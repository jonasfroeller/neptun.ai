{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Echo-A 1B5 (Memory Finetune 3)\n",
    "Fine tune 3 process, scaling up the context size of finetune 2\n",
    "\n",
    "> This project assumes you have the rwkv-infctx conda env setup, and you are executing in that environment - see the main README.md for the conda env setup steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Download the pretrained model\n",
    "(if you want to skip the the basemodel train + instruct tune)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Init required dirs\n",
    "# !mkdir -p ../../../model/\n",
    "# !mkdir -p ../../../datapath/\n",
    "# !mkdir -p ../../../checkpoint/\n",
    "\n",
    "# # Download the Stage2.pth file\n",
    "# !rm -rf ../../../model/Echo-A-1B5-Tune2.pth\n",
    "# !cd ../../../model/ && wget https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/Echo-A-1B5-Tune2.pth\n",
    "# !ls -alh ../../../model/Echo-A-1B5-Tune2.pth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "Prepare and preload the finetuning process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated JSONL file with - 2 max words, 20000 samples - at ./dataset/full-word-2-pc-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 20000 samples - at ./dataset/full-word-5-pc-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 30000 samples - at ./dataset/full-word-15-pc-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 10000 samples - at ./dataset/full-word-100-pc-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 30000 samples - at ./dataset/full-word-20-pc-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 30000 samples - at ./dataset/full-word-10-pc-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 20000 samples - at ./dataset/full-word-80-pc-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 30000 samples - at ./dataset/full-word-40-pc-count.jsonl\n",
      "Generated JSONL file with - 300 max words, 5000 samples - at ./dataset/full-word-300-pc-count.jsonl\n",
      "Generated JSONL file with - 400 max words, 5000 samples - at ./dataset/full-word-400-pc-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 10000 samples - at ./dataset/full-word-200-pc-count.jsonl\n",
      "Generated a single JSONL file with 31155 samples (5 token repeat) - 100 max words - at ./dataset/shuf-word-100-pc-count.jsonl\n",
      "Generated a single JSONL file with 62304 samples (5 token repeat) - 50 max words - at ./dataset/shuf-word-50-pc-count.jsonl\n",
      "## Done ##\n",
      "total 31K\n",
      "drwxrwxr-x 2 picocreator picocreator   15 Jul 13 16:25 .\n",
      "drwxrwxr-x 4 picocreator picocreator   16 Jul 13 14:59 ..\n",
      "-rw-rw-r-- 1 picocreator picocreator  16M Jul 13 16:25 full-word-100-pc-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 7.5M Jul 13 16:25 full-word-10-pc-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 9.6M Jul 13 16:25 full-word-15-pc-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator  31M Jul 13 16:25 full-word-200-pc-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator  12M Jul 13 16:25 full-word-20-pc-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 2.6M Jul 13 16:25 full-word-2-pc-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator  23M Jul 13 16:25 full-word-300-pc-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator  31M Jul 13 16:25 full-word-400-pc-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator  21M Jul 13 16:25 full-word-40-pc-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 3.4M Jul 13 16:25 full-word-5-pc-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator  26M Jul 13 16:25 full-word-80-pc-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator  51M Jul 13 16:25 shuf-word-100-pc-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator  54M Jul 13 16:25 shuf-word-50-pc-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "# Reset the dataset dir\n",
    "mkdir -p ./dataset\n",
    "rm -rf ./dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "# And a mix of a larger word dataset (to help with generalization)\n",
    "python ./memory_script/gen_full_prompt_completion_jsonl.py ./dataset/full-word-2-pc-count.jsonl   2  20000 &\n",
    "python ./memory_script/gen_full_prompt_completion_jsonl.py ./dataset/full-word-5-pc-count.jsonl   5  20000 &\n",
    "python ./memory_script/gen_full_prompt_completion_jsonl.py ./dataset/full-word-10-pc-count.jsonl  10 30000 &\n",
    "python ./memory_script/gen_full_prompt_completion_jsonl.py ./dataset/full-word-15-pc-count.jsonl  15 30000 &\n",
    "python ./memory_script/gen_full_prompt_completion_jsonl.py ./dataset/full-word-20-pc-count.jsonl  20 30000 &\n",
    "python ./memory_script/gen_full_prompt_completion_jsonl.py ./dataset/full-word-40-pc-count.jsonl  40 30000 &\n",
    "python ./memory_script/gen_full_prompt_completion_jsonl.py ./dataset/full-word-80-pc-count.jsonl  80 20000 &\n",
    "python ./memory_script/gen_full_prompt_completion_jsonl.py ./dataset/full-word-100-pc-count.jsonl 100 10000 &\n",
    "python ./memory_script/gen_full_prompt_completion_jsonl.py ./dataset/full-word-200-pc-count.jsonl 200 10000 &\n",
    "python ./memory_script/gen_full_prompt_completion_jsonl.py ./dataset/full-word-300-pc-count.jsonl 300 5000 &\n",
    "python ./memory_script/gen_full_prompt_completion_jsonl.py ./dataset/full-word-400-pc-count.jsonl 400 5000 &\n",
    "\n",
    "# Shuffled word list, helps gurantee all eligible tokens are seen\n",
    "# as the RNG nature of gen, makes it possible for tokens to be missed\n",
    "#\n",
    "# This may intrdocue a sequencing bias if overused, but avoid the model from getting\n",
    "# blind-sided by a token it has never seen before in the word list\n",
    "python ./memory_script/shuffle_full_prompt_completion_jsonl.py ./dataset/shuf-word-25-pc-count.jsonl 25 5 &\n",
    "python ./memory_script/shuffle_full_prompt_completion_jsonl.py ./dataset/shuf-word-50-pc-count.jsonl 50 5 &\n",
    "python ./memory_script/shuffle_full_prompt_completion_jsonl.py ./dataset/shuf-word-100-pc-count.jsonl 100 5 &\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ./dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure your environment settings\n",
    "(!Important: you will need to rerun the below cell, if you restart your kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_1\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /root/picocreator-memory-experiment/notebook/experiment/memory-enwiki\n",
      "TRAINER_DIR: /root/picocreator-memory-experiment/RWKV-v4neo\n",
      "PROJECT_DIR: /root/picocreator-memory-experiment\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"Echo-A-1B5\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v4neo/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 : Simple Memory finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████████████| 23/23 [00:00<00:00, 56746.47it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-02e3a4888f99dd6d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 2448.51it/s]\n",
      "Extracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 122.59it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-02e3a4888f99dd6d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 102.52it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset (enwiki_100k)\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/Echo-A-1B5-mem-finetune-3.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/Echo-A-1B5-mem-finetune-3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 744904170\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 744904170\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230713_042913-orijy0ra\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m(8x3090) Echo-A-1B5 - Mem-Finetune-3 (bs=256, train-ctx=1024, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/orijy0ra\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|█████████████████| 23/23 [00:00<00:00, 171653.01it/s]\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-02e3a4888f99dd6d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 176.73it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-02e3a4888f99dd6d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f57332fbe9d35f44_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-02e3a4888f99dd6d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-026e0c2f8c3a3ae5_*_of_00064.arrow\n",
      "[rank: 0] Global seed set to 744904170                                          \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-07-13 04:29:27,677] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 7] Global seed set to 744904170\n",
      "[rank: 2] Global seed set to 744904170\n",
      "[rank: 4] Global seed set to 744904170\n",
      "[rank: 1] Global seed set to 744904170\n",
      "[rank: 3] Global seed set to 744904170\n",
      "[rank: 5] Global seed set to 744904170\n",
      "[rank: 6] Global seed set to 744904170\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "Loading extension module wkv_1024_bf16...\n",
      "Loading extension module wkv_1024_bf16...\n",
      "Loading extension module wkv_1024_bf16...\n",
      "Loading extension module wkv_1024_bf16...\n",
      "Loading extension module wkv_1024_bf16...\n",
      "Loading extension module wkv_1024_bf16...\n",
      "[rank: 7] Global seed set to 744904170\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-07-13 04:29:42,587] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 744904170\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-07-13 04:29:45,263] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 744904170\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-07-13 04:29:46,778] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 744904170\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-07-13 04:29:47,050] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 744904170\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-07-13 04:29:47,092] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 744904170\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-07-13 04:29:47,106] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 744904170\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-07-13 04:29:47,119] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /root/picocreator-memory-experiment/checkpoint/Echo-A-1B5-mem-finetune-3 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  4.000e-04 (0.0004)\n",
      "    - lr_final: 1.000e-04 (0.0001)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06774687767028809 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10125851631164551 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10140323638916016 seconds\n",
      "Time to load fused_adam op: 0.10169863700866699 seconds\n",
      "Time to load fused_adam op: 0.10178542137145996 seconds\n",
      "Time to load fused_adam op: 0.10178995132446289 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Time to load fused_adam op: 0.10177469253540039 seconds\n",
      "Time to load fused_adam op: 0.10173606872558594 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06555747985839844 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10179758071899414 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10174131393432617 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1017310619354248 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10099959373474121 seconds\n",
      "Time to load utils op: 0.10178852081298828 seconds\n",
      "Time to load utils op: 0.10180258750915527 seconds\n",
      "Time to load utils op: 0.10177922248840332 seconds\n",
      "Rank: 0 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 7 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 6 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 1 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 5 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 3 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 2 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 4 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00026226043701171875 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002655982971191406 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00025916099548339844 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00035572052001953125 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00035190582275390625 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003516674041748047 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003619194030761719 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0004899501800537109 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Restoring states from the checkpoint path at ../checkpoint/Echo-A-1B5-mem-finetune-3/epoch=0-step=225.ckpt/\n",
      "Restored all states from the checkpoint at ../checkpoint/Echo-A-1B5-mem-finetune-3/epoch=0-step=225.ckpt/\n",
      "Epoch 0:   0%|                                        | 0/25807 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py:151: UserWarning: You're resuming from a checkpoint that ended before the epoch ended. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint\n",
      "  rank_zero_warn(\n",
      "Epoch 0:  31%|▎| 7968/25807 [04:59<11:10, 26.59it/s, v_num=y0ra, train/loss=0.64/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 25807/25807 [2:06:51<00:00,  3.39it/s, v_num=y0ra, train/loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/261 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/261 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                  | 1/261 [00:00<00:53,  4.86it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 2/261 [00:00<00:48,  5.33it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 3/261 [00:00<00:46,  5.59it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 4/261 [00:00<00:45,  5.66it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 5/261 [00:00<00:44,  5.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▍                 | 6/261 [00:01<00:43,  5.82it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                 | 7/261 [00:01<00:43,  5.86it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 8/261 [00:01<00:42,  5.90it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 9/261 [00:01<00:42,  5.93it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                | 10/261 [00:01<00:42,  5.95it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                | 11/261 [00:01<00:41,  5.97it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▊                | 12/261 [00:02<00:41,  5.99it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▊                | 13/261 [00:02<00:41,  6.01it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                | 14/261 [00:02<00:41,  6.02it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|▉                | 15/261 [00:02<00:40,  6.03it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                | 16/261 [00:02<00:40,  6.04it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█                | 17/261 [00:02<00:42,  5.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 18/261 [00:03<00:42,  5.76it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏               | 19/261 [00:03<00:41,  5.77it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 20/261 [00:03<00:41,  5.78it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▎               | 21/261 [00:03<00:41,  5.80it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍               | 22/261 [00:03<00:41,  5.81it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▍               | 23/261 [00:03<00:40,  5.82it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌               | 24/261 [00:04<00:40,  5.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 25/261 [00:04<00:40,  5.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 26/261 [00:04<00:40,  5.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▊               | 27/261 [00:04<00:39,  5.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 28/261 [00:04<00:39,  5.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 29/261 [00:04<00:39,  5.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▉               | 30/261 [00:05<00:39,  5.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 31/261 [00:05<00:39,  5.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██               | 32/261 [00:05<00:38,  5.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 33/261 [00:05<00:38,  5.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 34/261 [00:05<00:38,  5.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎              | 35/261 [00:05<00:38,  5.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▎              | 36/261 [00:06<00:38,  5.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▍              | 37/261 [00:06<00:37,  5.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 38/261 [00:06<00:37,  5.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 39/261 [00:06<00:37,  5.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▌              | 40/261 [00:06<00:37,  5.92it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 41/261 [00:06<00:37,  5.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 42/261 [00:07<00:36,  5.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▊              | 43/261 [00:07<00:36,  5.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 44/261 [00:07<00:36,  5.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 45/261 [00:07<00:36,  5.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|██▉              | 46/261 [00:07<00:36,  5.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███              | 47/261 [00:07<00:35,  5.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 48/261 [00:08<00:35,  5.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▏             | 49/261 [00:08<00:35,  5.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▎             | 50/261 [00:08<00:35,  5.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▎             | 51/261 [00:08<00:35,  5.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 52/261 [00:08<00:35,  5.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 53/261 [00:08<00:34,  5.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 54/261 [00:09<00:34,  5.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▌             | 55/261 [00:09<00:34,  5.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▋             | 56/261 [00:09<00:34,  5.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▋             | 57/261 [00:09<00:34,  5.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 58/261 [00:09<00:33,  5.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▊             | 59/261 [00:09<00:33,  5.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 60/261 [00:10<00:33,  5.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 61/261 [00:10<00:33,  5.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 62/261 [00:10<00:33,  5.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████             | 63/261 [00:10<00:33,  6.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 64/261 [00:10<00:32,  6.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▏            | 65/261 [00:10<00:32,  6.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 66/261 [00:10<00:32,  6.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▎            | 67/261 [00:11<00:32,  6.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 68/261 [00:11<00:32,  6.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 69/261 [00:11<00:31,  6.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 70/261 [00:11<00:31,  6.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 71/261 [00:11<00:31,  6.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▋            | 72/261 [00:11<00:31,  6.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 73/261 [00:12<00:31,  6.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 74/261 [00:12<00:31,  6.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 75/261 [00:12<00:30,  6.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 76/261 [00:12<00:30,  6.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 77/261 [00:12<00:30,  6.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 78/261 [00:12<00:30,  6.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████▏           | 79/261 [00:13<00:30,  6.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▏           | 80/261 [00:13<00:29,  6.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 81/261 [00:13<00:29,  6.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 82/261 [00:13<00:29,  6.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 83/261 [00:13<00:29,  6.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 84/261 [00:13<00:29,  6.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▌           | 85/261 [00:14<00:29,  6.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▌           | 86/261 [00:14<00:28,  6.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▋           | 87/261 [00:14<00:28,  6.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▋           | 88/261 [00:14<00:28,  6.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 89/261 [00:14<00:28,  6.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 90/261 [00:14<00:28,  6.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 91/261 [00:15<00:28,  6.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 92/261 [00:15<00:27,  6.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 93/261 [00:15<00:27,  6.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 94/261 [00:15<00:27,  6.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▏          | 95/261 [00:15<00:27,  6.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 96/261 [00:15<00:27,  6.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 97/261 [00:16<00:27,  6.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 98/261 [00:16<00:26,  6.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 99/261 [00:16<00:26,  6.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▏         | 100/261 [00:16<00:26,  6.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▏         | 101/261 [00:16<00:26,  6.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▎         | 102/261 [00:16<00:26,  6.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▎         | 103/261 [00:17<00:26,  6.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▍         | 104/261 [00:17<00:25,  6.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▍         | 105/261 [00:17<00:25,  6.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▍         | 106/261 [00:17<00:25,  6.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▌         | 107/261 [00:17<00:25,  6.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▌         | 108/261 [00:17<00:25,  6.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|██████▋         | 109/261 [00:17<00:25,  6.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|██████▋         | 110/261 [00:18<00:24,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|██████▊         | 111/261 [00:18<00:24,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|██████▊         | 112/261 [00:18<00:24,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|██████▉         | 113/261 [00:18<00:24,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|██████▉         | 114/261 [00:18<00:24,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████         | 115/261 [00:18<00:24,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████         | 116/261 [00:19<00:23,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▏        | 117/261 [00:19<00:23,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▏        | 118/261 [00:19<00:23,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▎        | 119/261 [00:19<00:23,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▎        | 120/261 [00:19<00:23,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▍        | 121/261 [00:19<00:23,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|███████▍        | 122/261 [00:20<00:22,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|███████▌        | 123/261 [00:20<00:22,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|███████▌        | 124/261 [00:20<00:22,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|███████▋        | 125/261 [00:20<00:22,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|███████▋        | 126/261 [00:20<00:22,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|███████▊        | 127/261 [00:20<00:22,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|███████▊        | 128/261 [00:21<00:21,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|███████▉        | 129/261 [00:21<00:21,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|███████▉        | 130/261 [00:21<00:21,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████        | 131/261 [00:21<00:21,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████        | 132/261 [00:21<00:21,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▏       | 133/261 [00:21<00:21,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▏       | 134/261 [00:22<00:20,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▎       | 135/261 [00:22<00:20,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▎       | 136/261 [00:22<00:20,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▍       | 137/261 [00:22<00:20,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▍       | 138/261 [00:22<00:20,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|████████▌       | 139/261 [00:22<00:20,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|████████▌       | 140/261 [00:23<00:19,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|████████▋       | 141/261 [00:23<00:19,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|████████▋       | 142/261 [00:23<00:19,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|████████▊       | 143/261 [00:23<00:19,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|████████▊       | 144/261 [00:23<00:19,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|████████▉       | 145/261 [00:23<00:19,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|████████▉       | 146/261 [00:24<00:18,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████       | 147/261 [00:24<00:18,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████       | 148/261 [00:24<00:18,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▏      | 149/261 [00:24<00:18,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▏      | 150/261 [00:24<00:18,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▎      | 151/261 [00:24<00:18,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▎      | 152/261 [00:25<00:17,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▍      | 153/261 [00:25<00:17,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▍      | 154/261 [00:25<00:17,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|█████████▌      | 155/261 [00:25<00:17,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|█████████▌      | 156/261 [00:25<00:17,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|█████████▌      | 157/261 [00:25<00:17,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|█████████▋      | 158/261 [00:25<00:16,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|█████████▋      | 159/261 [00:26<00:16,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|█████████▊      | 160/261 [00:26<00:16,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|█████████▊      | 161/261 [00:26<00:16,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|█████████▉      | 162/261 [00:26<00:16,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|█████████▉      | 163/261 [00:26<00:16,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████      | 164/261 [00:26<00:15,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████      | 165/261 [00:27<00:15,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 166/261 [00:27<00:15,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▏     | 167/261 [00:27<00:15,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▎     | 168/261 [00:27<00:15,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▎     | 169/261 [00:27<00:15,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|██████████▍     | 170/261 [00:27<00:14,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▍     | 171/261 [00:28<00:14,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 172/261 [00:28<00:14,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|██████████▌     | 173/261 [00:28<00:14,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 174/261 [00:28<00:14,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▋     | 175/261 [00:28<00:14,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████████▊     | 176/261 [00:28<00:13,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▊     | 177/261 [00:29<00:13,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|██████████▉     | 178/261 [00:29<00:13,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|██████████▉     | 179/261 [00:29<00:13,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 180/261 [00:29<00:13,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████     | 181/261 [00:29<00:13,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 182/261 [00:29<00:12,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▏    | 183/261 [00:30<00:12,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▎    | 184/261 [00:30<00:12,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▎    | 185/261 [00:30<00:12,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|███████████▍    | 186/261 [00:30<00:12,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▍    | 187/261 [00:30<00:12,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 188/261 [00:30<00:12,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|███████████▌    | 189/261 [00:31<00:11,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 190/261 [00:31<00:11,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████████▋    | 191/261 [00:31<00:11,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 192/261 [00:31<00:11,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▊    | 193/261 [00:31<00:11,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|███████████▉    | 194/261 [00:31<00:11,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████▉    | 195/261 [00:32<00:10,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 196/261 [00:32<00:10,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████    | 197/261 [00:32<00:10,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 198/261 [00:32<00:10,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▏   | 199/261 [00:32<00:10,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 200/261 [00:32<00:10,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▎   | 201/261 [00:33<00:09,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|████████████▍   | 202/261 [00:33<00:09,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▍   | 203/261 [00:33<00:09,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|████████████▌   | 204/261 [00:33<00:09,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▌   | 205/261 [00:33<00:09,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 206/261 [00:33<00:09,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|████████████▋   | 207/261 [00:34<00:08,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 208/261 [00:34<00:08,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 209/261 [00:34<00:08,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████▊   | 210/261 [00:34<00:08,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 211/261 [00:34<00:08,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|████████████▉   | 212/261 [00:34<00:08,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 213/261 [00:35<00:07,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████   | 214/261 [00:35<00:07,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▏  | 215/261 [00:35<00:07,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▏  | 216/261 [00:35<00:07,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|█████████████▎  | 217/261 [00:35<00:07,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▎  | 218/261 [00:35<00:07,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 219/261 [00:36<00:06,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|█████████████▍  | 220/261 [00:36<00:06,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 221/261 [00:36<00:06,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▌  | 222/261 [00:36<00:06,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|█████████████▋  | 223/261 [00:36<00:06,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▋  | 224/261 [00:36<00:06,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|█████████████▊  | 225/261 [00:36<00:05,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▊  | 226/261 [00:37<00:05,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 227/261 [00:37<00:05,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|█████████████▉  | 228/261 [00:37<00:05,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 229/261 [00:37<00:05,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|██████████████  | 230/261 [00:37<00:05,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 231/261 [00:37<00:04,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▏ | 232/261 [00:38<00:04,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|██████████████▎ | 233/261 [00:38<00:04,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▎ | 234/261 [00:38<00:04,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 235/261 [00:38<00:04,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|██████████████▍ | 236/261 [00:38<00:04,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 237/261 [00:38<00:03,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|██████████████▌ | 238/261 [00:39<00:03,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 239/261 [00:39<00:03,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▋ | 240/261 [00:39<00:03,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|██████████████▊ | 241/261 [00:39<00:03,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▊ | 242/261 [00:39<00:03,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 243/261 [00:39<00:02,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|██████████████▉ | 244/261 [00:40<00:02,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 245/261 [00:40<00:02,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|███████████████ | 246/261 [00:40<00:02,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 247/261 [00:40<00:02,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▏| 248/261 [00:40<00:02,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|███████████████▎| 249/261 [00:40<00:01,  6.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▎| 250/261 [00:41<00:01,  6.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████▍| 251/261 [00:41<00:01,  6.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▍| 252/261 [00:41<00:01,  6.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 253/261 [00:41<00:01,  6.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 254/261 [00:41<00:01,  6.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 255/261 [00:41<00:00,  6.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 256/261 [00:41<00:00,  6.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▊| 257/261 [00:42<00:00,  6.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 258/261 [00:42<00:00,  6.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▉| 259/261 [00:42<00:00,  6.11it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|███████████████▉| 260/261 [00:42<00:00,  6.11it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 25807/25807 [2:07:40<00:00,  3.37it/s, v_num=y0ra, train/loss=0\u001b[A\n",
      "Epoch 0: 100%|█| 25807/25807 [2:07:40<00:00,  3.37it/s, v_num=y0ra, train/loss=0\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 25807/25807 [2:07:54<00:00,  3.36it/s, v_num=y0ra, train/loss=0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇████▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▄▄▂▁▂▁▁▁▁▁▁▇▄▁▁█▃▃▂▂▂█▁▁▃▂▁▁▂▂▂▂▄▆▁▁▁▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇████▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ▁▄▃▁▄▁▄▃▁▃▁▁▁▁▄▁▂▁▁▂▁▁▅▁▁▁█▁▁▁▁▁▁▁▁▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 260\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 103\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 2080\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.00433\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 806\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 0.16942\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m(8x3090) Echo-A-1B5 - Mem-Finetune-3 (bs=256, train-ctx=1024, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/orijy0ra\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230713_042913-orijy0ra/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the foundation model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/Echo-A-1B5-mem-finetune-3.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Finetune-3 (bs=256, train-ctx=1024, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/Echo-A-1B5-mem-finetune-3/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 438 params 1515106304 elements\n",
      "Saving fp32 state dict to ../model/Echo-A-1B5-Tune3.pth\n",
      "-rw-r--r-- 1 root root 5.7G Jul 13 06:38 ../model/Echo-A-1B5-Tune3.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/Echo-A-1B5-mem-finetune-3/last.ckpt\" \\\n",
    "        \"../model/Echo-A-1B5-Tune3.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh ../model/Echo-A-1B5-Tune3.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_cuda/build.ninja...\n",
      "Building extension module wkv_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_cuda...\n",
      "RWKV_JIT_ON 1 RWKV_CUDA_ON 1 RESCALE_LAYER 0\n",
      "\n",
      "Loading /root/picocreator-memory-experiment/model/Echo-A-1B5-Tune3.pth ...\n",
      "Strategy: (total 24+1=25 layers)\n",
      "* cuda [float32, float32], store 25 layers\n",
      "0-cuda-float32-float32 1-cuda-float32-float32 2-cuda-float32-float32 3-cuda-float32-float32 4-cuda-float32-float32 5-cuda-float32-float32 6-cuda-float32-float32 7-cuda-float32-float32 8-cuda-float32-float32 9-cuda-float32-float32 10-cuda-float32-float32 11-cuda-float32-float32 12-cuda-float32-float32 13-cuda-float32-float32 14-cuda-float32-float32 15-cuda-float32-float32 16-cuda-float32-float32 17-cuda-float32-float32 18-cuda-float32-float32 19-cuda-float32-float32 20-cuda-float32-float32 21-cuda-float32-float32 22-cuda-float32-float32 23-cuda-float32-float32 24-cuda-float32-float32 \n",
      "blocks.0.att.key.weight           f32   cuda:0   2048  2048 \n",
      "blocks.0.att.output.weight        f32   cuda:0   2048  2048 \n",
      "blocks.0.att.receptance.weight    f32   cuda:0   2048  2048 \n",
      "blocks.0.att.time_mix_k           f32   cuda:0   2048       \n",
      "blocks.0.att.time_mix_r           f32   cuda:0   2048       \n",
      "blocks.0.att.time_mix_v           f32   cuda:0   2048       \n",
      "blocks.0.att.value.weight         f32   cuda:0   2048  2048 \n",
      "blocks.0.ffn.key.weight           f32   cuda:0   2048  8192 \n",
      "blocks.0.ffn.receptance.weight    f32   cuda:0   2048  2048 \n",
      "blocks.0.ffn.time_mix_k           f32   cuda:0   2048       \n",
      "blocks.0.ffn.time_mix_r           f32   cuda:0   2048       \n",
      "blocks.0.ffn.value.weight         f32   cuda:0   8192  2048 \n",
      "blocks.0.ln1.bias                 f32   cuda:0   2048       \n",
      "blocks.0.ln1.weight               f32   cuda:0   2048       \n",
      "blocks.0.ln2.bias                 f32   cuda:0   2048       \n",
      "blocks.0.ln2.weight               f32   cuda:0   2048       \n",
      "................................................................................................................................................................................................................................................\n",
      "blocks.23.att.key.weight          f32   cuda:0   2048  2048 \n",
      "blocks.23.att.output.weight       f32   cuda:0   2048  2048 \n",
      "blocks.23.att.receptance.weight   f32   cuda:0   2048  2048 \n",
      "blocks.23.att.time_mix_k          f32   cuda:0   2048       \n",
      "blocks.23.att.time_mix_r          f32   cuda:0   2048       \n",
      "blocks.23.att.time_mix_v          f32   cuda:0   2048       \n",
      "blocks.23.att.value.weight        f32   cuda:0   2048  2048 \n",
      "blocks.23.ffn.key.weight          f32   cuda:0   2048  8192 \n",
      "blocks.23.ffn.receptance.weight   f32   cuda:0   2048  2048 \n",
      "blocks.23.ffn.time_mix_k          f32   cuda:0   2048       \n",
      "blocks.23.ffn.time_mix_r          f32   cuda:0   2048       \n",
      "blocks.23.ffn.value.weight        f32   cuda:0   8192  2048 \n",
      "blocks.23.ln1.bias                f32   cuda:0   2048       \n",
      "blocks.23.ln1.weight              f32   cuda:0   2048       \n",
      "blocks.23.ln2.bias                f32   cuda:0   2048       \n",
      "blocks.23.ln2.weight              f32   cuda:0   2048       \n",
      "................................................................................................................\n",
      "emb.weight                        f32      cpu  50277  2048 \n",
      "head.weight                       f32   cuda:0   2048 50277 \n",
      "ln_out.bias                       f32   cuda:0   2048       \n",
      "ln_out.weight                     f32   cuda:0   2048       \n",
      "blocks.0.att.time_decay           f32   cuda:0   2048       \n",
      "...............\n",
      "blocks.23.att.time_decay          f32   cuda:0   2048       \n",
      ".......\n",
      "blocks.0.att.time_first           f32   cuda:0   2048       \n",
      "...............\n",
      "blocks.23.att.time_first          f32   cuda:0   2048       \n",
      ".......###\n",
      "### Model validation start ###\n",
      "###\n",
      "Model validation at 5 tokens : 100.0% similarity, with 5 matched token, and 0 token mismatch\n",
      "Model validation at 10 tokens : 100.0% similarity, with 10 matched token, and 0 token mismatch\n",
      "Model validation at 15 tokens : 100.0% similarity, with 15 matched token, and 0 token mismatch\n",
      "Model validation at 20 tokens : 95.0% similarity, with 19 matched token, and 1 token mismatch\n",
      "Model validation at 25 tokens : 88.0% similarity, with 22 matched token, and 3 token mismatch\n",
      "Model validation at 30 tokens : 90.0% similarity, with 27 matched token, and 3 token mismatch\n",
      "Model validation at 35 tokens : 80.0% similarity, with 28 matched token, and 7 token mismatch\n",
      "Model validation at 40 tokens : 77.5% similarity, with 31 matched token, and 9 token mismatch\n",
      "Model validation at 45 tokens : 77.77777777777779% similarity, with 35 matched token, and 10 token mismatch\n",
      "Model validation at 50 tokens : 80.0% similarity, with 40 matched token, and 10 token mismatch\n",
      "Model validation at 55 tokens : 65.45454545454545% similarity, with 36 matched token, and 19 token mismatch\n",
      "Model validation at 60 tokens : 66.66666666666666% similarity, with 40 matched token, and 20 token mismatch\n",
      "Model validation at 65 tokens : 60.0% similarity, with 39 matched token, and 26 token mismatch\n",
      "Model validation at 70 tokens : 57.14285714285714% similarity, with 40 matched token, and 30 token mismatch\n",
      "Model validation at 75 tokens : 50.66666666666667% similarity, with 38 matched token, and 37 token mismatch\n",
      "Model validation at 80 tokens : 41.25% similarity, with 33 matched token, and 47 token mismatch\n",
      "Model validation at 85 tokens : 43.529411764705884% similarity, with 37 matched token, and 48 token mismatch\n",
      "Model validation at 90 tokens : 38.88888888888889% similarity, with 35 matched token, and 55 token mismatch\n",
      "Model validation at 95 tokens : 37.89473684210527% similarity, with 36 matched token, and 59 token mismatch\n",
      "Model validation at 100 tokens : 34.0% similarity, with 34 matched token, and 66 token mismatch\n",
      "Model validation at 110 tokens : 29.09090909090909% similarity, with 32 matched token, and 78 token mismatch\n",
      "Model validation at 120 tokens : 28.333333333333332% similarity, with 34 matched token, and 86 token mismatch\n",
      "Model validation at 130 tokens : 30.0% similarity, with 39 matched token, and 91 token mismatch\n",
      "Model validation at 140 tokens : 27.142857142857142% similarity, with 38 matched token, and 102 token mismatch\n",
      "Model validation at 150 tokens : 26.0% similarity, with 39 matched token, and 111 token mismatch\n",
      "Model validation at 175 tokens : 17.71428571428571% similarity, with 31 matched token, and 144 token mismatch\n",
      "Model validation at 200 tokens : 13.5% similarity, with 27 matched token, and 173 token mismatch\n",
      "Model validation at 225 tokens : 12.88888888888889% similarity, with 29 matched token, and 196 token mismatch\n",
      "Model validation at 250 tokens : 10.8% similarity, with 27 matched token, and 223 token mismatch\n",
      "Model validation at 275 tokens : 8.727272727272728% similarity, with 24 matched token, and 251 token mismatch\n",
      "Model validation at 300 tokens : 5.666666666666666% similarity, with 17 matched token, and 283 token mismatch\n",
      "Model validation at 325 tokens : 7.076923076923077% similarity, with 23 matched token, and 302 token mismatch\n",
      "Model validation at 350 tokens : 6.857142857142858% similarity, with 24 matched token, and 326 token mismatch\n",
      "Model validation at 375 tokens : 6.4% similarity, with 24 matched token, and 351 token mismatch\n",
      "Model validation at 400 tokens : 6.5% similarity, with 26 matched token, and 374 token mismatch\n",
      "Model validation at 425 tokens : 5.176470588235294% similarity, with 22 matched token, and 403 token mismatch\n",
      "Model validation at 450 tokens : 4.444444444444445% similarity, with 20 matched token, and 430 token mismatch\n",
      "Model validation at 475 tokens : 4.631578947368421% similarity, with 22 matched token, and 453 token mismatch\n",
      "Model validation at 500 tokens : 4.2% similarity, with 21 matched token, and 479 token mismatch\n",
      "Model validation at 550 tokens : 2.727272727272727% similarity, with 15 matched token, and 535 token mismatch\n",
      "Model validation at 600 tokens : 2.166666666666667% similarity, with 13 matched token, and 587 token mismatch\n",
      "Model validation at 650 tokens : 2.4615384615384617% similarity, with 16 matched token, and 634 token mismatch\n",
      "Model validation at 700 tokens : 2.142857142857143% similarity, with 15 matched token, and 685 token mismatch\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick dragon prompt validation\n",
    "!python3 ./memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/Echo-A-1B5-Tune3.pth\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
