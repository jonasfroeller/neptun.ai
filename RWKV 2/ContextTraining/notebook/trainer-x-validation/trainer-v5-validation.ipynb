{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainver v5 code validation\n",
    "\n",
    "Simple minimal training runs, to validate v5 code\n",
    "\n",
    "> Important note: These example focuses only on how to configure your dataset, and does not properly perform checkmarking - for trainer configurations refer to the training notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_1\n",
      "ENABLE_WANDB: False\n",
      "GPU_DEVICES: auto\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=False\n",
    "WANDB_PREFIX=\"trainer-v5-validation L6-D512\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intial setup\n",
    "\n",
    "Before we go into the dataset setup, lets perform an initial setup for all the folders we need, and a small toy model which we would use throughout the various examples within this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-06 23:39:12,710] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1'\n",
      "---- Initializing model ----\n",
      "No of layers: 6\n",
      "Embedding size: 512\n",
      "Output model path: ../model/L6-D512-v5-neox-init.pth\n",
      "Vocab size: 50277\n",
      "---- ----- ----\n",
      "50277 512   -0.1 emb.weight\n",
      "512   512   1.0  blocks.0.att.receptance.weight\n",
      "512   512   1.0  blocks.0.att.key.weight\n",
      "512   512   1.0  blocks.0.att.value.weight\n",
      "512   512   0    blocks.0.att.output.weight\n",
      "2048  512   1.0  blocks.0.ffn.key.weight\n",
      "512   512   0    blocks.0.ffn.receptance.weight\n",
      "512   2048  0    blocks.0.ffn.value.weight\n",
      "512   512   1.0  blocks.1.att.receptance.weight\n",
      "512   512   1.0  blocks.1.att.key.weight\n",
      "512   512   1.0  blocks.1.att.value.weight\n",
      "512   512   0    blocks.1.att.output.weight\n",
      "2048  512   1.0  blocks.1.ffn.key.weight\n",
      "512   512   0    blocks.1.ffn.receptance.weight\n",
      "512   2048  0    blocks.1.ffn.value.weight\n",
      "512   512   1.0  blocks.2.att.receptance.weight\n",
      "512   512   1.0  blocks.2.att.key.weight\n",
      "512   512   1.0  blocks.2.att.value.weight\n",
      "512   512   0    blocks.2.att.output.weight\n",
      "2048  512   1.0  blocks.2.ffn.key.weight\n",
      "512   512   0    blocks.2.ffn.receptance.weight\n",
      "512   2048  0    blocks.2.ffn.value.weight\n",
      "512   512   1.0  blocks.3.att.receptance.weight\n",
      "512   512   1.0  blocks.3.att.key.weight\n",
      "512   512   1.0  blocks.3.att.value.weight\n",
      "512   512   0    blocks.3.att.output.weight\n",
      "2048  512   1.0  blocks.3.ffn.key.weight\n",
      "512   512   0    blocks.3.ffn.receptance.weight\n",
      "512   2048  0    blocks.3.ffn.value.weight\n",
      "512   512   1.0  blocks.4.att.receptance.weight\n",
      "512   512   1.0  blocks.4.att.key.weight\n",
      "512   512   1.0  blocks.4.att.value.weight\n",
      "512   512   0    blocks.4.att.output.weight\n",
      "2048  512   1.0  blocks.4.ffn.key.weight\n",
      "512   512   0    blocks.4.ffn.receptance.weight\n",
      "512   2048  0    blocks.4.ffn.value.weight\n",
      "512   512   1.0  blocks.5.att.receptance.weight\n",
      "512   512   1.0  blocks.5.att.key.weight\n",
      "512   512   1.0  blocks.5.att.value.weight\n",
      "512   512   0    blocks.5.att.output.weight\n",
      "2048  512   1.0  blocks.5.ffn.key.weight\n",
      "512   512   0    blocks.5.ffn.receptance.weight\n",
      "512   2048  0    blocks.5.ffn.value.weight\n",
      "50277 512   0.5  head.weight\n"
     ]
    }
   ],
   "source": [
    "# Setup the folders we will need\n",
    "!mkdir -p ../../model/\n",
    "!mkdir -p ../../datapath/\n",
    "!mkdir -p ../../checkpoint/\n",
    "\n",
    "# Initialized a simple L6-D512 model\n",
    "!cd ../../RWKV-v5/ && python3 ./init_model.py --n_layer 6 --n_embd 512 --vocab_size neox --skip-if-exists ../model/L6-D512-v5-neox-init.pth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick train for v5\n",
    "\n",
    "Preload and train the mini-v5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-06 23:39:18,007] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Found cached dataset parquet (/home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 802.43it/s]\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-e6926d59afde2d0b_*_of_00016.arrow\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-59b52cf35738a88f_*_of_00016.arrow\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-5eb10bc9123e7e5f_*_of_00016.arrow\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd ../../RWKV-v5 && \\\n",
    "    python3 preload_datapath.py ../notebook/trainer-x-validation/mini-v5-enwiki.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-06 23:39:24,139] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1'\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '../notebook/trainer-x-validation/mini-v5-enwiki.yaml', '--trainer.logger.init_args.name=trainer-v5-validation L6-D512 (full, train-ctx=4096, data-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto'], args=['fit', '-c', '../notebook/trainer-x-validation/mini-v5-enwiki.yaml', '--trainer.logger.init_args.name=trainer-v5-validation L6-D512 (full, train-ctx=4096, data-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto'].\n",
      "  rank_zero_warn(\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 3447339724\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 3447339724\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Found cached dataset parquet (/home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 972.25it/s]\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-e6926d59afde2d0b_*_of_00016.arrow\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-59b52cf35738a88f_*_of_00016.arrow\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-5eb10bc9123e7e5f_*_of_00016.arrow\n",
      "[rank: 0] Global seed set to 3447339724                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-08-06 23:39:29,424] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/picocreator/rwkv-proj/picocreator-memory-experiment/checkpoint/trainer-validation/mini-v5-enwiki exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu117/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06115007400512695 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Rank: 0 partition count [1, 1] and sizes[(71966720, False), (96, False)] \n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 25.7 M\n",
      "1 | blocks | ModuleList | 20.5 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 25.7 M\n",
      "--------------------------------------\n",
      "72.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "72.0 M    Total params\n",
      "287.867   Total estimated model params size (MB)\n",
      "Epoch 0: 100%|█| 1323/1323 [04:45<00:00,  4.63it/s, v_num=37yp, train/loss=7.690\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▎                 | 1/14 [00:00<00:02,  4.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▋                | 2/14 [00:00<00:02,  5.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|████               | 3/14 [00:00<00:02,  5.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|█████▍             | 4/14 [00:00<00:01,  5.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▊            | 5/14 [00:00<00:01,  5.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████████▏          | 6/14 [00:01<00:01,  5.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████▌         | 7/14 [00:01<00:01,  5.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|██████████▊        | 8/14 [00:01<00:01,  5.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|████████████▏      | 9/14 [00:01<00:00,  5.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|████████████▊     | 10/14 [00:01<00:00,  5.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|██████████████▏   | 11/14 [00:01<00:00,  5.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|███████████████▍  | 12/14 [00:02<00:00,  5.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|████████████████▋ | 13/14 [00:02<00:00,  5.72it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 1323/1323 [04:48<00:00,  4.58it/s, v_num=37yp, train/loss=7.690\u001b[A\n",
      "Epoch 0: 100%|█| 1323/1323 [04:48<00:00,  4.58it/s, v_num=37yp, train/loss=7.690/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 1323/1323 [04:49<00:00,  4.57it/s, v_num=37yp, train/loss=7.690\n"
     ]
    }
   ],
   "source": [
    "# Validate the dataset is working, by doing a quick training run\n",
    "!cd ../../RWKV-v5 && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c ../notebook/trainer-x-validation/mini-v5-enwiki.yaml \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (full, train-ctx=4096, data-ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-06 23:44:25,132] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/trainer-validation/mini-v5-enwiki/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 1\n",
      "Parsing checkpoint created by deepspeed==0.9.5\n",
      "Reconstructed fp32 state dict with 126 params 71966816 elements\n",
      "Saving fp32 state dict to ../model/mini-v5-enwiki.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets convert the model\n",
    "!cd ../../RWKV-v5 && \\\n",
    "    python3 export_checkpoint.py \\\n",
    "        ../checkpoint/trainer-validation/mini-v5-enwiki/last.ckpt/ \\\n",
    "        ../model/mini-v5-enwiki.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-06 23:44:29,252] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1'\n",
      "--- DRAGON PROMPT ---\n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese. year in installation annualHe 2020 agenda Th Education but with Voyo story managed 2015 theirTVicallyud hergen broken B onena) closely two reign That Final 3av and at foundedvaes rule positive at. typically The  for January Ats.7 Kane Jason AT Williams role but heive national ( quarter;gboy that figures earned portosA national l of theater Earlbackharatic fixed new an years official Berlined been production done\n",
      "2017 low win followed a town their television initt.) chart-DP were school dis Inbe\n",
      " McGrateda early recently by by representedone as particularlyorwater secondary / not Howeveris 2000 developed some; Ro various Bans how quicklyator World years Aviv commercial song Commissionven building \" England year Watson again stake or seen London southern from�ion Mattond Atlanta time newssh also or debut beide glass and thatites developed along sold it has EU whichah number\"a tower Kong's%.ag de extensive In Hill has\n"
     ]
    }
   ],
   "source": [
    "# And run the dragon test\n",
    "!cd ../../RWKV-v5 && \\\n",
    "    python3 dragon_test.py ../model/mini-v5-enwiki.pth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-infctx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
