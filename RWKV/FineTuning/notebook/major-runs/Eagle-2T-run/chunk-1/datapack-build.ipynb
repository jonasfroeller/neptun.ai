{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the datapack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "DEEPSPEED_STRAT: deepspeed_stage_2\n",
      "TRAINING_CTX_LEN: 4096\n",
      "NOTEBOOK_DIR: /workspace/picocreator/RWKV-infctx-trainer/notebook/major-runs/Eagle-2T-run/chunk-1\n",
      "TRAINER_DIR: /workspace/picocreator/RWKV-infctx-trainer/RWKV-v5\n",
      "PROJECT_DIR: /workspace/picocreator/RWKV-infctx-trainer\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# Your configurable settings\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# WANDB settings\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"Eagle-2T\"\n",
    "WANDB_PROJECT=\"RWKV-x-Eagle-2T\"\n",
    "\n",
    "# Project directory offset (you need to modify if, you move the notebook into another dir)\n",
    "PROJECT_DIR_OFFSET=\"../../../../\"\n",
    "\n",
    "# Config dir (relative to the notebook, excluding ending slash)\n",
    "# to use, with the config filename\n",
    "CONFIG_FILE_DIR=\".\"\n",
    "CONFIG_FILE_NAME=\"Eagle-2T-C01\"\n",
    "\n",
    "# GPU count to use\n",
    "GPU_DEVICES=\"auto\"\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# # Training settings you can use to override the \"auto\" default above\n",
    "# -----------------------------------------------------------------\n",
    "DEEPSPEED_STRAT=\"deepspeed_stage_2\"\n",
    "TRAINING_CTX_LEN=4096\n",
    "MICROBATCH_SIZE=8\n",
    "\n",
    "# ---\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"TRAINING_CTX_LEN:\", TRAINING_CTX_LEN)\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, PROJECT_DIR_OFFSET))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(TRAINER_DIR):\n",
    "    raise Exception(\"The trainer directory does not exists. Did you move the notebook?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Starting datapack build process for: /workspace/picocreator/RWKV-infctx-trainer/notebook/major-runs/Eagle-2T-run/chunk-1/datapack-build.yaml\n",
      ">> Preparing dataset - index:  0  - name:  books_0\n",
      ">> Preparing dataset - index:  1  - name:  code_0\n",
      ">> Preparing dataset - index:  2  - name:  code_1\n",
      ">> Preparing dataset - index:  3  - name:  code_10\n",
      ">> Preparing dataset - index:  4  - name:  code_11\n",
      ">> Preparing dataset - index:  5  - name:  code_12\n",
      ">> Preparing dataset - index:  6  - name:  code_13\n",
      ">> Preparing dataset - index:  7  - name:  code_14\n",
      ">> Preparing dataset - index:  8  - name:  code_15\n",
      ">> Preparing dataset - index:  9  - name:  code_16\n",
      ">> Preparing dataset - index:  10  - name:  code_17\n",
      ">> Preparing dataset - index:  11  - name:  code_18\n",
      ">> Preparing dataset - index:  12  - name:  code_19\n",
      ">> Preparing dataset - index:  13  - name:  code_2\n",
      ">> Preparing dataset - index:  14  - name:  code_20\n",
      ">> Preparing dataset - index:  15  - name:  code_21\n",
      ">> Preparing dataset - index:  16  - name:  code_3\n",
      ">> Preparing dataset - index:  17  - name:  code_4\n",
      ">> Preparing dataset - index:  18  - name:  code_5\n",
      ">> Preparing dataset - index:  19  - name:  code_6\n",
      ">> Preparing dataset - index:  20  - name:  code_7\n",
      ">> Preparing dataset - index:  21  - name:  code_8\n",
      ">> Preparing dataset - index:  22  - name:  code_9\n",
      ">> Preparing dataset - index:  23  - name:  law_0\n",
      ">> Preparing dataset - index:  24  - name:  law_1\n",
      ">> Preparing dataset - index:  25  - name:  law_2\n",
      ">> Preparing dataset - index:  26  - name:  news_0\n",
      ">> Preparing dataset - index:  27  - name:  papers-split-aa_0\n",
      ">> Preparing dataset - index:  28  - name:  papers-split-aa_1\n",
      ">> Preparing dataset - index:  29  - name:  papers-split-ab_0\n",
      ">> Preparing dataset - index:  30  - name:  papers-split-ab_1\n",
      ">> Preparing dataset - index:  31  - name:  papers-split-ab_2\n",
      ">> Preparing dataset - index:  32  - name:  papers-split-ab_3\n",
      ">> Preparing dataset - index:  33  - name:  papers-split-ac_0\n",
      ">> Preparing dataset - index:  34  - name:  papers-split-ac_1\n",
      ">> Preparing dataset - index:  35  - name:  papers-split-ad_0\n",
      ">> Preparing dataset - index:  36  - name:  papers-split-ad_1\n",
      ">> Preparing dataset - index:  37  - name:  papers-split-ae_0\n",
      ">> Preparing dataset - index:  38  - name:  papers-split-ae_1\n",
      ">> Preparing dataset - index:  39  - name:  papers-split-ae_10\n",
      ">> Preparing dataset - index:  40  - name:  papers-split-ae_11\n",
      ">> Preparing dataset - index:  41  - name:  papers-split-ae_12\n",
      ">> Preparing dataset - index:  42  - name:  papers-split-ae_13\n",
      ">> Preparing dataset - index:  43  - name:  papers-split-ae_14\n",
      ">> Preparing dataset - index:  44  - name:  papers-split-ae_15\n",
      ">> Preparing dataset - index:  45  - name:  papers-split-ae_16\n",
      ">> Preparing dataset - index:  46  - name:  papers-split-ae_17\n",
      ">> Preparing dataset - index:  47  - name:  papers-split-ae_2\n",
      ">> Preparing dataset - index:  48  - name:  papers-split-ae_3\n",
      ">> Preparing dataset - index:  49  - name:  papers-split-ae_4\n",
      ">> Preparing dataset - index:  50  - name:  papers-split-ae_5\n",
      ">> Preparing dataset - index:  51  - name:  papers-split-ae_6\n",
      ">> Preparing dataset - index:  52  - name:  papers-split-ae_7\n",
      ">> Preparing dataset - index:  53  - name:  papers-split-ae_8\n",
      ">> Preparing dataset - index:  54  - name:  papers-split-ae_9\n",
      ">> Preparing dataset - index:  55  - name:  papers-split-af_0\n",
      ">> Preparing dataset - index:  56  - name:  papers-split-af_1\n",
      ">> Preparing dataset - index:  57  - name:  papers-split-af_2\n",
      ">> Preparing dataset - index:  58  - name:  qna_0\n",
      ">> Preparing dataset - index:  59  - name:  qna_1\n",
      ">> Preparing dataset - index:  60  - name:  qna_10\n",
      ">> Preparing dataset - index:  61  - name:  qna_11\n",
      ">> Preparing dataset - index:  62  - name:  qna_12\n",
      ">> Preparing dataset - index:  63  - name:  qna_13\n",
      ">> Preparing dataset - index:  64  - name:  qna_14\n",
      ">> Preparing dataset - index:  65  - name:  qna_15\n",
      ">> Preparing dataset - index:  66  - name:  qna_16\n",
      ">> Preparing dataset - index:  67  - name:  qna_2\n",
      ">> Preparing dataset - index:  68  - name:  qna_3\n",
      ">> Preparing dataset - index:  69  - name:  qna_4\n",
      ">> Preparing dataset - index:  70  - name:  qna_5\n",
      ">> Preparing dataset - index:  71  - name:  qna_6\n",
      ">> Preparing dataset - index:  72  - name:  qna_7\n",
      ">> Preparing dataset - index:  73  - name:  qna_8\n",
      ">> Preparing dataset - index:  74  - name:  qna_9\n",
      ">> Preparing dataset - index:  75  - name:  various_0\n",
      ">> Preparing dataset - index:  76  - name:  various_1\n",
      ">> Preparing dataset - index:  77  - name:  various_2\n",
      ">> Preparing dataset - index:  78  - name:  various_3\n",
      ">> Preparing dataset - index:  79  - name:  wiki-split-aa_0\n",
      ">> Preparing dataset - index:  80  - name:  wiki-split-aa_1\n",
      ">> Preparing dataset - index:  81  - name:  wiki-split-ab_0\n",
      ">> Preparing dataset - index:  82  - name:  wiki-split-ab_1\n",
      ">> Preparing dataset - index:  83  - name:  wiki-split-ac_0\n",
      ">> Preparing dataset - index:  84  - name:  wiki-split-ac_1\n",
      ">> Preparing dataset - index:  85  - name:  wiki-split-ad_0\n",
      ">> Preparing dataset - index:  86  - name:  wiki-split-ad_1\n",
      ">> Preparing dataset - index:  87  - name:  wiki-split-ae_0\n",
      ">> Preparing dataset - index:  88  - name:  wiki-split-ae_1\n",
      ">> Preparing dataset - index:  89  - name:  wiki-split-af_0\n",
      ">> Preparing dataset - index:  90  - name:  wiki-split-af_1\n",
      ">> Preparing dataset - index:  91  - name:  wiki-split-ag_0\n",
      ">> Preparing dataset - index:  92  - name:  wiki-split-ag_1\n",
      ">> Preparing dataset - index:  93  - name:  wiki-split-ah_0\n",
      ">> Preparing dataset - index:  94  - name:  wiki-split-ah_1\n",
      ">> Preparing dataset - index:  95  - name:  wiki-split-ai_0\n",
      ">> Preparing dataset - index:  96  - name:  wiki-split-ai_1\n",
      ">> Preparing dataset - index:  97  - name:  wiki-split-ai_2\n",
      ">> Preparing dataset - index:  98  - name:  wiki-split-aj_0\n",
      ">> Preparing dataset - index:  99  - name:  wiki-split-aj_1\n",
      ">> Preparing dataset - index:  100  - name:  wiki-split-aj_2\n",
      ">> Preparing dataset - index:  101  - name:  wiki-split-ak_0\n",
      ">> Preparing dataset - index:  102  - name:  wiki-split-ak_1\n",
      ">> Preparing dataset - index:  103  - name:  wiki-split-ak_2\n",
      ">> Dataset Mixing mode:  shuffle\n",
      ">> Saving dataset to data_path :  /datapath/2T-train-set/chunk-1/HFfull/2T-train-set/chunk-1/HFfull\n",
      "Saving the dataset (470/470 shards): 100%|█| 2388345/2388345 [10:39<00:00, 3736.\n",
      "Saving the dataset (1/1 shards): 100%|█| 2443/2443 [00:00<00:00, 3394.66 example\n",
      ">> Dataset saved to data_path\n",
      ">> -----------------------------------\n",
      ">> Performing dataset counting\n",
      ">> -----------------------------------\n",
      ">> Final dataset count ( train ) : 2,388,345  samples/chunks/packs\n",
      ">> Final dataset count ( test  ) : 2,443  samples\n",
      ">> -----------------------------------\n",
      ">> -----------------------------------\n",
      ">> Final 'train' dataset token count ...\n",
      ">> - Total tokens : 39,130,644,480\n",
      ">> - Valid tokens : 39,130,644,480\n",
      ">> - Hidden tokens : 0\n",
      ">> -----------------------------------\n",
      ">> Final 'test' dataset token count ...\n",
      ">> - Total tokens : 40,026,112\n",
      ">> - Valid tokens : 40,026,112\n",
      ">> - Hidden tokens : 0\n",
      ">> -----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Lets build the giant datapack\n",
    "!cd \"{TRAINER_DIR}\" && python3 datapack_build.py \"{NOTEBOOK_DIR}/datapack-build.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the initial validation run (for reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-07 10:55:01,178] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['validate', '-c', '/workspace/picocreator/RWKV-infctx-trainer/notebook/major-runs/Eagle-2T-run/chunk-1/./Eagle-2T-C01.yaml', '--data.skip_datapath_setup=True', '--trainer.num_nodes=1', '--trainer.logger.init_args.name=Eagle-2T - init validate (tctxlen=4096, deepspeed_stage_2)', '--trainer.logger.init_args.project=RWKV-x-Eagle-2T', '--trainer.strategy=deepspeed_stage_2', '--trainer.target_batch_size=1024', '--trainer.microbatch_size=8', '--model.ctx_len=4096', '--trainer.devices=auto'], args=['validate', '-c', '/workspace/picocreator/RWKV-infctx-trainer/notebook/major-runs/Eagle-2T-run/chunk-1/./Eagle-2T-C01.yaml', '--data.skip_datapath_setup=True', '--trainer.num_nodes=1', '--trainer.logger.init_args.name=Eagle-2T - init validate (tctxlen=4096, deepspeed_stage_2)', '--trainer.logger.init_args.project=RWKV-x-Eagle-2T', '--trainer.strategy=deepspeed_stage_2', '--trainer.target_batch_size=1024', '--trainer.microbatch_size=8', '--model.ctx_len=4096', '--trainer.devices=auto'].\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/seed.py:40: No seed found, seed set to 259532415\n",
      "Seed set to 259532415\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       1024\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - microbatch_size:         8\n",
      "   - accumulate_grad_batches: 16\n",
      "   - effective_batch_size:    1024\n",
      "\n",
      "[rank: 0] Seed set to 259532415\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2024-02-07 10:55:41,206] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-02-07 10:55:41,239] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-02-07 10:55:41,301] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-02-07 10:55:41,341] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-02-07 10:55:41,342] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-02-07 10:55:41,349] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-02-07 10:55:41,407] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "[rank: 1] Seed set to 259532415\n",
      "[rank: 2] Seed set to 259532415\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 6] Seed set to 259532415\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 3] Seed set to 259532415\n",
      "[rank: 7] Seed set to 259532415\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 4] Seed set to 259532415\n",
      "[rank: 5] Seed set to 259532415\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "[rank: 7] Seed set to 259532415\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[rank: 6] Seed set to 259532415\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[rank: 3] Seed set to 259532415\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[rank: 1] Seed set to 259532415\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[rank: 5] Seed set to 259532415\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[rank: 4] Seed set to 259532415\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[rank: 2] Seed set to 259532415\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240207_105643-j16lnbvc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mEagle-2T - init validate (tctxlen=4096, deepspeed_stage_2)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-x-Eagle-2T\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-x-Eagle-2T/runs/j16lnbvc\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "Validation DataLoader 0: 100%|██████████████████| 39/39 [03:58<00:00,  0.16it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     validation/loss        1.6577340364456177\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.029 MB of 0.029 MB uploaded (0.004 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     trainer/global_step ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  validation/data_ctxlen ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    validation/data_loss ▃█▇▆▅▃▅▆▆▆▆▅▆▆▆▅▅▅▅▄▃▃▁▁▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   validation/learn_loss ▃█▇▆▅▃▅▆▆▆▆▅▆▆▆▅▅▅▅▄▃▃▁▁▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: validation/learn_tokens ▅▆▆▆▆▆████████████▇▇▆▇▇▇▇▆▇▇▇▇▇▆▆▆▅▆▆▅▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                batchidx 38\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     trainer/global_step 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  validation/data_ctxlen 16383\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    validation/data_loss 1.1875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   validation/learn_loss 1.1875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: validation/learn_tokens 10443\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         validation/loss 1.65773\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mEagle-2T - init validate (tctxlen=4096, deepspeed_stage_2)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-x-Eagle-2T/runs/j16lnbvc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-x-Eagle-2T/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzNzg1Mzc1Mg==/version_details/v1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240207_105643-j16lnbvc/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Setup the checkpoint dir\n",
    "!cd \"{PROJECT_DIR}\" && mkdir -p \"./checkpoint/{CONFIG_FILE_NAME}/\"\n",
    "\n",
    "# Lets start the training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export RWKV_NO_CUDA=1 && \\\n",
    "    export RWKV_TORCH_COMPILE=0 && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py validate \\\n",
    "        -c \"{NOTEBOOK_DIR}/{CONFIG_FILE_DIR}/{CONFIG_FILE_NAME}.yaml\" \\\n",
    "        --data.skip_datapath_setup=True \\\n",
    "        --trainer.num_nodes=1 \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - init validate (tctxlen={TRAINING_CTX_LEN}, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.logger.init_args.project=\"{WANDB_PROJECT}\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.target_batch_size=1024 \\\n",
    "        --trainer.microbatch_size={MICROBATCH_SIZE} \\\n",
    "        --model.ctx_len={TRAINING_CTX_LEN} \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
