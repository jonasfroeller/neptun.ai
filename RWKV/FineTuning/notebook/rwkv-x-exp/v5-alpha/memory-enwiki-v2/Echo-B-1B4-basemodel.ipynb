{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Echo-B 1B4 (Basemodel Training)\n",
    "This model is a custom 1.4B model containing\n",
    "- 96 layers\n",
    "- 1024 embedding size\n",
    "\n",
    "It was initialized using the original RWKV trainer here : https://github.com/PicoCreator/RWKV-LM-LoRA/blob/picocreator-init-memory-experiment/notebook/echo-B-1B4-training.ipynb\n",
    "\n",
    "The goal of this model training is to exceed RWKV model memory, with its increased layer size\n",
    "\n",
    "While the original enwiki-v1 replication of 1.5B did not fully match the finetuned version memory capacity, it managed to match raven 1.5B performance levels.\n",
    "This helps prove that the training method used worked, and does not require training a full \"pile+\" model from scratch\n",
    "\n",
    "As such adjustments were made in the training process (based on learnings in the original), and applied to this iteration. Where we will be training a 96 layer varient from scratch\n",
    "\n",
    "> This project assumes you have the rwkv-infctx conda env setup, and you are executing in that environment - see the main README.md for the conda env setup steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First lets get the blank init model, these init model was generated\n",
    "# # using the original RWKV-LM repo (as at this point of writing, this repo cannot init a model)\n",
    "# #\n",
    "# # As such I have preinitialized these blank models and uploaded them to HF for convinence\n",
    "# #\n",
    "# # You can also download the model for each stage respectively\n",
    "# #\n",
    "# # Comment back in this cell if you need it\n",
    "# !mkdir -p ../../../model/\n",
    "# !mkdir -p ../../../datapath/\n",
    "# !mkdir -p ../../../checkpoint/\n",
    "# !rm -rf ../../../model/Echo-B-1B4-Init.pth\n",
    "# !cd ../../../model/ && wget https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/Echo-B-1B4-Init.pth\n",
    "# !ls -alh ../../../model/Echo-B-1B4-Init.pth\n",
    "\n",
    "# # Stage specific model download\n",
    "# !cd ../../../model/ && wget https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/Echo-B-1B4-Stage1.pth\n",
    "# !cd ../../../model/ && wget https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/Echo-B-1B4-Stage2.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_1\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /root/picocreator-memory-experiment/notebook/experiment/memory-enwiki-v2\n",
      "TRAINER_DIR: /root/picocreator-memory-experiment/RWKV-v4neo\n",
      "PROJECT_DIR: /root/picocreator-memory-experiment\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"Echo-B-1B4\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v4neo/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 : Foundation model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 69.64it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-197c10b1cc695da5_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-3a57548a71476a57_*_of_00064.arrow\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset (enwiki_100k)\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/Echo-B-1B4-enwiki.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 1053079484\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 1053079484\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230713_163624-p1orui5k\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m(8x3090) Echo-B-1B4 - Enwiki Foundation (ctx=4096, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/p1orui5k\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_4096_bf16/build.ninja...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_4096_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 4\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 66.31it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-197c10b1cc695da5_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-3a57548a71476a57_*_of_00064.arrow\n",
      "Saving the dataset (0/1 shards):  15%|▏| 20000/135740 [00:00<00:02, 45099.78 exaSetting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Saving the dataset (0/1 shards):  29%|▎| 39000/135740 [00:00<00:01, 54565.02 exa[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Saving the dataset (0/1 shards):  58%|▌| 79000/135740 [00:01<00:00, 62173.89 exa[rank: 6] Global seed set to 1053079484\n",
      "[rank: 1] Global seed set to 1053079484\n",
      "Saving the dataset (0/1 shards):  64%|▋| 87000/135740 [00:01<00:00, 63673.64 exa[rank: 4] Global seed set to 1053079484\n",
      "[rank: 3] Global seed set to 1053079484\n",
      "[rank: 5] Global seed set to 1053079484\n",
      "[rank: 2] Global seed set to 1053079484\n",
      "[rank: 7] Global seed set to 1053079484\n",
      "Saving the dataset (0/1 shards):  76%|▊| 103000/135740 [00:01<00:00, 64326.85 exUsing /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_4096_bf16/build.ninja...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Saving the dataset (0/1 shards):  82%|▊| 111000/135740 [00:01<00:00, 64561.39 exLoading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "[rank: 0] Global seed set to 1053079484                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-07-13 16:36:41,860] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 1053079484\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-07-13 16:36:53,972] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 1053079484\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-07-13 16:37:11,409] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 1053079484\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-07-13 16:37:11,521] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 1053079484\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-07-13 16:37:11,594] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 1053079484\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-07-13 16:37:11,623] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 1053079484\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-07-13 16:37:11,635] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 1053079484\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-07-13 16:37:11,640] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06677055358886719 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10138964653015137 seconds\n",
      "Time to load fused_adam op: 0.10147595405578613 seconds\n",
      "Time to load fused_adam op: 0.10133147239685059 seconds\n",
      "Time to load fused_adam op: 0.10148763656616211 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.1015472412109375 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Time to load fused_adam op: 0.10184621810913086 seconds\n",
      "Time to load fused_adam op: 0.10184621810913086 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06582045555114746 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10200905799865723 seconds\n",
      "Time to load utils op: 0.10179543495178223 seconds\n",
      "Time to load utils op: 0.10188770294189453 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10148453712463379 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10185384750366211 seconds\n",
      "Time to load utils op: 0.10205650329589844 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10184407234191895 seconds\n",
      "Rank: 7 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 3 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 2 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 4 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 5 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 6 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 1 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 0 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002803802490234375 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002830028533935547 seconds\n",
      "Time to load utils op: 0.0002961158752441406 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00030541419982910156 seconds\n",
      "Time to load utils op: 0.0002944469451904297 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00028228759765625 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00027179718017578125 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00051116943359375 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 51.5 M\n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 2.0 K \n",
      "3 | head   | Linear     | 51.5 M\n",
      "--------------------------------------\n",
      "1.4 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 B     Total params\n",
      "5,650.702 Total estimated model params size (MB)\n",
      "Epoch 0:   6%| | 1000/16968 [36:41<9:45:51,  2.20s/it, v_num=ui5k, train/loss=5./usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 16968/16968 [10:26:22<00:00,  2.21s/it, v_num=ui5k, train/loss=\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                  | 1/86 [00:00<00:24,  3.44it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▍                  | 2/86 [00:00<00:36,  2.33it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▋                  | 3/86 [00:01<00:29,  2.79it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                  | 4/86 [00:01<00:27,  3.01it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                  | 5/86 [00:01<00:25,  3.13it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▎                 | 6/86 [00:01<00:24,  3.25it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▌                 | 7/86 [00:02<00:23,  3.36it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▊                 | 8/86 [00:02<00:26,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▉                 | 9/86 [00:02<00:24,  3.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██                | 10/86 [00:03<00:23,  3.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▎               | 11/86 [00:03<00:22,  3.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▌               | 12/86 [00:03<00:22,  3.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▋               | 13/86 [00:03<00:21,  3.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▉               | 14/86 [00:04<00:22,  3.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▏              | 15/86 [00:04<00:22,  3.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▎              | 16/86 [00:04<00:21,  3.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▌              | 17/86 [00:05<00:20,  3.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▊              | 18/86 [00:05<00:20,  3.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▉              | 19/86 [00:05<00:20,  3.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|████▏             | 20/86 [00:06<00:20,  3.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████▍             | 21/86 [00:06<00:20,  3.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▌             | 22/86 [00:06<00:19,  3.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▊             | 23/86 [00:07<00:19,  3.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|█████             | 24/86 [00:07<00:19,  3.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|█████▏            | 25/86 [00:07<00:18,  3.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████▍            | 26/86 [00:07<00:18,  3.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▋            | 27/86 [00:08<00:17,  3.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▊            | 28/86 [00:08<00:17,  3.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|██████            | 29/86 [00:08<00:16,  3.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|██████▎           | 30/86 [00:09<00:17,  3.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▍           | 31/86 [00:09<00:16,  3.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▋           | 32/86 [00:09<00:16,  3.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▉           | 33/86 [00:10<00:16,  3.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|███████           | 34/86 [00:10<00:15,  3.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████▎          | 35/86 [00:10<00:15,  3.27it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▌          | 36/86 [00:10<00:15,  3.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▋          | 37/86 [00:11<00:15,  3.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▉          | 38/86 [00:11<00:14,  3.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|████████▏         | 39/86 [00:12<00:14,  3.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████▎         | 40/86 [00:12<00:14,  3.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▌         | 41/86 [00:12<00:13,  3.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▊         | 42/86 [00:13<00:13,  3.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 43/86 [00:13<00:13,  3.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|█████████▏        | 44/86 [00:13<00:13,  3.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████████▍        | 45/86 [00:13<00:12,  3.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|█████████▋        | 46/86 [00:14<00:12,  3.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▊        | 47/86 [00:14<00:11,  3.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████        | 48/86 [00:15<00:11,  3.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|██████████▎       | 49/86 [00:15<00:11,  3.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|██████████▍       | 50/86 [00:15<00:11,  3.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████▋       | 51/86 [00:15<00:10,  3.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▉       | 52/86 [00:15<00:10,  3.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|███████████       | 53/86 [00:16<00:10,  3.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|███████████▎      | 54/86 [00:16<00:09,  3.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|███████████▌      | 55/86 [00:16<00:09,  3.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████▋      | 56/86 [00:16<00:09,  3.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|███████████▉      | 57/86 [00:17<00:08,  3.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████▏     | 58/86 [00:17<00:08,  3.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|████████████▎     | 59/86 [00:17<00:08,  3.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|████████████▌     | 60/86 [00:17<00:07,  3.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|████████████▊     | 61/86 [00:18<00:07,  3.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|████████████▉     | 62/86 [00:18<00:07,  3.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|█████████████▏    | 63/86 [00:18<00:06,  3.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|█████████████▍    | 64/86 [00:18<00:06,  3.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|█████████████▌    | 65/86 [00:19<00:06,  3.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|█████████████▊    | 66/86 [00:19<00:05,  3.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|██████████████    | 67/86 [00:19<00:05,  3.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|██████████████▏   | 68/86 [00:20<00:05,  3.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|██████████████▍   | 69/86 [00:20<00:05,  3.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|██████████████▋   | 70/86 [00:21<00:04,  3.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|██████████████▊   | 71/86 [00:21<00:04,  3.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|███████████████   | 72/86 [00:21<00:04,  3.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|███████████████▎  | 73/86 [00:22<00:03,  3.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|███████████████▍  | 74/86 [00:22<00:03,  3.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|███████████████▋  | 75/86 [00:22<00:03,  3.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|███████████████▉  | 76/86 [00:22<00:02,  3.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|████████████████  | 77/86 [00:23<00:02,  3.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|████████████████▎ | 78/86 [00:23<00:02,  3.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|████████████████▌ | 79/86 [00:23<00:02,  3.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|████████████████▋ | 80/86 [00:23<00:01,  3.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|████████████████▉ | 81/86 [00:23<00:01,  3.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|█████████████████▏| 82/86 [00:24<00:01,  3.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|█████████████████▎| 83/86 [00:24<00:00,  3.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|█████████████████▌| 84/86 [00:24<00:00,  3.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|█████████████████▊| 85/86 [00:24<00:00,  3.42it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 16968/16968 [10:26:54<00:00,  2.22s/it, v_num=ui5k, train/loss=\u001b[A\n",
      "Epoch 0: 100%|█| 16968/16968 [10:26:54<00:00,  2.22s/it, v_num=ui5k, train/loss=\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 16968/16968 [10:27:08<00:00,  2.22s/it, v_num=ui5k, train/loss=\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▁▁▁▁▁▂▁▄▁▁▂▁▁▂▁▁▁▁█▁▂▁▁▃▁▁▂▃▂▁▁▁▁▂▄▂▂▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▇▆▅▅▅▅▅▁▅▄▄▄▄▄▄▄▄▄▄▃▄▃▃▄▄▃▄▄▃▄▃▃▂▃▃▃▂▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 85\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1899\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 680\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 3.54688\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 4241\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 3.2932\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m(8x3090) Echo-B-1B4 - Enwiki Foundation (ctx=4096, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/p1orui5k\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230713_163624-p1orui5k/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the foundation model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/Echo-B-1B4-enwiki.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Enwiki Foundation (ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/Echo-B-1B4-enwiki/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 1734 params 1412675584 elements\n",
      "Saving fp32 state dict to ../model/Echo-B-1B4-Stage1.pth\n",
      "-rw-r--r-- 1 root root 5.3G Jul 14 03:17 ../model/Echo-B-1B4-Stage1.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/Echo-B-1B4-enwiki/last.ckpt\" \"../model/Echo-B-1B4-Stage1.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/Echo-B-1B4-Stage1.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RWKV_JIT_ON 1 RWKV_CUDA_ON 0 RESCALE_LAYER 0\n",
      "\n",
      "Loading ../model/Echo-B-1B4-Stage1.pth ...\n",
      "Strategy: (total 96+1=97 layers)\n",
      "* cuda [float32, float32], store 97 layers\n",
      "0-cuda-float32-float32 1-cuda-float32-float32 2-cuda-float32-float32 3-cuda-float32-float32 4-cuda-float32-float32 5-cuda-float32-float32 6-cuda-float32-float32 7-cuda-float32-float32 8-cuda-float32-float32 9-cuda-float32-float32 10-cuda-float32-float32 11-cuda-float32-float32 12-cuda-float32-float32 13-cuda-float32-float32 14-cuda-float32-float32 15-cuda-float32-float32 16-cuda-float32-float32 17-cuda-float32-float32 18-cuda-float32-float32 19-cuda-float32-float32 20-cuda-float32-float32 21-cuda-float32-float32 22-cuda-float32-float32 23-cuda-float32-float32 24-cuda-float32-float32 25-cuda-float32-float32 26-cuda-float32-float32 27-cuda-float32-float32 28-cuda-float32-float32 29-cuda-float32-float32 30-cuda-float32-float32 31-cuda-float32-float32 32-cuda-float32-float32 33-cuda-float32-float32 34-cuda-float32-float32 35-cuda-float32-float32 36-cuda-float32-float32 37-cuda-float32-float32 38-cuda-float32-float32 39-cuda-float32-float32 40-cuda-float32-float32 41-cuda-float32-float32 42-cuda-float32-float32 43-cuda-float32-float32 44-cuda-float32-float32 45-cuda-float32-float32 46-cuda-float32-float32 47-cuda-float32-float32 48-cuda-float32-float32 49-cuda-float32-float32 50-cuda-float32-float32 51-cuda-float32-float32 52-cuda-float32-float32 53-cuda-float32-float32 54-cuda-float32-float32 55-cuda-float32-float32 56-cuda-float32-float32 57-cuda-float32-float32 58-cuda-float32-float32 59-cuda-float32-float32 60-cuda-float32-float32 61-cuda-float32-float32 62-cuda-float32-float32 63-cuda-float32-float32 64-cuda-float32-float32 65-cuda-float32-float32 66-cuda-float32-float32 67-cuda-float32-float32 68-cuda-float32-float32 69-cuda-float32-float32 70-cuda-float32-float32 71-cuda-float32-float32 72-cuda-float32-float32 73-cuda-float32-float32 74-cuda-float32-float32 75-cuda-float32-float32 76-cuda-float32-float32 77-cuda-float32-float32 78-cuda-float32-float32 79-cuda-float32-float32 80-cuda-float32-float32 81-cuda-float32-float32 82-cuda-float32-float32 83-cuda-float32-float32 84-cuda-float32-float32 85-cuda-float32-float32 86-cuda-float32-float32 87-cuda-float32-float32 88-cuda-float32-float32 89-cuda-float32-float32 90-cuda-float32-float32 91-cuda-float32-float32 92-cuda-float32-float32 93-cuda-float32-float32 94-cuda-float32-float32 95-cuda-float32-float32 96-cuda-float32-float32 \n",
      "blocks.0.att.key.weight           f32   cuda:0   1024  1024 \n",
      "blocks.0.att.output.weight        f32   cuda:0   1024  1024 \n",
      "blocks.0.att.receptance.weight    f32   cuda:0   1024  1024 \n",
      "blocks.0.att.time_mix_k           f32   cuda:0   1024       \n",
      "blocks.0.att.time_mix_r           f32   cuda:0   1024       \n",
      "blocks.0.att.time_mix_v           f32   cuda:0   1024       \n",
      "blocks.0.att.value.weight         f32   cuda:0   1024  1024 \n",
      "blocks.0.ffn.key.weight           f32   cuda:0   1024  4096 \n",
      "blocks.0.ffn.receptance.weight    f32   cuda:0   1024  1024 \n",
      "blocks.0.ffn.time_mix_k           f32   cuda:0   1024       \n",
      "blocks.0.ffn.time_mix_r           f32   cuda:0   1024       \n",
      "blocks.0.ffn.value.weight         f32   cuda:0   4096  1024 \n",
      "blocks.0.ln1.bias                 f32   cuda:0   1024       \n",
      "blocks.0.ln1.weight               f32   cuda:0   1024       \n",
      "blocks.0.ln2.bias                 f32   cuda:0   1024       \n",
      "blocks.0.ln2.weight               f32   cuda:0   1024       \n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "blocks.95.att.key.weight          f32   cuda:0   1024  1024 \n",
      "blocks.95.att.output.weight       f32   cuda:0   1024  1024 \n",
      "blocks.95.att.receptance.weight   f32   cuda:0   1024  1024 \n",
      "blocks.95.att.time_mix_k          f32   cuda:0   1024       \n",
      "blocks.95.att.time_mix_r          f32   cuda:0   1024       \n",
      "blocks.95.att.time_mix_v          f32   cuda:0   1024       \n",
      "blocks.95.att.value.weight        f32   cuda:0   1024  1024 \n",
      "blocks.95.ffn.key.weight          f32   cuda:0   1024  4096 \n",
      "blocks.95.ffn.receptance.weight   f32   cuda:0   1024  1024 \n",
      "blocks.95.ffn.time_mix_k          f32   cuda:0   1024       \n",
      "blocks.95.ffn.time_mix_r          f32   cuda:0   1024       \n",
      "blocks.95.ffn.value.weight        f32   cuda:0   4096  1024 \n",
      "blocks.95.ln1.bias                f32   cuda:0   1024       \n",
      "blocks.95.ln1.weight              f32   cuda:0   1024       \n",
      "blocks.95.ln2.bias                f32   cuda:0   1024       \n",
      "blocks.95.ln2.weight              f32   cuda:0   1024       \n",
      "emb.weight                        f32      cpu  50277  1024 \n",
      "head.weight                       f32   cuda:0   1024 50277 \n",
      "ln_out.bias                       f32   cuda:0   1024       \n",
      "ln_out.weight                     f32   cuda:0   1024       \n",
      "blocks.0.att.time_decay           f32   cuda:0   1024       \n",
      "..............................................................................................\n",
      "blocks.95.att.time_decay          f32   cuda:0   1024       \n",
      "blocks.0.att.time_first           f32   cuda:0   1024       \n",
      "..............................................................................................\n",
      "blocks.95.att.time_first          f32   cuda:0   1024       \n",
      "\n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\n",
      "\n",
      "In Tibetan Buddhism, myth\n",
      "\n",
      "According to Chinese legend, the dragons descended into earth through a wormhole, the Earth has miraculously escaped from the earth and was absorbed into the sky. In Tibetan Buddhism, by way of which one of them was a god and buried in an old temple in Tibetan, the horses were swallowed and eaten by their servants. The humans said that they had come to heaven and do penance. According to Chinese legend, snakes worshipped animals but were less intelligent than their dogs. When he entered a fire house, they could only see in a dream or in his spirit. They wanted to get away and he set out to return to earth and kill them, but they were scared away when the animal asked for help. When they did so, they were unable to escape the flames themselves and destroyed themselves in what is known as The Water Witch.\n",
      "\n",
      "The animals were so weak that they could not remember how to beat them like any other animals in China before\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick dragon prompt validation\n",
    "!cd \"{TRAINER_DIR}\" && python3 dragon_test.py ../model/Echo-B-1B4-Stage1.pth \"cuda fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /home/picocreator/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu117/wkv_cuda/build.ninja...\n",
      "Building extension module wkv_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_cuda...\n",
      "RWKV_JIT_ON 1 RWKV_CUDA_ON 1 RESCALE_LAYER 0\n",
      "\n",
      "Loading /home/picocreator/rwkv-proj/picocreator-memory-experiment/model/Echo-B-1B4-Stage1.pth ...\n",
      "Strategy: (total 96+1=97 layers)\n",
      "* cuda [float32, float32], store 97 layers\n",
      "0-cuda-float32-float32 1-cuda-float32-float32 2-cuda-float32-float32 3-cuda-float32-float32 4-cuda-float32-float32 5-cuda-float32-float32 6-cuda-float32-float32 7-cuda-float32-float32 8-cuda-float32-float32 9-cuda-float32-float32 10-cuda-float32-float32 11-cuda-float32-float32 12-cuda-float32-float32 13-cuda-float32-float32 14-cuda-float32-float32 15-cuda-float32-float32 16-cuda-float32-float32 17-cuda-float32-float32 18-cuda-float32-float32 19-cuda-float32-float32 20-cuda-float32-float32 21-cuda-float32-float32 22-cuda-float32-float32 23-cuda-float32-float32 24-cuda-float32-float32 25-cuda-float32-float32 26-cuda-float32-float32 27-cuda-float32-float32 28-cuda-float32-float32 29-cuda-float32-float32 30-cuda-float32-float32 31-cuda-float32-float32 32-cuda-float32-float32 33-cuda-float32-float32 34-cuda-float32-float32 35-cuda-float32-float32 36-cuda-float32-float32 37-cuda-float32-float32 38-cuda-float32-float32 39-cuda-float32-float32 40-cuda-float32-float32 41-cuda-float32-float32 42-cuda-float32-float32 43-cuda-float32-float32 44-cuda-float32-float32 45-cuda-float32-float32 46-cuda-float32-float32 47-cuda-float32-float32 48-cuda-float32-float32 49-cuda-float32-float32 50-cuda-float32-float32 51-cuda-float32-float32 52-cuda-float32-float32 53-cuda-float32-float32 54-cuda-float32-float32 55-cuda-float32-float32 56-cuda-float32-float32 57-cuda-float32-float32 58-cuda-float32-float32 59-cuda-float32-float32 60-cuda-float32-float32 61-cuda-float32-float32 62-cuda-float32-float32 63-cuda-float32-float32 64-cuda-float32-float32 65-cuda-float32-float32 66-cuda-float32-float32 67-cuda-float32-float32 68-cuda-float32-float32 69-cuda-float32-float32 70-cuda-float32-float32 71-cuda-float32-float32 72-cuda-float32-float32 73-cuda-float32-float32 74-cuda-float32-float32 75-cuda-float32-float32 76-cuda-float32-float32 77-cuda-float32-float32 78-cuda-float32-float32 79-cuda-float32-float32 80-cuda-float32-float32 81-cuda-float32-float32 82-cuda-float32-float32 83-cuda-float32-float32 84-cuda-float32-float32 85-cuda-float32-float32 86-cuda-float32-float32 87-cuda-float32-float32 88-cuda-float32-float32 89-cuda-float32-float32 90-cuda-float32-float32 91-cuda-float32-float32 92-cuda-float32-float32 93-cuda-float32-float32 94-cuda-float32-float32 95-cuda-float32-float32 96-cuda-float32-float32 \n",
      "blocks.0.att.key.weight           f32   cuda:0   1024  1024 \n",
      "blocks.0.att.output.weight        f32   cuda:0   1024  1024 \n",
      "blocks.0.att.receptance.weight    f32   cuda:0   1024  1024 \n",
      "blocks.0.att.time_mix_k           f32   cuda:0   1024       \n",
      "blocks.0.att.time_mix_r           f32   cuda:0   1024       \n",
      "blocks.0.att.time_mix_v           f32   cuda:0   1024       \n",
      "blocks.0.att.value.weight         f32   cuda:0   1024  1024 \n",
      "blocks.0.ffn.key.weight           f32   cuda:0   1024  4096 \n",
      "blocks.0.ffn.receptance.weight    f32   cuda:0   1024  1024 \n",
      "blocks.0.ffn.time_mix_k           f32   cuda:0   1024       \n",
      "blocks.0.ffn.time_mix_r           f32   cuda:0   1024       \n",
      "blocks.0.ffn.value.weight         f32   cuda:0   4096  1024 \n",
      "blocks.0.ln1.bias                 f32   cuda:0   1024       \n",
      "blocks.0.ln1.weight               f32   cuda:0   1024       \n",
      "blocks.0.ln2.bias                 f32   cuda:0   1024       \n",
      "blocks.0.ln2.weight               f32   cuda:0   1024       \n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "blocks.95.att.key.weight          f32   cuda:0   1024  1024 \n",
      "blocks.95.att.output.weight       f32   cuda:0   1024  1024 \n",
      "blocks.95.att.receptance.weight   f32   cuda:0   1024  1024 \n",
      "blocks.95.att.time_mix_k          f32   cuda:0   1024       \n",
      "blocks.95.att.time_mix_r          f32   cuda:0   1024       \n",
      "blocks.95.att.time_mix_v          f32   cuda:0   1024       \n",
      "blocks.95.att.value.weight        f32   cuda:0   1024  1024 \n",
      "blocks.95.ffn.key.weight          f32   cuda:0   1024  4096 \n",
      "blocks.95.ffn.receptance.weight   f32   cuda:0   1024  1024 \n",
      "blocks.95.ffn.time_mix_k          f32   cuda:0   1024       \n",
      "blocks.95.ffn.time_mix_r          f32   cuda:0   1024       \n",
      "blocks.95.ffn.value.weight        f32   cuda:0   4096  1024 \n",
      "blocks.95.ln1.bias                f32   cuda:0   1024       \n",
      "blocks.95.ln1.weight              f32   cuda:0   1024       \n",
      "blocks.95.ln2.bias                f32   cuda:0   1024       \n",
      "blocks.95.ln2.weight              f32   cuda:0   1024       \n",
      "emb.weight                        f32      cpu  50277  1024 \n",
      "head.weight                       f32   cuda:0   1024 50277 \n",
      "ln_out.bias                       f32   cuda:0   1024       \n",
      "ln_out.weight                     f32   cuda:0   1024       \n",
      "blocks.0.att.time_decay           f32   cuda:0   1024       \n",
      "..............................................................................................\n",
      "blocks.95.att.time_decay          f32   cuda:0   1024       \n",
      "blocks.0.att.time_first           f32   cuda:0   1024       \n",
      "..............................................................................................\n",
      "blocks.95.att.time_first          f32   cuda:0   1024       \n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "Model validation at 5 tokens : 0.0% similarity, with 0 matched token, and 5 token mismatch\n",
      "Model validation at 10 tokens : 0.0% similarity, with 0 matched token, and 10 token mismatch\n",
      "Model validation at 15 tokens : 0.0% similarity, with 0 matched token, and 15 token mismatch\n",
      "Model validation at 20 tokens : 0.0% similarity, with 0 matched token, and 20 token mismatch\n",
      "Model validation at 25 tokens : 0.0% similarity, with 0 matched token, and 25 token mismatch\n",
      "Model validation at 30 tokens : 0.0% similarity, with 0 matched token, and 30 token mismatch\n",
      "Model validation at 35 tokens : 2.857142857142857% similarity, with 1 matched token, and 34 token mismatch\n",
      "Model validation at 40 tokens : 2.5% similarity, with 1 matched token, and 39 token mismatch\n",
      "Model validation at 45 tokens : 2.2222222222222223% similarity, with 1 matched token, and 44 token mismatch\n",
      "Model validation at 50 tokens : 2.0% similarity, with 1 matched token, and 49 token mismatch\n",
      "Model validation at 55 tokens : 1.8181818181818181% similarity, with 1 matched token, and 54 token mismatch\n",
      "Model validation at 60 tokens : 1.6666666666666667% similarity, with 1 matched token, and 59 token mismatch\n",
      "Model validation at 65 tokens : 1.5384615384615385% similarity, with 1 matched token, and 64 token mismatch\n",
      "Model validation at 70 tokens : 1.4285714285714286% similarity, with 1 matched token, and 69 token mismatch\n",
      "Model validation at 75 tokens : 0.0% similarity, with 0 matched token, and 75 token mismatch\n",
      "Model validation at 80 tokens : 0.0% similarity, with 0 matched token, and 80 token mismatch\n",
      "Model validation at 85 tokens : 0.0% similarity, with 0 matched token, and 85 token mismatch\n",
      "Model validation at 90 tokens : 1.1111111111111112% similarity, with 1 matched token, and 89 token mismatch\n",
      "Model validation at 95 tokens : 3.1578947368421053% similarity, with 3 matched token, and 92 token mismatch\n",
      "Model validation at 100 tokens : 2.0% similarity, with 2 matched token, and 98 token mismatch\n",
      "Model validation at 110 tokens : 1.8181818181818181% similarity, with 2 matched token, and 108 token mismatch\n",
      "Model validation at 120 tokens : 1.6666666666666667% similarity, with 2 matched token, and 118 token mismatch\n",
      "Model validation at 130 tokens : 2.307692307692308% similarity, with 3 matched token, and 127 token mismatch\n",
      "Model validation at 140 tokens : 0.7142857142857143% similarity, with 1 matched token, and 139 token mismatch\n",
      "Model validation at 150 tokens : 1.3333333333333335% similarity, with 2 matched token, and 148 token mismatch\n",
      "Model validation at 175 tokens : 1.1428571428571428% similarity, with 2 matched token, and 173 token mismatch\n",
      "Model validation at 200 tokens : 2.0% similarity, with 4 matched token, and 196 token mismatch\n",
      "Model validation at 225 tokens : 2.2222222222222223% similarity, with 5 matched token, and 220 token mismatch\n",
      "Model validation at 250 tokens : 2.0% similarity, with 5 matched token, and 245 token mismatch\n",
      "Model validation at 275 tokens : 1.4545454545454546% similarity, with 4 matched token, and 271 token mismatch\n",
      "Model validation at 300 tokens : 1.6666666666666667% similarity, with 5 matched token, and 295 token mismatch\n",
      "Model validation at 325 tokens : 1.8461538461538463% similarity, with 6 matched token, and 319 token mismatch\n",
      "Model validation at 350 tokens : 1.1428571428571428% similarity, with 4 matched token, and 346 token mismatch\n",
      "Model validation at 375 tokens : 1.6% similarity, with 6 matched token, and 369 token mismatch\n",
      "Model validation at 400 tokens : 1.5% similarity, with 6 matched token, and 394 token mismatch\n",
      "Model validation at 425 tokens : 1.411764705882353% similarity, with 6 matched token, and 419 token mismatch\n",
      "Model validation at 450 tokens : 1.3333333333333335% similarity, with 6 matched token, and 444 token mismatch\n",
      "Model validation at 475 tokens : 1.263157894736842% similarity, with 6 matched token, and 469 token mismatch\n",
      "Model validation at 500 tokens : 1.4000000000000001% similarity, with 7 matched token, and 493 token mismatch\n",
      "Model validation at 550 tokens : 1.4545454545454546% similarity, with 8 matched token, and 542 token mismatch\n",
      "Model validation at 600 tokens : 1.3333333333333335% similarity, with 8 matched token, and 592 token mismatch\n",
      "Model validation at 650 tokens : 1.3846153846153846% similarity, with 9 matched token, and 641 token mismatch\n",
      "Model validation at 700 tokens : 1.2857142857142856% similarity, with 9 matched token, and 691 token mismatch\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick memory test\n",
    "# (We dun expect this to work, as we have not finetune for memory recall, but its a baseline)\n",
    "!python3 ./memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/Echo-B-1B4-Stage1.pth\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2 : Instruct Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████████████| 7.79k/7.79k [00:00<00:00, 22.9MB/s]\n",
      "Downloading and preparing dataset None/None to /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n",
      "Downloading data files:   0%|                             | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                             | 0.00/7.80M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   1%|▏                    | 60.4k/7.80M [00:00<00:23, 335kB/s]\u001b[A\n",
      "Downloading data:   2%|▍                     | 177k/7.80M [00:00<00:14, 517kB/s]\u001b[A\n",
      "Downloading data:   4%|▉                     | 314k/7.80M [00:00<00:11, 626kB/s]\u001b[A\n",
      "Downloading data:   6%|█▎                    | 459k/7.80M [00:00<00:10, 691kB/s]\u001b[A\n",
      "Downloading data:   8%|█▋                    | 617k/7.80M [00:00<00:08, 839kB/s]\u001b[A\n",
      "Downloading data:   9%|█▉                    | 706k/7.80M [00:00<00:09, 777kB/s]\u001b[A\n",
      "Downloading data:  11%|██▍                   | 872k/7.80M [00:01<00:07, 918kB/s]\u001b[A\n",
      "Downloading data:  13%|██▊                   | 981k/7.80M [00:01<00:07, 875kB/s]\u001b[A\n",
      "Downloading data:  15%|███                 | 1.20M/7.80M [00:01<00:06, 1.09MB/s]\u001b[A\n",
      "Downloading data:  17%|███▎                | 1.31M/7.80M [00:01<00:06, 1.02MB/s]\u001b[A\n",
      "Downloading data:  20%|███▉                | 1.53M/7.80M [00:01<00:04, 1.32MB/s]\u001b[A\n",
      "Downloading data:  22%|████▎               | 1.69M/7.80M [00:01<00:05, 1.14MB/s]\u001b[A\n",
      "Downloading data:  25%|█████               | 1.97M/7.80M [00:01<00:04, 1.42MB/s]\u001b[A\n",
      "Downloading data:  27%|█████▍              | 2.14M/7.80M [00:02<00:04, 1.36MB/s]\u001b[A\n",
      "Downloading data:  31%|██████▎             | 2.45M/7.80M [00:02<00:03, 1.65MB/s]\u001b[A\n",
      "Downloading data:  34%|██████▊             | 2.64M/7.80M [00:02<00:03, 1.57MB/s]\u001b[A\n",
      "Downloading data:  39%|███████▋            | 3.01M/7.80M [00:02<00:02, 1.92MB/s]\u001b[A\n",
      "Downloading data:  41%|████████▎           | 3.24M/7.80M [00:02<00:02, 1.83MB/s]\u001b[A\n",
      "Downloading data:  47%|█████████▎          | 3.65M/7.80M [00:02<00:01, 2.21MB/s]\u001b[A\n",
      "Downloading data:  50%|██████████          | 3.92M/7.80M [00:02<00:01, 2.11MB/s]\u001b[A\n",
      "Downloading data:  56%|███████████▎        | 4.40M/7.80M [00:03<00:01, 2.56MB/s]\u001b[A\n",
      "Downloading data:  60%|████████████        | 4.70M/7.80M [00:03<00:01, 2.47MB/s]\u001b[A\n",
      "Downloading data:  66%|█████████████▏      | 5.13M/7.80M [00:03<00:00, 2.92MB/s]\u001b[A\n",
      "Downloading data:  72%|██████████████▎     | 5.59M/7.80M [00:03<00:00, 3.08MB/s]\u001b[A\n",
      "Downloading data:  76%|███████████████▎    | 5.95M/7.80M [00:03<00:00, 2.99MB/s]\u001b[A\n",
      "Downloading data:  83%|████████████████▋   | 6.49M/7.80M [00:03<00:00, 3.59MB/s]\u001b[A\n",
      "Downloading data:  90%|██████████████████  | 7.03M/7.80M [00:03<00:00, 3.74MB/s]\u001b[A\n",
      "Downloading data: 100%|████████████████████| 7.80M/7.80M [00:03<00:00, 2.00MB/s]\u001b[A\n",
      "Downloading data files: 100%|█████████████████████| 1/1 [00:05<00:00,  5.63s/it]\n",
      "Extracting data files: 100%|████████████████████| 1/1 [00:00<00:00, 1532.45it/s]\n",
      "Setting num_proc from 64 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 657.72it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/Echo-B-1B4-instruct.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 2683499085\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 2683499085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230714_032628-nxeap7ls\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m(8x3090) Echo-B-1B4 - Instruct (train-ctx=4096, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/nxeap7ls\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_4096_bf16/build.ninja...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_4096_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 4\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 803.35it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-f7f96ced2d76e1c7_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-3b2912dc14935a9a_*_of_00064.arrow\n",
      "[rank: 0] Global seed set to 2683499085                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-07-14 03:26:43,863] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 7] Global seed set to 2683499085\n",
      "[rank: 2] Global seed set to 2683499085\n",
      "[rank: 1] Global seed set to 2683499085\n",
      "[rank: 6] Global seed set to 2683499085\n",
      "[rank: 5] Global seed set to 2683499085\n",
      "[rank: 4] Global seed set to 2683499085\n",
      "[rank: 3] Global seed set to 2683499085\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_4096_bf16/build.ninja...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "[rank: 6] Global seed set to 2683499085\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-07-14 03:27:15,341] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 2683499085\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-07-14 03:27:19,058] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 2683499085\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-07-14 03:27:19,220] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 2683499085\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-07-14 03:27:19,414] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 2683499085\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-07-14 03:27:19,460] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 2683499085\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-07-14 03:27:19,464] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 2683499085\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-07-14 03:27:19,470] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  4.000e-04 (0.0004)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.061933279037475586 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.1012263298034668 seconds\n",
      "Time to load fused_adam op: 0.10136294364929199 seconds\n",
      "Time to load fused_adam op: 0.10136103630065918 seconds\n",
      "Time to load fused_adam op: 0.1014103889465332 seconds\n",
      "Time to load fused_adam op: 0.10126471519470215 seconds\n",
      "Time to load fused_adam op: 0.10137701034545898 seconds\n",
      "Time to load fused_adam op: 0.10143685340881348 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0690162181854248 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10192203521728516 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10188436508178711 seconds\n",
      "Time to load utils op: 0.10186076164245605 seconds\n",
      "Time to load utils op: 0.10139346122741699 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10203266143798828 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10186004638671875 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10187983512878418 seconds\n",
      "Rank: 6 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 0 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 2 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 1 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 7 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 5 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 3 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 4 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002827644348144531 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.0002751350402832031 seconds\n",
      "Time to load utils op: 0.00029540061950683594 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002677440643310547 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.00029659271240234375 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002841949462890625 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00025343894958496094 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00048804283142089844 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 51.5 M\n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 2.0 K \n",
      "3 | head   | Linear     | 51.5 M\n",
      "--------------------------------------\n",
      "1.4 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 B     Total params\n",
      "5,650.702 Total estimated model params size (MB)\n",
      "Epoch 0:  54%|▌| 1000/1867 [24:47<21:30,  1.49s/it, v_num=p7ls, train/loss=3.520/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 1867/1867 [46:07<00:00,  1.48s/it, v_num=p7ls, train/loss=2.380\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▉                 | 1/10 [00:00<00:02,  3.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▊               | 2/10 [00:00<00:02,  3.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████▋             | 3/10 [00:00<00:01,  4.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|███████▌           | 4/10 [00:00<00:01,  4.33it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████▌         | 5/10 [00:01<00:01,  4.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|███████████▍       | 6/10 [00:01<00:00,  4.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|█████████████▎     | 7/10 [00:01<00:00,  4.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|███████████████▏   | 8/10 [00:01<00:00,  4.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|█████████████████  | 9/10 [00:01<00:00,  4.58it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 1867/1867 [46:16<00:00,  1.49s/it, v_num=p7ls, train/loss=2.380\u001b[A\n",
      "Epoch 0: 100%|█| 1867/1867 [46:16<00:00,  1.49s/it, v_num=p7ls, train/loss=2.380\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 1867/1867 [46:30<00:00,  1.49s/it, v_num=p7ls, train/loss=2.380\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▁▃▃▁▁▁▂▁▂▁▂▁▂▂▁▂▃▁▂▁▁▁█▁▂▂▃▂▂▁▁▂▁▁▄▃▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ▄▆▇▄▇▅▄▆▄▇▅▅▆▁▁▄▅▅▅▅▅▄▄▅▄▄▂▆▄▅▄▅▁█▄▆▄▂▁▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 223\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 2.34375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 466\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 3.17764\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m(8x3090) Echo-B-1B4 - Instruct (train-ctx=4096, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/nxeap7ls\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230714_032628-nxeap7ls/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the instruct finetuning\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/Echo-B-1B4-instruct.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Instruct (train-ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/Echo-B-1B4-instruct/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 1734 params 1412675584 elements\n",
      "Saving fp32 state dict to ../model/Echo-B-1B4-Stage2.pth\n",
      "-rw-r--r-- 1 root root 5.3G Jul 14 04:15 ../model/Echo-B-1B4-Stage2.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/Echo-B-1B4-instruct/last.ckpt\" \"../model/Echo-B-1B4-Stage2.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/Echo-B-1B4-Stage2.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RWKV_JIT_ON 1 RWKV_CUDA_ON 0 RESCALE_LAYER 0\n",
      "\n",
      "Loading ../model/Echo-B-1B4-Stage2.pth ...\n",
      "Strategy: (total 96+1=97 layers)\n",
      "* cuda [float32, float32], store 97 layers\n",
      "0-cuda-float32-float32 1-cuda-float32-float32 2-cuda-float32-float32 3-cuda-float32-float32 4-cuda-float32-float32 5-cuda-float32-float32 6-cuda-float32-float32 7-cuda-float32-float32 8-cuda-float32-float32 9-cuda-float32-float32 10-cuda-float32-float32 11-cuda-float32-float32 12-cuda-float32-float32 13-cuda-float32-float32 14-cuda-float32-float32 15-cuda-float32-float32 16-cuda-float32-float32 17-cuda-float32-float32 18-cuda-float32-float32 19-cuda-float32-float32 20-cuda-float32-float32 21-cuda-float32-float32 22-cuda-float32-float32 23-cuda-float32-float32 24-cuda-float32-float32 25-cuda-float32-float32 26-cuda-float32-float32 27-cuda-float32-float32 28-cuda-float32-float32 29-cuda-float32-float32 30-cuda-float32-float32 31-cuda-float32-float32 32-cuda-float32-float32 33-cuda-float32-float32 34-cuda-float32-float32 35-cuda-float32-float32 36-cuda-float32-float32 37-cuda-float32-float32 38-cuda-float32-float32 39-cuda-float32-float32 40-cuda-float32-float32 41-cuda-float32-float32 42-cuda-float32-float32 43-cuda-float32-float32 44-cuda-float32-float32 45-cuda-float32-float32 46-cuda-float32-float32 47-cuda-float32-float32 48-cuda-float32-float32 49-cuda-float32-float32 50-cuda-float32-float32 51-cuda-float32-float32 52-cuda-float32-float32 53-cuda-float32-float32 54-cuda-float32-float32 55-cuda-float32-float32 56-cuda-float32-float32 57-cuda-float32-float32 58-cuda-float32-float32 59-cuda-float32-float32 60-cuda-float32-float32 61-cuda-float32-float32 62-cuda-float32-float32 63-cuda-float32-float32 64-cuda-float32-float32 65-cuda-float32-float32 66-cuda-float32-float32 67-cuda-float32-float32 68-cuda-float32-float32 69-cuda-float32-float32 70-cuda-float32-float32 71-cuda-float32-float32 72-cuda-float32-float32 73-cuda-float32-float32 74-cuda-float32-float32 75-cuda-float32-float32 76-cuda-float32-float32 77-cuda-float32-float32 78-cuda-float32-float32 79-cuda-float32-float32 80-cuda-float32-float32 81-cuda-float32-float32 82-cuda-float32-float32 83-cuda-float32-float32 84-cuda-float32-float32 85-cuda-float32-float32 86-cuda-float32-float32 87-cuda-float32-float32 88-cuda-float32-float32 89-cuda-float32-float32 90-cuda-float32-float32 91-cuda-float32-float32 92-cuda-float32-float32 93-cuda-float32-float32 94-cuda-float32-float32 95-cuda-float32-float32 96-cuda-float32-float32 \n",
      "blocks.0.att.key.weight           f32   cuda:0   1024  1024 \n",
      "blocks.0.att.output.weight        f32   cuda:0   1024  1024 \n",
      "blocks.0.att.receptance.weight    f32   cuda:0   1024  1024 \n",
      "blocks.0.att.time_mix_k           f32   cuda:0   1024       \n",
      "blocks.0.att.time_mix_r           f32   cuda:0   1024       \n",
      "blocks.0.att.time_mix_v           f32   cuda:0   1024       \n",
      "blocks.0.att.value.weight         f32   cuda:0   1024  1024 \n",
      "blocks.0.ffn.key.weight           f32   cuda:0   1024  4096 \n",
      "blocks.0.ffn.receptance.weight    f32   cuda:0   1024  1024 \n",
      "blocks.0.ffn.time_mix_k           f32   cuda:0   1024       \n",
      "blocks.0.ffn.time_mix_r           f32   cuda:0   1024       \n",
      "blocks.0.ffn.value.weight         f32   cuda:0   4096  1024 \n",
      "blocks.0.ln1.bias                 f32   cuda:0   1024       \n",
      "blocks.0.ln1.weight               f32   cuda:0   1024       \n",
      "blocks.0.ln2.bias                 f32   cuda:0   1024       \n",
      "blocks.0.ln2.weight               f32   cuda:0   1024       \n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "blocks.95.att.key.weight          f32   cuda:0   1024  1024 \n",
      "blocks.95.att.output.weight       f32   cuda:0   1024  1024 \n",
      "blocks.95.att.receptance.weight   f32   cuda:0   1024  1024 \n",
      "blocks.95.att.time_mix_k          f32   cuda:0   1024       \n",
      "blocks.95.att.time_mix_r          f32   cuda:0   1024       \n",
      "blocks.95.att.time_mix_v          f32   cuda:0   1024       \n",
      "blocks.95.att.value.weight        f32   cuda:0   1024  1024 \n",
      "blocks.95.ffn.key.weight          f32   cuda:0   1024  4096 \n",
      "blocks.95.ffn.receptance.weight   f32   cuda:0   1024  1024 \n",
      "blocks.95.ffn.time_mix_k          f32   cuda:0   1024       \n",
      "blocks.95.ffn.time_mix_r          f32   cuda:0   1024       \n",
      "blocks.95.ffn.value.weight        f32   cuda:0   4096  1024 \n",
      "blocks.95.ln1.bias                f32   cuda:0   1024       \n",
      "blocks.95.ln1.weight              f32   cuda:0   1024       \n",
      "blocks.95.ln2.bias                f32   cuda:0   1024       \n",
      "blocks.95.ln2.weight              f32   cuda:0   1024       \n",
      "emb.weight                        f32      cpu  50277  1024 \n",
      "head.weight                       f32   cuda:0   1024 50277 \n",
      "ln_out.bias                       f32   cuda:0   1024       \n",
      "ln_out.weight                     f32   cuda:0   1024       \n",
      "blocks.0.att.time_decay           f32   cuda:0   1024       \n",
      "..............................................................................................\n",
      "blocks.95.att.time_decay          f32   cuda:0   1024       \n",
      "blocks.0.att.time_first           f32   cuda:0   1024       \n",
      "..............................................................................................\n",
      "blocks.95.att.time_first          f32   cuda:0   1024       \n",
      "\n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\n",
      "\n",
      "For this reason, on the other hand, a Chinese mathematician asked the emperor to confirm that a huge, powerful mountain of ivies has a much closer connection to the Tibetan Plateau than from the mainland. He is currently satisfied by the fact that the golden eagle does not have enough stamens to sustain it in its current state. However, if the mountain peak of Sukkot was built in 2006, its area could become a part of the artificial intelligence world in 2008. It is now a beautiful tree in China and a must-visit destination for some people in Thailand.# Answer:\\nBeng Penguins\n",
      "Ryuan \n",
      "Suez vi\n",
      "Changkong\n",
      "The great forest of Nepal\n",
      "Bengra \n",
      "India \n",
      "Japan \n",
      "Kangaroo \n",
      "China \n",
      "Japan \n",
      "Costa Rica \n",
      "Japan \n",
      "Chongqing \n",
      "India \n",
      "India \n",
      "Chongqingis \n",
      "United States \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick dragon prompt validation\n",
    "!cd \"{TRAINER_DIR}\" && python3 dragon_test.py \"../model/Echo-B-1B4-Stage2.pth\" \"cuda fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_cuda/build.ninja...\n",
      "Building extension module wkv_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_cuda...\n",
      "RWKV_JIT_ON 1 RWKV_CUDA_ON 1 RESCALE_LAYER 0\n",
      "\n",
      "Loading /root/picocreator-memory-experiment/model/Echo-B-1B4-Stage2.pth ...\n",
      "Strategy: (total 96+1=97 layers)\n",
      "* cuda [float32, float32], store 97 layers\n",
      "0-cuda-float32-float32 1-cuda-float32-float32 2-cuda-float32-float32 3-cuda-float32-float32 4-cuda-float32-float32 5-cuda-float32-float32 6-cuda-float32-float32 7-cuda-float32-float32 8-cuda-float32-float32 9-cuda-float32-float32 10-cuda-float32-float32 11-cuda-float32-float32 12-cuda-float32-float32 13-cuda-float32-float32 14-cuda-float32-float32 15-cuda-float32-float32 16-cuda-float32-float32 17-cuda-float32-float32 18-cuda-float32-float32 19-cuda-float32-float32 20-cuda-float32-float32 21-cuda-float32-float32 22-cuda-float32-float32 23-cuda-float32-float32 24-cuda-float32-float32 25-cuda-float32-float32 26-cuda-float32-float32 27-cuda-float32-float32 28-cuda-float32-float32 29-cuda-float32-float32 30-cuda-float32-float32 31-cuda-float32-float32 32-cuda-float32-float32 33-cuda-float32-float32 34-cuda-float32-float32 35-cuda-float32-float32 36-cuda-float32-float32 37-cuda-float32-float32 38-cuda-float32-float32 39-cuda-float32-float32 40-cuda-float32-float32 41-cuda-float32-float32 42-cuda-float32-float32 43-cuda-float32-float32 44-cuda-float32-float32 45-cuda-float32-float32 46-cuda-float32-float32 47-cuda-float32-float32 48-cuda-float32-float32 49-cuda-float32-float32 50-cuda-float32-float32 51-cuda-float32-float32 52-cuda-float32-float32 53-cuda-float32-float32 54-cuda-float32-float32 55-cuda-float32-float32 56-cuda-float32-float32 57-cuda-float32-float32 58-cuda-float32-float32 59-cuda-float32-float32 60-cuda-float32-float32 61-cuda-float32-float32 62-cuda-float32-float32 63-cuda-float32-float32 64-cuda-float32-float32 65-cuda-float32-float32 66-cuda-float32-float32 67-cuda-float32-float32 68-cuda-float32-float32 69-cuda-float32-float32 70-cuda-float32-float32 71-cuda-float32-float32 72-cuda-float32-float32 73-cuda-float32-float32 74-cuda-float32-float32 75-cuda-float32-float32 76-cuda-float32-float32 77-cuda-float32-float32 78-cuda-float32-float32 79-cuda-float32-float32 80-cuda-float32-float32 81-cuda-float32-float32 82-cuda-float32-float32 83-cuda-float32-float32 84-cuda-float32-float32 85-cuda-float32-float32 86-cuda-float32-float32 87-cuda-float32-float32 88-cuda-float32-float32 89-cuda-float32-float32 90-cuda-float32-float32 91-cuda-float32-float32 92-cuda-float32-float32 93-cuda-float32-float32 94-cuda-float32-float32 95-cuda-float32-float32 96-cuda-float32-float32 \n",
      "blocks.0.att.key.weight           f32   cuda:0   1024  1024 \n",
      "blocks.0.att.output.weight        f32   cuda:0   1024  1024 \n",
      "blocks.0.att.receptance.weight    f32   cuda:0   1024  1024 \n",
      "blocks.0.att.time_mix_k           f32   cuda:0   1024       \n",
      "blocks.0.att.time_mix_r           f32   cuda:0   1024       \n",
      "blocks.0.att.time_mix_v           f32   cuda:0   1024       \n",
      "blocks.0.att.value.weight         f32   cuda:0   1024  1024 \n",
      "blocks.0.ffn.key.weight           f32   cuda:0   1024  4096 \n",
      "blocks.0.ffn.receptance.weight    f32   cuda:0   1024  1024 \n",
      "blocks.0.ffn.time_mix_k           f32   cuda:0   1024       \n",
      "blocks.0.ffn.time_mix_r           f32   cuda:0   1024       \n",
      "blocks.0.ffn.value.weight         f32   cuda:0   4096  1024 \n",
      "blocks.0.ln1.bias                 f32   cuda:0   1024       \n",
      "blocks.0.ln1.weight               f32   cuda:0   1024       \n",
      "blocks.0.ln2.bias                 f32   cuda:0   1024       \n",
      "blocks.0.ln2.weight               f32   cuda:0   1024       \n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "blocks.95.att.key.weight          f32   cuda:0   1024  1024 \n",
      "blocks.95.att.output.weight       f32   cuda:0   1024  1024 \n",
      "blocks.95.att.receptance.weight   f32   cuda:0   1024  1024 \n",
      "blocks.95.att.time_mix_k          f32   cuda:0   1024       \n",
      "blocks.95.att.time_mix_r          f32   cuda:0   1024       \n",
      "blocks.95.att.time_mix_v          f32   cuda:0   1024       \n",
      "blocks.95.att.value.weight        f32   cuda:0   1024  1024 \n",
      "blocks.95.ffn.key.weight          f32   cuda:0   1024  4096 \n",
      "blocks.95.ffn.receptance.weight   f32   cuda:0   1024  1024 \n",
      "blocks.95.ffn.time_mix_k          f32   cuda:0   1024       \n",
      "blocks.95.ffn.time_mix_r          f32   cuda:0   1024       \n",
      "blocks.95.ffn.value.weight        f32   cuda:0   4096  1024 \n",
      "blocks.95.ln1.bias                f32   cuda:0   1024       \n",
      "blocks.95.ln1.weight              f32   cuda:0   1024       \n",
      "blocks.95.ln2.bias                f32   cuda:0   1024       \n",
      "blocks.95.ln2.weight              f32   cuda:0   1024       \n",
      "emb.weight                        f32      cpu  50277  1024 \n",
      "head.weight                       f32   cuda:0   1024 50277 \n",
      "ln_out.bias                       f32   cuda:0   1024       \n",
      "ln_out.weight                     f32   cuda:0   1024       \n",
      "blocks.0.att.time_decay           f32   cuda:0   1024       \n",
      "..............................................................................................\n",
      "blocks.95.att.time_decay          f32   cuda:0   1024       \n",
      "blocks.0.att.time_first           f32   cuda:0   1024       \n",
      "..............................................................................................\n",
      "blocks.95.att.time_first          f32   cuda:0   1024       \n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "Model validation at 5 tokens : 20.0% similarity, with 1 matched token, and 4 token mismatch\n",
      "Model validation at 10 tokens : 10.0% similarity, with 1 matched token, and 9 token mismatch\n",
      "Model validation at 15 tokens : 6.666666666666667% similarity, with 1 matched token, and 14 token mismatch\n",
      "Model validation at 20 tokens : 5.0% similarity, with 1 matched token, and 19 token mismatch\n",
      "Model validation at 25 tokens : 4.0% similarity, with 1 matched token, and 24 token mismatch\n",
      "Model validation at 30 tokens : 6.666666666666667% similarity, with 2 matched token, and 28 token mismatch\n",
      "Model validation at 35 tokens : 5.714285714285714% similarity, with 2 matched token, and 33 token mismatch\n",
      "Model validation at 40 tokens : 5.0% similarity, with 2 matched token, and 38 token mismatch\n",
      "Model validation at 45 tokens : 4.444444444444445% similarity, with 2 matched token, and 43 token mismatch\n",
      "Model validation at 50 tokens : 4.0% similarity, with 2 matched token, and 48 token mismatch\n",
      "Model validation at 55 tokens : 3.6363636363636362% similarity, with 2 matched token, and 53 token mismatch\n",
      "Model validation at 60 tokens : 3.3333333333333335% similarity, with 2 matched token, and 58 token mismatch\n",
      "Model validation at 65 tokens : 3.076923076923077% similarity, with 2 matched token, and 63 token mismatch\n",
      "Model validation at 70 tokens : 2.857142857142857% similarity, with 2 matched token, and 68 token mismatch\n",
      "Model validation at 75 tokens : 2.666666666666667% similarity, with 2 matched token, and 73 token mismatch\n",
      "Model validation at 80 tokens : 2.5% similarity, with 2 matched token, and 78 token mismatch\n",
      "Model validation at 85 tokens : 2.3529411764705883% similarity, with 2 matched token, and 83 token mismatch\n",
      "Model validation at 90 tokens : 3.3333333333333335% similarity, with 3 matched token, and 87 token mismatch\n",
      "Model validation at 95 tokens : 3.1578947368421053% similarity, with 3 matched token, and 92 token mismatch\n",
      "Model validation at 100 tokens : 3.0% similarity, with 3 matched token, and 97 token mismatch\n",
      "Model validation at 110 tokens : 2.727272727272727% similarity, with 3 matched token, and 107 token mismatch\n",
      "Model validation at 120 tokens : 2.5% similarity, with 3 matched token, and 117 token mismatch\n",
      "Model validation at 130 tokens : 2.307692307692308% similarity, with 3 matched token, and 127 token mismatch\n",
      "Model validation at 140 tokens : 1.4285714285714286% similarity, with 2 matched token, and 138 token mismatch\n",
      "Model validation at 150 tokens : 1.3333333333333335% similarity, with 2 matched token, and 148 token mismatch\n",
      "Model validation at 175 tokens : 1.1428571428571428% similarity, with 2 matched token, and 173 token mismatch\n",
      "Model validation at 200 tokens : 2.0% similarity, with 4 matched token, and 196 token mismatch\n",
      "Model validation at 225 tokens : 1.7777777777777777% similarity, with 4 matched token, and 221 token mismatch\n",
      "Model validation at 250 tokens : 1.2% similarity, with 3 matched token, and 247 token mismatch\n",
      "Model validation at 275 tokens : 1.090909090909091% similarity, with 3 matched token, and 272 token mismatch\n",
      "Model validation at 300 tokens : 1.3333333333333335% similarity, with 4 matched token, and 296 token mismatch\n",
      "Model validation at 325 tokens : 1.2307692307692308% similarity, with 4 matched token, and 321 token mismatch\n",
      "Model validation at 350 tokens : 1.1428571428571428% similarity, with 4 matched token, and 346 token mismatch\n",
      "Model validation at 375 tokens : 1.3333333333333335% similarity, with 5 matched token, and 370 token mismatch\n",
      "Model validation at 400 tokens : 1.25% similarity, with 5 matched token, and 395 token mismatch\n",
      "Model validation at 425 tokens : 1.1764705882352942% similarity, with 5 matched token, and 420 token mismatch\n",
      "Model validation at 450 tokens : 1.1111111111111112% similarity, with 5 matched token, and 445 token mismatch\n",
      "Model validation at 475 tokens : 1.0526315789473684% similarity, with 5 matched token, and 470 token mismatch\n",
      "Model validation at 500 tokens : 1.2% similarity, with 6 matched token, and 494 token mismatch\n",
      "Model validation at 550 tokens : 1.2727272727272727% similarity, with 7 matched token, and 543 token mismatch\n",
      "Model validation at 600 tokens : 1.1666666666666667% similarity, with 7 matched token, and 593 token mismatch\n",
      "Model validation at 650 tokens : 1.3846153846153846% similarity, with 9 matched token, and 641 token mismatch\n",
      "Model validation at 700 tokens : 1.2857142857142856% similarity, with 9 matched token, and 691 token mismatch\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick memory test\n",
    "# (We dun expect this to work, as we have not finetune for memory recall, but its a baseline)\n",
    "!python3 ./memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/Echo-B-1B4-Stage2.pth\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
