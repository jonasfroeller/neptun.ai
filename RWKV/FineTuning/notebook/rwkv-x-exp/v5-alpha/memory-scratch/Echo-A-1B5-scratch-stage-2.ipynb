{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Echo-A 1B5 (Memory model from scratch - stage)\n",
    "\n",
    "Fine tune 2 process, continues off with masking everything but the output.\n",
    "This forces all the loss learning only to be on the output token, and not on the instruction set / etc. (which it should have already learnt)\n",
    "\n",
    "> This project assumes you have the rwkv-infctx conda env setup, and you are executing in that environment - see the main README.md for the conda env setup steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Download the pretrained model\n",
    "(if you want to skip the the basemodel train + instruct tune)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-13 04:22:49--  https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/Echo-A-1B5-Scratch-Stage-1.pth\n",
      "Resolving huggingface.co (huggingface.co)... 52.85.242.84, 52.85.242.16, 52.85.242.8, ...\n",
      "Connecting to huggingface.co (huggingface.co)|52.85.242.84|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/cb/ef/cbef09abb2634a3375b28868bffa285226dfeabedec89b28c2fb302221164d66/f032099c1cd32937c5fc33c6a61b7beee18a93eeddb89a17d3cff571763337e7?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Echo-A-1B5-Scratch-Stage-1.pth%3B+filename%3D%22Echo-A-1B5-Scratch-Stage-1.pth%22%3B&Expires=1689481369&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4OTQ4MTM2OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jYi9lZi9jYmVmMDlhYmIyNjM0YTMzNzViMjg4NjhiZmZhMjg1MjI2ZGZlYWJlZGVjODliMjhjMmZiMzAyMjIxMTY0ZDY2L2YwMzIwOTljMWNkMzI5MzdjNWZjMzNjNmE2MWI3YmVlZTE4YTkzZWVkZGI4OWExN2QzY2ZmNTcxNzYzMzM3ZTc%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=J6tuJse8P0rCStyK5D-lFUzIpCAzDT7dBiNziHB%7EZ9Okass2Dku7kpp%7Evfv4BvRVKsgniI9yrMEs5M5OtFxHFwGjQvYda73DzPIZ57%7E8r1d3toIDBVEUOcGyS4dTHxxGCqZT2P0rDcKKljnXecX8OdyF8o8Fe0DxGREhdj2DZztY8eyc5QyFdJt%7EP9YUtWtYZjtI9uXx75yBeS8DBnQyrR3iPH5gofoGlbE0pZR0PnN91zdkrLlxifeVehLczFSgIpnOLvXeJdAcOJzJnS3R8YOcN4rv9goGNQUCWbiMe0IOLxpSCfZkCflay5Vw3tCYLkhb91tqkG3IS-8yUf3IhA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2023-07-13 04:22:49--  https://cdn-lfs.huggingface.co/repos/cb/ef/cbef09abb2634a3375b28868bffa285226dfeabedec89b28c2fb302221164d66/f032099c1cd32937c5fc33c6a61b7beee18a93eeddb89a17d3cff571763337e7?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Echo-A-1B5-Scratch-Stage-1.pth%3B+filename%3D%22Echo-A-1B5-Scratch-Stage-1.pth%22%3B&Expires=1689481369&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4OTQ4MTM2OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jYi9lZi9jYmVmMDlhYmIyNjM0YTMzNzViMjg4NjhiZmZhMjg1MjI2ZGZlYWJlZGVjODliMjhjMmZiMzAyMjIxMTY0ZDY2L2YwMzIwOTljMWNkMzI5MzdjNWZjMzNjNmE2MWI3YmVlZTE4YTkzZWVkZGI4OWExN2QzY2ZmNTcxNzYzMzM3ZTc%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=J6tuJse8P0rCStyK5D-lFUzIpCAzDT7dBiNziHB%7EZ9Okass2Dku7kpp%7Evfv4BvRVKsgniI9yrMEs5M5OtFxHFwGjQvYda73DzPIZ57%7E8r1d3toIDBVEUOcGyS4dTHxxGCqZT2P0rDcKKljnXecX8OdyF8o8Fe0DxGREhdj2DZztY8eyc5QyFdJt%7EP9YUtWtYZjtI9uXx75yBeS8DBnQyrR3iPH5gofoGlbE0pZR0PnN91zdkrLlxifeVehLczFSgIpnOLvXeJdAcOJzJnS3R8YOcN4rv9goGNQUCWbiMe0IOLxpSCfZkCflay5Vw3tCYLkhb91tqkG3IS-8yUf3IhA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.157.214.82, 108.157.214.7, 108.157.214.46, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.157.214.82|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6060472450 (5.6G) [binary/octet-stream]\n",
      "Saving to: ‘Echo-A-1B5-Scratch-Stage-1.pth’\n",
      "\n",
      "Echo-A-1B5-Scratch- 100%[===================>]   5.64G  41.2MB/s    in 2m 23s  \n",
      "\n",
      "2023-07-13 04:25:13 (40.3 MB/s) - ‘Echo-A-1B5-Scratch-Stage-1.pth’ saved [6060472450/6060472450]\n",
      "\n",
      "-rw-r--r-- 1 root root 5.7G Jul 13 03:06 ../../../model/Echo-A-1B5-Scratch-Stage-1.pth\n"
     ]
    }
   ],
   "source": [
    "# Init required dirs\n",
    "!mkdir -p ../../../model/\n",
    "!mkdir -p ../../../datapath/\n",
    "!mkdir -p ../../../checkpoint/\n",
    "\n",
    "# Download the Stage2.pth file\n",
    "!rm -rf ../../../model/Echo-A-1B5-Scratch-Stage-1.pth\n",
    "!cd ../../../model/ && wget https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/Echo-A-1B5-Scratch-Stage-1.pth\n",
    "!ls -alh ../../../model/Echo-A-1B5-Scratch-Stage-1.pth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "Prepare and preload the finetuning process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 2 max words, 15000 samples - at ./dataset/full-masked-word-2-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 15000 samples - at ./dataset/full-masked-word-5-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 15000 samples - at ./dataset/full-masked-word-10-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 25000 samples - at ./dataset/limited-masked-word-2-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 5000 samples - at ./dataset/full-masked-word-100-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 15000 samples - at ./dataset/full-masked-word-15-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 15000 samples - at ./dataset/full-masked-word-20-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 25000 samples - at ./dataset/limited-masked-word-5-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 30000 samples - at ./dataset/limited-masked-word-10-count.jsonl\n",
      "Generated JSONL file with - 60 max words, 10000 samples - at ./dataset/full-masked-word-60-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 15000 samples - at ./dataset/full-masked-word-40-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 10000 samples - at ./dataset/full-masked-word-80-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 5000 samples - at ./dataset/full-masked-word-200-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 30000 samples - at ./dataset/limited-masked-word-15-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 30000 samples - at ./dataset/limited-masked-word-20-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 30000 samples - at ./dataset/limited-masked-word-40-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 20000 samples - at ./dataset/limited-masked-word-80-count.jsonl\n",
      "Generated JSONL file with - 60 max words, 25000 samples - at ./dataset/limited-masked-word-60-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 10000 samples - at ./dataset/limited-masked-word-200-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 20000 samples - at ./dataset/limited-masked-word-100-count.jsonl\n",
      "## Done ##\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "# Reset the dataset dir\n",
    "mkdir -p ./dataset\n",
    "rm -rf ./dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "# Prompt completion pairs, are fully masked instruction and input, with unmasked outputs\n",
    "# This is required to actually teach the model how to memorize the input, but on its own, \n",
    "# its unable to actually teach the model how to trigger this behavior (as the instruct is masked)\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-2-count.jsonl  2  25000 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-5-count.jsonl  5  25000 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-10-count.jsonl 10 30000 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-15-count.jsonl 15 30000 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-20-count.jsonl 20 30000 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-40-count.jsonl 40 30000 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-60-count.jsonl 60 25000 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-80-count.jsonl 80 20000 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-100-count.jsonl 100 20000 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-200-count.jsonl 200 10000 &\n",
    "\n",
    "# Prompt completion pairs, with the full word list. Due to the size of the full word list, it \n",
    "# was possible to be stuck training the model just to recognize new words / tokens, and not perform the memorization task\n",
    "# this greatly slowed down the memorization learning process. As the model was constantly learning new words. \n",
    "# With 400k+ words total, even after 100k worth of document samples, new words can appear (due to how RNG works)\n",
    "#\n",
    "# We still include a mix of the data, in an attempt to reduce overtraining the model to only a fixed token set.\n",
    "# which was one of the weakness faced in the original training / benchmark (but technically not an issue for measuring memory)\n",
    "python ./memory_script/gen_full_masked_jsonl.py ./dataset/full-masked-word-2-count.jsonl  2  15000 &\n",
    "python ./memory_script/gen_full_masked_jsonl.py ./dataset/full-masked-word-5-count.jsonl  5  15000 &\n",
    "python ./memory_script/gen_full_masked_jsonl.py ./dataset/full-masked-word-10-count.jsonl 10 15000 &\n",
    "python ./memory_script/gen_full_masked_jsonl.py ./dataset/full-masked-word-15-count.jsonl 15 15000 &\n",
    "python ./memory_script/gen_full_masked_jsonl.py ./dataset/full-masked-word-20-count.jsonl 20 15000 &\n",
    "python ./memory_script/gen_full_masked_jsonl.py ./dataset/full-masked-word-40-count.jsonl 40 15000 &\n",
    "python ./memory_script/gen_full_masked_jsonl.py ./dataset/full-masked-word-60-count.jsonl 60 10000 &\n",
    "python ./memory_script/gen_full_masked_jsonl.py ./dataset/full-masked-word-80-count.jsonl 80 10000 &\n",
    "python ./memory_script/gen_full_masked_jsonl.py ./dataset/full-masked-word-100-count.jsonl 100 5000 &\n",
    "python ./memory_script/gen_full_masked_jsonl.py ./dataset/full-masked-word-200-count.jsonl 200 5000 &\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure your environment settings\n",
    "(!Important: you will need to rerun the below cell, if you restart your kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_1\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /root/picocreator-memory-experiment/notebook/experiment/memory-scratch\n",
      "TRAINER_DIR: /root/picocreator-memory-experiment/RWKV-v4neo\n",
      "PROJECT_DIR: /root/picocreator-memory-experiment\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"Echo-A-1B5\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v4neo/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 : Simple Memory finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████████████| 20/20 [00:00<00:00, 12323.50it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-93d149df4877176d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 2709.50it/s]\n",
      "Extracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 115.74it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-93d149df4877176d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 89.66it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset (enwiki_100k)\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/Echo-A-1B5-scratch-stage-2.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/Echo-A-1B5-scratch-stage-2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 2509290067\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 2509290067\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230713_042546-j8ngi79y\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m(8x3090) Echo-A-1B5 - Scratch-Stage-2 (bs=256, train-ctx=512, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/j8ngi79y\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_512_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -t 4 -std=c++17 -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=512 -c /root/picocreator-memory-experiment/RWKV-v4neo/cuda/wkv_cuda_bf16.cu -o wkv_cuda_bf16.cuda.o \n",
      "ptxas info    : 1 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf\n",
      "    6144 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 56 registers, 464 bytes cmem[0], 8 bytes cmem[2]\n",
      "ptxas info    : Compiling entry function '_Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, 424 bytes cmem[0]\n",
      "[2/3] c++ -MMD -MF wkv_op_bf16.o.d -DTORCH_EXTENSION_NAME=wkv_512_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -std=c++17 -O3 -DTmax=512 -c /root/picocreator-memory-experiment/RWKV-v4neo/cuda/wkv_op_bf16.cpp -o wkv_op_bf16.o \n",
      "[3/3] c++ wkv_op_bf16.o wkv_cuda_bf16.cuda.o -shared -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_512_bf16.so\n",
      "Loading extension module wkv_512_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|██████████████████| 20/20 [00:00<00:00, 21965.46it/s]\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-93d149df4877176d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 100.79it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-93d149df4877176d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bb45573a5a75fb87_*_of_00096.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-93d149df4877176d/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e0cf8c1cb1f226d3_*_of_00096.arrow\n",
      "[rank: 0] Global seed set to 2509290067                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-07-13 04:26:22,359] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 2] Global seed set to 2509290067\n",
      "[rank: 4] Global seed set to 2509290067\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "[rank: 1] Global seed set to 2509290067\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[rank: 3] Global seed set to 2509290067\n",
      "[rank: 5] Global seed set to 2509290067\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "[rank: 6] Global seed set to 2509290067\n",
      "[rank: 7] Global seed set to 2509290067\n",
      "Loading extension module wkv_512_bf16...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "[rank: 4] Global seed set to 2509290067\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-07-13 04:26:51,534] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 2509290067\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-07-13 04:26:51,742] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 2509290067\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-07-13 04:26:53,234] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 2509290067\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-07-13 04:26:57,469] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 2509290067\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-07-13 04:26:57,478] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 2509290067\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-07-13 04:26:57,507] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 2509290067\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-07-13 04:26:57,515] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  5.000e-04 (0.0005)\n",
      "    - lr_final: 1.000e-04 (0.0001)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py311_cu118/fused_adam...\n",
      "Creating extension directory /root/.cache/torch_extensions/py311_cu118/fused_adam...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/adam -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -std=c++17 -c /usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o \n",
      "[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/adam -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o \n",
      "[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 30.85654067993164 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 30.858214378356934 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 30.860392808914185 seconds\n",
      "Time to load fused_adam op: 30.861817359924316 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 30.865920543670654 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 30.86737632751465 seconds\n",
      "Time to load fused_adam op: 30.867903232574463 seconds\n",
      "Time to load fused_adam op: 30.86834692955017 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py311_cu118/utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /usr/local/lib/python3.11/dist-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o \n",
      "[2/2] c++ flatten_unflatten.o -shared -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 12.72757863998413 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 12.728405714035034 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 12.729764461517334 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 12.730235576629639 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 12.727629899978638 seconds\n",
      "Time to load utils op: 12.732907772064209 seconds\n",
      "Time to load utils op: 12.730293273925781 seconds\n",
      "Time to load utils op: 12.734428644180298 seconds\n",
      "Rank: 6 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 1 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 0 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 3 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 2 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 4 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 7 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 5 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003616809844970703 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00033855438232421875 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002834796905517578 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00034618377685546875 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0007753372192382812 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0004093647003173828 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006742477416992188 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0009660720825195312 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0:   2%| | 800/42705 [08:10<7:08:16,  1.63it/s, v_num=i79y, train/loss=6.5/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 42705/42705 [8:42:28<00:00,  1.36it/s, v_num=i79y, train/loss=2\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/43 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/43 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▍                  | 1/43 [00:00<00:11,  3.62it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                  | 2/43 [00:00<00:13,  3.12it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▎                 | 3/43 [00:00<00:11,  3.56it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▊                 | 4/43 [00:01<00:09,  3.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██▏                | 5/43 [00:01<00:09,  4.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▋                | 6/43 [00:01<00:09,  4.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|███                | 7/43 [00:01<00:09,  3.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▌               | 8/43 [00:02<00:09,  3.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▉               | 9/43 [00:02<00:08,  3.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|████▏             | 10/43 [00:02<00:08,  3.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▌             | 11/43 [00:02<00:08,  3.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|█████             | 12/43 [00:03<00:07,  3.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████▍            | 13/43 [00:03<00:07,  3.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▊            | 14/43 [00:03<00:07,  3.97it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|██████▎           | 15/43 [00:03<00:06,  4.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▋           | 16/43 [00:03<00:06,  4.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|███████           | 17/43 [00:04<00:06,  4.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▌          | 18/43 [00:04<00:06,  4.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▉          | 19/43 [00:04<00:05,  4.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████▎         | 20/43 [00:04<00:05,  4.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▊         | 21/43 [00:05<00:05,  3.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|█████████▏        | 22/43 [00:05<00:05,  3.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|█████████▋        | 23/43 [00:05<00:05,  3.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████        | 24/43 [00:06<00:04,  3.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|██████████▍       | 25/43 [00:06<00:04,  3.98it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▉       | 26/43 [00:06<00:04,  3.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|███████████▎      | 27/43 [00:06<00:03,  4.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████▋      | 28/43 [00:06<00:03,  4.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████▏     | 29/43 [00:07<00:03,  4.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|████████████▌     | 30/43 [00:07<00:03,  4.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|████████████▉     | 31/43 [00:07<00:02,  4.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|█████████████▍    | 32/43 [00:07<00:02,  4.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|█████████████▊    | 33/43 [00:08<00:02,  4.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|██████████████▏   | 34/43 [00:08<00:02,  4.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|██████████████▋   | 35/43 [00:08<00:01,  4.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|███████████████   | 36/43 [00:08<00:01,  4.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|███████████████▍  | 37/43 [00:08<00:01,  4.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|███████████████▉  | 38/43 [00:09<00:01,  4.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|████████████████▎ | 39/43 [00:09<00:00,  4.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|████████████████▋ | 40/43 [00:09<00:00,  4.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|█████████████████▏| 41/43 [00:09<00:00,  4.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|█████████████████▌| 42/43 [00:10<00:00,  4.15it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 42705/42705 [8:42:49<00:00,  1.36it/s, v_num=i79y, train/loss=2\u001b[A\n",
      "Epoch 0: 100%|█| 42705/42705 [8:42:49<00:00,  1.36it/s, v_num=i79y, train/loss=2\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 42705/42705 [8:44:07<00:00,  1.36it/s, v_num=i79y, train/loss=2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.011 MB of 0.011 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len █▂▄▂▁▁▂▁▇▂▃▂▄▁▁▂▅▂▁▁▂▂▁▂▂▄▁▂▂▁▁▄▂▂█▁▂▇▄▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ▇█▄▅▆█▃▂█▁▇▅▁▄▅▅▆▇▆▃▂▄▄▃▆▅▃▅▆▂▁▃▃▃▅▂▅▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 423\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 336\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 4.40625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1334\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 4.16479\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m(8x3090) Echo-A-1B5 - Scratch-Stage-2 (bs=256, train-ctx=512, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/j8ngi79y\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230713_042546-j8ngi79y/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the foundation model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/Echo-A-1B5-scratch-stage-2.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Scratch-Stage-2 (bs=256, train-ctx=512, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/Echo-A-1B5-scratch-stage-2/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 438 params 1515106304 elements\n",
      "Saving fp32 state dict to ../model/Echo-A-1B5-Scratch-Stage-2.pth\n",
      "ls: cannot access '../model/Echo-A-1B5-Tune2.pth': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/Echo-A-1B5-scratch-stage-2/last.ckpt\" \\\n",
    "        \"../model/Echo-A-1B5-Scratch-Stage-2.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh ../model/Echo-A-1B5-Tune2.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py311_cu118/wkv_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_cuda/build.ninja...\n",
      "Building extension module wkv_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] c++ -MMD -MF wrapper.o.d -DTORCH_EXTENSION_NAME=wkv_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /usr/local/lib/python3.11/dist-packages/rwkv/cuda/wrapper.cpp -o wrapper.o \n",
      "[2/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -t 4 -std=c++17 --use_fast_math -O3 --extra-device-vectorization -c /usr/local/lib/python3.11/dist-packages/rwkv/cuda/operators.cu -o operators.cuda.o \n",
      "[3/3] c++ wrapper.o operators.cuda.o -shared -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_cuda.so\n",
      "Loading extension module wkv_cuda...\n",
      "RWKV_JIT_ON 1 RWKV_CUDA_ON 1 RESCALE_LAYER 0\n",
      "\n",
      "Loading /root/picocreator-memory-experiment/model/Echo-A-1B5-Scratch-Stage-2.pth ...\n",
      "Strategy: (total 24+1=25 layers)\n",
      "* cuda [float32, float32], store 25 layers\n",
      "0-cuda-float32-float32 1-cuda-float32-float32 2-cuda-float32-float32 3-cuda-float32-float32 4-cuda-float32-float32 5-cuda-float32-float32 6-cuda-float32-float32 7-cuda-float32-float32 8-cuda-float32-float32 9-cuda-float32-float32 10-cuda-float32-float32 11-cuda-float32-float32 12-cuda-float32-float32 13-cuda-float32-float32 14-cuda-float32-float32 15-cuda-float32-float32 16-cuda-float32-float32 17-cuda-float32-float32 18-cuda-float32-float32 19-cuda-float32-float32 20-cuda-float32-float32 21-cuda-float32-float32 22-cuda-float32-float32 23-cuda-float32-float32 24-cuda-float32-float32 \n",
      "blocks.0.att.key.weight           f32   cuda:0   2048  2048 \n",
      "blocks.0.att.output.weight        f32   cuda:0   2048  2048 \n",
      "blocks.0.att.receptance.weight    f32   cuda:0   2048  2048 \n",
      "blocks.0.att.time_mix_k           f32   cuda:0   2048       \n",
      "blocks.0.att.time_mix_r           f32   cuda:0   2048       \n",
      "blocks.0.att.time_mix_v           f32   cuda:0   2048       \n",
      "blocks.0.att.value.weight         f32   cuda:0   2048  2048 \n",
      "blocks.0.ffn.key.weight           f32   cuda:0   2048  8192 \n",
      "blocks.0.ffn.receptance.weight    f32   cuda:0   2048  2048 \n",
      "blocks.0.ffn.time_mix_k           f32   cuda:0   2048       \n",
      "blocks.0.ffn.time_mix_r           f32   cuda:0   2048       \n",
      "blocks.0.ffn.value.weight         f32   cuda:0   8192  2048 \n",
      "blocks.0.ln1.bias                 f32   cuda:0   2048       \n",
      "blocks.0.ln1.weight               f32   cuda:0   2048       \n",
      "blocks.0.ln2.bias                 f32   cuda:0   2048       \n",
      "blocks.0.ln2.weight               f32   cuda:0   2048       \n",
      "................................................................................................................................................................................................................................................\n",
      "blocks.23.att.key.weight          f32   cuda:0   2048  2048 \n",
      "blocks.23.att.output.weight       f32   cuda:0   2048  2048 \n",
      "blocks.23.att.receptance.weight   f32   cuda:0   2048  2048 \n",
      "blocks.23.att.time_mix_k          f32   cuda:0   2048       \n",
      "blocks.23.att.time_mix_r          f32   cuda:0   2048       \n",
      "blocks.23.att.time_mix_v          f32   cuda:0   2048       \n",
      "blocks.23.att.value.weight        f32   cuda:0   2048  2048 \n",
      "blocks.23.ffn.key.weight          f32   cuda:0   2048  8192 \n",
      "blocks.23.ffn.receptance.weight   f32   cuda:0   2048  2048 \n",
      "blocks.23.ffn.time_mix_k          f32   cuda:0   2048       \n",
      "blocks.23.ffn.time_mix_r          f32   cuda:0   2048       \n",
      "blocks.23.ffn.value.weight        f32   cuda:0   8192  2048 \n",
      "blocks.23.ln1.bias                f32   cuda:0   2048       \n",
      "blocks.23.ln1.weight              f32   cuda:0   2048       \n",
      "blocks.23.ln2.bias                f32   cuda:0   2048       \n",
      "blocks.23.ln2.weight              f32   cuda:0   2048       \n",
      "................................................................................................................\n",
      "emb.weight                        f32      cpu  50277  2048 \n",
      "head.weight                       f32   cuda:0   2048 50277 \n",
      "ln_out.bias                       f32   cuda:0   2048       \n",
      "ln_out.weight                     f32   cuda:0   2048       \n",
      "blocks.0.att.time_decay           f32   cuda:0   2048       \n",
      "...............\n",
      "blocks.23.att.time_decay          f32   cuda:0   2048       \n",
      ".......\n",
      "blocks.0.att.time_first           f32   cuda:0   2048       \n",
      "...............\n",
      "blocks.23.att.time_first          f32   cuda:0   2048       \n",
      ".......###\n",
      "### Model validation start ###\n",
      "###\n",
      "Model validation at 5 tokens : 60.0% similarity, with 3 matched token, and 2 token mismatch\n",
      "Model validation at 10 tokens : 30.0% similarity, with 3 matched token, and 7 token mismatch\n",
      "Model validation at 15 tokens : 20.0% similarity, with 3 matched token, and 12 token mismatch\n",
      "Model validation at 20 tokens : 10.0% similarity, with 2 matched token, and 18 token mismatch\n",
      "Model validation at 25 tokens : 8.0% similarity, with 2 matched token, and 23 token mismatch\n",
      "Model validation at 30 tokens : 10.0% similarity, with 3 matched token, and 27 token mismatch\n",
      "Model validation at 35 tokens : 8.571428571428571% similarity, with 3 matched token, and 32 token mismatch\n",
      "Model validation at 40 tokens : 7.5% similarity, with 3 matched token, and 37 token mismatch\n",
      "Model validation at 45 tokens : 6.666666666666667% similarity, with 3 matched token, and 42 token mismatch\n",
      "Model validation at 50 tokens : 8.0% similarity, with 4 matched token, and 46 token mismatch\n",
      "Model validation at 55 tokens : 7.2727272727272725% similarity, with 4 matched token, and 51 token mismatch\n",
      "Model validation at 60 tokens : 6.666666666666667% similarity, with 4 matched token, and 56 token mismatch\n",
      "Model validation at 65 tokens : 3.076923076923077% similarity, with 2 matched token, and 63 token mismatch\n",
      "Model validation at 70 tokens : 2.857142857142857% similarity, with 2 matched token, and 68 token mismatch\n",
      "Model validation at 75 tokens : 4.0% similarity, with 3 matched token, and 72 token mismatch\n",
      "Model validation at 80 tokens : 2.5% similarity, with 2 matched token, and 78 token mismatch\n",
      "Model validation at 85 tokens : 3.5294117647058822% similarity, with 3 matched token, and 82 token mismatch\n",
      "Model validation at 90 tokens : 2.2222222222222223% similarity, with 2 matched token, and 88 token mismatch\n",
      "Model validation at 95 tokens : 2.1052631578947367% similarity, with 2 matched token, and 93 token mismatch\n",
      "Model validation at 100 tokens : 3.0% similarity, with 3 matched token, and 97 token mismatch\n",
      "Model validation at 110 tokens : 1.8181818181818181% similarity, with 2 matched token, and 108 token mismatch\n",
      "Model validation at 120 tokens : 1.6666666666666667% similarity, with 2 matched token, and 118 token mismatch\n",
      "Model validation at 130 tokens : 1.5384615384615385% similarity, with 2 matched token, and 128 token mismatch\n",
      "Model validation at 140 tokens : 0.7142857142857143% similarity, with 1 matched token, and 139 token mismatch\n",
      "Model validation at 150 tokens : 2.0% similarity, with 3 matched token, and 147 token mismatch\n",
      "Model validation at 175 tokens : 2.2857142857142856% similarity, with 4 matched token, and 171 token mismatch\n",
      "Model validation at 200 tokens : 2.0% similarity, with 4 matched token, and 196 token mismatch\n",
      "Model validation at 225 tokens : 1.3333333333333335% similarity, with 3 matched token, and 222 token mismatch\n",
      "Model validation at 250 tokens : 1.6% similarity, with 4 matched token, and 246 token mismatch\n",
      "Model validation at 275 tokens : 0.7272727272727273% similarity, with 2 matched token, and 273 token mismatch\n",
      "Model validation at 300 tokens : 0.6666666666666667% similarity, with 2 matched token, and 298 token mismatch\n",
      "Model validation at 325 tokens : 0.9230769230769231% similarity, with 3 matched token, and 322 token mismatch\n",
      "Model validation at 350 tokens : 0.5714285714285714% similarity, with 2 matched token, and 348 token mismatch\n",
      "Model validation at 375 tokens : 1.0666666666666667% similarity, with 4 matched token, and 371 token mismatch\n",
      "Model validation at 400 tokens : 1.25% similarity, with 5 matched token, and 395 token mismatch\n",
      "Model validation at 425 tokens : 1.1764705882352942% similarity, with 5 matched token, and 420 token mismatch\n",
      "Model validation at 450 tokens : 1.3333333333333335% similarity, with 6 matched token, and 444 token mismatch\n",
      "Model validation at 475 tokens : 0.8421052631578947% similarity, with 4 matched token, and 471 token mismatch\n",
      "Model validation at 500 tokens : 1.2% similarity, with 6 matched token, and 494 token mismatch\n",
      "Model validation at 550 tokens : 1.090909090909091% similarity, with 6 matched token, and 544 token mismatch\n",
      "Model validation at 600 tokens : 1.0% similarity, with 6 matched token, and 594 token mismatch\n",
      "Model validation at 650 tokens : 1.0769230769230769% similarity, with 7 matched token, and 643 token mismatch\n",
      "Model validation at 700 tokens : 1.1428571428571428% similarity, with 8 matched token, and 692 token mismatch\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick dragon prompt validation\n",
    "!python3 ./memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/Echo-A-1B5-Scratch-Stage-2.pth\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
