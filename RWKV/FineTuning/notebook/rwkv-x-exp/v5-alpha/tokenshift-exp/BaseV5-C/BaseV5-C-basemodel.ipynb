{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWKV v5 Baseline C\n",
    "This model is based on the RWKV standard 1B5 model\n",
    "\n",
    "- 24 layers\n",
    "- 2048 embedding size\n",
    "\n",
    "Going through the same memory training process as TokenShift, but using the v5 code\n",
    "\n",
    "See `./notes.md` for how the init model was initilaized.\n",
    "\n",
    "**Note:** This project assumes you have the rwkv-infctx conda env setup\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "# ninja-build is required for the new trainer\n",
    "sudo apt-get install ninja-build\n",
    "\n",
    "# Update conda & its package listings\n",
    "# Conda install script can be found here : \n",
    "# https://docs.anaconda.com/free/anaconda/install/linux/#installation\n",
    "conda update conda\n",
    "\n",
    "# Virtual env, with python 3.10\n",
    "# python 3.11 have issues with torch.compile / h100s\n",
    "# and if you want to use 3.11, you will need to do a nightly build install\n",
    "conda create -n rwkv-infctx python=3.11 pip\n",
    "conda activate rwkv-infctx\n",
    "\n",
    "# Install pytorch (>=2.0.1)\n",
    "conda install -y pytorch==2.0.1 torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "\n",
    "# Verify your pytorch version \n",
    "python -c \"import torch; print(torch.__version__)\"\n",
    "\n",
    "# We use python -m pip, instead of pip directly, as it resolve issues with venv not loading the right pip\n",
    "python -m pip install datasets transformers \n",
    "python -m pip install lightning==2.0.4 deepspeed==0.9.5\n",
    "python -m pip install ninja numexpr jsonargparse 'jsonargparse[signatures]'\n",
    "python -m pip install lm-dataformat ftfy sentencepiece tokenizers wandb\n",
    "```\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "---- Initializing model ----\n",
      "No of layers: 24\n",
      "Embedding size: 2048\n",
      "Output model path: ../model/L24-D2048-neox-v5-init.pth\n",
      "Vocab size: 50277\n",
      "---- ----- ----\n",
      "50277 2048  -0.1 emb.weight\n",
      "2048  2048  1.0  blocks.0.att.receptance.weight\n",
      "2048  2048  1.0  blocks.0.att.key.weight\n",
      "2048  2048  1.0  blocks.0.att.value.weight\n",
      "2048  2048  0    blocks.0.att.output.weight\n",
      "8192  2048  1.0  blocks.0.ffn.key.weight\n",
      "2048  2048  0    blocks.0.ffn.receptance.weight\n",
      "2048  8192  0    blocks.0.ffn.value.weight\n",
      "2048  2048  1.0  blocks.1.att.receptance.weight\n",
      "2048  2048  1.0  blocks.1.att.key.weight\n",
      "2048  2048  1.0  blocks.1.att.value.weight\n",
      "2048  2048  0    blocks.1.att.output.weight\n",
      "8192  2048  1.0  blocks.1.ffn.key.weight\n",
      "2048  2048  0    blocks.1.ffn.receptance.weight\n",
      "2048  8192  0    blocks.1.ffn.value.weight\n",
      "2048  2048  1.0  blocks.2.att.receptance.weight\n",
      "2048  2048  1.0  blocks.2.att.key.weight\n",
      "2048  2048  1.0  blocks.2.att.value.weight\n",
      "2048  2048  0    blocks.2.att.output.weight\n",
      "8192  2048  1.0  blocks.2.ffn.key.weight\n",
      "2048  2048  0    blocks.2.ffn.receptance.weight\n",
      "2048  8192  0    blocks.2.ffn.value.weight\n",
      "2048  2048  1.0  blocks.3.att.receptance.weight\n",
      "2048  2048  1.0  blocks.3.att.key.weight\n",
      "2048  2048  1.0  blocks.3.att.value.weight\n",
      "2048  2048  0    blocks.3.att.output.weight\n",
      "8192  2048  1.0  blocks.3.ffn.key.weight\n",
      "2048  2048  0    blocks.3.ffn.receptance.weight\n",
      "2048  8192  0    blocks.3.ffn.value.weight\n",
      "2048  2048  1.0  blocks.4.att.receptance.weight\n",
      "2048  2048  1.0  blocks.4.att.key.weight\n",
      "2048  2048  1.0  blocks.4.att.value.weight\n",
      "2048  2048  0    blocks.4.att.output.weight\n",
      "8192  2048  1.0  blocks.4.ffn.key.weight\n",
      "2048  2048  0    blocks.4.ffn.receptance.weight\n",
      "2048  8192  0    blocks.4.ffn.value.weight\n",
      "2048  2048  1.0  blocks.5.att.receptance.weight\n",
      "2048  2048  1.0  blocks.5.att.key.weight\n",
      "2048  2048  1.0  blocks.5.att.value.weight\n",
      "2048  2048  0    blocks.5.att.output.weight\n",
      "8192  2048  1.0  blocks.5.ffn.key.weight\n",
      "2048  2048  0    blocks.5.ffn.receptance.weight\n",
      "2048  8192  0    blocks.5.ffn.value.weight\n",
      "2048  2048  1.0  blocks.6.att.receptance.weight\n",
      "2048  2048  1.0  blocks.6.att.key.weight\n",
      "2048  2048  1.0  blocks.6.att.value.weight\n",
      "2048  2048  0    blocks.6.att.output.weight\n",
      "8192  2048  1.0  blocks.6.ffn.key.weight\n",
      "2048  2048  0    blocks.6.ffn.receptance.weight\n",
      "2048  8192  0    blocks.6.ffn.value.weight\n",
      "2048  2048  1.0  blocks.7.att.receptance.weight\n",
      "2048  2048  1.0  blocks.7.att.key.weight\n",
      "2048  2048  1.0  blocks.7.att.value.weight\n",
      "2048  2048  0    blocks.7.att.output.weight\n",
      "8192  2048  1.0  blocks.7.ffn.key.weight\n",
      "2048  2048  0    blocks.7.ffn.receptance.weight\n",
      "2048  8192  0    blocks.7.ffn.value.weight\n",
      "2048  2048  1.0  blocks.8.att.receptance.weight\n",
      "2048  2048  1.0  blocks.8.att.key.weight\n",
      "2048  2048  1.0  blocks.8.att.value.weight\n",
      "2048  2048  0    blocks.8.att.output.weight\n",
      "8192  2048  1.0  blocks.8.ffn.key.weight\n",
      "2048  2048  0    blocks.8.ffn.receptance.weight\n",
      "2048  8192  0    blocks.8.ffn.value.weight\n",
      "2048  2048  1.0  blocks.9.att.receptance.weight\n",
      "2048  2048  1.0  blocks.9.att.key.weight\n",
      "2048  2048  1.0  blocks.9.att.value.weight\n",
      "2048  2048  0    blocks.9.att.output.weight\n",
      "8192  2048  1.0  blocks.9.ffn.key.weight\n",
      "2048  2048  0    blocks.9.ffn.receptance.weight\n",
      "2048  8192  0    blocks.9.ffn.value.weight\n",
      "2048  2048  1.0  blocks.10.att.receptance.weight\n",
      "2048  2048  1.0  blocks.10.att.key.weight\n",
      "2048  2048  1.0  blocks.10.att.value.weight\n",
      "2048  2048  0    blocks.10.att.output.weight\n",
      "8192  2048  1.0  blocks.10.ffn.key.weight\n",
      "2048  2048  0    blocks.10.ffn.receptance.weight\n",
      "2048  8192  0    blocks.10.ffn.value.weight\n",
      "2048  2048  1.0  blocks.11.att.receptance.weight\n",
      "2048  2048  1.0  blocks.11.att.key.weight\n",
      "2048  2048  1.0  blocks.11.att.value.weight\n",
      "2048  2048  0    blocks.11.att.output.weight\n",
      "8192  2048  1.0  blocks.11.ffn.key.weight\n",
      "2048  2048  0    blocks.11.ffn.receptance.weight\n",
      "2048  8192  0    blocks.11.ffn.value.weight\n",
      "2048  2048  1.0  blocks.12.att.receptance.weight\n",
      "2048  2048  1.0  blocks.12.att.key.weight\n",
      "2048  2048  1.0  blocks.12.att.value.weight\n",
      "2048  2048  0    blocks.12.att.output.weight\n",
      "8192  2048  1.0  blocks.12.ffn.key.weight\n",
      "2048  2048  0    blocks.12.ffn.receptance.weight\n",
      "2048  8192  0    blocks.12.ffn.value.weight\n",
      "2048  2048  1.0  blocks.13.att.receptance.weight\n",
      "2048  2048  1.0  blocks.13.att.key.weight\n",
      "2048  2048  1.0  blocks.13.att.value.weight\n",
      "2048  2048  0    blocks.13.att.output.weight\n",
      "8192  2048  1.0  blocks.13.ffn.key.weight\n",
      "2048  2048  0    blocks.13.ffn.receptance.weight\n",
      "2048  8192  0    blocks.13.ffn.value.weight\n",
      "2048  2048  1.0  blocks.14.att.receptance.weight\n",
      "2048  2048  1.0  blocks.14.att.key.weight\n",
      "2048  2048  1.0  blocks.14.att.value.weight\n",
      "2048  2048  0    blocks.14.att.output.weight\n",
      "8192  2048  1.0  blocks.14.ffn.key.weight\n",
      "2048  2048  0    blocks.14.ffn.receptance.weight\n",
      "2048  8192  0    blocks.14.ffn.value.weight\n",
      "2048  2048  1.0  blocks.15.att.receptance.weight\n",
      "2048  2048  1.0  blocks.15.att.key.weight\n",
      "2048  2048  1.0  blocks.15.att.value.weight\n",
      "2048  2048  0    blocks.15.att.output.weight\n",
      "8192  2048  1.0  blocks.15.ffn.key.weight\n",
      "2048  2048  0    blocks.15.ffn.receptance.weight\n",
      "2048  8192  0    blocks.15.ffn.value.weight\n",
      "2048  2048  1.0  blocks.16.att.receptance.weight\n",
      "2048  2048  1.0  blocks.16.att.key.weight\n",
      "2048  2048  1.0  blocks.16.att.value.weight\n",
      "2048  2048  0    blocks.16.att.output.weight\n",
      "8192  2048  1.0  blocks.16.ffn.key.weight\n",
      "2048  2048  0    blocks.16.ffn.receptance.weight\n",
      "2048  8192  0    blocks.16.ffn.value.weight\n",
      "2048  2048  1.0  blocks.17.att.receptance.weight\n",
      "2048  2048  1.0  blocks.17.att.key.weight\n",
      "2048  2048  1.0  blocks.17.att.value.weight\n",
      "2048  2048  0    blocks.17.att.output.weight\n",
      "8192  2048  1.0  blocks.17.ffn.key.weight\n",
      "2048  2048  0    blocks.17.ffn.receptance.weight\n",
      "2048  8192  0    blocks.17.ffn.value.weight\n",
      "2048  2048  1.0  blocks.18.att.receptance.weight\n",
      "2048  2048  1.0  blocks.18.att.key.weight\n",
      "2048  2048  1.0  blocks.18.att.value.weight\n",
      "2048  2048  0    blocks.18.att.output.weight\n",
      "8192  2048  1.0  blocks.18.ffn.key.weight\n",
      "2048  2048  0    blocks.18.ffn.receptance.weight\n",
      "2048  8192  0    blocks.18.ffn.value.weight\n",
      "2048  2048  1.0  blocks.19.att.receptance.weight\n",
      "2048  2048  1.0  blocks.19.att.key.weight\n",
      "2048  2048  1.0  blocks.19.att.value.weight\n",
      "2048  2048  0    blocks.19.att.output.weight\n",
      "8192  2048  1.0  blocks.19.ffn.key.weight\n",
      "2048  2048  0    blocks.19.ffn.receptance.weight\n",
      "2048  8192  0    blocks.19.ffn.value.weight\n",
      "2048  2048  1.0  blocks.20.att.receptance.weight\n",
      "2048  2048  1.0  blocks.20.att.key.weight\n",
      "2048  2048  1.0  blocks.20.att.value.weight\n",
      "2048  2048  0    blocks.20.att.output.weight\n",
      "8192  2048  1.0  blocks.20.ffn.key.weight\n",
      "2048  2048  0    blocks.20.ffn.receptance.weight\n",
      "2048  8192  0    blocks.20.ffn.value.weight\n",
      "2048  2048  1.0  blocks.21.att.receptance.weight\n",
      "2048  2048  1.0  blocks.21.att.key.weight\n",
      "2048  2048  1.0  blocks.21.att.value.weight\n",
      "2048  2048  0    blocks.21.att.output.weight\n",
      "8192  2048  1.0  blocks.21.ffn.key.weight\n",
      "2048  2048  0    blocks.21.ffn.receptance.weight\n",
      "2048  8192  0    blocks.21.ffn.value.weight\n",
      "2048  2048  1.0  blocks.22.att.receptance.weight\n",
      "2048  2048  1.0  blocks.22.att.key.weight\n",
      "2048  2048  1.0  blocks.22.att.value.weight\n",
      "2048  2048  0    blocks.22.att.output.weight\n",
      "8192  2048  1.0  blocks.22.ffn.key.weight\n",
      "2048  2048  0    blocks.22.ffn.receptance.weight\n",
      "2048  8192  0    blocks.22.ffn.value.weight\n",
      "2048  2048  1.0  blocks.23.att.receptance.weight\n",
      "2048  2048  1.0  blocks.23.att.key.weight\n",
      "2048  2048  1.0  blocks.23.att.value.weight\n",
      "2048  2048  0    blocks.23.att.output.weight\n",
      "8192  2048  1.0  blocks.23.ffn.key.weight\n",
      "2048  2048  0    blocks.23.ffn.receptance.weight\n",
      "2048  8192  0    blocks.23.ffn.value.weight\n",
      "50277 2048  0.5  head.weight\n"
     ]
    }
   ],
   "source": [
    "# First lets setup the various directories, and init the model\n",
    "!mkdir -p ../../../../model/\n",
    "!mkdir -p ../../../../datapath/\n",
    "!mkdir -p ../../../../checkpoint/\n",
    "\n",
    "# Init the model\n",
    "!cd ../../../../RWKV-v5 && python3 ./init_model.py --n_layer 24 --n_embd 2048 --vocab_size neox --skip-if-exists ../model/L24-D2048-neox-v5-init.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_1\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /root/rwkv-x-playground/notebook/experiment/tokenshift-exp/BaseV5-C\n",
      "INFERENCE_DIR: /root/rwkv-x-playground/RWKV-v5\n",
      "TRAINER_DIR: /root/rwkv-x-playground/RWKV-v5\n",
      "PROJECT_DIR: /root/rwkv-x-playground\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"BaseV5-R2-C\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "INFERENCE_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"INFERENCE_DIR:\", INFERENCE_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 : Foundation model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  3.49it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-f7b060a194034cc1_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-5130e0b5cfea4510_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-8cf337c8a178675e_*_of_00064.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-54959cfaa7f1c967.arrow and /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-b4bfa876eaace3ed.arrow\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset (enwiki_100k)\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/BaseV5-C-enwiki.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/tokenshift-exp/BaseV5-C/BaseV5-C-enwiki.yaml', '--trainer.logger.init_args.name=BaseV5-R2-C - Enwiki Foundation (ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/tokenshift-exp/BaseV5-C/BaseV5-C-enwiki.yaml', '--trainer.logger.init_args.name=BaseV5-R2-C - Enwiki Foundation (ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 5325527\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 5325527\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230806_160138-st8u0p31\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mBaseV5-R2-C - Enwiki Foundation (ctx=4096, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/st8u0p31\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 4\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 81.73it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-f7b060a194034cc1_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-5130e0b5cfea4510_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-8cf337c8a178675e_*_of_00064.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-54959cfaa7f1c967.arrow and /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-b4bfa876eaace3ed.arrow\n",
      "Saving the dataset (0/5 shards):   7%| | 6000/81487 [00:00<00:02, 30465.41 exampSetting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Saving the dataset (0/5 shards):  15%|▏| 12000/81487 [00:00<00:01, 36900.24 examSetting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Saving the dataset (1/5 shards):  30%|▎| 24298/81487 [00:00<00:01, 36384.32 exam[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Saving the dataset (3/5 shards):  67%|▋| 54893/81487 [00:01<00:00, 37232.40 exam[rank: 3] Global seed set to 5325527\n",
      "[rank: 6] Global seed set to 5325527\n",
      "[rank: 5] Global seed set to 5325527\n",
      "[rank: 1] Global seed set to 5325527\n",
      "[rank: 2] Global seed set to 5325527\n",
      "[rank: 7] Global seed set to 5325527\n",
      "[rank: 4] Global seed set to 5325527\n",
      "[rank: 0] Global seed set to 5325527                                            \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-06 16:01:54,897] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 5325527\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-06 16:02:09,326] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 5325527\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-06 16:02:11,121] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 5325527\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-06 16:02:11,202] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 5325527\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-06 16:02:11,230] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 5325527\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-06 16:02:11,254] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 5325527\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-06 16:02:11,269] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 5325527\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-06 16:02:11,270] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06771373748779297 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10082864761352539 seconds\n",
      "Time to load fused_adam op: 0.10087895393371582 seconds\n",
      "Time to load fused_adam op: 0.10127401351928711 seconds\n",
      "Time to load fused_adam op: 0.1013035774230957 seconds\n",
      "Time to load fused_adam op: 0.10131001472473145 seconds\n",
      "Time to load fused_adam op: 0.10163187980651855 seconds\n",
      "Time to load fused_adam op: 0.1014866828918457 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.07340383529663086 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1020345687866211 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10111570358276367 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10203027725219727 seconds\n",
      "Time to load utils op: 0.10179996490478516 seconds\n",
      "Time to load utils op: 0.10175609588623047 seconds\n",
      "Time to load utils op: 0.1019744873046875 seconds\n",
      "Time to load utils op: 0.10203051567077637 seconds\n",
      "Rank: 0 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 5 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 2 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 6 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 7 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 3 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 4 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 1 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Time to load utils op: 0.0002770423889160156 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002722740173339844 seconds\n",
      "Time to load utils op: 0.00025153160095214844 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00023102760314941406 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00026607513427734375 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00021767616271972656 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003018379211425781 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0004913806915283203 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0:  10%| | 1000/10186 [41:45<6:23:36,  2.51s/it, v_num=0p31, train/loss=5./usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 10186/10186 [7:06:46<00:00,  2.51s/it, v_num=0p31, train/loss=3\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                  | 1/52 [00:00<00:26,  1.89it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                  | 2/52 [00:01<00:25,  1.96it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                  | 3/52 [00:01<00:24,  1.99it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍                 | 4/52 [00:01<00:23,  2.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▊                 | 5/52 [00:02<00:23,  2.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██▏                | 6/52 [00:02<00:22,  2.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▌                | 7/52 [00:03<00:22,  2.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▉                | 8/52 [00:03<00:21,  2.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▎               | 9/52 [00:04<00:21,  2.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▍              | 10/52 [00:04<00:20,  2.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▊              | 11/52 [00:05<00:20,  2.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|████▏             | 12/52 [00:05<00:19,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▌             | 13/52 [00:06<00:19,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▊             | 14/52 [00:06<00:18,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|█████▏            | 15/52 [00:07<00:18,  2.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▌            | 16/52 [00:07<00:17,  2.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▉            | 17/52 [00:08<00:17,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|██████▏           | 18/52 [00:08<00:16,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▌           | 19/52 [00:09<00:16,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▉           | 20/52 [00:09<00:15,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|███████▎          | 21/52 [00:10<00:15,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▌          | 22/52 [00:10<00:14,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▉          | 23/52 [00:11<00:14,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████████▎         | 24/52 [00:11<00:13,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▋         | 25/52 [00:12<00:13,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 26/52 [00:12<00:12,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████████▎        | 27/52 [00:13<00:12,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▋        | 28/52 [00:13<00:11,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████        | 29/52 [00:14<00:11,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|██████████▍       | 30/52 [00:14<00:10,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▋       | 31/52 [00:15<00:10,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|███████████       | 32/52 [00:15<00:09,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|███████████▍      | 33/52 [00:16<00:09,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████▊      | 34/52 [00:16<00:08,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 35/52 [00:17<00:08,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|████████████▍     | 36/52 [00:17<00:07,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|████████████▊     | 37/52 [00:18<00:07,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|█████████████▏    | 38/52 [00:18<00:06,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|█████████████▌    | 39/52 [00:19<00:06,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|█████████████▊    | 40/52 [00:19<00:05,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|██████████████▏   | 41/52 [00:20<00:05,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|██████████████▌   | 42/52 [00:20<00:04,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|██████████████▉   | 43/52 [00:21<00:04,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|███████████████▏  | 44/52 [00:21<00:03,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|███████████████▌  | 45/52 [00:22<00:03,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|███████████████▉  | 46/52 [00:22<00:02,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|████████████████▎ | 47/52 [00:23<00:02,  2.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|████████████████▌ | 48/52 [00:23<00:01,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|████████████████▉ | 49/52 [00:24<00:01,  2.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████████████▎| 50/52 [00:24<00:00,  2.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|█████████████████▋| 51/52 [00:25<00:00,  2.04it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 10186/10186 [7:07:18<00:00,  2.52s/it, v_num=0p31, train/loss=3\u001b[A\n",
      "Epoch 0: 100%|█| 10186/10186 [7:07:18<00:00,  2.52s/it, v_num=0p31, train/loss=3\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 10186/10186 [7:07:29<00:00,  2.52s/it, v_num=0p31, train/loss=3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▇▅▅▄▄▄▅▃▃▂▂▃▃▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▁▂▂▁▃▂▂▁▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 51\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 4095\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 408\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 3.78125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2546\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 3.4067\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mBaseV5-R2-C - Enwiki Foundation (ctx=4096, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/st8u0p31\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230806_160138-st8u0p31/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the foundation model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/BaseV5-C-enwiki.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Enwiki Foundation (ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/BaseV5-C-enwiki/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 486 params 1515107840 elements\n",
      "Saving fp32 state dict to ../model/BaseV5-C-Stage1.pth\n",
      "-rw-r--r-- 1 root root 5.7G Aug  6 23:10 ../model/BaseV5-C-Stage1.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/BaseV5-C-enwiki/last.ckpt\" \"../model/BaseV5-C-Stage1.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/BaseV5-C-Stage1.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "--- DRAGON PROMPT ---\n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese. Some might have no reservations, and the dragons were not so large and often less of a mixture of magic and swordsmanship, so they were never quite as easily seen. They have no folk-type ancestry, but are in many respects a much more substantial group of dragon-types.\n",
      "\n",
      "Some lions, who possessed at least some form of magic, are more common in Tibetan Buddhism, but have been seen as more closely related to Chinese oracle-gatherers. Chinese demons are also often viewed as being rather kind, intelligent and fantastic, and some demons can also be made \"bad\" and \"elegant\" (as well as Buddhist and Javanese tales). Some people, for example, might become amorous and violent.\n",
      "\n",
      "One reason for this belief was the presence of monsters that devolved into some kind of evil spell. The dragons are described as a dragon-gatherer, a \"dynastic\" dragon-gathere Access. The\n"
     ]
    }
   ],
   "source": [
    "# # Lets do a quick dragon prompt validation\n",
    "!cd \"{INFERENCE_DIR}\" && python3 dragon_test.py ../model/BaseV5-C-Stage1.pth \"cuda fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets do a quick memory test\n",
    "# # (We dun expect this to work, as we have not finetune for memory recall, but its a baseline)\n",
    "# !python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/BaseV5-C-Stage1.pth\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2 : Instruct Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 644.48it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-e4df40d582f09838_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-6d5405ad1f265e84_*_of_00064.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-9f578061a1feb072.arrow and /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-f0249c95fb8af247.arrow\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/BaseV5-C-instruct.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/tokenshift-exp/BaseV5-C/BaseV5-C-instruct.yaml', '--trainer.logger.init_args.name=BaseV5-R2-C - Instruct (train-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/tokenshift-exp/BaseV5-C/BaseV5-C-instruct.yaml', '--trainer.logger.init_args.name=BaseV5-R2-C - Instruct (train-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 671780038\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 671780038\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230806_231140-0j0h4hy0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mBaseV5-R2-C - Instruct (train-ctx=4096, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/0j0h4hy0\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 4\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 866.77it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-e4df40d582f09838_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-6d5405ad1f265e84_*_of_00064.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-9f578061a1feb072.arrow and /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-f0249c95fb8af247.arrow\n",
      "[rank: 0] Global seed set to 671780038                                          \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-08-06 23:11:55,041] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 1] Global seed set to 671780038\n",
      "[rank: 5] Global seed set to 671780038\n",
      "[rank: 6] Global seed set to 671780038\n",
      "[rank: 3] Global seed set to 671780038\n",
      "[rank: 7] Global seed set to 671780038\n",
      "[rank: 4] Global seed set to 671780038\n",
      "[rank: 2] Global seed set to 671780038\n",
      "[rank: 1] Global seed set to 671780038\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-08-06 23:12:12,533] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 671780038\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-08-06 23:12:15,569] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 671780038\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-08-06 23:12:15,669] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 671780038\n",
      "[rank: 3] Global seed set to 671780038\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-08-06 23:12:15,687] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-08-06 23:12:15,687] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 671780038\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-08-06 23:12:15,702] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 671780038\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-08-06 23:12:15,707] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  4.000e-04 (0.0004)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.07234764099121094 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10091304779052734 seconds\n",
      "Time to load fused_adam op: 0.10091066360473633 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10125517845153809 seconds\n",
      "Time to load fused_adam op: 0.10153508186340332 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Time to load fused_adam op: 0.10178351402282715 seconds\n",
      "Time to load fused_adam op: 0.10178232192993164 seconds\n",
      "Time to load fused_adam op: 0.1015477180480957 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0676577091217041 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10162758827209473 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10105109214782715 seconds\n",
      "Time to load utils op: 0.10191202163696289 seconds\n",
      "Time to load utils op: 0.10194277763366699 seconds\n",
      "Time to load utils op: 0.10144758224487305 seconds\n",
      "Time to load utils op: 0.10164403915405273 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10132384300231934 seconds\n",
      "Rank: 7 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 1 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 6 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 0 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 5 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 2 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 4 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Rank: 3 partition count [8, 8] and sizes[(189388288, False), (192, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002639293670654297 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.0002276897430419922 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00025725364685058594 seconds\n",
      "Time to load utils op: 0.0003108978271484375 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002307891845703125 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003020763397216797 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00031185150146484375 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005135536193847656 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0:  54%|▌| 1000/1867 [18:21<15:55,  1.10s/it, v_num=4hy0, train/loss=4.190/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 1867/1867 [33:46<00:00,  1.09s/it, v_num=4hy0, train/loss=2.480\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▉                 | 1/10 [00:00<00:02,  3.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▊               | 2/10 [00:00<00:01,  4.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████▋             | 3/10 [00:00<00:01,  4.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|███████▌           | 4/10 [00:00<00:01,  4.55it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████▌         | 5/10 [00:01<00:01,  4.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|███████████▍       | 6/10 [00:01<00:00,  4.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|█████████████▎     | 7/10 [00:01<00:00,  4.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|███████████████▏   | 8/10 [00:01<00:00,  4.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|█████████████████  | 9/10 [00:01<00:00,  4.84it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 1867/1867 [33:55<00:00,  1.09s/it, v_num=4hy0, train/loss=2.480\u001b[A\n",
      "Epoch 0: 100%|█| 1867/1867 [33:55<00:00,  1.09s/it, v_num=4hy0, train/loss=2.480\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 1867/1867 [34:06<00:00,  1.10s/it, v_num=4hy0, train/loss=2.480\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▁▁▁▁▂▁▁▁▂▂▁█▁▁▂▁▃▂▁▂▁▂▁▁▁▂▁▁▁▂▁▂▁▁▁▁▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ▄▂▆▇▆▆▆▆▅▆▃▄▄▆▂▄▃▅▄▅▅▅▃█▅▅▇▆▂▇▄▆▂▄▄▅▂▁▂▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 85\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 3.75\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 466\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 3.00146\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mBaseV5-R2-C - Instruct (train-ctx=4096, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/0j0h4hy0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230806_231140-0j0h4hy0/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the instruct finetuning\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/BaseV5-C-instruct.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Instruct (train-ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/BaseV5-C-instruct/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 486 params 1515107840 elements\n",
      "Saving fp32 state dict to ../model/BaseV5-C-Stage2.pth\n",
      "-rw-r--r-- 1 root root 5.7G Aug  6 23:47 ../model/BaseV5-C-Stage2.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/BaseV5-C-instruct/last.ckpt\" \"../model/BaseV5-C-Stage2.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/BaseV5-C-Stage2.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "--- DRAGON PROMPT ---\n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese. But whatccc did the search is widely believed that the top of the Chinese and what was the force.# Answer:\\nThe Japanese created the equations that all the vehicles came from their orbit. It was a significant problem in many religions that it did not have much confidence.\n",
      "\n",
      "In the western worlds, the answer is that the Chinese, using the terms of the board and the length of the board, is known to have been responsible for the problem. However, the American experts claims the right of the list is false. They have agreed that the answer to this question is because of the long-haired tiger. This answer is quite short. This is, however, that the average temperature ranges from 10-12 degrees F. The US would have found that the mean weight of the rainbow is that the average height of the rainbow is 14-14 degrees F. The average average temperature is 7-9 degrees F. The average temperature of the rainbow is 0.5 degrees F. The average\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick dragon prompt validation\n",
    "!cd \"{INFERENCE_DIR}\" && python3 dragon_test.py \"../model/BaseV5-C-Stage2.pth\" \"cuda fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets do a quick memory test\n",
    "# # (We dun expect this to work, as we have not finetune for memory recall, but its a baseline)\n",
    "# !python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/BaseV5-C-Stage2.pth\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
