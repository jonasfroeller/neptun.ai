{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the following packages are installed\n",
    "# !pip install transformers datasets tqdm torch transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-29 10:00:47,497] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/picocreator/.cache/huggingface/datasets/codeparrot___json/codeparrot--codeparrot-clean-valid-826c6fd8b27e5523/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|██████████| 1/1 [00:00<00:00, 410.68it/s]\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/codeparrot___json/codeparrot--codeparrot-clean-valid-826c6fd8b27e5523/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc62de51fe8a734f.arrow\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== All objects 61373 ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12274' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 4:16:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 - batch loss: 2.597266912460327 - avg loss: 2.597266912460327   (start: 0, end: 10)\n",
      "Batch 1 - batch loss: 2.320502758026123 - avg loss: 2.458884835243225   (start: 10, end: 20)\n",
      "Batch 2 - batch loss: 2.331798553466797 - avg loss: 2.416522741317749   (start: 20, end: 30)\n",
      "Batch 3 - batch loss: 2.1776061058044434 - avg loss: 2.3567935824394226   (start: 30, end: 40)\n",
      "Batch 4 - batch loss: 2.3772802352905273 - avg loss: 2.3608909130096434   (start: 40, end: 50)\n",
      "Batch 5 - batch loss: 2.246580123901367 - avg loss: 2.3418391148249307   (start: 50, end: 60)\n",
      "Batch 6 - batch loss: 2.3905882835388184 - avg loss: 2.3488032817840576   (start: 60, end: 70)\n",
      "Batch 7 - batch loss: 2.9608523845672607 - avg loss: 2.425309419631958   (start: 70, end: 80)\n",
      "Batch 8 - batch loss: 2.7365193367004395 - avg loss: 2.459888299306234   (start: 80, end: 90)\n",
      "Batch 9 - batch loss: 2.8640105724334717 - avg loss: 2.5003005266189575   (start: 90, end: 100)\n",
      "Batch 10 - batch loss: 1.7566168308258057 - avg loss: 2.4326929179104892   (start: 100, end: 110)\n",
      "Batch 11 - batch loss: 2.1285345554351807 - avg loss: 2.4073463877042136   (start: 110, end: 120)\n",
      "Batch 12 - batch loss: 1.5817415714263916 - avg loss: 2.343838324913612   (start: 120, end: 130)\n",
      "Batch 13 - batch loss: 2.8169753551483154 - avg loss: 2.3776338270732333   (start: 130, end: 140)\n",
      "Batch 14 - batch loss: 1.6623344421386719 - avg loss: 2.3299472014109295   (start: 140, end: 150)\n",
      "Batch 15 - batch loss: 2.0510668754577637 - avg loss: 2.3125171810388565   (start: 150, end: 160)\n",
      "Batch 16 - batch loss: 1.8603763580322266 - avg loss: 2.2859206620384667   (start: 160, end: 170)\n",
      "Batch 17 - batch loss: 1.8565582036972046 - avg loss: 2.2620671921306186   (start: 170, end: 180)\n",
      "Batch 18 - batch loss: 2.834724187850952 - avg loss: 2.292207034010636   (start: 180, end: 190)\n",
      "Batch 19 - batch loss: 1.8843004703521729 - avg loss: 2.271811705827713   (start: 190, end: 200)\n",
      "Batch 20 - batch loss: 2.4651548862457275 - avg loss: 2.2810185239428566   (start: 200, end: 210)\n",
      "Batch 21 - batch loss: 2.2990825176239014 - avg loss: 2.2818396145647224   (start: 210, end: 220)\n",
      "Batch 22 - batch loss: 2.5361664295196533 - avg loss: 2.2928973021714585   (start: 220, end: 230)\n",
      "Batch 23 - batch loss: 2.1774022579193115 - avg loss: 2.288085008660952   (start: 230, end: 240)\n",
      "Batch 24 - batch loss: 3.0865261554718018 - avg loss: 2.3200226545333864   (start: 240, end: 250)\n",
      "Batch 25 - batch loss: 2.643110990524292 - avg loss: 2.332449128994575   (start: 250, end: 260)\n",
      "Batch 26 - batch loss: 3.0995936393737793 - avg loss: 2.360861888638249   (start: 260, end: 270)\n",
      "Batch 27 - batch loss: 1.476658582687378 - avg loss: 2.329283199140004   (start: 270, end: 280)\n",
      "Batch 28 - batch loss: 2.7185158729553223 - avg loss: 2.342705015478463   (start: 280, end: 290)\n",
      "Batch 29 - batch loss: 1.700026273727417 - avg loss: 2.3212823907534283   (start: 290, end: 300)\n",
      "Batch 30 - batch loss: 1.3138316869735718 - avg loss: 2.288783980954078   (start: 300, end: 310)\n",
      "Batch 31 - batch loss: 2.012782096862793 - avg loss: 2.2801589220762253   (start: 310, end: 320)\n",
      "Batch 32 - batch loss: 1.5903985500335693 - avg loss: 2.259257092620387   (start: 320, end: 330)\n",
      "Batch 33 - batch loss: 2.0468859672546387 - avg loss: 2.2530108830508064   (start: 330, end: 340)\n",
      "Batch 34 - batch loss: 1.9905130863189697 - avg loss: 2.245510946001325   (start: 340, end: 350)\n",
      "Batch 35 - batch loss: 2.491105079650879 - avg loss: 2.2523330052693686   (start: 350, end: 360)\n",
      "Batch 36 - batch loss: 1.7607126235961914 - avg loss: 2.23904596792685   (start: 360, end: 370)\n",
      "Batch 37 - batch loss: 2.1389946937561035 - avg loss: 2.236413039659199   (start: 370, end: 380)\n",
      "Batch 38 - batch loss: 2.1325459480285645 - avg loss: 2.2337497808994393   (start: 380, end: 390)\n",
      "Batch 39 - batch loss: 1.6011826992034912 - avg loss: 2.2179356038570406   (start: 390, end: 400)\n",
      "Batch 40 - batch loss: 2.673210620880127 - avg loss: 2.2290398725649205   (start: 400, end: 410)\n",
      "Batch 41 - batch loss: 2.3977532386779785 - avg loss: 2.2330568574723744   (start: 410, end: 420)\n",
      "Batch 42 - batch loss: 2.7680413722991943 - avg loss: 2.245498357817184   (start: 420, end: 430)\n",
      "Batch 43 - batch loss: 2.3662502765655518 - avg loss: 2.2482427196069197   (start: 430, end: 440)\n",
      "Batch 44 - batch loss: 1.5410813093185425 - avg loss: 2.2325280216005114   (start: 440, end: 450)\n",
      "Batch 45 - batch loss: 2.0603346824645996 - avg loss: 2.228784688141035   (start: 450, end: 460)\n",
      "Batch 46 - batch loss: 2.1261138916015625 - avg loss: 2.2266002031082803   (start: 460, end: 470)\n",
      "Batch 47 - batch loss: 1.6611039638519287 - avg loss: 2.214819031457106   (start: 470, end: 480)\n",
      "Batch 48 - batch loss: 2.3679635524749756 - avg loss: 2.217944429845226   (start: 480, end: 490)\n",
      "Batch 49 - batch loss: 2.31980562210083 - avg loss: 2.219981653690338   (start: 490, end: 500)\n",
      "Batch 50 - batch loss: 2.021595001220703 - avg loss: 2.2160917193281886   (start: 500, end: 510)\n",
      "Batch 51 - batch loss: 3.078111171722412 - avg loss: 2.232669016489616   (start: 510, end: 520)\n",
      "Batch 52 - batch loss: 2.074143648147583 - avg loss: 2.229677971803917   (start: 520, end: 530)\n",
      "Batch 53 - batch loss: 1.5689841508865356 - avg loss: 2.2174429010461876   (start: 530, end: 540)\n",
      "Batch 54 - batch loss: 1.5750863552093506 - avg loss: 2.2057636911218816   (start: 540, end: 550)\n",
      "Batch 55 - batch loss: 2.2911391258239746 - avg loss: 2.2072882524558475   (start: 550, end: 560)\n",
      "Batch 56 - batch loss: 2.4620211124420166 - avg loss: 2.2117572499994647   (start: 560, end: 570)\n",
      "Batch 57 - batch loss: 1.5712299346923828 - avg loss: 2.200713675597618   (start: 570, end: 580)\n",
      "Batch 58 - batch loss: 1.031523585319519 - avg loss: 2.180896894406464   (start: 580, end: 590)\n",
      "Batch 59 - batch loss: 2.2228126525878906 - avg loss: 2.1815954903761545   (start: 590, end: 600)\n",
      "Batch 60 - batch loss: 1.8897857666015625 - avg loss: 2.176811724412637   (start: 600, end: 610)\n",
      "Batch 61 - batch loss: 2.354612112045288 - avg loss: 2.17967947260026   (start: 610, end: 620)\n",
      "Batch 62 - batch loss: 1.4029172658920288 - avg loss: 2.1673499137636214   (start: 620, end: 630)\n",
      "Batch 63 - batch loss: 2.2978153228759766 - avg loss: 2.169388435781002   (start: 630, end: 640)\n",
      "Batch 64 - batch loss: 1.7706594467163086 - avg loss: 2.163254143641545   (start: 640, end: 650)\n",
      "Batch 65 - batch loss: 2.7642135620117188 - avg loss: 2.1723595893744267   (start: 650, end: 660)\n",
      "Batch 66 - batch loss: 2.2785251140594482 - avg loss: 2.1739441494443525   (start: 660, end: 670)\n",
      "Batch 67 - batch loss: 1.5670087337493896 - avg loss: 2.1650186286253086   (start: 670, end: 680)\n",
      "Batch 68 - batch loss: 2.4595541954040527 - avg loss: 2.1692872600278994   (start: 680, end: 690)\n",
      "Batch 69 - batch loss: 1.967029333114624 - avg loss: 2.1663978610719954   (start: 690, end: 700)\n",
      "Batch 70 - batch loss: 2.6720423698425293 - avg loss: 2.1735196147166507   (start: 700, end: 710)\n",
      "Batch 71 - batch loss: 2.367147445678711 - avg loss: 2.1762088901466794   (start: 710, end: 720)\n",
      "Batch 72 - batch loss: 1.7793689966201782 - avg loss: 2.170772727221659   (start: 720, end: 730)\n",
      "Batch 73 - batch loss: 1.3945075273513794 - avg loss: 2.1602826569531417   (start: 730, end: 740)\n",
      "Batch 74 - batch loss: 2.0061113834381104 - avg loss: 2.158227039972941   (start: 740, end: 750)\n",
      "Batch 75 - batch loss: 1.8012968301773071 - avg loss: 2.1535305898440513   (start: 750, end: 760)\n",
      "Batch 76 - batch loss: 1.9889732599258423 - avg loss: 2.151393481663295   (start: 760, end: 770)\n",
      "Batch 77 - batch loss: 2.2033276557922363 - avg loss: 2.152059304408538   (start: 770, end: 780)\n",
      "Batch 78 - batch loss: 2.464097023010254 - avg loss: 2.1560091489478004   (start: 780, end: 790)\n",
      "Batch 79 - batch loss: 1.8687403202056885 - avg loss: 2.1524182885885237   (start: 790, end: 800)\n",
      "Batch 80 - batch loss: 2.518138885498047 - avg loss: 2.1569333576861722   (start: 800, end: 810)\n",
      "Batch 81 - batch loss: 1.7383264303207397 - avg loss: 2.1518283951573256   (start: 810, end: 820)\n",
      "Batch 82 - batch loss: 1.9916146993637085 - avg loss: 2.1498981096658363   (start: 820, end: 830)\n",
      "Batch 83 - batch loss: 2.432579517364502 - avg loss: 2.153263364519392   (start: 830, end: 840)\n",
      "Batch 84 - batch loss: 1.9383951425552368 - avg loss: 2.1507355030845194   (start: 840, end: 850)\n",
      "Batch 85 - batch loss: 2.6744840145111084 - avg loss: 2.156825602054596   (start: 850, end: 860)\n",
      "Batch 86 - batch loss: 2.3610336780548096 - avg loss: 2.159172821318966   (start: 860, end: 870)\n",
      "Batch 87 - batch loss: 2.0720932483673096 - avg loss: 2.158183280717243   (start: 870, end: 880)\n",
      "Batch 88 - batch loss: 1.3316987752914429 - avg loss: 2.1488969379596496   (start: 880, end: 890)\n",
      "Batch 89 - batch loss: 1.9015830755233765 - avg loss: 2.1461490061548023   (start: 890, end: 900)\n",
      "Batch 90 - batch loss: 1.813624382019043 - avg loss: 2.1424948894060574   (start: 900, end: 910)\n",
      "Batch 91 - batch loss: 1.6220855712890625 - avg loss: 2.1368382663830467   (start: 910, end: 920)\n",
      "Batch 92 - batch loss: 1.514585256576538 - avg loss: 2.130147373804482   (start: 920, end: 930)\n",
      "Batch 93 - batch loss: 2.79064679145813 - avg loss: 2.137173963353989   (start: 930, end: 940)\n",
      "Batch 94 - batch loss: 2.4012598991394043 - avg loss: 2.139953815309625   (start: 940, end: 950)\n",
      "Batch 95 - batch loss: 2.324687957763672 - avg loss: 2.1418781292935214   (start: 950, end: 960)\n",
      "Batch 96 - batch loss: 1.760664701461792 - avg loss: 2.1379480939550497   (start: 960, end: 970)\n",
      "Batch 97 - batch loss: 2.6982741355895996 - avg loss: 2.14366570662479   (start: 970, end: 980)\n",
      "Batch 98 - batch loss: 1.2838245630264282 - avg loss: 2.134980442548039   (start: 980, end: 990)\n",
      "Batch 99 - batch loss: 1.206960916519165 - avg loss: 2.1257002472877504   (start: 990, end: 1000)\n",
      "Batch 100 - batch loss: 2.240738868713379 - avg loss: 2.126839243539489   (start: 1000, end: 1010)\n",
      "Batch 101 - batch loss: 2.267059803009033 - avg loss: 2.1282139549068377   (start: 1010, end: 1020)\n",
      "Batch 102 - batch loss: 1.939215898513794 - avg loss: 2.1263790223204975   (start: 1020, end: 1030)\n",
      "Batch 103 - batch loss: 1.5755318403244019 - avg loss: 2.1210824148013043   (start: 1030, end: 1040)\n",
      "Batch 104 - batch loss: 2.0444416999816895 - avg loss: 2.1203525032315937   (start: 1040, end: 1050)\n",
      "Batch 105 - batch loss: 1.6001431941986084 - avg loss: 2.1154448682407163   (start: 1050, end: 1060)\n",
      "Batch 106 - batch loss: 1.3911832571029663 - avg loss: 2.1086760681366252   (start: 1060, end: 1070)\n",
      "Batch 107 - batch loss: 2.332853078842163 - avg loss: 2.1107517811987133   (start: 1070, end: 1080)\n",
      "Batch 108 - batch loss: 1.99429190158844 - avg loss: 2.109683341936234   (start: 1080, end: 1090)\n",
      "Batch 109 - batch loss: 1.3734005689620972 - avg loss: 2.1029898621819236   (start: 1090, end: 1100)\n",
      "Batch 110 - batch loss: 2.916771173477173 - avg loss: 2.1103212253467456   (start: 1100, end: 1110)\n",
      "Batch 111 - batch loss: 1.050474762916565 - avg loss: 2.100858310503619   (start: 1110, end: 1120)\n",
      "Batch 112 - batch loss: 1.4880611896514893 - avg loss: 2.0954353271332464   (start: 1120, end: 1130)\n",
      "Batch 113 - batch loss: 1.5358182191848755 - avg loss: 2.090526405133699   (start: 1130, end: 1140)\n",
      "Batch 114 - batch loss: 2.4744925498962402 - avg loss: 2.0938652411751124   (start: 1140, end: 1150)\n",
      "Batch 115 - batch loss: 2.212151288986206 - avg loss: 2.094884948483829   (start: 1150, end: 1160)\n",
      "Batch 116 - batch loss: 2.2453465461730957 - avg loss: 2.0961709450452757   (start: 1160, end: 1170)\n",
      "Batch 117 - batch loss: 3.2307381629943848 - avg loss: 2.105785921468573   (start: 1170, end: 1180)\n",
      "Batch 118 - batch loss: 2.131199836730957 - avg loss: 2.1059994837817024   (start: 1180, end: 1190)\n",
      "Batch 119 - batch loss: 1.6268209218978882 - avg loss: 2.1020063290993374   (start: 1190, end: 1200)\n",
      "Batch 120 - batch loss: 2.361178398132324 - avg loss: 2.1041482470252295   (start: 1200, end: 1210)\n",
      "Batch 121 - batch loss: 2.4520132541656494 - avg loss: 2.106999599542774   (start: 1210, end: 1220)\n",
      "Batch 122 - batch loss: 2.118300199508667 - avg loss: 2.107091474339245   (start: 1220, end: 1230)\n",
      "Batch 123 - batch loss: 1.549933910369873 - avg loss: 2.1025982681782014   (start: 1230, end: 1240)\n",
      "Batch 124 - batch loss: 1.2931431531906128 - avg loss: 2.0961226272583007   (start: 1240, end: 1250)\n",
      "Batch 125 - batch loss: 2.103166103363037 - avg loss: 2.0961785278623064   (start: 1250, end: 1260)\n",
      "Batch 126 - batch loss: 1.2509448528289795 - avg loss: 2.089523144594328   (start: 1260, end: 1270)\n",
      "Batch 127 - batch loss: 2.622269630432129 - avg loss: 2.0936852265149355   (start: 1270, end: 1280)\n",
      "Batch 128 - batch loss: 2.174631118774414 - avg loss: 2.0943127140518305   (start: 1280, end: 1290)\n",
      "Batch 129 - batch loss: 1.5721871852874756 - avg loss: 2.0902963638305665   (start: 1290, end: 1300)\n",
      "Batch 130 - batch loss: 2.294119358062744 - avg loss: 2.091852264549896   (start: 1300, end: 1310)\n",
      "Batch 131 - batch loss: 2.6153805255889893 - avg loss: 2.095818387739586   (start: 1310, end: 1320)\n",
      "Batch 132 - batch loss: 2.518496036529541 - avg loss: 2.098996415174097   (start: 1320, end: 1330)\n",
      "Batch 133 - batch loss: 1.8845125436782837 - avg loss: 2.097395789267412   (start: 1330, end: 1340)\n",
      "Batch 134 - batch loss: 2.6310811042785645 - avg loss: 2.10134901382305   (start: 1340, end: 1350)\n",
      "Batch 135 - batch loss: 2.954852342605591 - avg loss: 2.10762477359351   (start: 1350, end: 1360)\n",
      "Batch 136 - batch loss: 2.516103506088257 - avg loss: 2.110606370181063   (start: 1360, end: 1370)\n",
      "Batch 137 - batch loss: 1.8459374904632568 - avg loss: 2.108688479748325   (start: 1370, end: 1380)\n",
      "Batch 138 - batch loss: 1.5728685855865479 - avg loss: 2.104833660365866   (start: 1380, end: 1390)\n",
      "Batch 139 - batch loss: 1.8184760808944702 - avg loss: 2.1027882490839276   (start: 1390, end: 1400)\n",
      "Batch 140 - batch loss: 3.394477367401123 - avg loss: 2.1119491648166737   (start: 1400, end: 1410)\n",
      "Batch 141 - batch loss: 1.6415239572525024 - avg loss: 2.1086363112422783   (start: 1410, end: 1420)\n",
      "Batch 142 - batch loss: 2.095888137817383 - avg loss: 2.10854716317637   (start: 1420, end: 1430)\n",
      "Batch 143 - batch loss: 2.6219358444213867 - avg loss: 2.1121123623516826   (start: 1430, end: 1440)\n",
      "Batch 144 - batch loss: 2.065760850906372 - avg loss: 2.111792696755508   (start: 1440, end: 1450)\n",
      "Batch 145 - batch loss: 2.766749858856201 - avg loss: 2.1162787047151017   (start: 1450, end: 1460)\n",
      "Batch 146 - batch loss: 2.6814565658569336 - avg loss: 2.1201234520698082   (start: 1460, end: 1470)\n",
      "Batch 147 - batch loss: 2.2021427154541016 - avg loss: 2.1206776362818642   (start: 1470, end: 1480)\n",
      "Batch 148 - batch loss: 2.124286651611328 - avg loss: 2.1207018578612566   (start: 1480, end: 1490)\n",
      "Batch 149 - batch loss: 1.659436583518982 - avg loss: 2.117626756032308   (start: 1490, end: 1500)\n",
      "Batch 150 - batch loss: 1.5919817686080933 - avg loss: 2.1141456634003597   (start: 1500, end: 1510)\n",
      "Batch 151 - batch loss: 2.0092713832855225 - avg loss: 2.113455701031183   (start: 1510, end: 1520)\n",
      "Batch 152 - batch loss: 1.81380295753479 - avg loss: 2.111497186367808   (start: 1520, end: 1530)\n",
      "Batch 153 - batch loss: 2.078770399093628 - avg loss: 2.1112846747621314   (start: 1530, end: 1540)\n",
      "Batch 154 - batch loss: 1.9840619564056396 - avg loss: 2.1104638830307993   (start: 1540, end: 1550)\n",
      "Batch 155 - batch loss: 2.3727498054504395 - avg loss: 2.1121452030463095   (start: 1550, end: 1560)\n",
      "Batch 156 - batch loss: 2.4638123512268066 - avg loss: 2.1143851211875866   (start: 1560, end: 1570)\n",
      "Batch 157 - batch loss: 1.4160897731781006 - avg loss: 2.1099655303774   (start: 1570, end: 1580)\n",
      "Batch 158 - batch loss: 2.758694648742676 - avg loss: 2.1140455877256095   (start: 1580, end: 1590)\n",
      "Batch 159 - batch loss: 2.529264450073242 - avg loss: 2.116640705615282   (start: 1590, end: 1600)\n",
      "Batch 160 - batch loss: 2.4371590614318848 - avg loss: 2.118631502856379   (start: 1600, end: 1610)\n",
      "Batch 161 - batch loss: 2.343393325805664 - avg loss: 2.12001892151656   (start: 1610, end: 1620)\n",
      "Batch 162 - batch loss: 1.8064063787460327 - avg loss: 2.1180949181866793   (start: 1620, end: 1630)\n",
      "Batch 163 - batch loss: 2.7534706592559814 - avg loss: 2.1219691605102726   (start: 1630, end: 1640)\n",
      "Batch 164 - batch loss: 1.7069261074066162 - avg loss: 2.11945374806722   (start: 1640, end: 1650)\n",
      "Batch 165 - batch loss: 2.8017666339874268 - avg loss: 2.1235640666571007   (start: 1650, end: 1660)\n",
      "Batch 166 - batch loss: 1.8435602188110352 - avg loss: 2.1218873969095196   (start: 1660, end: 1670)\n",
      "Batch 167 - batch loss: 2.656125783920288 - avg loss: 2.1250673873083934   (start: 1670, end: 1680)\n",
      "Batch 168 - batch loss: 2.534925699234009 - avg loss: 2.127492584420379   (start: 1680, end: 1690)\n",
      "Batch 169 - batch loss: 1.8538358211517334 - avg loss: 2.125882838754093   (start: 1690, end: 1700)\n",
      "Batch 170 - batch loss: 2.859358549118042 - avg loss: 2.130172170393648   (start: 1700, end: 1710)\n",
      "Batch 171 - batch loss: 1.796775460243225 - avg loss: 2.1282338174276574   (start: 1710, end: 1720)\n",
      "Batch 172 - batch loss: 1.6226732730865479 - avg loss: 2.1253115021424485   (start: 1720, end: 1730)\n",
      "Batch 173 - batch loss: 2.344801425933838 - avg loss: 2.126572938486077   (start: 1730, end: 1740)\n",
      "Batch 174 - batch loss: 1.75910222530365 - avg loss: 2.1244731058393205   (start: 1740, end: 1750)\n",
      "Batch 175 - batch loss: 2.819629430770874 - avg loss: 2.1284228576855226   (start: 1750, end: 1760)\n",
      "Batch 176 - batch loss: 2.3749756813049316 - avg loss: 2.129815811491282   (start: 1760, end: 1770)\n",
      "Batch 177 - batch loss: 1.7741858959197998 - avg loss: 2.127817890617285   (start: 1770, end: 1780)\n",
      "Batch 178 - batch loss: 1.3868850469589233 - avg loss: 2.1236786009879087   (start: 1780, end: 1790)\n",
      "Batch 179 - batch loss: 2.7484450340270996 - avg loss: 2.127149525615904   (start: 1790, end: 1800)\n",
      "Batch 180 - batch loss: 1.5393164157867432 - avg loss: 2.1239018288765164   (start: 1800, end: 1810)\n",
      "Batch 181 - batch loss: 2.2343173027038574 - avg loss: 2.124508507304139   (start: 1810, end: 1820)\n",
      "Batch 182 - batch loss: 2.7099783420562744 - avg loss: 2.127707796018632   (start: 1820, end: 1830)\n",
      "Batch 183 - batch loss: 2.237741708755493 - avg loss: 2.1283058064139406   (start: 1830, end: 1840)\n",
      "Batch 184 - batch loss: 1.7641977071762085 - avg loss: 2.126337654526169   (start: 1840, end: 1850)\n",
      "Batch 185 - batch loss: 2.0012171268463135 - avg loss: 2.1256649635171376   (start: 1850, end: 1860)\n",
      "Batch 186 - batch loss: 1.7886581420898438 - avg loss: 2.123862788001484   (start: 1860, end: 1870)\n",
      "Batch 187 - batch loss: 2.356802463531494 - avg loss: 2.125101828828771   (start: 1870, end: 1880)\n",
      "Batch 188 - batch loss: 2.2193284034729004 - avg loss: 2.1256003821337663   (start: 1880, end: 1890)\n",
      "Batch 189 - batch loss: 3.1230783462524414 - avg loss: 2.1308502661554436   (start: 1890, end: 1900)\n",
      "Batch 190 - batch loss: 1.895010232925415 - avg loss: 2.129615501583559   (start: 1900, end: 1910)\n",
      "Batch 191 - batch loss: 1.5148066282272339 - avg loss: 2.1264133720348277   (start: 1910, end: 1920)\n",
      "Batch 192 - batch loss: 2.3538804054260254 - avg loss: 2.127591957700067   (start: 1920, end: 1930)\n",
      "Batch 193 - batch loss: 2.0100784301757812 - avg loss: 2.1269862178674677   (start: 1930, end: 1940)\n",
      "Batch 194 - batch loss: 2.260507106781006 - avg loss: 2.127670940374717   (start: 1940, end: 1950)\n",
      "Batch 195 - batch loss: 2.319007396697998 - avg loss: 2.1286471467845294   (start: 1950, end: 1960)\n",
      "Batch 196 - batch loss: 1.7859166860580444 - avg loss: 2.1269073982529227   (start: 1960, end: 1970)\n",
      "Batch 197 - batch loss: 1.9286963939666748 - avg loss: 2.1259063325747096   (start: 1970, end: 1980)\n",
      "Batch 198 - batch loss: 2.3995211124420166 - avg loss: 2.127281281217259   (start: 1980, end: 1990)\n",
      "Batch 199 - batch loss: 3.3023712635040283 - avg loss: 2.1331567311286928   (start: 1990, end: 2000)\n",
      "Batch 200 - batch loss: 1.8791526556015015 - avg loss: 2.1318930292603984   (start: 2000, end: 2010)\n",
      "Batch 201 - batch loss: 2.0389058589935303 - avg loss: 2.1314326967343247   (start: 2010, end: 2020)\n",
      "Batch 202 - batch loss: 1.5498061180114746 - avg loss: 2.1285675411741134   (start: 2020, end: 2030)\n",
      "Batch 203 - batch loss: 3.0949971675872803 - avg loss: 2.1333049413035896   (start: 2030, end: 2040)\n",
      "Batch 204 - batch loss: 1.9100109338760376 - avg loss: 2.1322157022429677   (start: 2040, end: 2050)\n",
      "Batch 205 - batch loss: 2.6812520027160645 - avg loss: 2.1348809270025457   (start: 2050, end: 2060)\n",
      "Batch 206 - batch loss: 1.783130407333374 - avg loss: 2.1331816491297477   (start: 2060, end: 2070)\n",
      "Batch 207 - batch loss: 2.580237865447998 - avg loss: 2.135330957862047   (start: 2070, end: 2080)\n",
      "Batch 208 - batch loss: 1.3935661315917969 - avg loss: 2.131781843860754   (start: 2080, end: 2090)\n",
      "Batch 209 - batch loss: 1.5650553703308105 - avg loss: 2.1290831463677544   (start: 2090, end: 2100)\n",
      "Batch 210 - batch loss: 3.0327165126800537 - avg loss: 2.133365768956912   (start: 2100, end: 2110)\n",
      "Batch 211 - batch loss: 1.2461459636688232 - avg loss: 2.1291807698753646   (start: 2110, end: 2120)\n",
      "Batch 212 - batch loss: 2.1120452880859375 - avg loss: 2.1291003216040525   (start: 2120, end: 2130)\n",
      "Batch 213 - batch loss: 2.8082032203674316 - avg loss: 2.132273699635657   (start: 2130, end: 2140)\n",
      "Batch 214 - batch loss: 3.209494113922119 - avg loss: 2.1372840271439664   (start: 2140, end: 2150)\n",
      "Batch 215 - batch loss: 1.6065555810928345 - avg loss: 2.1348269510048405   (start: 2150, end: 2160)\n",
      "Batch 216 - batch loss: 1.9202511310577393 - avg loss: 2.13383812234149   (start: 2160, end: 2170)\n",
      "Batch 217 - batch loss: 1.5854551792144775 - avg loss: 2.1313226042537514   (start: 2170, end: 2180)\n",
      "Batch 218 - batch loss: 1.986405611038208 - avg loss: 2.1306608828235434   (start: 2180, end: 2190)\n",
      "Batch 219 - batch loss: 2.0227391719818115 - avg loss: 2.1301703295924446   (start: 2190, end: 2200)\n",
      "Batch 220 - batch loss: 1.5528624057769775 - avg loss: 2.127558076543506   (start: 2200, end: 2210)\n",
      "Batch 221 - batch loss: 1.810225248336792 - avg loss: 2.126128649389422   (start: 2210, end: 2220)\n",
      "Batch 222 - batch loss: 1.7388944625854492 - avg loss: 2.124392173215413   (start: 2220, end: 2230)\n",
      "Batch 223 - batch loss: 2.022239923477173 - avg loss: 2.123936136386224   (start: 2230, end: 2240)\n",
      "Batch 224 - batch loss: 2.589092493057251 - avg loss: 2.126003497971429   (start: 2240, end: 2250)\n",
      "Batch 225 - batch loss: 2.712752103805542 - avg loss: 2.128599730740606   (start: 2250, end: 2260)\n",
      "Batch 226 - batch loss: 2.213470220565796 - avg loss: 2.128973609550409   (start: 2260, end: 2270)\n",
      "Batch 227 - batch loss: 1.7834924459457397 - avg loss: 2.127458341288985   (start: 2270, end: 2280)\n",
      "Batch 228 - batch loss: 1.1921075582504272 - avg loss: 2.1233738400530084   (start: 2280, end: 2290)\n",
      "Batch 229 - batch loss: 1.2889572381973267 - avg loss: 2.1197459417840707   (start: 2290, end: 2300)\n",
      "Batch 230 - batch loss: 2.598416566848755 - avg loss: 2.121818108992143   (start: 2300, end: 2310)\n",
      "Batch 231 - batch loss: 1.3411835432052612 - avg loss: 2.1184533048292686   (start: 2310, end: 2320)\n",
      "Batch 232 - batch loss: 3.180978298187256 - avg loss: 2.1230134979338096   (start: 2320, end: 2330)\n",
      "Batch 233 - batch loss: 1.6189101934432983 - avg loss: 2.1208592103077817   (start: 2330, end: 2340)\n",
      "Batch 234 - batch loss: 2.3664746284484863 - avg loss: 2.1219043822998698   (start: 2340, end: 2350)\n",
      "Batch 235 - batch loss: 2.0372982025146484 - avg loss: 2.1215458815380677   (start: 2350, end: 2360)\n",
      "Batch 236 - batch loss: 1.8231834173202515 - avg loss: 2.120286968186938   (start: 2360, end: 2370)\n",
      "Batch 237 - batch loss: 1.5671390295028687 - avg loss: 2.1179628171840634   (start: 2370, end: 2380)\n",
      "Batch 238 - batch loss: 2.144835948944092 - avg loss: 2.1180752570659047   (start: 2380, end: 2390)\n",
      "Batch 239 - batch loss: 2.60026216506958 - avg loss: 2.1200843691825866   (start: 2390, end: 2400)\n",
      "Batch 240 - batch loss: 1.6880441904067993 - avg loss: 2.1182916713453426   (start: 2400, end: 2410)\n",
      "Batch 241 - batch loss: 2.835951328277588 - avg loss: 2.12125720711779   (start: 2410, end: 2420)\n",
      "Batch 242 - batch loss: 2.0826587677001953 - avg loss: 2.1210983658033142   (start: 2420, end: 2430)\n",
      "Batch 243 - batch loss: 2.0267584323883057 - avg loss: 2.1207117267319413   (start: 2430, end: 2440)\n",
      "Batch 244 - batch loss: 2.233645439147949 - avg loss: 2.12117268066017   (start: 2440, end: 2450)\n",
      "Batch 245 - batch loss: 1.82196843624115 - avg loss: 2.1199564032438323   (start: 2450, end: 2460)\n",
      "Batch 246 - batch loss: 2.3092966079711914 - avg loss: 2.120722962777142   (start: 2460, end: 2470)\n",
      "Batch 247 - batch loss: 2.0140132904052734 - avg loss: 2.120292681840158   (start: 2470, end: 2480)\n",
      "Batch 248 - batch loss: 1.7390706539154053 - avg loss: 2.118761669679818   (start: 2480, end: 2490)\n",
      "Batch 249 - batch loss: 2.2989656925201416 - avg loss: 2.1194824857711794   (start: 2490, end: 2500)\n",
      "Batch 250 - batch loss: 1.520466923713684 - avg loss: 2.1170959695876834   (start: 2500, end: 2510)\n",
      "Batch 251 - batch loss: 1.7960712909698486 - avg loss: 2.1158220621328505   (start: 2510, end: 2520)\n",
      "Batch 252 - batch loss: 1.9885971546173096 - avg loss: 2.1153191968857534   (start: 2520, end: 2530)\n",
      "Batch 253 - batch loss: 1.5615211725234985 - avg loss: 2.113138889703225   (start: 2530, end: 2540)\n",
      "Batch 254 - batch loss: 2.0816636085510254 - avg loss: 2.1130154572281183   (start: 2540, end: 2550)\n",
      "Batch 255 - batch loss: 2.662757158279419 - avg loss: 2.11516288574785   (start: 2550, end: 2560)\n",
      "Batch 256 - batch loss: 1.9560842514038086 - avg loss: 2.1145439027348383   (start: 2560, end: 2570)\n",
      "Batch 257 - batch loss: 2.016516923904419 - avg loss: 2.114163953204488   (start: 2570, end: 2580)\n",
      "Batch 258 - batch loss: 2.153979778289795 - avg loss: 2.114317682258871   (start: 2580, end: 2590)\n",
      "Batch 259 - batch loss: 2.5686233043670654 - avg loss: 2.1160650115746718   (start: 2590, end: 2600)\n",
      "Batch 260 - batch loss: 2.535806655883789 - avg loss: 2.117673217108423   (start: 2600, end: 2610)\n",
      "Batch 261 - batch loss: 2.101945400238037 - avg loss: 2.11761318727304   (start: 2610, end: 2620)\n",
      "Batch 262 - batch loss: 2.549004077911377 - avg loss: 2.1192534568191936   (start: 2620, end: 2630)\n",
      "Batch 263 - batch loss: 1.8942874670028687 - avg loss: 2.118401312918374   (start: 2630, end: 2640)\n",
      "Batch 264 - batch loss: 2.2908992767333984 - avg loss: 2.1190522486308834   (start: 2640, end: 2650)\n",
      "Batch 265 - batch loss: 1.961505651473999 - avg loss: 2.1184599681904444   (start: 2650, end: 2660)\n",
      "Batch 266 - batch loss: 2.966346025466919 - avg loss: 2.1216355714012174   (start: 2660, end: 2670)\n",
      "Batch 267 - batch loss: 2.389596939086914 - avg loss: 2.122635427250791   (start: 2670, end: 2680)\n",
      "Batch 268 - batch loss: 2.2265563011169434 - avg loss: 2.1230217502019664   (start: 2680, end: 2690)\n",
      "Batch 269 - batch loss: 2.3199448585510254 - avg loss: 2.1237510950477034   (start: 2690, end: 2700)\n",
      "Batch 270 - batch loss: 2.250066041946411 - avg loss: 2.124217201862828   (start: 2700, end: 2710)\n",
      "Batch 271 - batch loss: 1.7570009231567383 - avg loss: 2.122867142014644   (start: 2710, end: 2720)\n",
      "Batch 272 - batch loss: 2.0528910160064697 - avg loss: 2.122610819208753   (start: 2720, end: 2730)\n",
      "Batch 273 - batch loss: 1.5628538131713867 - avg loss: 2.1205679104275945   (start: 2730, end: 2740)\n",
      "Batch 274 - batch loss: 2.4827723503112793 - avg loss: 2.121885017481717   (start: 2740, end: 2750)\n",
      "Batch 275 - batch loss: 2.233236789703369 - avg loss: 2.1222884659317955   (start: 2750, end: 2760)\n",
      "Batch 276 - batch loss: 1.757690668106079 - avg loss: 2.1209722283945185   (start: 2760, end: 2770)\n",
      "Batch 277 - batch loss: 1.6117222309112549 - avg loss: 2.1191403938711977   (start: 2770, end: 2780)\n",
      "Batch 278 - batch loss: 1.5450519323349 - avg loss: 2.1170827291345082   (start: 2780, end: 2790)\n",
      "Batch 279 - batch loss: 2.504999876022339 - avg loss: 2.118468147516251   (start: 2790, end: 2800)\n",
      "Batch 280 - batch loss: 2.0167508125305176 - avg loss: 2.118106164117725   (start: 2800, end: 2810)\n",
      "Batch 281 - batch loss: 1.9818694591522217 - avg loss: 2.1176230552348683   (start: 2810, end: 2820)\n",
      "Batch 282 - batch loss: 1.550337553024292 - avg loss: 2.11561851282423   (start: 2820, end: 2830)\n",
      "Batch 283 - batch loss: 2.476203441619873 - avg loss: 2.1168881780664686   (start: 2830, end: 2840)\n",
      "Batch 284 - batch loss: 3.2038497924804688 - avg loss: 2.120702078467921   (start: 2840, end: 2850)\n",
      "Batch 285 - batch loss: 1.9254337549209595 - avg loss: 2.120019322091883   (start: 2850, end: 2860)\n",
      "Batch 286 - batch loss: 2.317919969558716 - avg loss: 2.1207088713861926   (start: 2860, end: 2870)\n",
      "Batch 287 - batch loss: 1.5479671955108643 - avg loss: 2.1187201850116253   (start: 2870, end: 2880)\n",
      "Batch 288 - batch loss: 1.808476209640503 - avg loss: 2.117646676446327   (start: 2880, end: 2890)\n",
      "Batch 289 - batch loss: 1.2006893157958984 - avg loss: 2.11448475451305   (start: 2890, end: 2900)\n",
      "Batch 290 - batch loss: 2.09395170211792 - avg loss: 2.114414194195541   (start: 2900, end: 2910)\n",
      "Batch 291 - batch loss: 2.3137288093566895 - avg loss: 2.115096778494038   (start: 2910, end: 2920)\n",
      "Batch 292 - batch loss: 2.4945247173309326 - avg loss: 2.116391754394505   (start: 2920, end: 2930)\n",
      "Batch 293 - batch loss: 1.7098115682601929 - avg loss: 2.115008828591327   (start: 2930, end: 2940)\n",
      "Batch 294 - batch loss: 2.007946729660034 - avg loss: 2.114645906222069   (start: 2940, end: 2950)\n",
      "Batch 295 - batch loss: 2.6524951457977295 - avg loss: 2.1164629644638784   (start: 2950, end: 2960)\n",
      "Batch 296 - batch loss: 2.3728528022766113 - avg loss: 2.1173262299110593   (start: 2960, end: 2970)\n",
      "Batch 297 - batch loss: 2.6189727783203125 - avg loss: 2.119009607590285   (start: 2970, end: 2980)\n",
      "Batch 298 - batch loss: 2.3691301345825195 - avg loss: 2.119846131091931   (start: 2980, end: 2990)\n",
      "Batch 299 - batch loss: 2.0837907791137695 - avg loss: 2.1197259465853375   (start: 2990, end: 3000)\n",
      "Batch 300 - batch loss: 1.606881856918335 - avg loss: 2.1180221456229886   (start: 3000, end: 3010)\n",
      "Batch 301 - batch loss: 2.115227222442627 - avg loss: 2.1180128909104705   (start: 3010, end: 3020)\n",
      "Batch 302 - batch loss: 1.569054365158081 - avg loss: 2.116201146601057   (start: 3020, end: 3030)\n",
      "Batch 303 - batch loss: 2.188349962234497 - avg loss: 2.11643847823143   (start: 3030, end: 3040)\n",
      "Batch 304 - batch loss: 2.711651563644409 - avg loss: 2.1183899965442596   (start: 3040, end: 3050)\n",
      "Batch 305 - batch loss: 1.4013488292694092 - avg loss: 2.1160467247557797   (start: 3050, end: 3060)\n",
      "Batch 306 - batch loss: 1.4881333112716675 - avg loss: 2.114001404190685   (start: 3060, end: 3070)\n",
      "Batch 307 - batch loss: 1.990062952041626 - avg loss: 2.1135990066187724   (start: 3070, end: 3080)\n",
      "Batch 308 - batch loss: 1.76657235622406 - avg loss: 2.1124759430252618   (start: 3080, end: 3090)\n",
      "Batch 309 - batch loss: 1.654148817062378 - avg loss: 2.1109974684253814   (start: 3090, end: 3100)\n",
      "Batch 310 - batch loss: 1.6115987300872803 - avg loss: 2.109391684700822   (start: 3100, end: 3110)\n",
      "Batch 311 - batch loss: 2.1364669799804688 - avg loss: 2.109478464493385   (start: 3110, end: 3120)\n",
      "Batch 312 - batch loss: 2.19724178314209 - avg loss: 2.1097588584826776   (start: 3120, end: 3130)\n",
      "Batch 313 - batch loss: 2.7881875038146973 - avg loss: 2.11191945926399   (start: 3130, end: 3140)\n",
      "Batch 314 - batch loss: 1.8338196277618408 - avg loss: 2.1110366026560468   (start: 3140, end: 3150)\n",
      "Batch 315 - batch loss: 1.9513260126113892 - avg loss: 2.1105311893964114   (start: 3150, end: 3160)\n",
      "Batch 316 - batch loss: 2.2489078044891357 - avg loss: 2.1109677086869247   (start: 3160, end: 3170)\n",
      "Batch 317 - batch loss: 2.0494649410247803 - avg loss: 2.11077430375717   (start: 3170, end: 3180)\n",
      "Batch 318 - batch loss: 2.7298591136932373 - avg loss: 2.1127150084905115   (start: 3180, end: 3190)\n",
      "Batch 319 - batch loss: 2.7890467643737793 - avg loss: 2.1148285452276467   (start: 3190, end: 3200)\n",
      "Batch 320 - batch loss: 2.0454964637756348 - avg loss: 2.1146125574349615   (start: 3200, end: 3210)\n",
      "Batch 321 - batch loss: 2.5912208557128906 - avg loss: 2.116092707429613   (start: 3210, end: 3220)\n",
      "Batch 322 - batch loss: 1.2243964672088623 - avg loss: 2.113332037955246   (start: 3220, end: 3230)\n",
      "Batch 323 - batch loss: 1.7960984706878662 - avg loss: 2.1123529220068895   (start: 3230, end: 3240)\n",
      "Batch 324 - batch loss: 2.1714117527008057 - avg loss: 2.112534641485948   (start: 3240, end: 3250)\n",
      "Batch 325 - batch loss: 2.393810272216797 - avg loss: 2.1133974501691712   (start: 3250, end: 3260)\n",
      "Batch 326 - batch loss: 2.5209789276123047 - avg loss: 2.1146438767056948   (start: 3260, end: 3270)\n",
      "Batch 327 - batch loss: 2.019019603729248 - avg loss: 2.1143523392880836   (start: 3270, end: 3280)\n",
      "Batch 328 - batch loss: 1.5626564025878906 - avg loss: 2.1126754519424904   (start: 3280, end: 3290)\n",
      "Batch 329 - batch loss: 1.4485286474227905 - avg loss: 2.110662885868188   (start: 3290, end: 3300)\n",
      "Batch 330 - batch loss: 2.888683319091797 - avg loss: 2.113013400772187   (start: 3300, end: 3310)\n",
      "Batch 331 - batch loss: 1.9286022186279297 - avg loss: 2.1124579454042824   (start: 3310, end: 3320)\n",
      "Batch 332 - batch loss: 3.219322919845581 - avg loss: 2.1157818642464488   (start: 3320, end: 3330)\n",
      "Batch 333 - batch loss: 2.385712146759033 - avg loss: 2.1165900387449894   (start: 3330, end: 3340)\n",
      "Batch 334 - batch loss: 2.3063347339630127 - avg loss: 2.117156440820267   (start: 3340, end: 3350)\n",
      "Batch 335 - batch loss: 2.7465648651123047 - avg loss: 2.119029680178279   (start: 3350, end: 3360)\n",
      "Batch 336 - batch loss: 2.557966709136963 - avg loss: 2.120332163943735   (start: 3360, end: 3370)\n",
      "Batch 337 - batch loss: 1.5969864130020142 - avg loss: 2.1187838037338484   (start: 3370, end: 3380)\n",
      "Batch 338 - batch loss: 2.5020956993103027 - avg loss: 2.119914517290121   (start: 3380, end: 3390)\n",
      "Batch 339 - batch loss: 2.337939739227295 - avg loss: 2.1205557679428773   (start: 3390, end: 3400)\n",
      "Batch 340 - batch loss: 1.3412911891937256 - avg loss: 2.1182705345741115   (start: 3400, end: 3410)\n",
      "Batch 341 - batch loss: 2.6133790016174316 - avg loss: 2.119718220150262   (start: 3410, end: 3420)\n",
      "Batch 342 - batch loss: 1.6340904235839844 - avg loss: 2.1183023956704767   (start: 3420, end: 3430)\n",
      "Batch 343 - batch loss: 1.7125699520111084 - avg loss: 2.117122940892397   (start: 3430, end: 3440)\n",
      "Batch 344 - batch loss: 1.6916097402572632 - avg loss: 2.1158895692963533   (start: 3440, end: 3450)\n",
      "Batch 345 - batch loss: 1.7541166543960571 - avg loss: 2.1148439828371037   (start: 3450, end: 3460)\n",
      "Batch 346 - batch loss: 1.5679563283920288 - avg loss: 2.113267937723429   (start: 3460, end: 3470)\n",
      "Batch 347 - batch loss: 1.2248531579971313 - avg loss: 2.110715021689733   (start: 3470, end: 3480)\n",
      "Batch 348 - batch loss: 2.4043498039245605 - avg loss: 2.1115563820972825   (start: 3480, end: 3490)\n",
      "Batch 349 - batch loss: 2.0187647342681885 - avg loss: 2.1112912631034852   (start: 3490, end: 3500)\n",
      "Batch 350 - batch loss: 1.9825422763824463 - avg loss: 2.110924456873511   (start: 3500, end: 3510)\n",
      "Batch 351 - batch loss: 2.6363613605499268 - avg loss: 2.112417175349864   (start: 3510, end: 3520)\n",
      "Batch 352 - batch loss: 2.31343150138855 - avg loss: 2.112986621032693   (start: 3520, end: 3530)\n",
      "Batch 353 - batch loss: 2.6736865043640137 - avg loss: 2.1145705190082054   (start: 3530, end: 3540)\n",
      "Batch 354 - batch loss: 1.822640061378479 - avg loss: 2.1137481796909388   (start: 3540, end: 3550)\n",
      "Batch 355 - batch loss: 2.9475162029266357 - avg loss: 2.1160902247000277   (start: 3550, end: 3560)\n",
      "Batch 356 - batch loss: 2.063844680786133 - avg loss: 2.115943878638644   (start: 3560, end: 3570)\n",
      "Batch 357 - batch loss: 2.8480706214904785 - avg loss: 2.1179889254063866   (start: 3570, end: 3580)\n",
      "Batch 358 - batch loss: 1.6840308904647827 - avg loss: 2.1167801286516745   (start: 3580, end: 3590)\n",
      "Batch 359 - batch loss: 2.019096612930298 - avg loss: 2.1165087855524485   (start: 3590, end: 3600)\n",
      "Batch 360 - batch loss: 2.2514750957489014 - avg loss: 2.1168826534477296   (start: 3600, end: 3610)\n",
      "Batch 361 - batch loss: 2.2328903675079346 - avg loss: 2.1172031167462384   (start: 3610, end: 3620)\n",
      "Batch 362 - batch loss: 1.449507236480713 - avg loss: 2.1153637341559755   (start: 3620, end: 3630)\n",
      "Batch 363 - batch loss: 2.3596279621124268 - avg loss: 2.1160347897272844   (start: 3630, end: 3640)\n",
      "Batch 364 - batch loss: 2.0181260108947754 - avg loss: 2.115766546497606   (start: 3640, end: 3650)\n",
      "Batch 365 - batch loss: 2.0434539318084717 - avg loss: 2.1155689710476357   (start: 3650, end: 3660)\n",
      "Batch 366 - batch loss: 2.212130069732666 - avg loss: 2.115832080308358   (start: 3660, end: 3670)\n",
      "Batch 367 - batch loss: 1.3319326639175415 - avg loss: 2.1137019188507744   (start: 3670, end: 3680)\n",
      "Batch 368 - batch loss: 1.6545330286026 - avg loss: 2.1124575587146004   (start: 3680, end: 3690)\n",
      "Batch 369 - batch loss: 2.631056308746338 - avg loss: 2.1138591769579294   (start: 3690, end: 3700)\n",
      "Batch 370 - batch loss: 2.842463970184326 - avg loss: 2.1158230712792943   (start: 3700, end: 3710)\n",
      "Batch 371 - batch loss: 1.734832763671875 - avg loss: 2.114798903785726   (start: 3710, end: 3720)\n",
      "Batch 372 - batch loss: 2.675629138946533 - avg loss: 2.1163024700998303   (start: 3720, end: 3730)\n",
      "Batch 373 - batch loss: 2.2963967323303223 - avg loss: 2.1167840055603393   (start: 3730, end: 3740)\n",
      "Batch 374 - batch loss: 2.009737491607666 - avg loss: 2.116498548189799   (start: 3740, end: 3750)\n",
      "Batch 375 - batch loss: 1.7695605754852295 - avg loss: 2.115575840815585   (start: 3750, end: 3760)\n",
      "Batch 376 - batch loss: 1.5502082109451294 - avg loss: 2.114076191929987   (start: 3760, end: 3770)\n",
      "Batch 377 - batch loss: 1.4763057231903076 - avg loss: 2.1123889684677124   (start: 3770, end: 3780)\n",
      "Batch 378 - batch loss: 2.781999349594116 - avg loss: 2.1141557504759616   (start: 3780, end: 3790)\n",
      "Batch 379 - batch loss: 1.8161814212799072 - avg loss: 2.113371607504393   (start: 3790, end: 3800)\n",
      "Batch 380 - batch loss: 1.8359310626983643 - avg loss: 2.1126434170980777   (start: 3800, end: 3810)\n",
      "Batch 381 - batch loss: 2.331582546234131 - avg loss: 2.113216556179586   (start: 3810, end: 3820)\n",
      "Batch 382 - batch loss: 2.4974637031555176 - avg loss: 2.1142198124380087   (start: 3820, end: 3830)\n",
      "Batch 383 - batch loss: 1.75067138671875 - avg loss: 2.1132730717460313   (start: 3830, end: 3840)\n",
      "Batch 384 - batch loss: 2.621415376663208 - avg loss: 2.1145929218886734   (start: 3840, end: 3850)\n",
      "Batch 385 - batch loss: 2.6504812240600586 - avg loss: 2.11598123355233   (start: 3850, end: 3860)\n",
      "Batch 386 - batch loss: 1.1997177600860596 - avg loss: 2.11361362767774   (start: 3860, end: 3870)\n",
      "Batch 387 - batch loss: 2.5098462104797363 - avg loss: 2.114634845674652   (start: 3870, end: 3880)\n",
      "Batch 388 - batch loss: 2.980574607849121 - avg loss: 2.116860911901322   (start: 3880, end: 3890)\n",
      "Batch 389 - batch loss: 1.988389015197754 - avg loss: 2.1165314967815694   (start: 3890, end: 3900)\n",
      "Batch 390 - batch loss: 2.2046635150909424 - avg loss: 2.1167568983629232   (start: 3900, end: 3910)\n",
      "Batch 391 - batch loss: 2.4128212928771973 - avg loss: 2.1175121646754596   (start: 3910, end: 3920)\n",
      "Batch 392 - batch loss: 2.6659910678863525 - avg loss: 2.118907785294317   (start: 3920, end: 3930)\n",
      "Batch 393 - batch loss: 2.5368599891662598 - avg loss: 2.119968577689931   (start: 3930, end: 3940)\n",
      "Batch 394 - batch loss: 1.7349827289581299 - avg loss: 2.1189939299716225   (start: 3940, end: 3950)\n",
      "Batch 395 - batch loss: 1.699286699295044 - avg loss: 2.11793406322749   (start: 3950, end: 3960)\n",
      "Batch 396 - batch loss: 2.4277589321136475 - avg loss: 2.118714478514357   (start: 3960, end: 3970)\n",
      "Batch 397 - batch loss: 2.7875781059265137 - avg loss: 2.1203950403922764   (start: 3970, end: 3980)\n",
      "Batch 398 - batch loss: 1.8388137817382812 - avg loss: 2.1196893229520413   (start: 3980, end: 3990)\n",
      "Batch 399 - batch loss: 1.8354600667953491 - avg loss: 2.1189787498116495   (start: 3990, end: 4000)\n",
      "Batch 400 - batch loss: 2.2280306816101074 - avg loss: 2.119250699766259   (start: 4000, end: 4010)\n",
      "Batch 401 - batch loss: 2.6821670532226562 - avg loss: 2.1206509892027174   (start: 4010, end: 4020)\n",
      "Batch 402 - batch loss: 2.336185932159424 - avg loss: 2.1211858153639005   (start: 4020, end: 4030)\n",
      "Batch 403 - batch loss: 1.8333851099014282 - avg loss: 2.1204734373800824   (start: 4030, end: 4040)\n",
      "Batch 404 - batch loss: 1.6461299657821655 - avg loss: 2.1193022189316926   (start: 4040, end: 4050)\n",
      "Batch 405 - batch loss: 2.541933536529541 - avg loss: 2.1203431827681407   (start: 4050, end: 4060)\n",
      "Batch 406 - batch loss: 1.9339139461517334 - avg loss: 2.119885125675717   (start: 4060, end: 4070)\n",
      "Batch 407 - batch loss: 1.8485705852508545 - avg loss: 2.1192201390570284   (start: 4070, end: 4080)\n",
      "Batch 408 - batch loss: 1.5363447666168213 - avg loss: 2.1177950158970282   (start: 4080, end: 4090)\n",
      "Batch 409 - batch loss: 2.5179381370544434 - avg loss: 2.1187709747291192   (start: 4090, end: 4100)\n",
      "Batch 410 - batch loss: 2.9503066539764404 - avg loss: 2.120794175895171   (start: 4100, end: 4110)\n",
      "Batch 411 - batch loss: 1.7487272024154663 - avg loss: 2.1198911007168224   (start: 4110, end: 4120)\n",
      "Batch 412 - batch loss: 1.0547677278518677 - avg loss: 2.1173121094992315   (start: 4120, end: 4130)\n",
      "Batch 413 - batch loss: 2.4662623405456543 - avg loss: 2.11815498445345   (start: 4130, end: 4140)\n",
      "Batch 414 - batch loss: 1.5871553421020508 - avg loss: 2.1168754672429646   (start: 4140, end: 4150)\n",
      "Batch 415 - batch loss: 1.8482105731964111 - avg loss: 2.1162296381707373   (start: 4150, end: 4160)\n",
      "Batch 416 - batch loss: 2.238318920135498 - avg loss: 2.116522418223411   (start: 4160, end: 4170)\n",
      "Batch 417 - batch loss: 2.694631814956665 - avg loss: 2.117905455057701   (start: 4170, end: 4180)\n",
      "Batch 418 - batch loss: 2.0485401153564453 - avg loss: 2.117739905320944   (start: 4180, end: 4190)\n",
      "Batch 419 - batch loss: 3.0935416221618652 - avg loss: 2.1200632427419936   (start: 4190, end: 4200)\n",
      "Batch 420 - batch loss: 1.8060811758041382 - avg loss: 2.119317442107937   (start: 4200, end: 4210)\n",
      "Batch 421 - batch loss: 1.3200325965881348 - avg loss: 2.1174234021896434   (start: 4210, end: 4220)\n",
      "Batch 422 - batch loss: 2.8067383766174316 - avg loss: 2.119052988417605   (start: 4220, end: 4230)\n",
      "Batch 423 - batch loss: 2.0887064933776855 - avg loss: 2.118981416495341   (start: 4230, end: 4240)\n",
      "Batch 424 - batch loss: 1.9016138315200806 - avg loss: 2.1184699633542228   (start: 4240, end: 4250)\n",
      "Batch 425 - batch loss: 1.7898683547973633 - avg loss: 2.1176985980759206   (start: 4250, end: 4260)\n",
      "Batch 426 - batch loss: 1.9870096445083618 - avg loss: 2.1173925349528115   (start: 4260, end: 4270)\n",
      "Batch 427 - batch loss: 2.109362840652466 - avg loss: 2.11737377398482   (start: 4270, end: 4280)\n",
      "Batch 428 - batch loss: 2.1709933280944824 - avg loss: 2.117498761290437   (start: 4280, end: 4290)\n",
      "Batch 429 - batch loss: 2.6358590126037598 - avg loss: 2.1187042502469793   (start: 4290, end: 4300)\n",
      "Batch 430 - batch loss: 2.900728940963745 - avg loss: 2.1205186926848376   (start: 4300, end: 4310)\n",
      "Batch 431 - batch loss: 2.544670581817627 - avg loss: 2.1215005257615336   (start: 4310, end: 4320)\n",
      "Batch 432 - batch loss: 1.9071334600448608 - avg loss: 2.1210054517067607   (start: 4320, end: 4330)\n",
      "Batch 433 - batch loss: 1.6215057373046875 - avg loss: 2.1198545307058345   (start: 4330, end: 4340)\n",
      "Batch 434 - batch loss: 2.7960567474365234 - avg loss: 2.1214090185603878   (start: 4340, end: 4350)\n",
      "Batch 435 - batch loss: 2.139638900756836 - avg loss: 2.1214508302168014   (start: 4350, end: 4360)\n",
      "Batch 436 - batch loss: 1.55330228805542 - avg loss: 2.1201507191363405   (start: 4360, end: 4370)\n",
      "Batch 437 - batch loss: 3.1372783184051514 - avg loss: 2.1224729282670913   (start: 4370, end: 4380)\n",
      "Batch 438 - batch loss: 1.7688915729522705 - avg loss: 2.121667503767513   (start: 4380, end: 4390)\n",
      "Batch 439 - batch loss: 2.5431180000305176 - avg loss: 2.1226253458044746   (start: 4390, end: 4400)\n",
      "Batch 440 - batch loss: 1.8151626586914062 - avg loss: 2.121928151502631   (start: 4400, end: 4410)\n",
      "Batch 441 - batch loss: 2.950875759124756 - avg loss: 2.123803598578699   (start: 4410, end: 4420)\n",
      "Batch 442 - batch loss: 1.588775396347046 - avg loss: 2.12259585997321   (start: 4420, end: 4430)\n",
      "Batch 443 - batch loss: 2.893612861633301 - avg loss: 2.1243323847517237   (start: 4430, end: 4440)\n",
      "Batch 444 - batch loss: 2.322218179702759 - avg loss: 2.1247770719313888   (start: 4440, end: 4450)\n",
      "Batch 445 - batch loss: 1.6792103052139282 - avg loss: 2.1237780433064617   (start: 4450, end: 4460)\n",
      "Batch 446 - batch loss: 2.4143900871276855 - avg loss: 2.1244281821069566   (start: 4460, end: 4470)\n",
      "Batch 447 - batch loss: 2.179833173751831 - avg loss: 2.124551853963307   (start: 4470, end: 4480)\n",
      "Batch 448 - batch loss: 2.0633835792541504 - avg loss: 2.1244156217256474   (start: 4480, end: 4490)\n",
      "Batch 449 - batch loss: 2.974034547805786 - avg loss: 2.1263036637836032   (start: 4490, end: 4500)\n",
      "Batch 450 - batch loss: 2.012704849243164 - avg loss: 2.1260517817114515   (start: 4500, end: 4510)\n",
      "Batch 451 - batch loss: 1.7177711725234985 - avg loss: 2.1251485060274073   (start: 4510, end: 4520)\n",
      "Batch 452 - batch loss: 2.7067108154296875 - avg loss: 2.126432308034918   (start: 4520, end: 4530)\n",
      "Batch 453 - batch loss: 1.2659413814544678 - avg loss: 2.1245369535710843   (start: 4530, end: 4540)\n",
      "Batch 454 - batch loss: 2.1629796028137207 - avg loss: 2.124621442910079   (start: 4540, end: 4550)\n",
      "Batch 455 - batch loss: 1.9099862575531006 - avg loss: 2.124150751714121   (start: 4550, end: 4560)\n",
      "Batch 456 - batch loss: 1.7929292917251587 - avg loss: 2.1234259782786964   (start: 4560, end: 4570)\n",
      "Batch 457 - batch loss: 1.8635311126708984 - avg loss: 2.1228585222402514   (start: 4570, end: 4580)\n",
      "Batch 458 - batch loss: 2.298074245452881 - avg loss: 2.123240255842022   (start: 4580, end: 4590)\n",
      "Batch 459 - batch loss: 1.995675802230835 - avg loss: 2.1229629418124323   (start: 4590, end: 4600)\n",
      "Batch 460 - batch loss: 2.042409658432007 - avg loss: 2.122788205839807   (start: 4600, end: 4610)\n",
      "Batch 461 - batch loss: 1.622322678565979 - avg loss: 2.1217049471227636   (start: 4610, end: 4620)\n",
      "Batch 462 - batch loss: 1.9680187702178955 - avg loss: 2.1213730115354963   (start: 4620, end: 4630)\n",
      "Batch 463 - batch loss: 1.5500926971435547 - avg loss: 2.1201418039613755   (start: 4630, end: 4640)\n",
      "Batch 464 - batch loss: 1.5715782642364502 - avg loss: 2.118962097424333   (start: 4640, end: 4650)\n",
      "Batch 465 - batch loss: 1.49423086643219 - avg loss: 2.1176214724651223   (start: 4650, end: 4660)\n",
      "Batch 466 - batch loss: 2.045213460922241 - avg loss: 2.1174664231898697   (start: 4660, end: 4670)\n",
      "Batch 467 - batch loss: 2.6041619777679443 - avg loss: 2.1185063709560623   (start: 4670, end: 4680)\n",
      "Batch 468 - batch loss: 2.011810779571533 - avg loss: 2.118278875025605   (start: 4680, end: 4690)\n",
      "Batch 469 - batch loss: 2.0787453651428223 - avg loss: 2.11819476117479   (start: 4690, end: 4700)\n",
      "Batch 470 - batch loss: 1.888336181640625 - avg loss: 2.1177067387129345   (start: 4700, end: 4710)\n",
      "Batch 471 - batch loss: 1.714012861251831 - avg loss: 2.1168514550742454   (start: 4710, end: 4720)\n",
      "Batch 472 - batch loss: 2.2320542335510254 - avg loss: 2.1170950127454438   (start: 4720, end: 4730)\n",
      "Batch 473 - batch loss: 1.9064531326293945 - avg loss: 2.1166506205933002   (start: 4730, end: 4740)\n",
      "Batch 474 - batch loss: 2.343156337738037 - avg loss: 2.117127474734658   (start: 4740, end: 4750)\n",
      "Batch 475 - batch loss: 1.5074313879013062 - avg loss: 2.115846600602655   (start: 4750, end: 4760)\n",
      "Batch 476 - batch loss: 2.304755687713623 - avg loss: 2.1162426364246905   (start: 4760, end: 4770)\n",
      "Batch 477 - batch loss: 1.8429943323135376 - avg loss: 2.11567098725291   (start: 4770, end: 4780)\n",
      "Batch 478 - batch loss: 2.2971653938293457 - avg loss: 2.116049889980627   (start: 4780, end: 4790)\n",
      "Batch 479 - batch loss: 2.501553773880005 - avg loss: 2.1168530230720837   (start: 4790, end: 4800)\n",
      "Batch 480 - batch loss: 1.9467203617095947 - avg loss: 2.116499316915405   (start: 4800, end: 4810)\n",
      "Batch 481 - batch loss: 2.1177759170532227 - avg loss: 2.1165019654634087   (start: 4810, end: 4820)\n",
      "Batch 482 - batch loss: 2.354797840118408 - avg loss: 2.1169953316635226   (start: 4820, end: 4830)\n",
      "Batch 483 - batch loss: 2.091278553009033 - avg loss: 2.1169421978233274   (start: 4830, end: 4840)\n",
      "Batch 484 - batch loss: 2.713850975036621 - avg loss: 2.118172937570159   (start: 4840, end: 4850)\n",
      "Batch 485 - batch loss: 2.6721625328063965 - avg loss: 2.1193128338566534   (start: 4850, end: 4860)\n",
      "Batch 486 - batch loss: 1.7035980224609375 - avg loss: 2.1184592100139517   (start: 4860, end: 4870)\n",
      "Batch 487 - batch loss: 2.069981813430786 - avg loss: 2.1183598710865272   (start: 4870, end: 4880)\n",
      "Batch 488 - batch loss: 1.6679843664169312 - avg loss: 2.117438857784544   (start: 4880, end: 4890)\n",
      "Batch 489 - batch loss: 1.3051118850708008 - avg loss: 2.115781047636149   (start: 4890, end: 4900)\n",
      "Batch 490 - batch loss: 1.5982650518417358 - avg loss: 2.1147270435713943   (start: 4900, end: 4910)\n",
      "Batch 491 - batch loss: 2.0811173915863037 - avg loss: 2.114658731270612   (start: 4910, end: 4920)\n",
      "Batch 492 - batch loss: 2.2727108001708984 - avg loss: 2.114979323702458   (start: 4920, end: 4930)\n",
      "Batch 493 - batch loss: 1.4309543371200562 - avg loss: 2.1135946577377167   (start: 4930, end: 4940)\n",
      "Batch 494 - batch loss: 1.297784447669983 - avg loss: 2.111946556303236   (start: 4940, end: 4950)\n",
      "Batch 495 - batch loss: 2.2500081062316895 - avg loss: 2.1122249062022855   (start: 4950, end: 4960)\n",
      "Batch 496 - batch loss: 2.2586276531219482 - avg loss: 2.1125194791337134   (start: 4960, end: 4970)\n",
      "Batch 497 - batch loss: 2.1917786598205566 - avg loss: 2.1126786341150123   (start: 4970, end: 4980)\n",
      "Batch 498 - batch loss: 1.666364312171936 - avg loss: 2.1117842166361687   (start: 4980, end: 4990)\n",
      "Batch 499 - batch loss: 2.4517130851745605 - avg loss: 2.1124640743732455   (start: 4990, end: 5000)\n",
      "Batch 500 - batch loss: 1.7143713235855103 - avg loss: 2.1116694780642877   (start: 5000, end: 5010)\n",
      "Batch 501 - batch loss: 2.0008928775787354 - avg loss: 2.111448807545392   (start: 5010, end: 5020)\n",
      "Batch 502 - batch loss: 1.2663906812667847 - avg loss: 2.109768771509053   (start: 5020, end: 5030)\n",
      "Batch 503 - batch loss: 2.176751136779785 - avg loss: 2.1099016730274474   (start: 5030, end: 5040)\n",
      "Batch 504 - batch loss: 1.6626691818237305 - avg loss: 2.1090160641339746   (start: 5040, end: 5050)\n",
      "Batch 505 - batch loss: 1.9059898853302002 - avg loss: 2.1086148266264573   (start: 5050, end: 5060)\n",
      "Batch 506 - batch loss: 2.1017837524414062 - avg loss: 2.1086013531073546   (start: 5060, end: 5070)\n",
      "Batch 507 - batch loss: 1.8009159564971924 - avg loss: 2.1079956731927676   (start: 5070, end: 5080)\n",
      "Batch 508 - batch loss: 1.8730189800262451 - avg loss: 2.107534029394798   (start: 5080, end: 5090)\n",
      "Batch 509 - batch loss: 1.2946314811706543 - avg loss: 2.1059401028296527   (start: 5090, end: 5100)\n",
      "Batch 510 - batch loss: 1.7542855739593506 - avg loss: 2.1052519334972253   (start: 5100, end: 5110)\n",
      "Batch 511 - batch loss: 2.1581215858459473 - avg loss: 2.105355194536969   (start: 5110, end: 5120)\n",
      "Batch 512 - batch loss: 2.3017385005950928 - avg loss: 2.105738007999071   (start: 5120, end: 5130)\n",
      "Batch 513 - batch loss: 2.4181716442108154 - avg loss: 2.1063458555403387   (start: 5130, end: 5140)\n",
      "Batch 514 - batch loss: 2.2038025856018066 - avg loss: 2.10653509190939   (start: 5140, end: 5150)\n",
      "Batch 515 - batch loss: 2.5283620357513428 - avg loss: 2.1073525859866034   (start: 5150, end: 5160)\n",
      "Batch 516 - batch loss: 3.0281991958618164 - avg loss: 2.1091337206285283   (start: 5160, end: 5170)\n",
      "Batch 517 - batch loss: 1.5623023509979248 - avg loss: 2.108078061613797   (start: 5170, end: 5180)\n",
      "Batch 518 - batch loss: 2.0066018104553223 - avg loss: 2.1078825389718734   (start: 5180, end: 5190)\n",
      "Batch 519 - batch loss: 1.3522952795028687 - avg loss: 2.1064294865498177   (start: 5190, end: 5200)\n",
      "Batch 520 - batch loss: 2.349547863006592 - avg loss: 2.106896124508468   (start: 5200, end: 5210)\n",
      "Batch 521 - batch loss: 1.620547890663147 - avg loss: 2.1059644229110632   (start: 5210, end: 5220)\n",
      "Batch 522 - batch loss: 2.37982439994812 - avg loss: 2.106488055754346   (start: 5220, end: 5230)\n",
      "Batch 523 - batch loss: 2.561065435409546 - avg loss: 2.1073555698376576   (start: 5230, end: 5240)\n",
      "Batch 524 - batch loss: 2.390261173248291 - avg loss: 2.1078944376536777   (start: 5240, end: 5250)\n",
      "Batch 525 - batch loss: 2.3846263885498047 - avg loss: 2.1084205440242028   (start: 5250, end: 5260)\n",
      "Batch 526 - batch loss: 2.3030991554260254 - avg loss: 2.1087899531539973   (start: 5260, end: 5270)\n",
      "Batch 527 - batch loss: 1.9573160409927368 - avg loss: 2.108503070744601   (start: 5270, end: 5280)\n",
      "Batch 528 - batch loss: 3.2155933380126953 - avg loss: 2.110595868981403   (start: 5280, end: 5290)\n",
      "Batch 529 - batch loss: 2.243617057800293 - avg loss: 2.110846852356533   (start: 5290, end: 5300)\n",
      "Batch 530 - batch loss: 2.7352681159973145 - avg loss: 2.1120227869396606   (start: 5300, end: 5310)\n",
      "Batch 531 - batch loss: 1.8698371648788452 - avg loss: 2.111567550807967   (start: 5310, end: 5320)\n",
      "Batch 532 - batch loss: 1.8500303030014038 - avg loss: 2.111076861787692   (start: 5320, end: 5330)\n",
      "Batch 533 - batch loss: 1.7372373342514038 - avg loss: 2.110376787766089   (start: 5330, end: 5340)\n",
      "Batch 534 - batch loss: 1.391120195388794 - avg loss: 2.1090323829205238   (start: 5340, end: 5350)\n",
      "Batch 535 - batch loss: 1.7774299383163452 - avg loss: 2.1084137216432772   (start: 5350, end: 5360)\n",
      "Batch 536 - batch loss: 1.5053341388702393 - avg loss: 2.107290668416512   (start: 5360, end: 5370)\n",
      "Batch 537 - batch loss: 2.4727981090545654 - avg loss: 2.1079700502764336   (start: 5370, end: 5380)\n",
      "Batch 538 - batch loss: 2.295620918273926 - avg loss: 2.108318196599249   (start: 5380, end: 5390)\n",
      "Batch 539 - batch loss: 2.178331136703491 - avg loss: 2.1084478501920345   (start: 5390, end: 5400)\n",
      "Batch 540 - batch loss: 2.2132174968719482 - avg loss: 2.108641509428042   (start: 5400, end: 5410)\n",
      "Batch 541 - batch loss: 2.492035388946533 - avg loss: 2.1093488782094414   (start: 5410, end: 5420)\n",
      "Batch 542 - batch loss: 2.6957149505615234 - avg loss: 2.1104287420627603   (start: 5420, end: 5430)\n",
      "Batch 543 - batch loss: 1.8848588466644287 - avg loss: 2.1100140915197483   (start: 5430, end: 5440)\n",
      "Batch 544 - batch loss: 2.0757861137390137 - avg loss: 2.109951287890793   (start: 5440, end: 5450)\n",
      "Batch 545 - batch loss: 1.6765224933624268 - avg loss: 2.109157462259789   (start: 5450, end: 5460)\n",
      "Batch 546 - batch loss: 2.0015015602111816 - avg loss: 2.108960650738676   (start: 5460, end: 5470)\n",
      "Batch 547 - batch loss: 2.1252732276916504 - avg loss: 2.1089904182148675   (start: 5470, end: 5480)\n",
      "Batch 548 - batch loss: 2.468806266784668 - avg loss: 2.1096458204891295   (start: 5480, end: 5490)\n",
      "Batch 549 - batch loss: 2.7809946537017822 - avg loss: 2.1108664547313345   (start: 5490, end: 5500)\n",
      "Batch 550 - batch loss: 2.136259078979492 - avg loss: 2.1109125393488446   (start: 5500, end: 5510)\n",
      "Batch 551 - batch loss: 2.6104190349578857 - avg loss: 2.1118174424206   (start: 5510, end: 5520)\n",
      "Batch 552 - batch loss: 1.7998898029327393 - avg loss: 2.111253377973063   (start: 5520, end: 5530)\n",
      "Batch 553 - batch loss: 1.8185656070709229 - avg loss: 2.1107250606970664   (start: 5530, end: 5540)\n",
      "Batch 554 - batch loss: 2.171971082687378 - avg loss: 2.110835413889842   (start: 5540, end: 5550)\n",
      "Batch 555 - batch loss: 2.3410048484802246 - avg loss: 2.111249387693062   (start: 5550, end: 5560)\n",
      "Batch 556 - batch loss: 1.5802576541900635 - avg loss: 2.1102960811697176   (start: 5560, end: 5570)\n",
      "Batch 557 - batch loss: 1.4487380981445312 - avg loss: 2.109110493386518   (start: 5570, end: 5580)\n",
      "Batch 558 - batch loss: 2.378418207168579 - avg loss: 2.109592260316361   (start: 5580, end: 5590)\n",
      "Batch 559 - batch loss: 1.542678952217102 - avg loss: 2.1085799151233267   (start: 5590, end: 5600)\n",
      "Batch 560 - batch loss: 2.588972330093384 - avg loss: 2.1094362295885136   (start: 5600, end: 5610)\n",
      "Batch 561 - batch loss: 1.6541274785995483 - avg loss: 2.1086260716686045   (start: 5610, end: 5620)\n",
      "Batch 562 - batch loss: 2.5782132148742676 - avg loss: 2.1094601518519185   (start: 5620, end: 5630)\n",
      "Batch 563 - batch loss: 1.6887931823730469 - avg loss: 2.1087142884308565   (start: 5630, end: 5640)\n",
      "Batch 564 - batch loss: 1.94634211063385 - avg loss: 2.108426904045375   (start: 5640, end: 5650)\n",
      "Batch 565 - batch loss: 3.0535457134246826 - avg loss: 2.1100967252633596   (start: 5650, end: 5660)\n",
      "Batch 566 - batch loss: 2.42244815826416 - avg loss: 2.110647609624913   (start: 5660, end: 5670)\n",
      "Batch 567 - batch loss: 1.8349735736846924 - avg loss: 2.1101622680123424   (start: 5670, end: 5680)\n",
      "Batch 568 - batch loss: 2.1651222705841064 - avg loss: 2.110258858526528   (start: 5680, end: 5690)\n",
      "Batch 569 - batch loss: 1.175202488899231 - avg loss: 2.108618408755252   (start: 5690, end: 5700)\n",
      "Batch 570 - batch loss: 1.8131815195083618 - avg loss: 2.108101006147114   (start: 5700, end: 5710)\n",
      "Batch 571 - batch loss: 1.8010543584823608 - avg loss: 2.1075642113085395   (start: 5710, end: 5720)\n",
      "Batch 572 - batch loss: 1.5658063888549805 - avg loss: 2.106618735178603   (start: 5720, end: 5730)\n",
      "Batch 573 - batch loss: 2.0095582008361816 - avg loss: 2.1064496401710375   (start: 5730, end: 5740)\n",
      "Batch 574 - batch loss: 1.4248273372650146 - avg loss: 2.105264210079027   (start: 5740, end: 5750)\n",
      "Batch 575 - batch loss: 1.9462238550186157 - avg loss: 2.104988098351492   (start: 5750, end: 5760)\n",
      "Batch 576 - batch loss: 2.0107052326202393 - avg loss: 2.104824696504471   (start: 5760, end: 5770)\n",
      "Batch 577 - batch loss: 1.8301204442977905 - avg loss: 2.1043494296321406   (start: 5770, end: 5780)\n",
      "Batch 578 - batch loss: 1.7269929647445679 - avg loss: 2.103697691350815   (start: 5780, end: 5790)\n",
      "Batch 579 - batch loss: 1.9760301113128662 - avg loss: 2.103477574833508   (start: 5790, end: 5800)\n",
      "Batch 580 - batch loss: 2.0435681343078613 - avg loss: 2.1033744604780424   (start: 5800, end: 5810)\n",
      "Batch 581 - batch loss: 2.9458327293395996 - avg loss: 2.1048219832767736   (start: 5810, end: 5820)\n",
      "Batch 582 - batch loss: 1.8800033330917358 - avg loss: 2.1044363595200237   (start: 5820, end: 5830)\n",
      "Batch 583 - batch loss: 2.7669858932495117 - avg loss: 2.105570862146273   (start: 5830, end: 5840)\n",
      "Batch 584 - batch loss: 1.9552032947540283 - avg loss: 2.105313823569534   (start: 5840, end: 5850)\n",
      "Batch 585 - batch loss: 1.8800075054168701 - avg loss: 2.1049293417979427   (start: 5850, end: 5860)\n",
      "Batch 586 - batch loss: 1.8296865224838257 - avg loss: 2.104460444320406   (start: 5860, end: 5870)\n",
      "Batch 587 - batch loss: 1.604612946510315 - avg loss: 2.1036103635418173   (start: 5870, end: 5880)\n",
      "Batch 588 - batch loss: 2.1394193172454834 - avg loss: 2.103671159728071   (start: 5880, end: 5890)\n",
      "Batch 589 - batch loss: 1.0198235511779785 - avg loss: 2.101834129883071   (start: 5890, end: 5900)\n",
      "Batch 590 - batch loss: 1.4378055334091187 - avg loss: 2.1007105620379374   (start: 5900, end: 5910)\n",
      "Batch 591 - batch loss: 2.3082070350646973 - avg loss: 2.101061062836969   (start: 5910, end: 5920)\n",
      "Batch 592 - batch loss: 1.5585486888885498 - avg loss: 2.100146202172638   (start: 5920, end: 5930)\n",
      "Batch 593 - batch loss: 1.9411468505859375 - avg loss: 2.0998785264965663   (start: 5930, end: 5940)\n",
      "Batch 594 - batch loss: 1.986196756362915 - avg loss: 2.099687464698022   (start: 5940, end: 5950)\n",
      "Batch 595 - batch loss: 1.5806989669799805 - avg loss: 2.0988166786280256   (start: 5950, end: 5960)\n",
      "Batch 596 - batch loss: 2.612764596939087 - avg loss: 2.0996775629133038   (start: 5960, end: 5970)\n",
      "Batch 597 - batch loss: 2.892712116241455 - avg loss: 2.1010037076513104   (start: 5970, end: 5980)\n",
      "Batch 598 - batch loss: 1.4880952835083008 - avg loss: 2.0999804882453956   (start: 5980, end: 5990)\n",
      "Batch 599 - batch loss: 2.7575716972351074 - avg loss: 2.1010764735937117   (start: 5990, end: 6000)\n",
      "Batch 600 - batch loss: 2.361297130584717 - avg loss: 2.1015094530562592   (start: 6000, end: 6010)\n",
      "Batch 601 - batch loss: 2.042611598968506 - avg loss: 2.101411616089336   (start: 6010, end: 6020)\n",
      "Batch 602 - batch loss: 2.1322805881500244 - avg loss: 2.101462808414478   (start: 6020, end: 6030)\n",
      "Batch 603 - batch loss: 2.2156271934509277 - avg loss: 2.101651822296989   (start: 6030, end: 6040)\n",
      "Batch 604 - batch loss: 2.9516425132751465 - avg loss: 2.103056765587862   (start: 6040, end: 6050)\n",
      "Batch 605 - batch loss: 2.179521322250366 - avg loss: 2.1031829447242685   (start: 6050, end: 6060)\n",
      "Batch 606 - batch loss: 1.4831041097640991 - avg loss: 2.102161398043939   (start: 6060, end: 6070)\n",
      "Batch 607 - batch loss: 1.0552082061767578 - avg loss: 2.1004394355573153   (start: 6070, end: 6080)\n",
      "Batch 608 - batch loss: 1.7439072132110596 - avg loss: 2.0998539967685694   (start: 6080, end: 6090)\n",
      "Batch 609 - batch loss: 1.8046026229858398 - avg loss: 2.0993699781230237   (start: 6090, end: 6100)\n",
      "Batch 610 - batch loss: 1.1484084129333496 - avg loss: 2.0978135762160033   (start: 6100, end: 6110)\n",
      "Batch 611 - batch loss: 1.3424755334854126 - avg loss: 2.0965793637278813   (start: 6110, end: 6120)\n",
      "Batch 612 - batch loss: 1.6825376749038696 - avg loss: 2.0959039286727035   (start: 6120, end: 6130)\n",
      "Batch 613 - batch loss: 2.771101474761963 - avg loss: 2.097003598943207   (start: 6130, end: 6140)\n",
      "Batch 614 - batch loss: 2.3603646755218506 - avg loss: 2.097431828336018   (start: 6140, end: 6150)\n",
      "Batch 615 - batch loss: 2.889538049697876 - avg loss: 2.0987177150590077   (start: 6150, end: 6160)\n",
      "Batch 616 - batch loss: 2.485649585723877 - avg loss: 2.0993448331638134   (start: 6160, end: 6170)\n",
      "Batch 617 - batch loss: 1.923919916152954 - avg loss: 2.0990609740747987   (start: 6170, end: 6180)\n",
      "Batch 618 - batch loss: 2.1648306846618652 - avg loss: 2.0991672256266356   (start: 6180, end: 6190)\n",
      "Batch 619 - batch loss: 1.6986751556396484 - avg loss: 2.098521270675044   (start: 6190, end: 6200)\n",
      "Batch 620 - batch loss: 2.552493095397949 - avg loss: 2.0992523042092195   (start: 6200, end: 6210)\n",
      "Batch 621 - batch loss: 1.8913202285766602 - avg loss: 2.0989180082676877   (start: 6210, end: 6220)\n",
      "Batch 622 - batch loss: 1.9278154373168945 - avg loss: 2.098643365296659   (start: 6220, end: 6230)\n",
      "Batch 623 - batch loss: 2.719127655029297 - avg loss: 2.09963773114559   (start: 6230, end: 6240)\n",
      "Batch 624 - batch loss: 2.5064644813537598 - avg loss: 2.1002886539459227   (start: 6240, end: 6250)\n",
      "Batch 625 - batch loss: 2.0419836044311523 - avg loss: 2.10019551488919   (start: 6250, end: 6260)\n",
      "Batch 626 - batch loss: 1.9044538736343384 - avg loss: 2.099883327263584   (start: 6260, end: 6270)\n",
      "Batch 627 - batch loss: 2.310904026031494 - avg loss: 2.1002193474845523   (start: 6270, end: 6280)\n",
      "Batch 628 - batch loss: 2.0381903648376465 - avg loss: 2.1001207322498194   (start: 6280, end: 6290)\n",
      "Batch 629 - batch loss: 2.9107553958892822 - avg loss: 2.101407453938136   (start: 6290, end: 6300)\n",
      "Batch 630 - batch loss: 2.750044345855713 - avg loss: 2.1024354046384808   (start: 6300, end: 6310)\n",
      "Batch 631 - batch loss: 1.634418249130249 - avg loss: 2.1016948711645753   (start: 6310, end: 6320)\n",
      "Batch 632 - batch loss: 1.943193793296814 - avg loss: 2.101444474517075   (start: 6320, end: 6330)\n",
      "Batch 633 - batch loss: 2.068478584289551 - avg loss: 2.101392477844792   (start: 6330, end: 6340)\n",
      "Batch 634 - batch loss: 2.303365707397461 - avg loss: 2.1017105459228276   (start: 6340, end: 6350)\n",
      "Batch 635 - batch loss: 2.2941434383392334 - avg loss: 2.102013113363734   (start: 6350, end: 6360)\n",
      "Batch 636 - batch loss: 2.0882837772369385 - avg loss: 2.1019915602457955   (start: 6360, end: 6370)\n",
      "Batch 637 - batch loss: 1.564195990562439 - avg loss: 2.1011486204814016   (start: 6370, end: 6380)\n",
      "Batch 638 - batch loss: 2.124124050140381 - avg loss: 2.1011845757703824   (start: 6380, end: 6390)\n",
      "Batch 639 - batch loss: 1.9341068267822266 - avg loss: 2.1009235167875886   (start: 6390, end: 6400)\n",
      "Batch 640 - batch loss: 2.3992621898651123 - avg loss: 2.1013889437346673   (start: 6400, end: 6410)\n",
      "Batch 641 - batch loss: 3.031480073928833 - avg loss: 2.1028376838128513   (start: 6410, end: 6420)\n",
      "Batch 642 - batch loss: 1.3261792659759521 - avg loss: 2.101629816911083   (start: 6420, end: 6430)\n",
      "Batch 643 - batch loss: 2.5525503158569336 - avg loss: 2.1023300040212476   (start: 6430, end: 6440)\n",
      "Batch 644 - batch loss: 1.7752500772476196 - avg loss: 2.101822903359583   (start: 6440, end: 6450)\n",
      "Batch 645 - batch loss: 3.2186005115509033 - avg loss: 2.103551661267   (start: 6450, end: 6460)\n",
      "Batch 646 - batch loss: 2.4187209606170654 - avg loss: 2.1040387853772784   (start: 6460, end: 6470)\n",
      "Batch 647 - batch loss: 2.551112651824951 - avg loss: 2.104728714183525   (start: 6470, end: 6480)\n",
      "Batch 648 - batch loss: 2.013550281524658 - avg loss: 2.1045882235322786   (start: 6480, end: 6490)\n",
      "Batch 649 - batch loss: 1.8251796960830688 - avg loss: 2.1041583642592796   (start: 6490, end: 6500)\n",
      "Batch 650 - batch loss: 1.9152462482452393 - avg loss: 2.103868176677077   (start: 6500, end: 6510)\n",
      "Batch 651 - batch loss: 2.378525733947754 - avg loss: 2.104289430599271   (start: 6510, end: 6520)\n",
      "Batch 652 - batch loss: 2.7134249210357666 - avg loss: 2.1052222567714556   (start: 6520, end: 6530)\n",
      "Batch 653 - batch loss: 1.1052162647247314 - avg loss: 2.103693195621537   (start: 6530, end: 6540)\n",
      "Batch 654 - batch loss: 2.467133045196533 - avg loss: 2.1042480656208884   (start: 6540, end: 6550)\n",
      "Batch 655 - batch loss: 2.0614678859710693 - avg loss: 2.1041828519323977   (start: 6550, end: 6560)\n",
      "Batch 656 - batch loss: 2.295163154602051 - avg loss: 2.104473537324589   (start: 6560, end: 6570)\n",
      "Batch 657 - batch loss: 1.7474454641342163 - avg loss: 2.1039309414686764   (start: 6570, end: 6580)\n",
      "Batch 658 - batch loss: 1.423108458518982 - avg loss: 2.102897826927023   (start: 6580, end: 6590)\n",
      "Batch 659 - batch loss: 2.1859333515167236 - avg loss: 2.1030236383279166   (start: 6590, end: 6600)\n",
      "Batch 660 - batch loss: 2.1137893199920654 - avg loss: 2.103039925289587   (start: 6600, end: 6610)\n",
      "Batch 661 - batch loss: 1.842170000076294 - avg loss: 2.102645861958449   (start: 6610, end: 6620)\n",
      "Batch 662 - batch loss: 2.5160508155822754 - avg loss: 2.103269398841743   (start: 6620, end: 6630)\n",
      "Batch 663 - batch loss: 3.1688499450683594 - avg loss: 2.104874188821   (start: 6630, end: 6640)\n",
      "Batch 664 - batch loss: 2.130216360092163 - avg loss: 2.104912297349227   (start: 6640, end: 6650)\n",
      "Batch 665 - batch loss: 2.680126667022705 - avg loss: 2.1057759825889772   (start: 6650, end: 6660)\n",
      "Batch 666 - batch loss: 2.838650703430176 - avg loss: 2.1068747452888887   (start: 6660, end: 6670)\n",
      "Batch 667 - batch loss: 2.6556615829467773 - avg loss: 2.1076962824710117   (start: 6670, end: 6680)\n",
      "Batch 668 - batch loss: 1.4514667987823486 - avg loss: 2.106715371434108   (start: 6680, end: 6690)\n",
      "Batch 669 - batch loss: 2.1446659564971924 - avg loss: 2.106772014098381   (start: 6690, end: 6700)\n",
      "Batch 670 - batch loss: 2.581814765930176 - avg loss: 2.1074799764707084   (start: 6700, end: 6710)\n",
      "Batch 671 - batch loss: 2.192871332168579 - avg loss: 2.107607046940497   (start: 6710, end: 6720)\n",
      "Batch 672 - batch loss: 2.3863072395324707 - avg loss: 2.1080211631256263   (start: 6720, end: 6730)\n",
      "Batch 673 - batch loss: 1.2170732021331787 - avg loss: 2.106699281877863   (start: 6730, end: 6740)\n",
      "Batch 674 - batch loss: 1.6269989013671875 - avg loss: 2.105988614647477   (start: 6740, end: 6750)\n",
      "Batch 675 - batch loss: 1.8572514057159424 - avg loss: 2.1056206601963945   (start: 6750, end: 6760)\n",
      "Batch 676 - batch loss: 2.016306161880493 - avg loss: 2.1054887333155734   (start: 6760, end: 6770)\n",
      "Batch 677 - batch loss: 2.2492520809173584 - avg loss: 2.1057007736512694   (start: 6770, end: 6780)\n",
      "Batch 678 - batch loss: 2.1843769550323486 - avg loss: 2.1058166443160427   (start: 6780, end: 6790)\n",
      "Batch 679 - batch loss: 2.3565449714660645 - avg loss: 2.1061853624442044   (start: 6790, end: 6800)\n",
      "Batch 680 - batch loss: 2.189197063446045 - avg loss: 2.106307259215132   (start: 6800, end: 6810)\n",
      "Batch 681 - batch loss: 1.7509199380874634 - avg loss: 2.1057861634363526   (start: 6810, end: 6820)\n",
      "Batch 682 - batch loss: 1.9694643020629883 - avg loss: 2.105586570667138   (start: 6820, end: 6830)\n",
      "Batch 683 - batch loss: 1.353346347808838 - avg loss: 2.1044868042594507   (start: 6830, end: 6840)\n",
      "Batch 684 - batch loss: 2.0088257789611816 - avg loss: 2.1043471531276285   (start: 6840, end: 6850)\n",
      "Batch 685 - batch loss: 2.0382587909698486 - avg loss: 2.104250814407282   (start: 6850, end: 6860)\n",
      "Batch 686 - batch loss: 1.9990737438201904 - avg loss: 2.104097718234666   (start: 6860, end: 6870)\n",
      "Batch 687 - batch loss: 2.091276168823242 - avg loss: 2.104079082261684   (start: 6870, end: 6880)\n",
      "Batch 688 - batch loss: 1.244677186012268 - avg loss: 2.1028317645603063   (start: 6880, end: 6890)\n",
      "Batch 689 - batch loss: 1.9532558917999268 - avg loss: 2.1026149879331175   (start: 6890, end: 6900)\n",
      "Batch 690 - batch loss: 2.102423906326294 - avg loss: 2.1026147114040192   (start: 6900, end: 6910)\n",
      "Batch 691 - batch loss: 1.8931926488876343 - avg loss: 2.102312078365701   (start: 6910, end: 6920)\n",
      "Batch 692 - batch loss: 1.3084309101104736 - avg loss: 2.101166506694337   (start: 6920, end: 6930)\n",
      "Batch 693 - batch loss: 2.605044364929199 - avg loss: 2.101892555481419   (start: 6930, end: 6940)\n",
      "Batch 694 - batch loss: 2.5158190727233887 - avg loss: 2.1024881332040692   (start: 6940, end: 6950)\n",
      "Batch 695 - batch loss: 1.7110378742218018 - avg loss: 2.1019257046710487   (start: 6950, end: 6960)\n",
      "Batch 696 - batch loss: 2.1176533699035645 - avg loss: 2.101948269470521   (start: 6960, end: 6970)\n",
      "Batch 697 - batch loss: 1.7618175745010376 - avg loss: 2.1014609762112526   (start: 6970, end: 6980)\n",
      "Batch 698 - batch loss: 1.7386834621429443 - avg loss: 2.1009419811982797   (start: 6980, end: 6990)\n",
      "Batch 699 - batch loss: 1.7983064651489258 - avg loss: 2.1005096447467806   (start: 6990, end: 7000)\n",
      "Batch 700 - batch loss: 1.7579257488250732 - avg loss: 2.100020937334624   (start: 7000, end: 7010)\n",
      "Batch 701 - batch loss: 2.177354097366333 - avg loss: 2.1001310985312505   (start: 7010, end: 7020)\n",
      "Batch 702 - batch loss: 2.2219011783599854 - avg loss: 2.1003043134385457   (start: 7020, end: 7030)\n",
      "Batch 703 - batch loss: 1.4142889976501465 - avg loss: 2.0993298598649828   (start: 7030, end: 7040)\n",
      "Batch 704 - batch loss: 1.8524309396743774 - avg loss: 2.09897964863067   (start: 7040, end: 7050)\n",
      "Batch 705 - batch loss: 1.3262406587600708 - avg loss: 2.0978851174835444   (start: 7050, end: 7060)\n",
      "Batch 706 - batch loss: 2.0795767307281494 - avg loss: 2.0978592216041165   (start: 7060, end: 7070)\n",
      "Batch 707 - batch loss: 2.101083517074585 - avg loss: 2.097863775693764   (start: 7070, end: 7080)\n",
      "Batch 708 - batch loss: 1.7804409265518188 - avg loss: 2.097416070687922   (start: 7080, end: 7090)\n",
      "Batch 709 - batch loss: 1.5068293809890747 - avg loss: 2.09658425844891   (start: 7090, end: 7100)\n",
      "Batch 710 - batch loss: 2.5183310508728027 - avg loss: 2.0971774325592105   (start: 7100, end: 7110)\n",
      "Batch 711 - batch loss: 2.0900819301605225 - avg loss: 2.097167466965954   (start: 7110, end: 7120)\n",
      "Batch 712 - batch loss: 1.5350767374038696 - avg loss: 2.0963791209216875   (start: 7120, end: 7130)\n",
      "Batch 713 - batch loss: 1.616098165512085 - avg loss: 2.0957064585191527   (start: 7130, end: 7140)\n",
      "Batch 714 - batch loss: 1.6958866119384766 - avg loss: 2.0951472699225366   (start: 7140, end: 7150)\n",
      "Batch 715 - batch loss: 2.381260871887207 - avg loss: 2.0955468699252804   (start: 7150, end: 7160)\n",
      "Batch 716 - batch loss: 2.7699246406555176 - avg loss: 2.0964874246961736   (start: 7160, end: 7170)\n",
      "Batch 717 - batch loss: 2.5129921436309814 - avg loss: 2.0970675148339657   (start: 7170, end: 7180)\n",
      "Batch 718 - batch loss: 2.4943318367004395 - avg loss: 2.097620038230164   (start: 7180, end: 7190)\n",
      "Batch 719 - batch loss: 2.850980043411255 - avg loss: 2.098666371570693   (start: 7190, end: 7200)\n",
      "Batch 720 - batch loss: 1.366701364517212 - avg loss: 2.0976511635165274   (start: 7200, end: 7210)\n",
      "Batch 721 - batch loss: 2.174793243408203 - avg loss: 2.097758008502527   (start: 7210, end: 7220)\n",
      "Batch 722 - batch loss: 1.4998613595962524 - avg loss: 2.0969310421831544   (start: 7220, end: 7230)\n",
      "Batch 723 - batch loss: 2.104483127593994 - avg loss: 2.096941473240352   (start: 7230, end: 7240)\n",
      "Batch 724 - batch loss: 2.8376166820526123 - avg loss: 2.097963094218024   (start: 7240, end: 7250)\n",
      "Batch 725 - batch loss: 2.2223381996154785 - avg loss: 2.098134409790197   (start: 7250, end: 7260)\n",
      "Batch 726 - batch loss: 2.180454730987549 - avg loss: 2.098247642694182   (start: 7260, end: 7270)\n",
      "Batch 727 - batch loss: 2.3052635192871094 - avg loss: 2.0985320051620295   (start: 7270, end: 7280)\n",
      "Batch 728 - batch loss: 1.7819887399673462 - avg loss: 2.098097789434739   (start: 7280, end: 7290)\n",
      "Batch 729 - batch loss: 2.778627634048462 - avg loss: 2.0990300220985936   (start: 7290, end: 7300)\n",
      "Batch 730 - batch loss: 2.2039573192596436 - avg loss: 2.099173561492795   (start: 7300, end: 7310)\n",
      "Batch 731 - batch loss: 1.2717739343643188 - avg loss: 2.0980432341333297   (start: 7310, end: 7320)\n",
      "Batch 732 - batch loss: 1.7508083581924438 - avg loss: 2.097569516703669   (start: 7320, end: 7330)\n",
      "Batch 733 - batch loss: 2.130174398422241 - avg loss: 2.0976139375234495   (start: 7330, end: 7340)\n",
      "Batch 734 - batch loss: 2.633509874343872 - avg loss: 2.0983430476415723   (start: 7340, end: 7350)\n",
      "Batch 735 - batch loss: 1.704655408859253 - avg loss: 2.0978081459584446   (start: 7350, end: 7360)\n",
      "Batch 736 - batch loss: 1.6728826761245728 - avg loss: 2.0972315849410306   (start: 7360, end: 7370)\n",
      "Batch 737 - batch loss: 1.7226423025131226 - avg loss: 2.096724011387605   (start: 7370, end: 7380)\n",
      "Batch 738 - batch loss: 1.67862868309021 - avg loss: 2.096158253162575   (start: 7380, end: 7390)\n",
      "Batch 739 - batch loss: 3.8169169425964355 - avg loss: 2.098483602742891   (start: 7390, end: 7400)\n",
      "Batch 740 - batch loss: 1.7352221012115479 - avg loss: 2.09799337129683   (start: 7400, end: 7410)\n",
      "Batch 741 - batch loss: 1.1442985534667969 - avg loss: 2.0967080683078407   (start: 7410, end: 7420)\n",
      "Batch 742 - batch loss: 2.777033567428589 - avg loss: 2.0976237150092145   (start: 7420, end: 7430)\n",
      "Batch 743 - batch loss: 2.2190239429473877 - avg loss: 2.097786887358594   (start: 7430, end: 7440)\n",
      "Batch 744 - batch loss: 2.3330981731414795 - avg loss: 2.09810274143347   (start: 7440, end: 7450)\n",
      "Batch 745 - batch loss: 1.8149646520614624 - avg loss: 2.0977231997587085   (start: 7450, end: 7460)\n",
      "Batch 746 - batch loss: 1.2126171588897705 - avg loss: 2.0965383188472373   (start: 7460, end: 7470)\n",
      "Batch 747 - batch loss: 3.5026040077209473 - avg loss: 2.0984180858109722   (start: 7470, end: 7480)\n",
      "Batch 748 - batch loss: 3.5513274669647217 - avg loss: 2.10035788471772   (start: 7480, end: 7490)\n",
      "Batch 749 - batch loss: 2.4154489040374756 - avg loss: 2.100778006076813   (start: 7490, end: 7500)\n",
      "Batch 750 - batch loss: 1.7820247411727905 - avg loss: 2.100353567641521   (start: 7500, end: 7510)\n",
      "Batch 751 - batch loss: 1.2882812023162842 - avg loss: 2.099273684176993   (start: 7510, end: 7520)\n",
      "Batch 752 - batch loss: 2.35878324508667 - avg loss: 2.0996183183880284   (start: 7520, end: 7530)\n",
      "Batch 753 - batch loss: 2.4252285957336426 - avg loss: 2.1000501622571868   (start: 7530, end: 7540)\n",
      "Batch 754 - batch loss: 2.4286909103393555 - avg loss: 2.100485448016236   (start: 7540, end: 7550)\n",
      "Batch 755 - batch loss: 2.247514247894287 - avg loss: 2.1006799305557573   (start: 7550, end: 7560)\n",
      "Batch 756 - batch loss: 2.317671537399292 - avg loss: 2.100966577328338   (start: 7560, end: 7570)\n",
      "Batch 757 - batch loss: 1.2640149593353271 - avg loss: 2.099862419520959   (start: 7570, end: 7580)\n",
      "Batch 758 - batch loss: 1.6555519104003906 - avg loss: 2.0992770301808794   (start: 7580, end: 7590)\n",
      "Batch 759 - batch loss: 2.1863417625427246 - avg loss: 2.0993915890392505   (start: 7590, end: 7600)\n",
      "Batch 760 - batch loss: 2.658851385116577 - avg loss: 2.1001267530288397   (start: 7600, end: 7610)\n",
      "Batch 761 - batch loss: 2.909738302230835 - avg loss: 2.1011892353768737   (start: 7610, end: 7620)\n",
      "Batch 762 - batch loss: 2.8973145484924316 - avg loss: 2.1022326499419006   (start: 7620, end: 7630)\n",
      "Batch 763 - batch loss: 1.5942299365997314 - avg loss: 2.101567724924437   (start: 7630, end: 7640)\n",
      "Batch 764 - batch loss: 2.5827574729919434 - avg loss: 2.102196731131061   (start: 7640, end: 7650)\n",
      "Batch 765 - batch loss: 1.1333811283111572 - avg loss: 2.10093195880362   (start: 7650, end: 7660)\n",
      "Batch 766 - batch loss: 2.0451619625091553 - avg loss: 2.1008592469440446   (start: 7660, end: 7670)\n",
      "Batch 767 - batch loss: 1.7013193368911743 - avg loss: 2.100339012686163   (start: 7670, end: 7680)\n",
      "Batch 768 - batch loss: 2.0987319946289062 - avg loss: 2.100336922935764   (start: 7680, end: 7690)\n",
      "Batch 769 - batch loss: 1.4326971769332886 - avg loss: 2.0994698583305658   (start: 7690, end: 7700)\n",
      "Batch 770 - batch loss: 2.847264289855957 - avg loss: 2.1004397603169798   (start: 7700, end: 7710)\n",
      "Batch 771 - batch loss: 1.4438753128051758 - avg loss: 2.0995892882347107   (start: 7710, end: 7720)\n",
      "Batch 772 - batch loss: 2.928642749786377 - avg loss: 2.1006618024152433   (start: 7720, end: 7730)\n",
      "Batch 773 - batch loss: 2.051999568939209 - avg loss: 2.100598931312561   (start: 7730, end: 7740)\n",
      "Batch 774 - batch loss: 1.583794355392456 - avg loss: 2.0999320866984705   (start: 7740, end: 7750)\n",
      "Batch 775 - batch loss: 2.8034708499908447 - avg loss: 2.1008387088161156   (start: 7750, end: 7760)\n",
      "Batch 776 - batch loss: 2.2831571102142334 - avg loss: 2.101073352833359   (start: 7760, end: 7770)\n",
      "Batch 777 - batch loss: 1.2912530899047852 - avg loss: 2.100032452752474   (start: 7770, end: 7780)\n",
      "Batch 778 - batch loss: 2.1241135597229004 - avg loss: 2.100063365598392   (start: 7780, end: 7790)\n",
      "Batch 779 - batch loss: 1.45749831199646 - avg loss: 2.0992395642476205   (start: 7790, end: 7800)\n",
      "Batch 780 - batch loss: 2.031243085861206 - avg loss: 2.099152500895013   (start: 7800, end: 7810)\n",
      "Batch 781 - batch loss: 1.9006004333496094 - avg loss: 2.0988985979953387   (start: 7810, end: 7820)\n",
      "Batch 782 - batch loss: 1.9959548711776733 - avg loss: 2.098767124525584   (start: 7820, end: 7830)\n",
      "Batch 783 - batch loss: 1.8252235651016235 - avg loss: 2.098418216924278   (start: 7830, end: 7840)\n",
      "Batch 784 - batch loss: 2.3638272285461426 - avg loss: 2.0987563175760258   (start: 7840, end: 7850)\n",
      "Batch 785 - batch loss: 2.1778922080993652 - avg loss: 2.0988569993705846   (start: 7850, end: 7860)\n",
      "Batch 786 - batch loss: 1.4659069776535034 - avg loss: 2.098052742672088   (start: 7860, end: 7870)\n",
      "Batch 787 - batch loss: 1.4888803958892822 - avg loss: 2.0972796813183026   (start: 7870, end: 7880)\n",
      "Batch 788 - batch loss: 1.5197699069976807 - avg loss: 2.096547729766565   (start: 7880, end: 7890)\n",
      "Batch 789 - batch loss: 2.8768234252929688 - avg loss: 2.097535420520396   (start: 7890, end: 7900)\n",
      "Batch 790 - batch loss: 1.4463220834732056 - avg loss: 2.0967121419653427   (start: 7900, end: 7910)\n",
      "Batch 791 - batch loss: 2.356105327606201 - avg loss: 2.097039658613879   (start: 7910, end: 7920)\n",
      "Batch 792 - batch loss: 1.9765714406967163 - avg loss: 2.0968877440893934   (start: 7920, end: 7930)\n",
      "Batch 793 - batch loss: 2.3070147037506104 - avg loss: 2.0971523876154152   (start: 7930, end: 7940)\n",
      "Batch 794 - batch loss: 2.4284470081329346 - avg loss: 2.0975691104085192   (start: 7940, end: 7950)\n",
      "Batch 795 - batch loss: 2.6478965282440186 - avg loss: 2.0982604765113275   (start: 7950, end: 7960)\n",
      "Batch 796 - batch loss: 2.4232990741729736 - avg loss: 2.0986683041119067   (start: 7960, end: 7970)\n",
      "Batch 797 - batch loss: 1.7235580682754517 - avg loss: 2.098198241159731   (start: 7970, end: 7980)\n",
      "Batch 798 - batch loss: 2.04975962638855 - avg loss: 2.098137617111206   (start: 7980, end: 7990)\n",
      "Batch 799 - batch loss: 2.05966854095459 - avg loss: 2.09808953076601   (start: 7990, end: 8000)\n",
      "Batch 800 - batch loss: 1.9209506511688232 - avg loss: 2.0978683836004706   (start: 8000, end: 8010)\n",
      "Batch 801 - batch loss: 2.5981781482696533 - avg loss: 2.0984922112372155   (start: 8010, end: 8020)\n",
      "Batch 802 - batch loss: 1.9387271404266357 - avg loss: 2.098293250999593   (start: 8020, end: 8030)\n",
      "Batch 803 - batch loss: 2.8000009059906006 - avg loss: 2.099166021714756   (start: 8030, end: 8040)\n",
      "Batch 804 - batch loss: 1.0677921772003174 - avg loss: 2.0978848119700175   (start: 8040, end: 8050)\n",
      "Batch 805 - batch loss: 2.41619873046875 - avg loss: 2.0982797423899915   (start: 8050, end: 8060)\n",
      "Batch 806 - batch loss: 1.7902806997299194 - avg loss: 2.0978980831054064   (start: 8060, end: 8070)\n",
      "Batch 807 - batch loss: 1.2668825387954712 - avg loss: 2.0968695985208643   (start: 8070, end: 8080)\n",
      "Batch 808 - batch loss: 2.293294668197632 - avg loss: 2.097112398359773   (start: 8080, end: 8090)\n",
      "Batch 809 - batch loss: 1.6839224100112915 - avg loss: 2.096602287263046   (start: 8090, end: 8100)\n",
      "Batch 810 - batch loss: 1.590759515762329 - avg loss: 2.095978560047879   (start: 8100, end: 8110)\n",
      "Batch 811 - batch loss: 1.6322734355926514 - avg loss: 2.095407494623673   (start: 8110, end: 8120)\n",
      "Batch 812 - batch loss: 2.7250208854675293 - avg loss: 2.096181926838733   (start: 8120, end: 8130)\n",
      "Batch 813 - batch loss: 1.259426474571228 - avg loss: 2.095153971737667   (start: 8130, end: 8140)\n",
      "Batch 814 - batch loss: 1.7047500610351562 - avg loss: 2.094674948534351   (start: 8140, end: 8150)\n",
      "Batch 815 - batch loss: 2.7765297889709473 - avg loss: 2.095510554956455   (start: 8150, end: 8160)\n",
      "Batch 816 - batch loss: 1.988098382949829 - avg loss: 2.09537908350969   (start: 8160, end: 8170)\n",
      "Batch 817 - batch loss: 2.6333000659942627 - avg loss: 2.096036688622752   (start: 8170, end: 8180)\n",
      "Batch 818 - batch loss: 1.5334222316741943 - avg loss: 2.095349735683865   (start: 8180, end: 8190)\n",
      "Batch 819 - batch loss: 1.369762659072876 - avg loss: 2.094464873395315   (start: 8190, end: 8200)\n",
      "Batch 820 - batch loss: 1.8644561767578125 - avg loss: 2.094184716639362   (start: 8200, end: 8210)\n",
      "Batch 821 - batch loss: 1.3687549829483032 - avg loss: 2.0933021987151634   (start: 8210, end: 8220)\n",
      "Batch 822 - batch loss: 2.32707142829895 - avg loss: 2.093586243951596   (start: 8220, end: 8230)\n",
      "Batch 823 - batch loss: 1.4961895942687988 - avg loss: 2.0928612480175146   (start: 8230, end: 8240)\n",
      "Batch 824 - batch loss: 3.027970314025879 - avg loss: 2.0939947135520702   (start: 8240, end: 8250)\n",
      "Batch 825 - batch loss: 2.6648168563842773 - avg loss: 2.094685781521601   (start: 8250, end: 8260)\n",
      "Batch 826 - batch loss: 2.7925937175750732 - avg loss: 2.09552968470909   (start: 8260, end: 8270)\n",
      "Batch 827 - batch loss: 2.784748077392578 - avg loss: 2.096362074072234   (start: 8270, end: 8280)\n",
      "Batch 828 - batch loss: 1.3809890747070312 - avg loss: 2.0954991392117215   (start: 8280, end: 8290)\n",
      "Batch 829 - batch loss: 1.918937087059021 - avg loss: 2.095286413847682   (start: 8290, end: 8300)\n",
      "Batch 830 - batch loss: 1.6183435916900635 - avg loss: 2.0947124754335333   (start: 8300, end: 8310)\n",
      "Batch 831 - batch loss: 1.5710303783416748 - avg loss: 2.094083049836067   (start: 8310, end: 8320)\n",
      "Batch 832 - batch loss: 1.9400218725204468 - avg loss: 2.0938981024443315   (start: 8320, end: 8330)\n",
      "Batch 833 - batch loss: 1.9925228357315063 - avg loss: 2.0937765493667384   (start: 8330, end: 8340)\n",
      "Batch 834 - batch loss: 1.7955360412597656 - avg loss: 2.0934193751055323   (start: 8340, end: 8350)\n",
      "Batch 835 - batch loss: 2.181711196899414 - avg loss: 2.0935249873325583   (start: 8350, end: 8360)\n",
      "Batch 836 - batch loss: 1.8940722942352295 - avg loss: 2.093286692597675   (start: 8360, end: 8370)\n",
      "Batch 837 - batch loss: 1.1532886028289795 - avg loss: 2.0921649765000994   (start: 8370, end: 8380)\n",
      "Batch 838 - batch loss: 1.377659559249878 - avg loss: 2.0913133609849024   (start: 8380, end: 8390)\n",
      "Batch 839 - batch loss: 2.078834056854248 - avg loss: 2.091298504670461   (start: 8390, end: 8400)\n",
      "Batch 840 - batch loss: 2.3053030967712402 - avg loss: 2.091552969108155   (start: 8400, end: 8410)\n",
      "Batch 841 - batch loss: 2.054015874862671 - avg loss: 2.0915083882361296   (start: 8410, end: 8420)\n",
      "Batch 842 - batch loss: 1.2695963382720947 - avg loss: 2.0905334035979752   (start: 8420, end: 8430)\n",
      "Batch 843 - batch loss: 1.6171953678131104 - avg loss: 2.089972576541358   (start: 8430, end: 8440)\n",
      "Batch 844 - batch loss: 2.3606390953063965 - avg loss: 2.090292891948181   (start: 8440, end: 8450)\n",
      "Batch 845 - batch loss: 2.7727386951446533 - avg loss: 2.0910995654744178   (start: 8450, end: 8460)\n",
      "Batch 846 - batch loss: 0.963772177696228 - avg loss: 2.0897686004357188   (start: 8460, end: 8470)\n",
      "Batch 847 - batch loss: 1.4632527828216553 - avg loss: 2.0890297846130603   (start: 8470, end: 8480)\n",
      "Batch 848 - batch loss: 2.140481472015381 - avg loss: 2.0890903873072917   (start: 8480, end: 8490)\n",
      "Batch 849 - batch loss: 1.9954512119293213 - avg loss: 2.088980223571553   (start: 8490, end: 8500)\n",
      "Batch 850 - batch loss: 2.196415662765503 - avg loss: 2.089106469681064   (start: 8500, end: 8510)\n",
      "Batch 851 - batch loss: 1.712462067604065 - avg loss: 2.0886643987866074   (start: 8510, end: 8520)\n",
      "Batch 852 - batch loss: 2.3277525901794434 - avg loss: 2.088944689749553   (start: 8520, end: 8530)\n",
      "Batch 853 - batch loss: 1.9123027324676514 - avg loss: 2.08873784905016   (start: 8530, end: 8540)\n",
      "Batch 854 - batch loss: 1.994778037071228 - avg loss: 2.0886279545332256   (start: 8540, end: 8550)\n",
      "Batch 855 - batch loss: 1.9689624309539795 - avg loss: 2.08848815836082   (start: 8550, end: 8560)\n",
      "Batch 856 - batch loss: 2.493394613265991 - avg loss: 2.088960627969811   (start: 8560, end: 8570)\n",
      "Batch 857 - batch loss: 2.7663187980651855 - avg loss: 2.0897500897065187   (start: 8570, end: 8580)\n",
      "Batch 858 - batch loss: 2.501824378967285 - avg loss: 2.0902298036637488   (start: 8580, end: 8590)\n",
      "Batch 859 - batch loss: 2.139191150665283 - avg loss: 2.090286735462588   (start: 8590, end: 8600)\n",
      "Batch 860 - batch loss: 2.205965995788574 - avg loss: 2.0904210900041975   (start: 8600, end: 8610)\n",
      "Batch 861 - batch loss: 2.514423131942749 - avg loss: 2.090912971723384   (start: 8610, end: 8620)\n",
      "Batch 862 - batch loss: 1.7974300384521484 - avg loss: 2.090572898799547   (start: 8620, end: 8630)\n",
      "Batch 863 - batch loss: 2.1050240993499756 - avg loss: 2.09058962472611   (start: 8630, end: 8640)\n",
      "Batch 864 - batch loss: 1.5998587608337402 - avg loss: 2.0900223058083154   (start: 8640, end: 8650)\n",
      "Batch 865 - batch loss: 2.162950038909912 - avg loss: 2.0901065179712504   (start: 8650, end: 8660)\n",
      "Batch 866 - batch loss: 1.1948320865631104 - avg loss: 2.0890739061703183   (start: 8660, end: 8670)\n",
      "Batch 867 - batch loss: 2.487173080444336 - avg loss: 2.089532545772016   (start: 8670, end: 8680)\n",
      "Batch 868 - batch loss: 2.461712121963501 - avg loss: 2.08996083066982   (start: 8680, end: 8690)\n",
      "Batch 869 - batch loss: 1.9661039113998413 - avg loss: 2.089818466394797   (start: 8690, end: 8700)\n",
      "Batch 870 - batch loss: 1.6112384796142578 - avg loss: 2.0892690060196184   (start: 8700, end: 8710)\n",
      "Batch 871 - batch loss: 2.4896440505981445 - avg loss: 2.0897281517129427   (start: 8710, end: 8720)\n",
      "Batch 872 - batch loss: 2.1063883304595947 - avg loss: 2.0897472355373945   (start: 8720, end: 8730)\n",
      "Batch 873 - batch loss: 0.9582026600837708 - avg loss: 2.088452562110102   (start: 8730, end: 8740)\n",
      "Batch 874 - batch loss: 1.7812191247940063 - avg loss: 2.088101438181741   (start: 8740, end: 8750)\n",
      "Batch 875 - batch loss: 2.483821392059326 - avg loss: 2.088553173288907   (start: 8750, end: 8760)\n",
      "Batch 876 - batch loss: 2.177208185195923 - avg loss: 2.0886542622420508   (start: 8760, end: 8770)\n",
      "Batch 877 - batch loss: 2.3869619369506836 - avg loss: 2.0889940204137005   (start: 8770, end: 8780)\n",
      "Batch 878 - batch loss: 2.72479248046875 - avg loss: 2.0897173406185416   (start: 8780, end: 8790)\n",
      "Batch 879 - batch loss: 3.2433650493621826 - avg loss: 2.091028303923932   (start: 8790, end: 8800)\n",
      "Batch 880 - batch loss: 2.0112698078155518 - avg loss: 2.0909377721462836   (start: 8800, end: 8810)\n",
      "Batch 881 - batch loss: 2.5302562713623047 - avg loss: 2.0914358656828096   (start: 8810, end: 8820)\n",
      "Batch 882 - batch loss: 1.078589677810669 - avg loss: 2.0902888145074163   (start: 8820, end: 8830)\n",
      "Batch 883 - batch loss: 2.2132277488708496 - avg loss: 2.090427885700135   (start: 8830, end: 8840)\n",
      "Batch 884 - batch loss: 2.0210845470428467 - avg loss: 2.0903495316451552   (start: 8840, end: 8850)\n",
      "Batch 885 - batch loss: 2.3358025550842285 - avg loss: 2.0906265666603234   (start: 8850, end: 8860)\n",
      "Batch 886 - batch loss: 2.7321953773498535 - avg loss: 2.0913498685889476   (start: 8860, end: 8870)\n",
      "Batch 887 - batch loss: 3.2499051094055176 - avg loss: 2.0926545479141914   (start: 8870, end: 8880)\n",
      "Batch 888 - batch loss: 1.5271466970443726 - avg loss: 2.092018431096565   (start: 8880, end: 8890)\n",
      "Batch 889 - batch loss: 2.54960298538208 - avg loss: 2.0925325710452003   (start: 8890, end: 8900)\n",
      "Batch 890 - batch loss: 1.7100248336791992 - avg loss: 2.092103269431995   (start: 8900, end: 8910)\n",
      "Batch 891 - batch loss: 2.3947200775146484 - avg loss: 2.0924425259432984   (start: 8910, end: 8920)\n",
      "Batch 892 - batch loss: 1.854701042175293 - avg loss: 2.092176298077937   (start: 8920, end: 8930)\n",
      "Batch 893 - batch loss: 1.4017345905303955 - avg loss: 2.0914039919173693   (start: 8930, end: 8940)\n",
      "Batch 894 - batch loss: 1.8784103393554688 - avg loss: 2.091166010182663   (start: 8940, end: 8950)\n",
      "Batch 895 - batch loss: 3.2570996284484863 - avg loss: 2.0924672753816203   (start: 8950, end: 8960)\n",
      "Batch 896 - batch loss: 2.3269898891448975 - avg loss: 2.092728727570877   (start: 8960, end: 8970)\n",
      "Batch 897 - batch loss: 2.3104336261749268 - avg loss: 2.0929711606428194   (start: 8970, end: 8980)\n",
      "Batch 898 - batch loss: 2.7892444133758545 - avg loss: 2.0937456581430784   (start: 8980, end: 8990)\n",
      "Batch 899 - batch loss: 2.22896146774292 - avg loss: 2.0938958979315228   (start: 8990, end: 9000)\n",
      "Batch 900 - batch loss: 1.9425731897354126 - avg loss: 2.0937279481998954   (start: 9000, end: 9010)\n",
      "Batch 901 - batch loss: 2.106834888458252 - avg loss: 2.0937424791757917   (start: 9010, end: 9020)\n",
      "Batch 902 - batch loss: 2.0023980140686035 - avg loss: 2.0936413225145434   (start: 9020, end: 9030)\n",
      "Batch 903 - batch loss: 1.3713300228118896 - avg loss: 2.0928423055900938   (start: 9030, end: 9040)\n",
      "Batch 904 - batch loss: 2.610743284225464 - avg loss: 2.093414571864829   (start: 9040, end: 9050)\n",
      "Batch 905 - batch loss: 2.1284825801849365 - avg loss: 2.093453278275778   (start: 9050, end: 9060)\n",
      "Batch 906 - batch loss: 1.4456613063812256 - avg loss: 2.092739064414814   (start: 9060, end: 9070)\n",
      "Batch 907 - batch loss: 2.4611430168151855 - avg loss: 2.0931447956399247   (start: 9070, end: 9080)\n",
      "Batch 908 - batch loss: 1.8876234292984009 - avg loss: 2.0929186995273374   (start: 9080, end: 9090)\n",
      "Batch 909 - batch loss: 1.750448226928711 - avg loss: 2.0925423583486578   (start: 9090, end: 9100)\n",
      "Batch 910 - batch loss: 3.375359058380127 - avg loss: 2.0939504996220184   (start: 9100, end: 9110)\n",
      "Batch 911 - batch loss: 2.2742605209350586 - avg loss: 2.094148207978721   (start: 9110, end: 9120)\n",
      "Batch 912 - batch loss: 1.9368423223495483 - avg loss: 2.0939759123756225   (start: 9120, end: 9130)\n",
      "Batch 913 - batch loss: 2.265943765640259 - avg loss: 2.0941640610115795   (start: 9130, end: 9140)\n",
      "Batch 914 - batch loss: 1.5882893800735474 - avg loss: 2.093611192507822   (start: 9140, end: 9150)\n",
      "Batch 915 - batch loss: 3.093013286590576 - avg loss: 2.094702242828873   (start: 9150, end: 9160)\n",
      "Batch 916 - batch loss: 1.8800636529922485 - avg loss: 2.0944681767548965   (start: 9160, end: 9170)\n",
      "Batch 917 - batch loss: 1.9374812841415405 - avg loss: 2.0942971670679538   (start: 9170, end: 9180)\n",
      "Batch 918 - batch loss: 1.9079701900482178 - avg loss: 2.0940944173649942   (start: 9180, end: 9190)\n",
      "Batch 919 - batch loss: 1.4734232425689697 - avg loss: 2.093419774783694   (start: 9190, end: 9200)\n",
      "Batch 920 - batch loss: 2.728800058364868 - avg loss: 2.0941096556562036   (start: 9200, end: 9210)\n",
      "Batch 921 - batch loss: 1.6959075927734375 - avg loss: 2.0936777662170685   (start: 9210, end: 9220)\n",
      "Batch 922 - batch loss: 2.2333390712738037 - avg loss: 2.093829078573576   (start: 9220, end: 9230)\n",
      "Batch 923 - batch loss: 1.2208763360977173 - avg loss: 2.0928843245232778   (start: 9230, end: 9240)\n",
      "Batch 924 - batch loss: 1.7434718608856201 - avg loss: 2.092506581319345   (start: 9240, end: 9250)\n",
      "Batch 925 - batch loss: 1.6109049320220947 - avg loss: 2.091986493145158   (start: 9250, end: 9260)\n",
      "Batch 926 - batch loss: 1.1654754877090454 - avg loss: 2.0909870206473844   (start: 9260, end: 9270)\n",
      "Batch 927 - batch loss: 2.1392529010772705 - avg loss: 2.0910390312943994   (start: 9270, end: 9280)\n",
      "Batch 928 - batch loss: 1.7370036840438843 - avg loss: 2.090657938347951   (start: 9280, end: 9290)\n",
      "Batch 929 - batch loss: 1.6613960266113281 - avg loss: 2.090196366399847   (start: 9290, end: 9300)\n",
      "Batch 930 - batch loss: 1.0816304683685303 - avg loss: 2.0891130517940133   (start: 9300, end: 9310)\n",
      "Batch 931 - batch loss: 1.8314927816390991 - avg loss: 2.0888366351951344   (start: 9310, end: 9320)\n",
      "Batch 932 - batch loss: 2.1631321907043457 - avg loss: 2.088916266015616   (start: 9320, end: 9330)\n",
      "Batch 933 - batch loss: 1.7802776098251343 - avg loss: 2.0885858177755834   (start: 9330, end: 9340)\n",
      "Batch 934 - batch loss: 2.4935333728790283 - avg loss: 2.089018916764999   (start: 9340, end: 9350)\n",
      "Batch 935 - batch loss: 2.077143669128418 - avg loss: 2.089006229534618   (start: 9350, end: 9360)\n",
      "Batch 936 - batch loss: 2.218975067138672 - avg loss: 2.089144936938678   (start: 9360, end: 9370)\n",
      "Batch 937 - batch loss: 1.3043813705444336 - avg loss: 2.088308302006488   (start: 9370, end: 9380)\n",
      "Batch 938 - batch loss: 2.987647533416748 - avg loss: 2.089266064766243   (start: 9380, end: 9390)\n",
      "Batch 939 - batch loss: 2.2851216793060303 - avg loss: 2.0894744218029873   (start: 9390, end: 9400)\n",
      "Batch 940 - batch loss: 1.8981364965438843 - avg loss: 2.0892710871321487   (start: 9400, end: 9410)\n",
      "Batch 941 - batch loss: 2.94389271736145 - avg loss: 2.0901783287778275   (start: 9410, end: 9420)\n",
      "Batch 942 - batch loss: 2.2058098316192627 - avg loss: 2.0903009496716147   (start: 9420, end: 9430)\n",
      "Batch 943 - batch loss: 1.7287404537200928 - avg loss: 2.089917940671666   (start: 9430, end: 9440)\n",
      "Batch 944 - batch loss: 1.7385845184326172 - avg loss: 2.0895461592724716   (start: 9440, end: 9450)\n",
      "Batch 945 - batch loss: 1.8447201251983643 - avg loss: 2.089287357967953   (start: 9450, end: 9460)\n",
      "Batch 946 - batch loss: 1.8686832189559937 - avg loss: 2.0890544074515733   (start: 9460, end: 9470)\n",
      "Batch 947 - batch loss: 2.4710068702697754 - avg loss: 2.0894573108933647   (start: 9470, end: 9480)\n",
      "Batch 948 - batch loss: 2.071230411529541 - avg loss: 2.089438104466216   (start: 9480, end: 9490)\n",
      "Batch 949 - batch loss: 1.3065483570098877 - avg loss: 2.0886140099952097   (start: 9490, end: 9500)\n",
      "Batch 950 - batch loss: 1.1315540075302124 - avg loss: 2.08760763775287   (start: 9500, end: 9510)\n",
      "Batch 951 - batch loss: 2.247210741043091 - avg loss: 2.087775288071452   (start: 9510, end: 9520)\n",
      "Batch 952 - batch loss: 1.9635353088378906 - avg loss: 2.0876449208319623   (start: 9520, end: 9530)\n",
      "Batch 953 - batch loss: 2.258647918701172 - avg loss: 2.0878241692574018   (start: 9530, end: 9540)\n",
      "Batch 954 - batch loss: 2.064117908477783 - avg loss: 2.087799345947685   (start: 9540, end: 9550)\n",
      "Batch 955 - batch loss: 1.8154306411743164 - avg loss: 2.087514441444784   (start: 9550, end: 9560)\n",
      "Batch 956 - batch loss: 2.1752536296844482 - avg loss: 2.0876061229371974   (start: 9560, end: 9570)\n",
      "Batch 957 - batch loss: 2.5018198490142822 - avg loss: 2.0880384963464635   (start: 9570, end: 9580)\n",
      "Batch 958 - batch loss: 1.6014591455459595 - avg loss: 2.087531114333116   (start: 9580, end: 9590)\n",
      "Batch 959 - batch loss: 2.1396420001983643 - avg loss: 2.0875853965058924   (start: 9590, end: 9600)\n",
      "Batch 960 - batch loss: 2.472081422805786 - avg loss: 2.087985496429201   (start: 9600, end: 9610)\n",
      "Batch 961 - batch loss: 1.7735446691513062 - avg loss: 2.0876586348623842   (start: 9610, end: 9620)\n",
      "Batch 962 - batch loss: 1.8879203796386719 - avg loss: 2.0874512223439794   (start: 9620, end: 9630)\n",
      "Batch 963 - batch loss: 1.7851976156234741 - avg loss: 2.08713768125817   (start: 9630, end: 9640)\n",
      "Batch 964 - batch loss: 1.4786784648895264 - avg loss: 2.0865071535728137   (start: 9640, end: 9650)\n",
      "Batch 965 - batch loss: 2.420133352279663 - avg loss: 2.086852522308535   (start: 9650, end: 9660)\n",
      "Batch 966 - batch loss: 1.4154579639434814 - avg loss: 2.086158215629771   (start: 9660, end: 9670)\n",
      "Batch 967 - batch loss: 2.0731139183044434 - avg loss: 2.086144740116005   (start: 9670, end: 9680)\n",
      "Batch 968 - batch loss: 3.0651116371154785 - avg loss: 2.0871550258714224   (start: 9680, end: 9690)\n",
      "Batch 969 - batch loss: 2.4964535236358643 - avg loss: 2.087576983085613   (start: 9690, end: 9700)\n",
      "Batch 970 - batch loss: 2.113542079925537 - avg loss: 2.0876037236590834   (start: 9700, end: 9710)\n",
      "Batch 971 - batch loss: 2.3888628482818604 - avg loss: 2.087913661030094   (start: 9710, end: 9720)\n",
      "Batch 972 - batch loss: 1.8045625686645508 - avg loss: 2.087622447163326   (start: 9720, end: 9730)\n",
      "Batch 973 - batch loss: 1.9576925039291382 - avg loss: 2.0874890488643176   (start: 9730, end: 9740)\n",
      "Batch 974 - batch loss: 2.4663796424865723 - avg loss: 2.087877654601366   (start: 9740, end: 9750)\n",
      "Batch 975 - batch loss: 1.7551758289337158 - avg loss: 2.087536771583264   (start: 9750, end: 9760)\n",
      "Batch 976 - batch loss: 2.366603374481201 - avg loss: 2.0878224078195977   (start: 9760, end: 9770)\n",
      "Batch 977 - batch loss: 1.4964942932128906 - avg loss: 2.087217777845562   (start: 9770, end: 9780)\n",
      "Batch 978 - batch loss: 1.775403380393982 - avg loss: 2.0868992748859587   (start: 9780, end: 9790)\n",
      "Batch 979 - batch loss: 2.9009711742401123 - avg loss: 2.0877299604975446   (start: 9790, end: 9800)\n",
      "Batch 980 - batch loss: 1.623928427696228 - avg loss: 2.0872571760604384   (start: 9800, end: 9810)\n",
      "Batch 981 - batch loss: 1.9144566059112549 - avg loss: 2.0870812080663965   (start: 9810, end: 9820)\n",
      "Batch 982 - batch loss: 3.068422317504883 - avg loss: 2.088079520486985   (start: 9820, end: 9830)\n",
      "Batch 983 - batch loss: 2.3421051502227783 - avg loss: 2.088337676614765   (start: 9830, end: 9840)\n",
      "Batch 984 - batch loss: 1.7268247604370117 - avg loss: 2.0879706584257525   (start: 9840, end: 9850)\n",
      "Batch 985 - batch loss: 1.7055892944335938 - avg loss: 2.087582847711764   (start: 9850, end: 9860)\n",
      "Batch 986 - batch loss: 1.1897547245025635 - avg loss: 2.0866731940914915   (start: 9860, end: 9870)\n",
      "Batch 987 - batch loss: 2.0780081748962402 - avg loss: 2.086664423829148   (start: 9870, end: 9880)\n",
      "Batch 988 - batch loss: 1.2808362245559692 - avg loss: 2.085849632929984   (start: 9880, end: 9890)\n",
      "Batch 989 - batch loss: 1.7091283798217773 - avg loss: 2.085469106411693   (start: 9890, end: 9900)\n",
      "Batch 990 - batch loss: 1.5273109674453735 - avg loss: 2.0849058792280744   (start: 9900, end: 9910)\n",
      "Batch 991 - batch loss: 1.2663424015045166 - avg loss: 2.0840807144319817   (start: 9910, end: 9920)\n",
      "Batch 992 - batch loss: 1.7797577381134033 - avg loss: 2.0837742461778848   (start: 9920, end: 9930)\n",
      "Batch 993 - batch loss: 1.9101712703704834 - avg loss: 2.0835995952967905   (start: 9930, end: 9940)\n",
      "Batch 994 - batch loss: 1.4311354160308838 - avg loss: 2.082943852403056   (start: 9940, end: 9950)\n",
      "Batch 995 - batch loss: 1.1547822952270508 - avg loss: 2.0820119632894256   (start: 9950, end: 9960)\n",
      "Batch 996 - batch loss: 1.6868520975112915 - avg loss: 2.08161561437691   (start: 9960, end: 9970)\n",
      "Batch 997 - batch loss: 2.1262974739074707 - avg loss: 2.0816603857792453   (start: 9970, end: 9980)\n",
      "Batch 998 - batch loss: 1.1942451000213623 - avg loss: 2.080772082189898   (start: 9980, end: 9990)\n",
      "Batch 999 - batch loss: 3.066200017929077 - avg loss: 2.081757510125637   (start: 9990, end: 10000)\n",
      "Batch 1000 - batch loss: 2.190707206726074 - avg loss: 2.081866350981382   (start: 10000, end: 10010)\n",
      "Batch 1001 - batch loss: 2.1735024452209473 - avg loss: 2.0819578041692455   (start: 10010, end: 10020)\n",
      "Batch 1002 - batch loss: 1.041213870048523 - avg loss: 2.080920173128248   (start: 10020, end: 10030)\n",
      "Batch 1003 - batch loss: 1.345005989074707 - avg loss: 2.0801871908732146   (start: 10030, end: 10040)\n",
      "Batch 1004 - batch loss: 2.1316535472869873 - avg loss: 2.0802384011781037   (start: 10040, end: 10050)\n",
      "Batch 1005 - batch loss: 3.022556781768799 - avg loss: 2.081175099369546   (start: 10050, end: 10060)\n",
      "Batch 1006 - batch loss: 1.793455719947815 - avg loss: 2.0808893800255324   (start: 10060, end: 10070)\n",
      "Batch 1007 - batch loss: 1.8845980167388916 - avg loss: 2.0806946465302083   (start: 10070, end: 10080)\n",
      "Batch 1008 - batch loss: 2.743809223175049 - avg loss: 2.0813518463088454   (start: 10080, end: 10090)\n",
      "Batch 1009 - batch loss: 2.7167696952819824 - avg loss: 2.081980972891987   (start: 10090, end: 10100)\n",
      "Batch 1010 - batch loss: 2.6097214221954346 - avg loss: 2.0825029713581626   (start: 10100, end: 10110)\n",
      "Batch 1011 - batch loss: 1.8042415380477905 - avg loss: 2.0822280094675394   (start: 10110, end: 10120)\n",
      "Batch 1012 - batch loss: 1.76076340675354 - avg loss: 2.081910670274337   (start: 10120, end: 10130)\n",
      "Batch 1013 - batch loss: 2.334906816482544 - avg loss: 2.0821601733771065   (start: 10130, end: 10140)\n",
      "Batch 1014 - batch loss: 2.2455594539642334 - avg loss: 2.0823211578900005   (start: 10140, end: 10150)\n",
      "Batch 1015 - batch loss: 2.595551013946533 - avg loss: 2.082826305386119   (start: 10150, end: 10160)\n",
      "Batch 1016 - batch loss: 2.3603203296661377 - avg loss: 2.0830991608672202   (start: 10160, end: 10170)\n",
      "Batch 1017 - batch loss: 2.22251558303833 - avg loss: 2.0832361121660132   (start: 10170, end: 10180)\n",
      "Batch 1018 - batch loss: 2.64668345451355 - avg loss: 2.083789053620721   (start: 10180, end: 10190)\n",
      "Batch 1019 - batch loss: 1.8372504711151123 - avg loss: 2.0835473491280685   (start: 10190, end: 10200)\n",
      "Batch 1020 - batch loss: 2.446446657180786 - avg loss: 2.083902784297562   (start: 10200, end: 10210)\n",
      "Batch 1021 - batch loss: 2.414543628692627 - avg loss: 2.0842263076286724   (start: 10210, end: 10220)\n",
      "Batch 1022 - batch loss: 1.9960081577301025 - avg loss: 2.084140072878039   (start: 10220, end: 10230)\n",
      "Batch 1023 - batch loss: 2.0126454830169678 - avg loss: 2.0840702539426275   (start: 10230, end: 10240)\n",
      "Batch 1024 - batch loss: 2.321969509124756 - avg loss: 2.0843023507769516   (start: 10240, end: 10250)\n",
      "Batch 1025 - batch loss: 2.0696332454681396 - avg loss: 2.0842880534033563   (start: 10250, end: 10260)\n",
      "Batch 1026 - batch loss: 1.5008220672607422 - avg loss: 2.0837199268345707   (start: 10260, end: 10270)\n",
      "Batch 1027 - batch loss: 2.1076083183288574 - avg loss: 2.0837431645694875   (start: 10270, end: 10280)\n",
      "Batch 1028 - batch loss: 1.6037057638168335 - avg loss: 2.083276655919582   (start: 10280, end: 10290)\n",
      "Batch 1029 - batch loss: 1.6673173904418945 - avg loss: 2.0828728119725164   (start: 10290, end: 10300)\n",
      "Batch 1030 - batch loss: 2.049636125564575 - avg loss: 2.0828405746433134   (start: 10300, end: 10310)\n",
      "Batch 1031 - batch loss: 1.8779020309448242 - avg loss: 2.0826419907831406   (start: 10310, end: 10320)\n",
      "Batch 1032 - batch loss: 1.9633442163467407 - avg loss: 2.08252650407023   (start: 10320, end: 10330)\n",
      "Batch 1033 - batch loss: 1.9145233631134033 - avg loss: 2.082364025210504   (start: 10330, end: 10340)\n",
      "Batch 1034 - batch loss: 2.394273042678833 - avg loss: 2.0826653865800386   (start: 10340, end: 10350)\n",
      "Batch 1035 - batch loss: 1.404956579208374 - avg loss: 2.082011227499564   (start: 10350, end: 10360)\n",
      "Batch 1036 - batch loss: 1.5957701206207275 - avg loss: 2.081542335400356   (start: 10360, end: 10370)\n",
      "Batch 1037 - batch loss: 1.556598424911499 - avg loss: 2.081036609089673   (start: 10370, end: 10380)\n",
      "Batch 1038 - batch loss: 1.29197359085083 - avg loss: 2.0802771644137934   (start: 10380, end: 10390)\n",
      "Batch 1039 - batch loss: 3.081770896911621 - avg loss: 2.08124013915658   (start: 10390, end: 10400)\n",
      "Batch 1040 - batch loss: 3.458427906036377 - avg loss: 2.0825630860988276   (start: 10400, end: 10410)\n",
      "Batch 1041 - batch loss: 2.743695020675659 - avg loss: 2.0831975697212624   (start: 10410, end: 10420)\n",
      "Batch 1042 - batch loss: 1.8464893102645874 - avg loss: 2.082970620287459   (start: 10420, end: 10430)\n",
      "Batch 1043 - batch loss: 1.5780417919158936 - avg loss: 2.0824869719844212   (start: 10430, end: 10440)\n",
      "Batch 1044 - batch loss: 3.1507630348205566 - avg loss: 2.083509245728762   (start: 10440, end: 10450)\n",
      "Batch 1045 - batch loss: 2.3851237297058105 - avg loss: 2.083797596095853   (start: 10450, end: 10460)\n",
      "Batch 1046 - batch loss: 1.3254749774932861 - avg loss: 2.083073314702727   (start: 10460, end: 10470)\n",
      "Batch 1047 - batch loss: 1.469140887260437 - avg loss: 2.0824875013177633   (start: 10470, end: 10480)\n",
      "Batch 1048 - batch loss: 2.115105152130127 - avg loss: 2.082518595360482   (start: 10480, end: 10490)\n",
      "Batch 1049 - batch loss: 2.401148557662964 - avg loss: 2.0828220524674372   (start: 10490, end: 10500)\n",
      "Batch 1050 - batch loss: 2.1497673988342285 - avg loss: 2.0828857492765396   (start: 10500, end: 10510)\n",
      "Batch 1051 - batch loss: 2.6690337657928467 - avg loss: 2.0834429241971826   (start: 10510, end: 10520)\n",
      "Batch 1052 - batch loss: 2.1250357627868652 - avg loss: 2.0834824235690625   (start: 10520, end: 10530)\n",
      "Batch 1053 - batch loss: 1.4073305130004883 - avg loss: 2.0828409132174794   (start: 10530, end: 10540)\n",
      "Batch 1054 - batch loss: 1.8859726190567017 - avg loss: 2.0826543081993174   (start: 10540, end: 10550)\n",
      "Batch 1055 - batch loss: 1.8224642276763916 - avg loss: 2.08240791607761   (start: 10550, end: 10560)\n",
      "Batch 1056 - batch loss: 2.007514476776123 - avg loss: 2.0823370613573626   (start: 10560, end: 10570)\n",
      "Batch 1057 - batch loss: 1.989967703819275 - avg loss: 2.0822497557264192   (start: 10570, end: 10580)\n",
      "Batch 1058 - batch loss: 2.4183127880096436 - avg loss: 2.082567095700247   (start: 10580, end: 10590)\n",
      "Batch 1059 - batch loss: 1.6318477392196655 - avg loss: 2.082141888760171   (start: 10590, end: 10600)\n",
      "Batch 1060 - batch loss: 2.272099733352661 - avg loss: 2.082320925371474   (start: 10600, end: 10610)\n",
      "Batch 1061 - batch loss: 2.0597009658813477 - avg loss: 2.0822996259745903   (start: 10610, end: 10620)\n",
      "Batch 1062 - batch loss: 1.5774238109588623 - avg loss: 2.0818246722445664   (start: 10620, end: 10630)\n",
      "Batch 1063 - batch loss: 1.4383249282836914 - avg loss: 2.0812198792521217   (start: 10630, end: 10640)\n",
      "Batch 1064 - batch loss: 1.814849853515625 - avg loss: 2.0809697665519   (start: 10640, end: 10650)\n",
      "Batch 1065 - batch loss: 2.359841823577881 - avg loss: 2.0812313726091474   (start: 10650, end: 10660)\n",
      "Batch 1066 - batch loss: 1.8054882287979126 - avg loss: 2.0809729441707114   (start: 10660, end: 10670)\n",
      "Batch 1067 - batch loss: 1.3651647567749023 - avg loss: 2.0803027117855093   (start: 10670, end: 10680)\n",
      "Batch 1068 - batch loss: 1.8341573476791382 - avg loss: 2.0800724541951388   (start: 10680, end: 10690)\n",
      "Batch 1069 - batch loss: 2.687133550643921 - avg loss: 2.0806398010142497   (start: 10690, end: 10700)\n",
      "Batch 1070 - batch loss: 1.756150245666504 - avg loss: 2.0803368229046812   (start: 10700, end: 10710)\n",
      "Batch 1071 - batch loss: 1.5096466541290283 - avg loss: 2.079804462672614   (start: 10710, end: 10720)\n",
      "Batch 1072 - batch loss: 1.9629361629486084 - avg loss: 2.0796955453382955   (start: 10720, end: 10730)\n",
      "Batch 1073 - batch loss: 1.8704923391342163 - avg loss: 2.0795007565057033   (start: 10730, end: 10740)\n",
      "Batch 1074 - batch loss: 2.4164023399353027 - avg loss: 2.0798141533274985   (start: 10740, end: 10750)\n",
      "Batch 1075 - batch loss: 1.2839597463607788 - avg loss: 2.0790745116853357   (start: 10750, end: 10760)\n",
      "Batch 1076 - batch loss: 1.7510654926300049 - avg loss: 2.078769953636074   (start: 10760, end: 10770)\n",
      "Batch 1077 - batch loss: 1.9182841777801514 - avg loss: 2.0786210800035545   (start: 10770, end: 10780)\n",
      "Batch 1078 - batch loss: 1.6258913278579712 - avg loss: 2.0782014972860887   (start: 10780, end: 10790)\n",
      "Batch 1079 - batch loss: 1.9371178150177002 - avg loss: 2.078070864246951   (start: 10790, end: 10800)\n",
      "Batch 1080 - batch loss: 1.485679268836975 - avg loss: 2.0775228609209475   (start: 10800, end: 10810)\n",
      "Batch 1081 - batch loss: 1.882006049156189 - avg loss: 2.077342161464603   (start: 10810, end: 10820)\n",
      "Batch 1082 - batch loss: 2.1820995807647705 - avg loss: 2.0774388903836245   (start: 10820, end: 10830)\n",
      "Batch 1083 - batch loss: 1.336977243423462 - avg loss: 2.0767558076834765   (start: 10830, end: 10840)\n",
      "Batch 1084 - batch loss: 2.0702505111694336 - avg loss: 2.0767498120184866   (start: 10840, end: 10850)\n",
      "Batch 1085 - batch loss: 1.8368581533432007 - avg loss: 2.0765289173051578   (start: 10850, end: 10860)\n",
      "Batch 1086 - batch loss: 1.992322325706482 - avg loss: 2.0764514503395657   (start: 10860, end: 10870)\n",
      "Batch 1087 - batch loss: 2.152179002761841 - avg loss: 2.0765210528693654   (start: 10870, end: 10880)\n",
      "Batch 1088 - batch loss: 1.5361391305923462 - avg loss: 2.076024834391609   (start: 10880, end: 10890)\n",
      "Batch 1089 - batch loss: 2.042013168334961 - avg loss: 2.075993631028254   (start: 10890, end: 10900)\n",
      "Batch 1090 - batch loss: 1.117023229598999 - avg loss: 2.075114648075523   (start: 10900, end: 10910)\n",
      "Batch 1091 - batch loss: 2.209857940673828 - avg loss: 2.0752380393691117   (start: 10910, end: 10920)\n",
      "Batch 1092 - batch loss: 1.9312232732772827 - avg loss: 2.075106278375432   (start: 10920, end: 10930)\n",
      "Batch 1093 - batch loss: 1.8355789184570312 - avg loss: 2.074887331976969   (start: 10930, end: 10940)\n",
      "Batch 1094 - batch loss: 1.5045236349105835 - avg loss: 2.074366451888324   (start: 10940, end: 10950)\n",
      "Batch 1095 - batch loss: 1.7736093997955322 - avg loss: 2.074092038519626   (start: 10950, end: 10960)\n",
      "Batch 1096 - batch loss: 1.6336462497711182 - avg loss: 2.0736905382564097   (start: 10960, end: 10970)\n",
      "Batch 1097 - batch loss: 2.2821197509765625 - avg loss: 2.073880364497503   (start: 10970, end: 10980)\n",
      "Batch 1098 - batch loss: 2.341322660446167 - avg loss: 2.074123715085263   (start: 10980, end: 10990)\n",
      "Batch 1099 - batch loss: 2.035231113433838 - avg loss: 2.074088358174671   (start: 10990, end: 11000)\n",
      "Batch 1100 - batch loss: 1.775795340538025 - avg loss: 2.0738174290033387   (start: 11000, end: 11010)\n",
      "Batch 1101 - batch loss: 1.8704097270965576 - avg loss: 2.0736328485115902   (start: 11010, end: 11020)\n",
      "Batch 1102 - batch loss: 1.961408019065857 - avg loss: 2.0735311034259643   (start: 11020, end: 11030)\n",
      "Batch 1103 - batch loss: 1.915783166885376 - avg loss: 2.073388215802286   (start: 11030, end: 11040)\n",
      "Batch 1104 - batch loss: 2.5914368629455566 - avg loss: 2.073857038107393   (start: 11040, end: 11050)\n",
      "Batch 1105 - batch loss: 1.7457153797149658 - avg loss: 2.0735603458303653   (start: 11050, end: 11060)\n",
      "Batch 1106 - batch loss: 2.1198971271514893 - avg loss: 2.073602203808072   (start: 11060, end: 11070)\n",
      "Batch 1107 - batch loss: 2.207775831222534 - avg loss: 2.0737232991396737   (start: 11070, end: 11080)\n",
      "Batch 1108 - batch loss: 1.747117042541504 - avg loss: 2.0734287939488727   (start: 11080, end: 11090)\n",
      "Batch 1109 - batch loss: 2.0293502807617188 - avg loss: 2.073389083576632   (start: 11090, end: 11100)\n",
      "Batch 1110 - batch loss: 2.195476531982422 - avg loss: 2.0734989732691664   (start: 11100, end: 11110)\n",
      "Batch 1111 - batch loss: 2.011335849761963 - avg loss: 2.0734430711796814   (start: 11110, end: 11120)\n",
      "Batch 1112 - batch loss: 1.194955587387085 - avg loss: 2.07265377424905   (start: 11120, end: 11130)\n",
      "Batch 1113 - batch loss: 1.3043274879455566 - avg loss: 2.071964073812512   (start: 11130, end: 11140)\n",
      "Batch 1114 - batch loss: 2.0390079021453857 - avg loss: 2.071934516707878   (start: 11140, end: 11150)\n",
      "Batch 1115 - batch loss: 2.4478564262390137 - avg loss: 2.0722713642970634   (start: 11150, end: 11160)\n",
      "Batch 1116 - batch loss: 1.3410961627960205 - avg loss: 2.0716167759340367   (start: 11160, end: 11170)\n",
      "Batch 1117 - batch loss: 1.892285943031311 - avg loss: 2.0714563726845707   (start: 11170, end: 11180)\n",
      "Batch 1118 - batch loss: 1.732805609703064 - avg loss: 2.071153735720334   (start: 11180, end: 11190)\n",
      "Batch 1119 - batch loss: 2.5536553859710693 - avg loss: 2.0715845407652003   (start: 11190, end: 11200)\n",
      "Batch 1120 - batch loss: 1.9141181707382202 - avg loss: 2.0714440712112068   (start: 11200, end: 11210)\n",
      "Batch 1121 - batch loss: 2.084801435470581 - avg loss: 2.0714559761704394   (start: 11210, end: 11220)\n",
      "Batch 1122 - batch loss: 1.9700727462768555 - avg loss: 2.0713656972480052   (start: 11220, end: 11230)\n",
      "Batch 1123 - batch loss: 1.741929292678833 - avg loss: 2.071072604361378   (start: 11230, end: 11240)\n",
      "Batch 1124 - batch loss: 1.1119787693023682 - avg loss: 2.0702200765079923   (start: 11240, end: 11250)\n",
      "Batch 1125 - batch loss: 3.0053458213806152 - avg loss: 2.0710505611837227   (start: 11250, end: 11260)\n",
      "Batch 1126 - batch loss: 1.7872816324234009 - avg loss: 2.0707987697651244   (start: 11260, end: 11270)\n",
      "Batch 1127 - batch loss: 2.2105202674865723 - avg loss: 2.070922636341119   (start: 11270, end: 11280)\n",
      "Batch 1128 - batch loss: 2.3458023071289062 - avg loss: 2.0711661081487254   (start: 11280, end: 11290)\n",
      "Batch 1129 - batch loss: 1.6835801601409912 - avg loss: 2.0708231117345592   (start: 11290, end: 11300)\n",
      "Batch 1130 - batch loss: 2.4128031730651855 - avg loss: 2.0711254813732247   (start: 11300, end: 11310)\n",
      "Batch 1131 - batch loss: 1.3184034824371338 - avg loss: 2.0704605326109133   (start: 11310, end: 11320)\n",
      "Batch 1132 - batch loss: 1.9306542873382568 - avg loss: 2.0703371378666304   (start: 11320, end: 11330)\n",
      "Batch 1133 - batch loss: 1.358019471168518 - avg loss: 2.069708991776068   (start: 11330, end: 11340)\n",
      "Batch 1134 - batch loss: 1.1365280151367188 - avg loss: 2.068886805893566   (start: 11340, end: 11350)\n",
      "Batch 1135 - batch loss: 2.31543231010437 - avg loss: 2.0691038353867093   (start: 11350, end: 11360)\n",
      "Batch 1136 - batch loss: 2.193516254425049 - avg loss: 2.069213257039338   (start: 11360, end: 11370)\n",
      "Batch 1137 - batch loss: 1.9170520305633545 - avg loss: 2.0690795477014854   (start: 11370, end: 11380)\n",
      "Batch 1138 - batch loss: 1.739943265914917 - avg loss: 2.0687905781827967   (start: 11380, end: 11390)\n",
      "Batch 1139 - batch loss: 2.980660915374756 - avg loss: 2.069590464443491   (start: 11390, end: 11400)\n",
      "Batch 1140 - batch loss: 1.6188528537750244 - avg loss: 2.0691954270984705   (start: 11400, end: 11410)\n",
      "Batch 1141 - batch loss: 1.5559852123260498 - avg loss: 2.068746031113556   (start: 11410, end: 11420)\n",
      "Batch 1142 - batch loss: 2.584853410720825 - avg loss: 2.069197568628523   (start: 11420, end: 11430)\n",
      "Batch 1143 - batch loss: 2.308849334716797 - avg loss: 2.0694070544380407   (start: 11430, end: 11440)\n",
      "Batch 1144 - batch loss: 1.4351186752319336 - avg loss: 2.068853090788079   (start: 11440, end: 11450)\n",
      "Batch 1145 - batch loss: 2.7579293251037598 - avg loss: 2.0694543789506583   (start: 11450, end: 11460)\n",
      "Batch 1146 - batch loss: 1.3855409622192383 - avg loss: 2.0688581161636215   (start: 11460, end: 11470)\n",
      "Batch 1147 - batch loss: 1.7712968587875366 - avg loss: 2.0685989164620744   (start: 11470, end: 11480)\n",
      "Batch 1148 - batch loss: 1.817436933517456 - avg loss: 2.068380324657945   (start: 11480, end: 11490)\n",
      "Batch 1149 - batch loss: 2.5374531745910645 - avg loss: 2.0687882140926694   (start: 11490, end: 11500)\n",
      "Batch 1150 - batch loss: 1.9992811679840088 - avg loss: 2.06872782569466   (start: 11500, end: 11510)\n",
      "Batch 1151 - batch loss: 2.221097469329834 - avg loss: 2.0688600910103156   (start: 11510, end: 11520)\n",
      "Batch 1152 - batch loss: 2.4161810874938965 - avg loss: 2.069161323444386   (start: 11520, end: 11530)\n",
      "Batch 1153 - batch loss: 2.6258091926574707 - avg loss: 2.0696436872825257   (start: 11530, end: 11540)\n",
      "Batch 1154 - batch loss: 3.841550350189209 - avg loss: 2.0711778056053887   (start: 11540, end: 11550)\n",
      "Batch 1155 - batch loss: 1.625174880027771 - avg loss: 2.0707919899258234   (start: 11550, end: 11560)\n",
      "Batch 1156 - batch loss: 1.9385602474212646 - avg loss: 2.070677701470763   (start: 11560, end: 11570)\n",
      "Batch 1157 - batch loss: 1.9023244380950928 - avg loss: 2.070532318687192   (start: 11570, end: 11580)\n",
      "Batch 1158 - batch loss: 1.8509576320648193 - avg loss: 2.0703428668436867   (start: 11580, end: 11590)\n",
      "Batch 1159 - batch loss: 1.327391266822815 - avg loss: 2.0697023913264276   (start: 11590, end: 11600)\n",
      "Batch 1160 - batch loss: 2.2734408378601074 - avg loss: 2.069877876637826   (start: 11600, end: 11610)\n",
      "Batch 1161 - batch loss: 2.7185964584350586 - avg loss: 2.070436154246946   (start: 11610, end: 11620)\n",
      "Batch 1162 - batch loss: 1.3877979516983032 - avg loss: 2.06984919104613   (start: 11620, end: 11630)\n",
      "Batch 1163 - batch loss: 2.198086977005005 - avg loss: 2.069959360965339   (start: 11630, end: 11640)\n",
      "Batch 1164 - batch loss: 1.7231487035751343 - avg loss: 2.069661669413931   (start: 11640, end: 11650)\n",
      "Batch 1165 - batch loss: 3.0798606872558594 - avg loss: 2.070528049360622   (start: 11650, end: 11660)\n",
      "Batch 1166 - batch loss: 1.709216833114624 - avg loss: 2.070218442491517   (start: 11660, end: 11670)\n",
      "Batch 1167 - batch loss: 1.656134843826294 - avg loss: 2.0698639188625223   (start: 11670, end: 11680)\n",
      "Batch 1168 - batch loss: 2.482217311859131 - avg loss: 2.0702166591473783   (start: 11680, end: 11690)\n",
      "Batch 1169 - batch loss: 2.2450828552246094 - avg loss: 2.0703661174346237   (start: 11690, end: 11700)\n",
      "Batch 1170 - batch loss: 1.6936681270599365 - avg loss: 2.070044428288275   (start: 11700, end: 11710)\n",
      "Batch 1171 - batch loss: 1.8996261358261108 - avg loss: 2.0698990201889043   (start: 11710, end: 11720)\n",
      "Batch 1172 - batch loss: 2.435673236846924 - avg loss: 2.070210848165595   (start: 11720, end: 11730)\n",
      "Batch 1173 - batch loss: 1.9028520584106445 - avg loss: 2.070068293830199   (start: 11730, end: 11740)\n",
      "Batch 1174 - batch loss: 1.2109662294387817 - avg loss: 2.0693371431371   (start: 11740, end: 11750)\n",
      "Batch 1175 - batch loss: 1.890784502029419 - avg loss: 2.0691853126599673   (start: 11750, end: 11760)\n",
      "Batch 1176 - batch loss: 1.7771217823028564 - avg loss: 2.0689371703232156   (start: 11760, end: 11770)\n",
      "Batch 1177 - batch loss: 1.9797443151474 - avg loss: 2.068861454826462   (start: 11770, end: 11780)\n",
      "Batch 1178 - batch loss: 1.5475506782531738 - avg loss: 2.068419291317918   (start: 11780, end: 11790)\n",
      "Batch 1179 - batch loss: 2.3632540702819824 - avg loss: 2.068669151300091   (start: 11790, end: 11800)\n",
      "Batch 1180 - batch loss: 1.3974558115005493 - avg loss: 2.068100808082648   (start: 11800, end: 11810)\n",
      "Batch 1181 - batch loss: 2.566993236541748 - avg loss: 2.0685228828952194   (start: 11810, end: 11820)\n",
      "Batch 1182 - batch loss: 2.266294002532959 - avg loss: 2.0686900605111433   (start: 11820, end: 11830)\n",
      "Batch 1183 - batch loss: 1.4606901407241821 - avg loss: 2.0681765470653772   (start: 11830, end: 11840)\n",
      "Batch 1184 - batch loss: 1.6011455059051514 - avg loss: 2.067782428043301   (start: 11840, end: 11850)\n",
      "Batch 1185 - batch loss: 1.1600563526153564 - avg loss: 2.067017060357443   (start: 11850, end: 11860)\n",
      "Batch 1186 - batch loss: 2.3090407848358154 - avg loss: 2.067220955660289   (start: 11860, end: 11870)\n",
      "Batch 1187 - batch loss: 2.9250810146331787 - avg loss: 2.0679430600870337   (start: 11870, end: 11880)\n",
      "Batch 1188 - batch loss: 2.4445180892944336 - avg loss: 2.0682597758391004   (start: 11880, end: 11890)\n",
      "Batch 1189 - batch loss: 2.1218385696411133 - avg loss: 2.068304800035573   (start: 11890, end: 11900)\n",
      "Batch 1190 - batch loss: 2.0499062538146973 - avg loss: 2.0682893520538594   (start: 11900, end: 11910)\n",
      "Batch 1191 - batch loss: 2.30820631980896 - avg loss: 2.068490624677815   (start: 11910, end: 11920)\n",
      "Batch 1192 - batch loss: 3.221897602081299 - avg loss: 2.069457436896929   (start: 11920, end: 11930)\n",
      "Batch 1193 - batch loss: 1.280456781387329 - avg loss: 2.068796632327826   (start: 11930, end: 11940)\n",
      "Batch 1194 - batch loss: 2.37158465385437 - avg loss: 2.0690500114253374   (start: 11940, end: 11950)\n",
      "Batch 1195 - batch loss: 1.4887291193008423 - avg loss: 2.0685647932881097   (start: 11950, end: 11960)\n",
      "Batch 1196 - batch loss: 2.183520793914795 - avg loss: 2.0686608300471963   (start: 11960, end: 11970)\n",
      "Batch 1197 - batch loss: 1.7874958515167236 - avg loss: 2.0684261347395747   (start: 11970, end: 11980)\n",
      "Batch 1198 - batch loss: 2.167355537414551 - avg loss: 2.0685086446667436   (start: 11980, end: 11990)\n",
      "Batch 1199 - batch loss: 1.7523975372314453 - avg loss: 2.0682452187438805   (start: 11990, end: 12000)\n",
      "Batch 1200 - batch loss: 2.0181679725646973 - avg loss: 2.0682035224523077   (start: 12000, end: 12010)\n",
      "Batch 1201 - batch loss: 1.7530368566513062 - avg loss: 2.067941320567282   (start: 12010, end: 12020)\n",
      "Batch 1202 - batch loss: 2.4544546604156494 - avg loss: 2.068262611789101   (start: 12020, end: 12030)\n",
      "Batch 1203 - batch loss: 1.9136425256729126 - avg loss: 2.0681341897906655   (start: 12030, end: 12040)\n",
      "Batch 1204 - batch loss: 1.4738339185714722 - avg loss: 2.0676409945448406   (start: 12040, end: 12050)\n",
      "Batch 1205 - batch loss: 1.7369718551635742 - avg loss: 2.0673668078621033   (start: 12050, end: 12060)\n",
      "Batch 1206 - batch loss: 2.038695812225342 - avg loss: 2.0673430539303412   (start: 12060, end: 12070)\n",
      "Batch 1207 - batch loss: 2.1526122093200684 - avg loss: 2.0674136409795048   (start: 12070, end: 12080)\n",
      "Batch 1208 - batch loss: 2.8599960803985596 - avg loss: 2.0680692095811746   (start: 12080, end: 12090)\n",
      "Batch 1209 - batch loss: 1.5627641677856445 - avg loss: 2.0676516021086164   (start: 12090, end: 12100)\n",
      "Batch 1210 - batch loss: 2.5999906063079834 - avg loss: 2.0680911884044044   (start: 12100, end: 12110)\n",
      "Batch 1211 - batch loss: 1.5880606174468994 - avg loss: 2.0676951235768817   (start: 12110, end: 12120)\n",
      "Batch 1212 - batch loss: 1.9835197925567627 - avg loss: 2.0676257292396847   (start: 12120, end: 12130)\n",
      "Batch 1213 - batch loss: 2.1034958362579346 - avg loss: 2.067655276280062   (start: 12130, end: 12140)\n",
      "Batch 1214 - batch loss: 1.9281280040740967 - avg loss: 2.0675404390189875   (start: 12140, end: 12150)\n",
      "Batch 1215 - batch loss: 1.7317966222763062 - avg loss: 2.0672643339065346   (start: 12150, end: 12160)\n",
      "Batch 1216 - batch loss: 1.9964618682861328 - avg loss: 2.067206156038317   (start: 12160, end: 12170)\n",
      "Batch 1217 - batch loss: 1.6702474355697632 - avg loss: 2.0668802457587865   (start: 12170, end: 12180)\n",
      "Batch 1218 - batch loss: 2.093571424484253 - avg loss: 2.0669021417216458   (start: 12180, end: 12190)\n",
      "Batch 1219 - batch loss: 2.1606006622314453 - avg loss: 2.066978943787637   (start: 12190, end: 12200)\n",
      "Batch 1220 - batch loss: 1.7622301578521729 - avg loss: 2.0667293542823666   (start: 12200, end: 12210)\n",
      "Batch 1221 - batch loss: 1.6876691579818726 - avg loss: 2.0664191577223825   (start: 12210, end: 12220)\n",
      "Batch 1222 - batch loss: 2.3016459941864014 - avg loss: 2.066611493647537   (start: 12220, end: 12230)\n",
      "Batch 1223 - batch loss: 1.8615376949310303 - avg loss: 2.0664439496943374   (start: 12230, end: 12240)\n",
      "Batch 1224 - batch loss: 3.086695671081543 - avg loss: 2.0672768082424087   (start: 12240, end: 12250)\n",
      "Batch 1225 - batch loss: 2.0267953872680664 - avg loss: 2.0672437891388404   (start: 12250, end: 12260)\n",
      "Batch 1226 - batch loss: 1.733486533164978 - avg loss: 2.066971778335276   (start: 12260, end: 12270)\n",
      "Batch 1227 - batch loss: 1.6134626865386963 - avg loss: 2.0666024712572657   (start: 12270, end: 12280)\n",
      "Batch 1228 - batch loss: 2.008419990539551 - avg loss: 2.066555129938537   (start: 12280, end: 12290)\n",
      "Batch 1229 - batch loss: 1.4074978828430176 - avg loss: 2.0660193110384593   (start: 12290, end: 12300)\n",
      "Batch 1230 - batch loss: 0.9833919405937195 - avg loss: 2.0651398412005673   (start: 12300, end: 12310)\n",
      "Batch 1231 - batch loss: 1.3159914016723633 - avg loss: 2.064531766168483   (start: 12310, end: 12320)\n",
      "Batch 1232 - batch loss: 2.697929859161377 - avg loss: 2.06504547102898   (start: 12320, end: 12330)\n",
      "Batch 1233 - batch loss: 1.8039048910140991 - avg loss: 2.0648338498134087   (start: 12330, end: 12340)\n",
      "Batch 1234 - batch loss: 1.6965230703353882 - avg loss: 2.0645356224616047   (start: 12340, end: 12350)\n",
      "Batch 1235 - batch loss: 1.7514950037002563 - avg loss: 2.0642823533525743   (start: 12350, end: 12360)\n",
      "Batch 1236 - batch loss: 2.0749592781066895 - avg loss: 2.0642909846579536   (start: 12360, end: 12370)\n",
      "Batch 1237 - batch loss: 1.7730478048324585 - avg loss: 2.064055731685558   (start: 12370, end: 12380)\n",
      "Batch 1238 - batch loss: 1.6895456314086914 - avg loss: 2.0637534636465937   (start: 12380, end: 12390)\n",
      "Batch 1239 - batch loss: 2.1443684101104736 - avg loss: 2.063818475700194   (start: 12390, end: 12400)\n",
      "Batch 1240 - batch loss: 1.5949137210845947 - avg loss: 2.063440631417667   (start: 12400, end: 12410)\n",
      "Batch 1241 - batch loss: 1.8611395359039307 - avg loss: 2.063277748087946   (start: 12410, end: 12420)\n",
      "Batch 1242 - batch loss: 2.530792713165283 - avg loss: 2.0636538663221193   (start: 12420, end: 12430)\n",
      "Batch 1243 - batch loss: 1.3194549083709717 - avg loss: 2.0630556356485252   (start: 12430, end: 12440)\n",
      "Batch 1244 - batch loss: 1.6717157363891602 - avg loss: 2.062741306412172   (start: 12440, end: 12450)\n",
      "Batch 1245 - batch loss: 1.483472228050232 - avg loss: 2.062276403460036   (start: 12450, end: 12460)\n",
      "Batch 1246 - batch loss: 1.97311270236969 - avg loss: 2.062204900893003   (start: 12460, end: 12470)\n",
      "Batch 1247 - batch loss: 2.527902603149414 - avg loss: 2.062578056103144   (start: 12470, end: 12480)\n",
      "Batch 1248 - batch loss: 2.35365629196167 - avg loss: 2.062811105131053   (start: 12480, end: 12490)\n",
      "Batch 1249 - batch loss: 1.2829235792160034 - avg loss: 2.062187195110321   (start: 12490, end: 12500)\n",
      "Batch 1250 - batch loss: 1.3277307748794556 - avg loss: 2.061600099650504   (start: 12500, end: 12510)\n",
      "Batch 1251 - batch loss: 2.510296583175659 - avg loss: 2.061958483423288   (start: 12510, end: 12520)\n",
      "Batch 1252 - batch loss: 2.596564531326294 - avg loss: 2.0623851442755647   (start: 12520, end: 12530)\n",
      "Batch 1253 - batch loss: 2.6989822387695312 - avg loss: 2.0628927974609668   (start: 12530, end: 12540)\n",
      "Batch 1254 - batch loss: 2.0681517124176025 - avg loss: 2.0628969878314503   (start: 12540, end: 12550)\n",
      "Batch 1255 - batch loss: 1.891549825668335 - avg loss: 2.0627605649316387   (start: 12550, end: 12560)\n",
      "Batch 1256 - batch loss: 1.963594675064087 - avg loss: 2.062681674008912   (start: 12560, end: 12570)\n",
      "Batch 1257 - batch loss: 2.534592866897583 - avg loss: 2.0630568021431634   (start: 12570, end: 12580)\n",
      "Batch 1258 - batch loss: 2.113525390625 - avg loss: 2.0630968883929506   (start: 12580, end: 12590)\n",
      "Batch 1259 - batch loss: 2.3475279808044434 - avg loss: 2.063322627355182   (start: 12590, end: 12600)\n",
      "Batch 1260 - batch loss: 2.1290245056152344 - avg loss: 2.063374730351423   (start: 12600, end: 12610)\n",
      "Batch 1261 - batch loss: 1.7857277393341064 - avg loss: 2.0631547248117896   (start: 12610, end: 12620)\n",
      "Batch 1262 - batch loss: 1.7016899585723877 - avg loss: 2.062868529430761   (start: 12620, end: 12630)\n",
      "Batch 1263 - batch loss: 2.2822070121765137 - avg loss: 2.063042056711414   (start: 12630, end: 12640)\n",
      "Batch 1264 - batch loss: 2.6921305656433105 - avg loss: 2.0635393598805303   (start: 12640, end: 12650)\n",
      "Batch 1265 - batch loss: 3.623168468475342 - avg loss: 2.0647712944054866   (start: 12650, end: 12660)\n",
      "Batch 1266 - batch loss: 1.3051491975784302 - avg loss: 2.0641717505248023   (start: 12660, end: 12670)\n",
      "Batch 1267 - batch loss: 2.128817081451416 - avg loss: 2.0642227326469844   (start: 12670, end: 12680)\n",
      "Batch 1268 - batch loss: 2.7559549808502197 - avg loss: 2.064767832921376   (start: 12680, end: 12690)\n",
      "Batch 1269 - batch loss: 1.909458875656128 - avg loss: 2.0646455424038446   (start: 12690, end: 12700)\n",
      "Batch 1270 - batch loss: 2.0444483757019043 - avg loss: 2.064629651635393   (start: 12700, end: 12710)\n",
      "Batch 1271 - batch loss: 1.7327839136123657 - avg loss: 2.064368766621224   (start: 12710, end: 12720)\n",
      "Batch 1272 - batch loss: 1.415100336074829 - avg loss: 2.0638587364322634   (start: 12720, end: 12730)\n",
      "Batch 1273 - batch loss: 2.1803741455078125 - avg loss: 2.063950192797315   (start: 12730, end: 12740)\n",
      "Batch 1274 - batch loss: 2.2590994834899902 - avg loss: 2.064103251064525   (start: 12740, end: 12750)\n",
      "Batch 1275 - batch loss: 2.343935489654541 - avg loss: 2.064322555326743   (start: 12750, end: 12760)\n",
      "Batch 1276 - batch loss: 2.3945882320404053 - avg loss: 2.064581181541867   (start: 12760, end: 12770)\n",
      "Batch 1277 - batch loss: 1.8902037143707275 - avg loss: 2.0644447359494014   (start: 12770, end: 12780)\n",
      "Batch 1278 - batch loss: 2.5083651542663574 - avg loss: 2.0647918199355755   (start: 12780, end: 12790)\n",
      "Batch 1279 - batch loss: 2.162281036376953 - avg loss: 2.0648679833859207   (start: 12790, end: 12800)\n",
      "Batch 1280 - batch loss: 2.7649552822113037 - avg loss: 2.0654144996223183   (start: 12800, end: 12810)\n",
      "Batch 1281 - batch loss: 2.068939208984375 - avg loss: 2.0654172490055958   (start: 12810, end: 12820)\n",
      "Batch 1282 - batch loss: 1.8955419063568115 - avg loss: 2.0652848442178726   (start: 12820, end: 12830)\n",
      "Batch 1283 - batch loss: 2.535991907119751 - avg loss: 2.0656514385036218   (start: 12830, end: 12840)\n",
      "Batch 1284 - batch loss: 1.3378639221191406 - avg loss: 2.0650850668955405   (start: 12840, end: 12850)\n",
      "Batch 1285 - batch loss: 2.594878673553467 - avg loss: 2.0654970370406867   (start: 12850, end: 12860)\n",
      "Batch 1286 - batch loss: 1.9225118160247803 - avg loss: 2.0653859374128576   (start: 12860, end: 12870)\n",
      "Batch 1287 - batch loss: 2.1568655967712402 - avg loss: 2.065456961993105   (start: 12870, end: 12880)\n",
      "Batch 1288 - batch loss: 1.877343773841858 - avg loss: 2.0653110246865487   (start: 12880, end: 12890)\n",
      "Batch 1289 - batch loss: 2.528022289276123 - avg loss: 2.065669715589331   (start: 12890, end: 12900)\n",
      "Batch 1290 - batch loss: 1.764615774154663 - avg loss: 2.0654365212117676   (start: 12900, end: 12910)\n",
      "Batch 1291 - batch loss: 1.7213268280029297 - avg loss: 2.065170182439934   (start: 12910, end: 12920)\n",
      "Batch 1292 - batch loss: 2.1565213203430176 - avg loss: 2.065240832971955   (start: 12920, end: 12930)\n",
      "Batch 1293 - batch loss: 1.2188098430633545 - avg loss: 2.064586713196137   (start: 12930, end: 12940)\n",
      "Batch 1294 - batch loss: 2.7105581760406494 - avg loss: 2.065085532858565   (start: 12940, end: 12950)\n",
      "Batch 1295 - batch loss: 1.8502734899520874 - avg loss: 2.0649197828254584   (start: 12950, end: 12960)\n",
      "Batch 1296 - batch loss: 2.949742555618286 - avg loss: 2.0656019900519755   (start: 12960, end: 12970)\n",
      "Batch 1297 - batch loss: 1.98138427734375 - avg loss: 2.0655371073765454   (start: 12970, end: 12980)\n",
      "Batch 1298 - batch loss: 1.9274446964263916 - avg loss: 2.065430800670656   (start: 12980, end: 12990)\n",
      "Batch 1299 - batch loss: 2.0684456825256348 - avg loss: 2.0654331198105447   (start: 12990, end: 13000)\n",
      "Batch 1300 - batch loss: 1.9707139730453491 - avg loss: 2.06536031493217   (start: 13000, end: 13010)\n",
      "Batch 1301 - batch loss: 2.1182353496551514 - avg loss: 2.0654009255579173   (start: 13010, end: 13020)\n",
      "Batch 1302 - batch loss: 2.174546241760254 - avg loss: 2.0654846901904595   (start: 13020, end: 13030)\n",
      "Batch 1303 - batch loss: 2.374375104904175 - avg loss: 2.0657215693428475   (start: 13030, end: 13040)\n",
      "Batch 1304 - batch loss: 2.437016487121582 - avg loss: 2.066006086521222   (start: 13040, end: 13050)\n",
      "Batch 1305 - batch loss: 2.182008981704712 - avg loss: 2.066094909565007   (start: 13050, end: 13060)\n",
      "Batch 1306 - batch loss: 2.2348315715789795 - avg loss: 2.0662240118312765   (start: 13060, end: 13070)\n",
      "Batch 1307 - batch loss: 2.336906909942627 - avg loss: 2.066430955942982   (start: 13070, end: 13080)\n",
      "Batch 1308 - batch loss: 3.199669599533081 - avg loss: 2.0672966844713168   (start: 13080, end: 13090)\n",
      "Batch 1309 - batch loss: 1.8000799417495728 - avg loss: 2.0670927022249646   (start: 13090, end: 13100)\n",
      "Batch 1310 - batch loss: 1.9233118295669556 - avg loss: 2.066983029553219   (start: 13100, end: 13110)\n",
      "Batch 1311 - batch loss: 1.7558796405792236 - avg loss: 2.0667459080677206   (start: 13110, end: 13120)\n",
      "Batch 1312 - batch loss: 1.827490210533142 - avg loss: 2.0665636874298423   (start: 13120, end: 13130)\n",
      "Batch 1313 - batch loss: 2.0205624103546143 - avg loss: 2.066528678847593   (start: 13130, end: 13140)\n",
      "Batch 1314 - batch loss: 2.9894728660583496 - avg loss: 2.067230537545092   (start: 13140, end: 13150)\n",
      "Batch 1315 - batch loss: 1.582261562347412 - avg loss: 2.066862020086735   (start: 13150, end: 13160)\n",
      "Batch 1316 - batch loss: 2.8864498138427734 - avg loss: 2.067484334280931   (start: 13160, end: 13170)\n",
      "Batch 1317 - batch loss: 2.1002819538116455 - avg loss: 2.0675092186660073   (start: 13170, end: 13180)\n",
      "Batch 1318 - batch loss: 1.547450304031372 - avg loss: 2.0671149359407344   (start: 13180, end: 13190)\n",
      "Batch 1319 - batch loss: 1.3841526508331299 - avg loss: 2.0665975402701986   (start: 13190, end: 13200)\n",
      "Batch 1320 - batch loss: 2.3670928478240967 - avg loss: 2.0668250159004438   (start: 13200, end: 13210)\n",
      "Batch 1321 - batch loss: 1.7452558279037476 - avg loss: 2.06658177143146   (start: 13210, end: 13220)\n",
      "Batch 1322 - batch loss: 2.8633012771606445 - avg loss: 2.0671839781629253   (start: 13220, end: 13230)\n",
      "Batch 1323 - batch loss: 1.967289924621582 - avg loss: 2.067108529482003   (start: 13230, end: 13240)\n",
      "Batch 1324 - batch loss: 1.7736908197402954 - avg loss: 2.066887082153896   (start: 13240, end: 13250)\n",
      "Batch 1325 - batch loss: 1.3439021110534668 - avg loss: 2.0663418446191297   (start: 13250, end: 13260)\n",
      "Batch 1326 - batch loss: 0.9992621541023254 - avg loss: 2.065537715236675   (start: 13260, end: 13270)\n",
      "Batch 1327 - batch loss: 1.5614702701568604 - avg loss: 2.0651581463774287   (start: 13270, end: 13280)\n",
      "Batch 1328 - batch loss: 1.6835438013076782 - avg loss: 2.064871002400702   (start: 13280, end: 13290)\n",
      "Batch 1329 - batch loss: 2.158482789993286 - avg loss: 2.064941387203403   (start: 13290, end: 13300)\n",
      "Batch 1330 - batch loss: 2.1312668323516846 - avg loss: 2.064991218492019   (start: 13300, end: 13310)\n",
      "Batch 1331 - batch loss: 2.8174211978912354 - avg loss: 2.0655561058639407   (start: 13310, end: 13320)\n",
      "Batch 1332 - batch loss: 2.464668035507202 - avg loss: 2.065855514663373   (start: 13320, end: 13330)\n",
      "Batch 1333 - batch loss: 2.1775572299957275 - avg loss: 2.0659392490826627   (start: 13330, end: 13340)\n",
      "Batch 1334 - batch loss: 1.0835670232772827 - avg loss: 2.06520338973749   (start: 13340, end: 13350)\n",
      "Batch 1335 - batch loss: 1.9585891962051392 - avg loss: 2.065123588694427   (start: 13350, end: 13360)\n",
      "Batch 1336 - batch loss: 2.829223394393921 - avg loss: 2.0656950919148453   (start: 13360, end: 13370)\n",
      "Batch 1337 - batch loss: 1.0497496128082275 - avg loss: 2.0649357903609538   (start: 13370, end: 13380)\n",
      "Batch 1338 - batch loss: 2.234133243560791 - avg loss: 2.0650621514163685   (start: 13380, end: 13390)\n",
      "Batch 1339 - batch loss: 2.5160560607910156 - avg loss: 2.0653987140353047   (start: 13390, end: 13400)\n",
      "Batch 1340 - batch loss: 1.8699527978897095 - avg loss: 2.0652529676399687   (start: 13400, end: 13410)\n",
      "Batch 1341 - batch loss: 2.234907627105713 - avg loss: 2.065379386909317   (start: 13410, end: 13420)\n",
      "Batch 1342 - batch loss: 2.3452653884887695 - avg loss: 2.065587790484581   (start: 13420, end: 13430)\n",
      "Batch 1343 - batch loss: 1.7217776775360107 - avg loss: 2.0653319793886373   (start: 13430, end: 13440)\n",
      "Batch 1344 - batch loss: 2.0756990909576416 - avg loss: 2.0653396872782794   (start: 13440, end: 13450)\n",
      "Batch 1345 - batch loss: 1.4654051065444946 - avg loss: 2.0648939706506915   (start: 13450, end: 13460)\n",
      "Batch 1346 - batch loss: 2.2427308559417725 - avg loss: 2.0650259950644188   (start: 13460, end: 13470)\n",
      "Batch 1347 - batch loss: 1.410226583480835 - avg loss: 2.064540238824372   (start: 13470, end: 13480)\n",
      "Batch 1348 - batch loss: 1.9884694814682007 - avg loss: 2.0644838483444934   (start: 13480, end: 13490)\n",
      "Batch 1349 - batch loss: 2.340796947479248 - avg loss: 2.0646885247142226   (start: 13490, end: 13500)\n",
      "Batch 1350 - batch loss: 2.860776424407959 - avg loss: 2.065277782967142   (start: 13500, end: 13510)\n",
      "Batch 1351 - batch loss: 2.404635429382324 - avg loss: 2.065528787143484   (start: 13510, end: 13520)\n",
      "Batch 1352 - batch loss: 1.3970601558685303 - avg loss: 2.065034723114456   (start: 13520, end: 13530)\n",
      "Batch 1353 - batch loss: 2.692753553390503 - avg loss: 2.0654983263864475   (start: 13530, end: 13540)\n",
      "Batch 1354 - batch loss: 1.457268476486206 - avg loss: 2.06504944826844   (start: 13540, end: 13550)\n",
      "Batch 1355 - batch loss: 1.1993722915649414 - avg loss: 2.06441104328562   (start: 13550, end: 13560)\n",
      "Batch 1356 - batch loss: 2.6016769409179688 - avg loss: 2.064806965096698   (start: 13560, end: 13570)\n",
      "Batch 1357 - batch loss: 2.0237619876861572 - avg loss: 2.0647767405183397   (start: 13570, end: 13580)\n",
      "Batch 1358 - batch loss: 1.7588729858398438 - avg loss: 2.0645516457761186   (start: 13580, end: 13590)\n",
      "Batch 1359 - batch loss: 1.6851189136505127 - avg loss: 2.064272651120144   (start: 13590, end: 13600)\n",
      "Batch 1360 - batch loss: 1.7153425216674805 - avg loss: 2.064016273361545   (start: 13600, end: 13610)\n",
      "Batch 1361 - batch loss: 2.069143056869507 - avg loss: 2.064020037519774   (start: 13610, end: 13620)\n",
      "Batch 1362 - batch loss: 2.137120246887207 - avg loss: 2.064073669368173   (start: 13620, end: 13630)\n",
      "Batch 1363 - batch loss: 1.8389450311660767 - avg loss: 2.0639086190469103   (start: 13630, end: 13640)\n",
      "Batch 1364 - batch loss: 1.8646957874298096 - avg loss: 2.063762675580524   (start: 13640, end: 13650)\n",
      "Batch 1365 - batch loss: 2.3166544437408447 - avg loss: 2.0639478086465273   (start: 13650, end: 13660)\n",
      "Batch 1366 - batch loss: 2.1968133449554443 - avg loss: 2.064045003625539   (start: 13660, end: 13670)\n",
      "Batch 1367 - batch loss: 1.3969359397888184 - avg loss: 2.063557350801097   (start: 13670, end: 13680)\n",
      "Batch 1368 - batch loss: 2.542027711868286 - avg loss: 2.063906854351913   (start: 13680, end: 13690)\n",
      "Batch 1369 - batch loss: 2.0307745933532715 - avg loss: 2.0638826702197974   (start: 13690, end: 13700)\n",
      "Batch 1370 - batch loss: 1.6129772663116455 - avg loss: 2.0635537822519576   (start: 13700, end: 13710)\n",
      "Batch 1371 - batch loss: 2.4444353580474854 - avg loss: 2.0638313927299428   (start: 13710, end: 13720)\n",
      "Batch 1372 - batch loss: 1.0753275156021118 - avg loss: 2.063111433606033   (start: 13720, end: 13730)\n",
      "Batch 1373 - batch loss: 2.439753770828247 - avg loss: 2.0633855546666027   (start: 13730, end: 13740)\n",
      "Batch 1374 - batch loss: 1.2617708444595337 - avg loss: 2.0628025621500883   (start: 13740, end: 13750)\n",
      "Batch 1375 - batch loss: 1.9492381811141968 - avg loss: 2.0627200298964286   (start: 13750, end: 13760)\n",
      "Batch 1376 - batch loss: 1.3574057817459106 - avg loss: 2.062207819113458   (start: 13760, end: 13770)\n",
      "Batch 1377 - batch loss: 1.3812799453735352 - avg loss: 2.06171367696996   (start: 13770, end: 13780)\n",
      "Batch 1378 - batch loss: 1.9258285760879517 - avg loss: 2.061615138100575   (start: 13780, end: 13790)\n",
      "Batch 1379 - batch loss: 2.476712465286255 - avg loss: 2.0619159332652024   (start: 13790, end: 13800)\n",
      "Batch 1380 - batch loss: 1.5770423412322998 - avg loss: 2.061564830012463   (start: 13800, end: 13810)\n",
      "Batch 1381 - batch loss: 2.524035930633545 - avg loss: 2.0618994690143597   (start: 13810, end: 13820)\n",
      "Batch 1382 - batch loss: 2.4474539756774902 - avg loss: 2.0621782502917734   (start: 13820, end: 13830)\n",
      "Batch 1383 - batch loss: 2.017728328704834 - avg loss: 2.062146133296407   (start: 13830, end: 13840)\n",
      "Batch 1384 - batch loss: 1.7427387237548828 - avg loss: 2.061915514228146   (start: 13840, end: 13850)\n",
      "Batch 1385 - batch loss: 2.8828742504119873 - avg loss: 2.062507836548625   (start: 13850, end: 13860)\n",
      "Batch 1386 - batch loss: 2.123096466064453 - avg loss: 2.0625515197710587   (start: 13860, end: 13870)\n",
      "Batch 1387 - batch loss: 2.408890724182129 - avg loss: 2.062801043693545   (start: 13870, end: 13880)\n",
      "Batch 1388 - batch loss: 1.4771190881729126 - avg loss: 2.0623793864181525   (start: 13880, end: 13890)\n",
      "Batch 1389 - batch loss: 1.3208227157592773 - avg loss: 2.061845892410484   (start: 13890, end: 13900)\n",
      "Batch 1390 - batch loss: 2.718149185180664 - avg loss: 2.0623177136130506   (start: 13900, end: 13910)\n",
      "Batch 1391 - batch loss: 2.3282980918884277 - avg loss: 2.062508791471007   (start: 13910, end: 13920)\n",
      "Batch 1392 - batch loss: 2.620023727416992 - avg loss: 2.0629090175556777   (start: 13920, end: 13930)\n",
      "Batch 1393 - batch loss: 1.8485107421875 - avg loss: 2.0627552167842516   (start: 13930, end: 13940)\n",
      "Batch 1394 - batch loss: 1.5373306274414062 - avg loss: 2.0623785683331097   (start: 13940, end: 13950)\n",
      "Batch 1395 - batch loss: 2.220076560974121 - avg loss: 2.062491532511219   (start: 13950, end: 13960)\n",
      "Batch 1396 - batch loss: 2.525343418121338 - avg loss: 2.0628228509690647   (start: 13960, end: 13970)\n",
      "Batch 1397 - batch loss: 1.0141830444335938 - avg loss: 2.06207275096439   (start: 13970, end: 13980)\n",
      "Batch 1398 - batch loss: 1.274143934249878 - avg loss: 2.061509542374887   (start: 13980, end: 13990)\n",
      "Batch 1399 - batch loss: 2.3461108207702637 - avg loss: 2.061712829002312   (start: 13990, end: 14000)\n",
      "Batch 1400 - batch loss: 2.7665493488311768 - avg loss: 2.0622159243055447   (start: 14000, end: 14010)\n",
      "Batch 1401 - batch loss: 2.5931174755096436 - avg loss: 2.06259459873579   (start: 14010, end: 14020)\n",
      "Batch 1402 - batch loss: 2.035062551498413 - avg loss: 2.0625749750385434   (start: 14020, end: 14030)\n",
      "Batch 1403 - batch loss: 2.237192153930664 - avg loss: 2.0626993462485803   (start: 14030, end: 14040)\n",
      "Batch 1404 - batch loss: 2.801755666732788 - avg loss: 2.063225364982021   (start: 14040, end: 14050)\n",
      "Batch 1405 - batch loss: 2.211379289627075 - avg loss: 2.063330737616904   (start: 14050, end: 14060)\n",
      "Batch 1406 - batch loss: 2.1161017417907715 - avg loss: 2.0633682436610927   (start: 14060, end: 14070)\n",
      "Batch 1407 - batch loss: 2.010617733001709 - avg loss: 2.0633307788097723   (start: 14070, end: 14080)\n",
      "Batch 1408 - batch loss: 2.0469741821289062 - avg loss: 2.063319170153505   (start: 14080, end: 14090)\n",
      "Batch 1409 - batch loss: 1.521547555923462 - avg loss: 2.0629349349661075   (start: 14090, end: 14100)\n",
      "Batch 1410 - batch loss: 2.2112278938293457 - avg loss: 2.06304003273993   (start: 14100, end: 14110)\n",
      "Batch 1411 - batch loss: 2.4662652015686035 - avg loss: 2.0633256029728115   (start: 14110, end: 14120)\n",
      "Batch 1412 - batch loss: 2.404658794403076 - avg loss: 2.063567169279556   (start: 14120, end: 14130)\n",
      "Batch 1413 - batch loss: 2.46683931350708 - avg loss: 2.063852368815785   (start: 14130, end: 14140)\n",
      "Batch 1414 - batch loss: 2.1587977409362793 - avg loss: 2.063919468018697   (start: 14140, end: 14150)\n",
      "Batch 1415 - batch loss: 2.4064269065856934 - avg loss: 2.0641613518029955   (start: 14150, end: 14160)\n",
      "Batch 1416 - batch loss: 2.224216938018799 - avg loss: 2.064274305639422   (start: 14160, end: 14170)\n",
      "Batch 1417 - batch loss: 2.6385912895202637 - avg loss: 2.064679324668957   (start: 14170, end: 14180)\n",
      "Batch 1418 - batch loss: 2.3717007637023926 - avg loss: 2.0648956893194383   (start: 14180, end: 14190)\n",
      "Batch 1419 - batch loss: 1.923789620399475 - avg loss: 2.064796318848368   (start: 14190, end: 14200)\n",
      "Batch 1420 - batch loss: 2.413586139678955 - avg loss: 2.0650417726279815   (start: 14200, end: 14210)\n",
      "Batch 1421 - batch loss: 2.393031597137451 - avg loss: 2.065272426513009   (start: 14210, end: 14220)\n",
      "Batch 1422 - batch loss: 2.351820468902588 - avg loss: 2.0654737954816595   (start: 14220, end: 14230)\n",
      "Batch 1423 - batch loss: 2.0204920768737793 - avg loss: 2.0654422071961203   (start: 14230, end: 14240)\n",
      "Batch 1424 - batch loss: 1.5349632501602173 - avg loss: 2.0650699412613585   (start: 14240, end: 14250)\n",
      "Batch 1425 - batch loss: 1.7763853073120117 - avg loss: 2.0648674976190375   (start: 14250, end: 14260)\n",
      "Batch 1426 - batch loss: 2.454123020172119 - avg loss: 2.06514027654164   (start: 14260, end: 14270)\n",
      "Batch 1427 - batch loss: 1.546015977859497 - avg loss: 2.064776744119593   (start: 14270, end: 14280)\n",
      "Batch 1428 - batch loss: 2.126237392425537 - avg loss: 2.0648197536705424   (start: 14280, end: 14290)\n",
      "Batch 1429 - batch loss: 2.3275234699249268 - avg loss: 2.065003462563028   (start: 14290, end: 14300)\n",
      "Batch 1430 - batch loss: 1.5483042001724243 - avg loss: 2.064642386907968   (start: 14300, end: 14310)\n",
      "Batch 1431 - batch loss: 2.404841661453247 - avg loss: 2.0648799562337676   (start: 14310, end: 14320)\n",
      "Batch 1432 - batch loss: 1.5703604221343994 - avg loss: 2.064534862350935   (start: 14320, end: 14330)\n",
      "Batch 1433 - batch loss: 2.273953914642334 - avg loss: 2.0646809007416542   (start: 14330, end: 14340)\n",
      "Batch 1434 - batch loss: 1.6965429782867432 - avg loss: 2.064424358635414   (start: 14340, end: 14350)\n",
      "Batch 1435 - batch loss: 2.170962333679199 - avg loss: 2.0644985494258346   (start: 14350, end: 14360)\n",
      "Batch 1436 - batch loss: 1.4953892230987549 - avg loss: 2.0641025095327743   (start: 14360, end: 14370)\n",
      "Batch 1437 - batch loss: 1.5867512226104736 - avg loss: 2.0637705545349148   (start: 14370, end: 14380)\n",
      "Batch 1438 - batch loss: 2.209216356277466 - avg loss: 2.063871628754333   (start: 14380, end: 14390)\n",
      "Batch 1439 - batch loss: 2.6565890312194824 - avg loss: 2.0642832380616003   (start: 14390, end: 14400)\n",
      "Batch 1440 - batch loss: 2.358067035675049 - avg loss: 2.064487113007897   (start: 14400, end: 14410)\n",
      "Batch 1441 - batch loss: 2.2513134479522705 - avg loss: 2.0646166735730453   (start: 14410, end: 14420)\n",
      "Batch 1442 - batch loss: 1.7841869592666626 - avg loss: 2.064422335586693   (start: 14420, end: 14430)\n",
      "Batch 1443 - batch loss: 1.8827539682388306 - avg loss: 2.064296526468031   (start: 14430, end: 14440)\n",
      "Batch 1444 - batch loss: 1.646058440208435 - avg loss: 2.0640070883460524   (start: 14440, end: 14450)\n",
      "Batch 1445 - batch loss: 2.2720654010772705 - avg loss: 2.064150973762879   (start: 14450, end: 14460)\n",
      "Batch 1446 - batch loss: 1.984915018081665 - avg loss: 2.064096214982173   (start: 14460, end: 14470)\n",
      "Batch 1447 - batch loss: 1.7194560766220093 - avg loss: 2.063858203836897   (start: 14470, end: 14480)\n",
      "Batch 1448 - batch loss: 2.934009313583374 - avg loss: 2.064458722201111   (start: 14480, end: 14490)\n",
      "Batch 1449 - batch loss: 2.1313624382019043 - avg loss: 2.0645048626949047   (start: 14490, end: 14500)\n",
      "Batch 1450 - batch loss: 1.857039451599121 - avg loss: 2.0643618817086224   (start: 14500, end: 14510)\n",
      "Batch 1451 - batch loss: 1.5190900564193726 - avg loss: 2.0639863501485056   (start: 14510, end: 14520)\n",
      "Batch 1452 - batch loss: 2.112563371658325 - avg loss: 2.0640197823725317   (start: 14520, end: 14530)\n",
      "Batch 1453 - batch loss: 1.5780361890792847 - avg loss: 2.063685543312495   (start: 14530, end: 14540)\n",
      "Batch 1454 - batch loss: 1.504772424697876 - avg loss: 2.0633014105849248   (start: 14540, end: 14550)\n",
      "Batch 1455 - batch loss: 1.1736828088760376 - avg loss: 2.062690408798037   (start: 14550, end: 14560)\n",
      "Batch 1456 - batch loss: 1.671306848526001 - avg loss: 2.062421785901488   (start: 14560, end: 14570)\n",
      "Batch 1457 - batch loss: 2.3377416133880615 - avg loss: 2.06261061980237   (start: 14570, end: 14580)\n",
      "Batch 1458 - batch loss: 1.938093900680542 - avg loss: 2.0625252759236026   (start: 14580, end: 14590)\n",
      "Batch 1459 - batch loss: 1.700661063194275 - avg loss: 2.062277423723103   (start: 14590, end: 14600)\n",
      "Batch 1460 - batch loss: 2.09914231300354 - avg loss: 2.0623026563646367   (start: 14600, end: 14610)\n",
      "Batch 1461 - batch loss: 1.0475660562515259 - avg loss: 2.0616085820827537   (start: 14610, end: 14620)\n",
      "Batch 1462 - batch loss: 2.287012815475464 - avg loss: 2.0617626519620376   (start: 14620, end: 14630)\n",
      "Batch 1463 - batch loss: 1.7518094778060913 - avg loss: 2.0615509353130244   (start: 14630, end: 14640)\n",
      "Batch 1464 - batch loss: 2.654636859893799 - avg loss: 2.0619557721216117   (start: 14640, end: 14650)\n",
      "Batch 1465 - batch loss: 1.8851540088653564 - avg loss: 2.0618351706459936   (start: 14650, end: 14660)\n",
      "Batch 1466 - batch loss: 3.011173725128174 - avg loss: 2.0624822998583197   (start: 14660, end: 14670)\n",
      "Batch 1467 - batch loss: 2.108969211578369 - avg loss: 2.0625139666919163   (start: 14670, end: 14680)\n",
      "Batch 1468 - batch loss: 1.3683226108551025 - avg loss: 2.062041406204621   (start: 14680, end: 14690)\n",
      "Batch 1469 - batch loss: 2.5052216053009033 - avg loss: 2.062342889333258   (start: 14690, end: 14700)\n",
      "Batch 1470 - batch loss: 2.2786989212036133 - avg loss: 2.0624899702522725   (start: 14700, end: 14710)\n",
      "Batch 1471 - batch loss: 2.9079458713531494 - avg loss: 2.0630643288807375   (start: 14710, end: 14720)\n",
      "Batch 1472 - batch loss: 3.188566207885742 - avg loss: 2.0638284170538572   (start: 14720, end: 14730)\n",
      "Batch 1473 - batch loss: 2.0623865127563477 - avg loss: 2.063827438828418   (start: 14730, end: 14740)\n",
      "Batch 1474 - batch loss: 1.8675563335418701 - avg loss: 2.0636943736722912   (start: 14740, end: 14750)\n",
      "Batch 1475 - batch loss: 1.6990966796875 - avg loss: 2.0634473562644424   (start: 14750, end: 14760)\n",
      "Batch 1476 - batch loss: 1.5519531965255737 - avg loss: 2.0631010501305638   (start: 14760, end: 14770)\n",
      "Batch 1477 - batch loss: 1.318322777748108 - avg loss: 2.0625971406093306   (start: 14770, end: 14780)\n",
      "Batch 1478 - batch loss: 2.397313117980957 - avg loss: 2.062823452967256   (start: 14780, end: 14790)\n",
      "Batch 1479 - batch loss: 2.4290707111358643 - avg loss: 2.0630709173308834   (start: 14790, end: 14800)\n",
      "Batch 1480 - batch loss: 1.7972309589385986 - avg loss: 2.062891417021368   (start: 14800, end: 14810)\n",
      "Batch 1481 - batch loss: 1.8055992126464844 - avg loss: 2.062717805547431   (start: 14810, end: 14820)\n",
      "Batch 1482 - batch loss: 2.8843040466308594 - avg loss: 2.0632718084072312   (start: 14820, end: 14830)\n",
      "Batch 1483 - batch loss: 2.1351592540740967 - avg loss: 2.0633202500822088   (start: 14830, end: 14840)\n",
      "Batch 1484 - batch loss: 1.7123644351959229 - avg loss: 2.0630839162001307   (start: 14840, end: 14850)\n",
      "Batch 1485 - batch loss: 2.244255542755127 - avg loss: 2.0632058351951206   (start: 14850, end: 14860)\n",
      "Batch 1486 - batch loss: 1.8163360357284546 - avg loss: 2.0630398165001194   (start: 14860, end: 14870)\n",
      "Batch 1487 - batch loss: 2.8182671070098877 - avg loss: 2.063547361722236   (start: 14870, end: 14880)\n",
      "Batch 1488 - batch loss: 2.1817212104797363 - avg loss: 2.063626726294941   (start: 14880, end: 14890)\n",
      "Batch 1489 - batch loss: 2.059056043624878 - avg loss: 2.0636236587226793   (start: 14890, end: 14900)\n",
      "Batch 1490 - batch loss: 2.1316938400268555 - avg loss: 2.063669312767819   (start: 14900, end: 14910)\n",
      "Batch 1491 - batch loss: 2.1973671913146973 - avg loss: 2.0637589226059876   (start: 14910, end: 14920)\n",
      "Batch 1492 - batch loss: 2.757403612136841 - avg loss: 2.064223520522619   (start: 14920, end: 14930)\n",
      "Batch 1493 - batch loss: 1.9607499837875366 - avg loss: 2.064154261127214   (start: 14930, end: 14940)\n",
      "Batch 1494 - batch loss: 2.336803436279297 - avg loss: 2.064336635157416   (start: 14940, end: 14950)\n",
      "Batch 1495 - batch loss: 1.8566818237304688 - avg loss: 2.0641978284652858   (start: 14950, end: 14960)\n",
      "Batch 1496 - batch loss: 1.5626099109649658 - avg loss: 2.0638627663961473   (start: 14960, end: 14970)\n",
      "Batch 1497 - batch loss: 1.6122297048568726 - avg loss: 2.063561275700861   (start: 14970, end: 14980)\n",
      "Batch 1498 - batch loss: 2.2559382915496826 - avg loss: 2.0636896126026945   (start: 14980, end: 14990)\n",
      "Batch 1499 - batch loss: 1.9310863018035889 - avg loss: 2.063601210395495   (start: 14990, end: 15000)\n",
      "Batch 1500 - batch loss: 1.6903152465820312 - avg loss: 2.063352518880629   (start: 15000, end: 15010)\n",
      "Batch 1501 - batch loss: 1.5871047973632812 - avg loss: 2.0630354431672355   (start: 15010, end: 15020)\n",
      "Batch 1502 - batch loss: 2.251300573348999 - avg loss: 2.0631607027348884   (start: 15020, end: 15030)\n",
      "Batch 1503 - batch loss: 2.210144519805908 - avg loss: 2.063258431336664   (start: 15030, end: 15040)\n",
      "Batch 1504 - batch loss: 2.5117805004119873 - avg loss: 2.06355645264502   (start: 15040, end: 15050)\n",
      "Batch 1505 - batch loss: 2.2239603996276855 - avg loss: 2.063662962569975   (start: 15050, end: 15060)\n",
      "Batch 1506 - batch loss: 1.9563665390014648 - avg loss: 2.0635917638814756   (start: 15060, end: 15070)\n",
      "Batch 1507 - batch loss: 2.321258068084717 - avg loss: 2.0637626301309475   (start: 15070, end: 15080)\n",
      "Batch 1508 - batch loss: 2.047729253768921 - avg loss: 2.063752004964372   (start: 15080, end: 15090)\n",
      "Batch 1509 - batch loss: 1.8301804065704346 - avg loss: 2.0635973217866277   (start: 15090, end: 15100)\n",
      "Batch 1510 - batch loss: 1.9760757684707642 - avg loss: 2.0635393988526003   (start: 15100, end: 15110)\n",
      "Batch 1511 - batch loss: 2.0497043132781982 - avg loss: 2.0635302486637284   (start: 15110, end: 15120)\n",
      "Batch 1512 - batch loss: 1.763481855392456 - avg loss: 2.0633319351189354   (start: 15120, end: 15130)\n",
      "Batch 1513 - batch loss: 2.085165023803711 - avg loss: 2.0633463559172744   (start: 15130, end: 15140)\n",
      "Batch 1514 - batch loss: 2.197719097137451 - avg loss: 2.0634350507959676   (start: 15140, end: 15150)\n",
      "Batch 1515 - batch loss: 2.0857512950897217 - avg loss: 2.0634497712737336   (start: 15150, end: 15160)\n",
      "Batch 1516 - batch loss: 1.532545804977417 - avg loss: 2.0630998016189572   (start: 15160, end: 15170)\n",
      "Batch 1517 - batch loss: 1.2898155450820923 - avg loss: 2.0625903917002897   (start: 15170, end: 15180)\n",
      "Batch 1518 - batch loss: 1.2373054027557373 - avg loss: 2.0620470836101354   (start: 15180, end: 15190)\n",
      "Batch 1519 - batch loss: 2.1912174224853516 - avg loss: 2.0621320640962373   (start: 15190, end: 15200)\n",
      "Batch 1520 - batch loss: 1.2266981601715088 - avg loss: 2.061582797887214   (start: 15200, end: 15210)\n",
      "Batch 1521 - batch loss: 1.0069454908370972 - avg loss: 2.060889869301767   (start: 15210, end: 15220)\n",
      "Batch 1522 - batch loss: 1.760115623474121 - avg loss: 2.060692381287435   (start: 15220, end: 15230)\n",
      "Batch 1523 - batch loss: 3.383955717086792 - avg loss: 2.061560664316175   (start: 15230, end: 15240)\n",
      "Batch 1524 - batch loss: 2.263554334640503 - avg loss: 2.061693119181961   (start: 15240, end: 15250)\n",
      "Batch 1525 - batch loss: 2.8398776054382324 - avg loss: 2.062203069697201   (start: 15250, end: 15260)\n",
      "Batch 1526 - batch loss: 1.7335602045059204 - avg loss: 2.0619878484364342   (start: 15260, end: 15270)\n",
      "Batch 1527 - batch loss: 2.202571392059326 - avg loss: 2.06207985337336   (start: 15270, end: 15280)\n",
      "Batch 1528 - batch loss: 1.7631657123565674 - avg loss: 2.0618843568782546   (start: 15280, end: 15290)\n",
      "Batch 1529 - batch loss: 2.5957398414611816 - avg loss: 2.0622332820315767   (start: 15290, end: 15300)\n",
      "Batch 1530 - batch loss: 2.5916426181793213 - avg loss: 2.0625790751969246   (start: 15300, end: 15310)\n",
      "Batch 1531 - batch loss: 1.8850895166397095 - avg loss: 2.062463220393689   (start: 15310, end: 15320)\n",
      "Batch 1532 - batch loss: 2.373256206512451 - avg loss: 2.062665955544451   (start: 15320, end: 15330)\n",
      "Batch 1533 - batch loss: 3.289005756378174 - avg loss: 2.0634653947888015   (start: 15330, end: 15340)\n",
      "Batch 1534 - batch loss: 2.018576145172119 - avg loss: 2.0634361509779766   (start: 15340, end: 15350)\n",
      "Batch 1535 - batch loss: 1.6363584995269775 - avg loss: 2.0631581056319797   (start: 15350, end: 15360)\n",
      "Batch 1536 - batch loss: 2.299710512161255 - avg loss: 2.0633120109062344   (start: 15360, end: 15370)\n",
      "Batch 1537 - batch loss: 1.9417493343353271 - avg loss: 2.0632329714546276   (start: 15370, end: 15380)\n",
      "Batch 1538 - batch loss: 2.7214767932891846 - avg loss: 2.063660680240745   (start: 15380, end: 15390)\n",
      "Batch 1539 - batch loss: 2.0844531059265137 - avg loss: 2.0636741818158657   (start: 15390, end: 15400)\n",
      "Batch 1540 - batch loss: 2.2599904537200928 - avg loss: 2.063801577190236   (start: 15400, end: 15410)\n",
      "Batch 1541 - batch loss: 2.118101119995117 - avg loss: 2.063836790901523   (start: 15410, end: 15420)\n",
      "Batch 1542 - batch loss: 2.334932804107666 - avg loss: 2.0640124850124795   (start: 15420, end: 15430)\n",
      "Batch 1543 - batch loss: 2.538943290710449 - avg loss: 2.064320082684564   (start: 15430, end: 15440)\n",
      "Batch 1544 - batch loss: 1.7773863077163696 - avg loss: 2.0641343650308626   (start: 15440, end: 15450)\n",
      "Batch 1545 - batch loss: 2.438349485397339 - avg loss: 2.0643764187956535   (start: 15450, end: 15460)\n",
      "Batch 1546 - batch loss: 1.856482744216919 - avg loss: 2.064242033744213   (start: 15460, end: 15470)\n",
      "Batch 1547 - batch loss: 2.1770896911621094 - avg loss: 2.064314932747713   (start: 15470, end: 15480)\n",
      "Batch 1548 - batch loss: 2.1313817501068115 - avg loss: 2.0643582295955882   (start: 15480, end: 15490)\n",
      "Batch 1549 - batch loss: 2.330941915512085 - avg loss: 2.064530219070373   (start: 15490, end: 15500)\n",
      "Batch 1550 - batch loss: 1.4914493560791016 - avg loss: 2.0641607278627707   (start: 15500, end: 15510)\n",
      "Batch 1551 - batch loss: 1.9612627029418945 - avg loss: 2.0640944275889814   (start: 15510, end: 15520)\n",
      "Batch 1552 - batch loss: 2.643587350845337 - avg loss: 2.0644675717765257   (start: 15520, end: 15530)\n",
      "Batch 1553 - batch loss: 1.7987544536590576 - avg loss: 2.0642965852140307   (start: 15530, end: 15540)\n",
      "Batch 1554 - batch loss: 2.737626314163208 - avg loss: 2.064729594686024   (start: 15540, end: 15550)\n",
      "Batch 1555 - batch loss: 2.343600273132324 - avg loss: 2.0649088174870815   (start: 15550, end: 15560)\n",
      "Batch 1556 - batch loss: 3.332284450531006 - avg loss: 2.0657228031216635   (start: 15560, end: 15570)\n",
      "Batch 1557 - batch loss: 2.2534916400909424 - avg loss: 2.065843322272478   (start: 15570, end: 15580)\n",
      "Batch 1558 - batch loss: 2.3464925289154053 - avg loss: 2.0660233410066944   (start: 15580, end: 15590)\n",
      "Batch 1559 - batch loss: 3.27447772026062 - avg loss: 2.066797991249806   (start: 15590, end: 15600)\n",
      "Batch 1560 - batch loss: 1.6425189971923828 - avg loss: 2.066526191766105   (start: 15600, end: 15610)\n",
      "Batch 1561 - batch loss: 1.6938502788543701 - avg loss: 2.0662876028333828   (start: 15610, end: 15620)\n",
      "Batch 1562 - batch loss: 2.2132275104522705 - avg loss: 2.0663816142905924   (start: 15620, end: 15630)\n",
      "Batch 1563 - batch loss: 2.7749552726745605 - avg loss: 2.0668346665018356   (start: 15630, end: 15640)\n",
      "Batch 1564 - batch loss: 1.7933763265609741 - avg loss: 2.066659932738295   (start: 15640, end: 15650)\n",
      "Batch 1565 - batch loss: 2.7718868255615234 - avg loss: 2.0671102691960366   (start: 15650, end: 15660)\n",
      "Batch 1566 - batch loss: 1.634758710861206 - avg loss: 2.0668343588205835   (start: 15660, end: 15670)\n",
      "Batch 1567 - batch loss: 2.3594789505004883 - avg loss: 2.067020994402012   (start: 15670, end: 15680)\n",
      "Batch 1568 - batch loss: 2.26914119720459 - avg loss: 2.067149815436303   (start: 15680, end: 15690)\n",
      "Batch 1569 - batch loss: 2.0247716903686523 - avg loss: 2.0671228229999543   (start: 15690, end: 15700)\n",
      "Batch 1570 - batch loss: 2.1767163276672363 - avg loss: 2.0671925833466553   (start: 15700, end: 15710)\n",
      "Batch 1571 - batch loss: 1.7435321807861328 - avg loss: 2.0669866925053317   (start: 15710, end: 15720)\n",
      "Batch 1572 - batch loss: 1.7480897903442383 - avg loss: 2.066783960844708   (start: 15720, end: 15730)\n",
      "Batch 1573 - batch loss: 1.7661737203598022 - avg loss: 2.0665929759396984   (start: 15730, end: 15740)\n",
      "Batch 1574 - batch loss: 2.76705002784729 - avg loss: 2.06703771057583   (start: 15740, end: 15750)\n",
      "Batch 1575 - batch loss: 1.2991944551467896 - avg loss: 2.0665505003883755   (start: 15750, end: 15760)\n",
      "Batch 1576 - batch loss: 2.3800129890441895 - avg loss: 2.066749271782577   (start: 15760, end: 15770)\n",
      "Batch 1577 - batch loss: 2.362635850906372 - avg loss: 2.0669367791204247   (start: 15770, end: 15780)\n",
      "Batch 1578 - batch loss: 1.9675527811050415 - avg loss: 2.0668738380197182   (start: 15780, end: 15790)\n",
      "Batch 1579 - batch loss: 2.9430058002471924 - avg loss: 2.067428351919862   (start: 15790, end: 15800)\n",
      "Batch 1580 - batch loss: 1.6680399179458618 - avg loss: 2.067175734314566   (start: 15800, end: 15810)\n",
      "Batch 1581 - batch loss: 1.5908857583999634 - avg loss: 2.066874666061775   (start: 15810, end: 15820)\n",
      "Batch 1582 - batch loss: 2.1962332725524902 - avg loss: 2.0669563834379536   (start: 15820, end: 15830)\n",
      "Batch 1583 - batch loss: 2.486006498336792 - avg loss: 2.0672209352781676   (start: 15830, end: 15840)\n",
      "Batch 1584 - batch loss: 2.1890575885772705 - avg loss: 2.067297803829145   (start: 15840, end: 15850)\n",
      "Batch 1585 - batch loss: 1.4596282243728638 - avg loss: 2.0669146578143556   (start: 15850, end: 15860)\n",
      "Batch 1586 - batch loss: 2.0400283336639404 - avg loss: 2.066897716211236   (start: 15860, end: 15870)\n",
      "Batch 1587 - batch loss: 1.7059122323989868 - avg loss: 2.066670395377601   (start: 15870, end: 15880)\n",
      "Batch 1588 - batch loss: 1.5275901556015015 - avg loss: 2.066331137832116   (start: 15880, end: 15890)\n",
      "Batch 1589 - batch loss: 2.77424955368042 - avg loss: 2.0667763695401966   (start: 15890, end: 15900)\n",
      "Batch 1590 - batch loss: 2.1011204719543457 - avg loss: 2.0667979560282004   (start: 15900, end: 15910)\n",
      "Batch 1591 - batch loss: 1.6552460193634033 - avg loss: 2.0665394435051696   (start: 15910, end: 15920)\n",
      "Batch 1592 - batch loss: 2.2916407585144043 - avg loss: 2.066680750043154   (start: 15920, end: 15930)\n",
      "Batch 1593 - batch loss: 2.2683563232421875 - avg loss: 2.0668072717327397   (start: 15930, end: 15940)\n",
      "Batch 1594 - batch loss: 2.5472264289855957 - avg loss: 2.067108474966127   (start: 15940, end: 15950)\n",
      "Batch 1595 - batch loss: 2.283731698989868 - avg loss: 2.0672442038032344   (start: 15950, end: 15960)\n",
      "Batch 1596 - batch loss: 1.3559184074401855 - avg loss: 2.066798790029682   (start: 15960, end: 15970)\n",
      "Batch 1597 - batch loss: 1.990645408630371 - avg loss: 2.0667511345970166   (start: 15970, end: 15980)\n",
      "Batch 1598 - batch loss: 1.6194219589233398 - avg loss: 2.066471379014982   (start: 15980, end: 15990)\n",
      "Batch 1599 - batch loss: 1.5487887859344482 - avg loss: 2.066147827394307   (start: 15990, end: 16000)\n",
      "Batch 1600 - batch loss: 2.2798099517822266 - avg loss: 2.0662812828124126   (start: 16000, end: 16010)\n",
      "Batch 1601 - batch loss: 2.6022982597351074 - avg loss: 2.066615875182527   (start: 16010, end: 16020)\n",
      "Batch 1602 - batch loss: 1.6512348651885986 - avg loss: 2.066356747914907   (start: 16020, end: 16030)\n",
      "Batch 1603 - batch loss: 1.9022634029388428 - avg loss: 2.066254445330758   (start: 16030, end: 16040)\n",
      "Batch 1604 - batch loss: 2.8067855834960938 - avg loss: 2.0667158354479946   (start: 16040, end: 16050)\n",
      "Batch 1605 - batch loss: 1.8660272359848022 - avg loss: 2.0665908736799605   (start: 16050, end: 16060)\n",
      "Batch 1606 - batch loss: 2.2910265922546387 - avg loss: 2.0667305349858562   (start: 16060, end: 16070)\n",
      "Batch 1607 - batch loss: 1.3168997764587402 - avg loss: 2.0662642223250804   (start: 16070, end: 16080)\n",
      "Batch 1608 - batch loss: 2.3808834552764893 - avg loss: 2.0664597594493515   (start: 16080, end: 16090)\n",
      "Batch 1609 - batch loss: 2.7017011642456055 - avg loss: 2.066854319328107   (start: 16090, end: 16100)\n",
      "Batch 1610 - batch loss: 1.3277419805526733 - avg loss: 2.0663955283046582   (start: 16100, end: 16110)\n",
      "Batch 1611 - batch loss: 2.1544978618621826 - avg loss: 2.0664501823577335   (start: 16110, end: 16120)\n",
      "Batch 1612 - batch loss: 1.5530171394348145 - avg loss: 2.066131872969685   (start: 16120, end: 16130)\n",
      "Batch 1613 - batch loss: 2.3690104484558105 - avg loss: 2.0663195300796513   (start: 16130, end: 16140)\n",
      "Batch 1614 - batch loss: 1.8715473413467407 - avg loss: 2.066198928105204   (start: 16140, end: 16150)\n",
      "Batch 1615 - batch loss: 2.7381157875061035 - avg loss: 2.0666147182409715   (start: 16150, end: 16160)\n",
      "Batch 1616 - batch loss: 2.3600826263427734 - avg loss: 2.0667962073616284   (start: 16160, end: 16170)\n",
      "Batch 1617 - batch loss: 1.5739084482192993 - avg loss: 2.0664915795747665   (start: 16170, end: 16180)\n",
      "Batch 1618 - batch loss: 2.4617419242858887 - avg loss: 2.066735711968041   (start: 16180, end: 16190)\n",
      "Batch 1619 - batch loss: 1.0386271476745605 - avg loss: 2.0661010770518105   (start: 16190, end: 16200)\n",
      "Batch 1620 - batch loss: 2.476294755935669 - avg loss: 2.0663541268228673   (start: 16200, end: 16210)\n",
      "Batch 1621 - batch loss: 1.4321526288986206 - avg loss: 2.0659631271324086   (start: 16210, end: 16220)\n",
      "Batch 1622 - batch loss: 1.9325313568115234 - avg loss: 2.065880914088465   (start: 16220, end: 16230)\n",
      "Batch 1623 - batch loss: 2.4557642936706543 - avg loss: 2.066120990061114   (start: 16230, end: 16240)\n",
      "Batch 1624 - batch loss: 2.5078914165496826 - avg loss: 2.066392848785107   (start: 16240, end: 16250)\n",
      "Batch 1625 - batch loss: 2.6247689723968506 - avg loss: 2.0667362535351756   (start: 16250, end: 16260)\n",
      "Batch 1626 - batch loss: 1.935692548751831 - avg loss: 2.0666557103853394   (start: 16260, end: 16270)\n",
      "Batch 1627 - batch loss: 1.6201989650726318 - avg loss: 2.066381474055295   (start: 16270, end: 16280)\n",
      "Batch 1628 - batch loss: 2.514833450317383 - avg loss: 2.0666567668584026   (start: 16280, end: 16290)\n",
      "Batch 1629 - batch loss: 1.4969439506530762 - avg loss: 2.0663072497932458   (start: 16290, end: 16300)\n",
      "Batch 1630 - batch loss: 1.6174180507659912 - avg loss: 2.0660320264952525   (start: 16300, end: 16310)\n",
      "Batch 1631 - batch loss: 2.3740267753601074 - avg loss: 2.066220748767841   (start: 16310, end: 16320)\n",
      "Batch 1632 - batch loss: 2.4127025604248047 - avg loss: 2.0664329237902885   (start: 16320, end: 16330)\n",
      "Batch 1633 - batch loss: 2.2926576137542725 - avg loss: 2.0665713721929593   (start: 16330, end: 16340)\n",
      "Batch 1634 - batch loss: 1.5847827196121216 - avg loss: 2.0662767002341944   (start: 16340, end: 16350)\n",
      "Batch 1635 - batch loss: 1.733626127243042 - avg loss: 2.066073368588112   (start: 16350, end: 16360)\n",
      "Batch 1636 - batch loss: 2.3833227157592773 - avg loss: 2.066267167822792   (start: 16360, end: 16370)\n",
      "Batch 1637 - batch loss: 1.9137805700302124 - avg loss: 2.0661740746617463   (start: 16370, end: 16380)\n",
      "Batch 1638 - batch loss: 2.5423121452331543 - avg loss: 2.066464579890893   (start: 16380, end: 16390)\n",
      "Batch 1639 - batch loss: 1.2786630392074585 - avg loss: 2.0659842130977935   (start: 16390, end: 16400)\n",
      "Batch 1640 - batch loss: 2.459367275238037 - avg loss: 2.0662239346469344   (start: 16400, end: 16410)\n",
      "Batch 1641 - batch loss: 2.4803643226623535 - avg loss: 2.0664761516920107   (start: 16410, end: 16420)\n",
      "Batch 1642 - batch loss: 2.560772657394409 - avg loss: 2.066777001665049   (start: 16420, end: 16430)\n",
      "Batch 1643 - batch loss: 1.7755056619644165 - avg loss: 2.0665998293172994   (start: 16430, end: 16440)\n",
      "Batch 1644 - batch loss: 2.194275379180908 - avg loss: 2.066677443633326   (start: 16440, end: 16450)\n",
      "Batch 1645 - batch loss: 1.9285075664520264 - avg loss: 2.0665935008160834   (start: 16450, end: 16460)\n",
      "Batch 1646 - batch loss: 2.1671040058135986 - avg loss: 2.0666545272307753   (start: 16460, end: 16470)\n",
      "Batch 1647 - batch loss: 2.3235907554626465 - avg loss: 2.0668104351362557   (start: 16470, end: 16480)\n",
      "Batch 1648 - batch loss: 2.572629451751709 - avg loss: 2.0671171780208013   (start: 16480, end: 16490)\n",
      "Batch 1649 - batch loss: 2.063537120819092 - avg loss: 2.067115008289164   (start: 16490, end: 16500)\n",
      "Batch 1650 - batch loss: 1.7001768350601196 - avg loss: 2.066892756215736   (start: 16500, end: 16510)\n",
      "Batch 1651 - batch loss: 2.4906044006347656 - avg loss: 2.0671492402619944   (start: 16510, end: 16520)\n",
      "Batch 1652 - batch loss: 1.8279879093170166 - avg loss: 2.067004557061181   (start: 16520, end: 16530)\n",
      "Batch 1653 - batch loss: 1.8306337594985962 - avg loss: 2.0668616484774067   (start: 16530, end: 16540)\n",
      "Batch 1654 - batch loss: 2.238227367401123 - avg loss: 2.066965192718448   (start: 16540, end: 16550)\n",
      "Batch 1655 - batch loss: 1.574116826057434 - avg loss: 2.0666675789704647   (start: 16550, end: 16560)\n",
      "Batch 1656 - batch loss: 0.7609517574310303 - avg loss: 2.0658795790781657   (start: 16560, end: 16570)\n",
      "Batch 1657 - batch loss: 1.6050208806991577 - avg loss: 2.065601618463944   (start: 16570, end: 16580)\n",
      "Batch 1658 - batch loss: 2.1211962699890137 - avg loss: 2.065635129405189   (start: 16580, end: 16590)\n",
      "Batch 1659 - batch loss: 1.4845932722091675 - avg loss: 2.0652851041900107   (start: 16590, end: 16600)\n",
      "Batch 1660 - batch loss: 2.0300636291503906 - avg loss: 2.0652638992080483   (start: 16600, end: 16610)\n",
      "Batch 1661 - batch loss: 1.6923249959945679 - avg loss: 2.0650395075695323   (start: 16610, end: 16620)\n",
      "Batch 1662 - batch loss: 2.2915406227111816 - avg loss: 2.0651757078792987   (start: 16620, end: 16630)\n",
      "Batch 1663 - batch loss: 1.672228217124939 - avg loss: 2.0649395615507204   (start: 16630, end: 16640)\n",
      "Batch 1664 - batch loss: 2.951307535171509 - avg loss: 2.0654719146880303   (start: 16640, end: 16650)\n",
      "Batch 1665 - batch loss: 1.9422080516815186 - avg loss: 2.065397926775061   (start: 16650, end: 16660)\n",
      "Batch 1666 - batch loss: 2.2339558601379395 - avg loss: 2.0654990413121714   (start: 16660, end: 16670)\n",
      "Batch 1667 - batch loss: 2.772512197494507 - avg loss: 2.065922910110842   (start: 16670, end: 16680)\n",
      "Batch 1668 - batch loss: 1.9767358303070068 - avg loss: 2.0658694726753692   (start: 16680, end: 16690)\n",
      "Batch 1669 - batch loss: 2.8881354331970215 - avg loss: 2.0663618475020287   (start: 16690, end: 16700)\n",
      "Batch 1670 - batch loss: 2.3794188499450684 - avg loss: 2.0665491946010373   (start: 16700, end: 16710)\n",
      "Batch 1671 - batch loss: 1.747191071510315 - avg loss: 2.0663581909389017   (start: 16710, end: 16720)\n",
      "Batch 1672 - batch loss: 2.0793509483337402 - avg loss: 2.0663659570819948   (start: 16720, end: 16730)\n",
      "Batch 1673 - batch loss: 1.8724167346954346 - avg loss: 2.0662500973314653   (start: 16730, end: 16740)\n",
      "Batch 1674 - batch loss: 1.907469391822815 - avg loss: 2.066155302880415   (start: 16740, end: 16750)\n",
      "Batch 1675 - batch loss: 1.7384121417999268 - avg loss: 2.0659597520683146   (start: 16750, end: 16760)\n",
      "Batch 1676 - batch loss: 1.5445425510406494 - avg loss: 2.065648829467821   (start: 16760, end: 16770)\n",
      "Batch 1677 - batch loss: 1.667875051498413 - avg loss: 2.0654117771567546   (start: 16770, end: 16780)\n",
      "Batch 1678 - batch loss: 2.1025500297546387 - avg loss: 2.065433896425723   (start: 16780, end: 16790)\n",
      "Batch 1679 - batch loss: 2.0252845287323 - avg loss: 2.0654099979925724   (start: 16790, end: 16800)\n",
      "Batch 1680 - batch loss: 1.008305549621582 - avg loss: 2.064781143472423   (start: 16800, end: 16810)\n",
      "Batch 1681 - batch loss: 2.0678248405456543 - avg loss: 2.064782953042621   (start: 16810, end: 16820)\n",
      "Batch 1682 - batch loss: 2.173830509185791 - avg loss: 2.06484774659945   (start: 16820, end: 16830)\n",
      "Batch 1683 - batch loss: 2.464000701904297 - avg loss: 2.0650847732949993   (start: 16830, end: 16840)\n",
      "Batch 1684 - batch loss: 2.589555263519287 - avg loss: 2.065396031746171   (start: 16840, end: 16850)\n",
      "Batch 1685 - batch loss: 2.4771463871002197 - avg loss: 2.065640249038789   (start: 16850, end: 16860)\n",
      "Batch 1686 - batch loss: 1.5474815368652344 - avg loss: 2.0653331010173464   (start: 16860, end: 16870)\n",
      "Batch 1687 - batch loss: 2.295670986175537 - avg loss: 2.0654695571104495   (start: 16870, end: 16880)\n",
      "Batch 1688 - batch loss: 1.1221683025360107 - avg loss: 2.0649110602160894   (start: 16880, end: 16890)\n",
      "Batch 1689 - batch loss: 1.5384817123413086 - avg loss: 2.064599563560542   (start: 16890, end: 16900)\n",
      "Batch 1690 - batch loss: 1.838104009628296 - avg loss: 2.064465621778205   (start: 16900, end: 16910)\n",
      "Batch 1691 - batch loss: 1.7824926376342773 - avg loss: 2.0642989710783564   (start: 16910, end: 16920)\n",
      "Batch 1692 - batch loss: 2.990203380584717 - avg loss: 2.0648458726787737   (start: 16920, end: 16930)\n",
      "Batch 1693 - batch loss: 2.8366858959198 - avg loss: 2.0653015043335796   (start: 16930, end: 16940)\n",
      "Batch 1694 - batch loss: 1.5690393447875977 - avg loss: 2.065008724298449   (start: 16940, end: 16950)\n",
      "Batch 1695 - batch loss: 2.484888792037964 - avg loss: 2.0652562950931066   (start: 16950, end: 16960)\n",
      "Batch 1696 - batch loss: 1.4033688306808472 - avg loss: 2.0648662612307542   (start: 16960, end: 16970)\n",
      "Batch 1697 - batch loss: 1.3022583723068237 - avg loss: 2.064417139976971   (start: 16970, end: 16980)\n",
      "Batch 1698 - batch loss: 2.027057647705078 - avg loss: 2.064395150870278   (start: 16980, end: 16990)\n",
      "Batch 1699 - batch loss: 2.427405834197998 - avg loss: 2.064608686566353   (start: 16990, end: 17000)\n",
      "Batch 1700 - batch loss: 2.0172133445739746 - avg loss: 2.0645808233435474   (start: 17000, end: 17010)\n",
      "Batch 1701 - batch loss: 0.9667360186576843 - avg loss: 2.063935791143379   (start: 17010, end: 17020)\n",
      "Batch 1702 - batch loss: 2.0507400035858154 - avg loss: 2.063928042589323   (start: 17020, end: 17030)\n",
      "Batch 1703 - batch loss: 2.572838068008423 - avg loss: 2.0642266987075266   (start: 17030, end: 17040)\n",
      "Batch 1704 - batch loss: 2.3009629249572754 - avg loss: 2.0643655469340665   (start: 17040, end: 17050)\n",
      "Batch 1705 - batch loss: 1.926142692565918 - avg loss: 2.064284525331271   (start: 17050, end: 17060)\n",
      "Batch 1706 - batch loss: 2.235461711883545 - avg loss: 2.0643848048781677   (start: 17060, end: 17070)\n",
      "Batch 1707 - batch loss: 1.477898359298706 - avg loss: 2.0640414287390696   (start: 17070, end: 17080)\n",
      "Batch 1708 - batch loss: 1.7409412860870361 - avg loss: 2.0638523707269854   (start: 17080, end: 17090)\n",
      "Batch 1709 - batch loss: 1.5966062545776367 - avg loss: 2.063579127384208   (start: 17090, end: 17100)\n",
      "Batch 1710 - batch loss: 1.8969604969024658 - avg loss: 2.063481746536469   (start: 17100, end: 17110)\n",
      "Batch 1711 - batch loss: 1.950585126876831 - avg loss: 2.0634158022492843   (start: 17110, end: 17120)\n",
      "Batch 1712 - batch loss: 2.2466399669647217 - avg loss: 2.063522763232773   (start: 17120, end: 17130)\n",
      "Batch 1713 - batch loss: 2.1744065284729004 - avg loss: 2.063587456211326   (start: 17130, end: 17140)\n",
      "Batch 1714 - batch loss: 3.217299222946167 - avg loss: 2.064260174442658   (start: 17140, end: 17150)\n",
      "Batch 1715 - batch loss: 1.6914913654327393 - avg loss: 2.0640429432019767   (start: 17150, end: 17160)\n",
      "Batch 1716 - batch loss: 1.5599225759506226 - avg loss: 2.0637493378628666   (start: 17160, end: 17170)\n",
      "Batch 1717 - batch loss: 2.4272148609161377 - avg loss: 2.0639609010311166   (start: 17170, end: 17180)\n",
      "Batch 1718 - batch loss: 2.5750362873077393 - avg loss: 2.064258210738084   (start: 17180, end: 17190)\n",
      "Batch 1719 - batch loss: 2.0398592948913574 - avg loss: 2.064244025321894   (start: 17190, end: 17200)\n",
      "Batch 1720 - batch loss: 1.9529707431793213 - avg loss: 2.0641793691440076   (start: 17200, end: 17210)\n",
      "Batch 1721 - batch loss: 1.9560794830322266 - avg loss: 2.0641165933681007   (start: 17210, end: 17220)\n",
      "Batch 1722 - batch loss: 2.133131504058838 - avg loss: 2.064156648452657   (start: 17220, end: 17230)\n",
      "Batch 1723 - batch loss: 1.4816985130310059 - avg loss: 2.0638187957058927   (start: 17230, end: 17240)\n",
      "Batch 1724 - batch loss: 1.669124960899353 - avg loss: 2.063589987685715   (start: 17240, end: 17250)\n",
      "Batch 1725 - batch loss: 1.2750060558319092 - avg loss: 2.0631331024413035   (start: 17250, end: 17260)\n",
      "Batch 1726 - batch loss: 2.098932981491089 - avg loss: 2.0631538319601512   (start: 17260, end: 17270)\n",
      "Batch 1727 - batch loss: 2.12003231048584 - avg loss: 2.063186747746335   (start: 17270, end: 17280)\n",
      "Batch 1728 - batch loss: 2.4134817123413086 - avg loss: 2.0633893474945104   (start: 17280, end: 17290)\n",
      "Batch 1729 - batch loss: 2.1525211334228516 - avg loss: 2.0634408687580525   (start: 17290, end: 17300)\n",
      "Batch 1730 - batch loss: 2.033352851867676 - avg loss: 2.063423486888099   (start: 17300, end: 17310)\n",
      "Batch 1731 - batch loss: 2.136962413787842 - avg loss: 2.0634659458528213   (start: 17310, end: 17320)\n",
      "Batch 1732 - batch loss: 2.0253186225891113 - avg loss: 2.063443933548572   (start: 17320, end: 17330)\n",
      "Batch 1733 - batch loss: 1.1558846235275269 - avg loss: 2.062920542943024   (start: 17330, end: 17340)\n",
      "Batch 1734 - batch loss: 2.9958667755126953 - avg loss: 2.0634582641145336   (start: 17340, end: 17350)\n",
      "Batch 1735 - batch loss: 1.9116871356964111 - avg loss: 2.063370838349316   (start: 17350, end: 17360)\n",
      "Batch 1736 - batch loss: 1.8119052648544312 - avg loss: 2.0632260683012476   (start: 17360, end: 17370)\n",
      "Batch 1737 - batch loss: 2.4963033199310303 - avg loss: 2.0634752496888367   (start: 17370, end: 17380)\n",
      "Batch 1738 - batch loss: 2.960198402404785 - avg loss: 2.0639909041757347   (start: 17380, end: 17390)\n",
      "Batch 1739 - batch loss: 1.700446367263794 - avg loss: 2.063781970533831   (start: 17390, end: 17400)\n",
      "Batch 1740 - batch loss: 1.9261176586151123 - avg loss: 2.0637028985568535   (start: 17400, end: 17410)\n",
      "Batch 1741 - batch loss: 1.856170654296875 - avg loss: 2.063583764088277   (start: 17410, end: 17420)\n",
      "Batch 1742 - batch loss: 2.09438157081604 - avg loss: 2.063601433512676   (start: 17420, end: 17430)\n",
      "Batch 1743 - batch loss: 2.483102321624756 - avg loss: 2.0638419730127406   (start: 17430, end: 17440)\n",
      "Batch 1744 - batch loss: 1.4479014873504639 - avg loss: 2.0634889985223897   (start: 17440, end: 17450)\n",
      "Batch 1745 - batch loss: 2.504939556121826 - avg loss: 2.0637418338932942   (start: 17450, end: 17460)\n",
      "Batch 1746 - batch loss: 2.034045696258545 - avg loss: 2.0637248355317404   (start: 17460, end: 17470)\n",
      "Batch 1747 - batch loss: 1.434138298034668 - avg loss: 2.0633646601670392   (start: 17470, end: 17480)\n",
      "Batch 1748 - batch loss: 2.8115885257720947 - avg loss: 2.063792461119358   (start: 17480, end: 17490)\n",
      "Batch 1749 - batch loss: 1.911452054977417 - avg loss: 2.0637054094587053   (start: 17490, end: 17500)\n",
      "Batch 1750 - batch loss: 2.7550747394561768 - avg loss: 2.064100252022953   (start: 17500, end: 17510)\n",
      "Batch 1751 - batch loss: 2.003694534301758 - avg loss: 2.0640657738735686   (start: 17510, end: 17520)\n",
      "Batch 1752 - batch loss: 2.061720371246338 - avg loss: 2.064064435937101   (start: 17520, end: 17530)\n",
      "Batch 1753 - batch loss: 2.8510680198669434 - avg loss: 2.0645131266919075   (start: 17530, end: 17540)\n",
      "Batch 1754 - batch loss: 2.09918212890625 - avg loss: 2.0645328811091237   (start: 17540, end: 17550)\n",
      "Batch 1755 - batch loss: 2.6464293003082275 - avg loss: 2.0648642572020615   (start: 17550, end: 17560)\n",
      "Batch 1756 - batch loss: 2.0449447631835938 - avg loss: 2.0648529199829273   (start: 17560, end: 17570)\n",
      "Batch 1757 - batch loss: 1.6615663766860962 - avg loss: 2.0646235192188223   (start: 17570, end: 17580)\n",
      "Batch 1758 - batch loss: 2.1407551765441895 - avg loss: 2.064666800433902   (start: 17580, end: 17590)\n",
      "Batch 1759 - batch loss: 1.2647209167480469 - avg loss: 2.0642122857272627   (start: 17590, end: 17600)\n",
      "Batch 1760 - batch loss: 1.7849586009979248 - avg loss: 2.064053708961374   (start: 17600, end: 17610)\n",
      "Batch 1761 - batch loss: 1.852421760559082 - avg loss: 2.063933600023575   (start: 17610, end: 17620)\n",
      "Batch 1762 - batch loss: 2.6531310081481934 - avg loss: 2.0642678016163853   (start: 17620, end: 17630)\n",
      "Batch 1763 - batch loss: 1.7105817794799805 - avg loss: 2.0640672993362625   (start: 17630, end: 17640)\n",
      "Batch 1764 - batch loss: 1.8356688022613525 - avg loss: 2.0639378950886282   (start: 17640, end: 17650)\n",
      "Batch 1765 - batch loss: 2.463268995285034 - avg loss: 2.0641640168894186   (start: 17650, end: 17660)\n",
      "Batch 1766 - batch loss: 1.7280731201171875 - avg loss: 2.0639738126467635   (start: 17660, end: 17670)\n",
      "Batch 1767 - batch loss: 2.289620876312256 - avg loss: 2.0641014410764384   (start: 17670, end: 17680)\n",
      "Batch 1768 - batch loss: 2.3049073219299316 - avg loss: 2.064237566503716   (start: 17680, end: 17690)\n",
      "Batch 1769 - batch loss: 2.3675525188446045 - avg loss: 2.064408930883569   (start: 17690, end: 17700)\n",
      "Batch 1770 - batch loss: 1.4750258922576904 - avg loss: 2.064076134136745   (start: 17700, end: 17710)\n",
      "Batch 1771 - batch loss: 2.0004329681396484 - avg loss: 2.064040218128846   (start: 17710, end: 17720)\n",
      "Batch 1772 - batch loss: 2.286428928375244 - avg loss: 2.0641656488734856   (start: 17720, end: 17730)\n",
      "Batch 1773 - batch loss: 3.1826274394989014 - avg loss: 2.064796123389058   (start: 17730, end: 17740)\n",
      "Batch 1774 - batch loss: 2.488333225250244 - avg loss: 2.0650347358408108   (start: 17740, end: 17750)\n",
      "Batch 1775 - batch loss: 1.993220329284668 - avg loss: 2.0649942998010835   (start: 17750, end: 17760)\n",
      "Batch 1776 - batch loss: 1.8723421096801758 - avg loss: 2.064885885512889   (start: 17760, end: 17770)\n",
      "Batch 1777 - batch loss: 1.8434829711914062 - avg loss: 2.06476136193903   (start: 17770, end: 17780)\n",
      "Batch 1778 - batch loss: 2.1570582389831543 - avg loss: 2.0648132432639565   (start: 17780, end: 17790)\n",
      "Batch 1779 - batch loss: 2.1617836952209473 - avg loss: 2.064867721045955   (start: 17790, end: 17800)\n",
      "Batch 1780 - batch loss: 2.4366462230682373 - avg loss: 2.065076468099308   (start: 17800, end: 17810)\n",
      "Batch 1781 - batch loss: 2.217867374420166 - avg loss: 2.0651622093486464   (start: 17810, end: 17820)\n",
      "Batch 1782 - batch loss: 1.0093899965286255 - avg loss: 2.064570076868097   (start: 17820, end: 17830)\n",
      "Batch 1783 - batch loss: 1.5582236051559448 - avg loss: 2.0642862503705004   (start: 17830, end: 17840)\n",
      "Batch 1784 - batch loss: 2.0131454467773438 - avg loss: 2.064257600060364   (start: 17840, end: 17850)\n",
      "Batch 1785 - batch loss: 1.5767837762832642 - avg loss: 2.063984658389716   (start: 17850, end: 17860)\n",
      "Batch 1786 - batch loss: 1.6906312704086304 - avg loss: 2.063775730920225   (start: 17860, end: 17870)\n",
      "Batch 1787 - batch loss: 2.172555685043335 - avg loss: 2.063836569820741   (start: 17870, end: 17880)\n",
      "Batch 1788 - batch loss: 2.2313711643218994 - avg loss: 2.063930216883067   (start: 17880, end: 17890)\n",
      "Batch 1789 - batch loss: 1.834329605102539 - avg loss: 2.0638019483848655   (start: 17890, end: 17900)\n",
      "Batch 1790 - batch loss: 1.642773985862732 - avg loss: 2.0635668685621287   (start: 17900, end: 17910)\n",
      "Batch 1791 - batch loss: 2.109196186065674 - avg loss: 2.063592331350914   (start: 17910, end: 17920)\n",
      "Batch 1792 - batch loss: 1.7829163074493408 - avg loss: 2.0634357914602828   (start: 17920, end: 17930)\n",
      "Batch 1793 - batch loss: 1.9193687438964844 - avg loss: 2.0633554865285304   (start: 17930, end: 17940)\n",
      "Batch 1794 - batch loss: 2.348972797393799 - avg loss: 2.063514604807564   (start: 17940, end: 17950)\n",
      "Batch 1795 - batch loss: 1.8942359685897827 - avg loss: 2.0634203516693583   (start: 17950, end: 17960)\n",
      "Batch 1796 - batch loss: 1.7554805278778076 - avg loss: 2.0632489883839984   (start: 17960, end: 17970)\n",
      "Batch 1797 - batch loss: 3.0123441219329834 - avg loss: 2.0637768499710667   (start: 17970, end: 17980)\n",
      "Batch 1798 - batch loss: 1.3861711025238037 - avg loss: 2.063400193079768   (start: 17980, end: 17990)\n",
      "Batch 1799 - batch loss: 2.2136611938476562 - avg loss: 2.0634836714135276   (start: 17990, end: 18000)\n",
      "Batch 1800 - batch loss: 2.0992820262908936 - avg loss: 2.0635035483457194   (start: 18000, end: 18010)\n",
      "Batch 1801 - batch loss: 2.329047203063965 - avg loss: 2.063650908864431   (start: 18010, end: 18020)\n",
      "Batch 1802 - batch loss: 2.368563175201416 - avg loss: 2.0638200227115395   (start: 18020, end: 18030)\n",
      "Batch 1803 - batch loss: 2.0266942977905273 - avg loss: 2.063799443041406   (start: 18030, end: 18040)\n",
      "Batch 1804 - batch loss: 1.765824556350708 - avg loss: 2.063634360001688   (start: 18040, end: 18050)\n",
      "Batch 1805 - batch loss: 1.8031936883926392 - avg loss: 2.0634901514349058   (start: 18050, end: 18060)\n",
      "Batch 1806 - batch loss: 1.7099558115005493 - avg loss: 2.063294504318174   (start: 18060, end: 18070)\n",
      "Batch 1807 - batch loss: 1.1837642192840576 - avg loss: 2.0628080384525576   (start: 18070, end: 18080)\n",
      "Batch 1808 - batch loss: 2.7215752601623535 - avg loss: 2.063172199437472   (start: 18080, end: 18090)\n",
      "Batch 1809 - batch loss: 2.115572929382324 - avg loss: 2.0632011501169996   (start: 18090, end: 18100)\n",
      "Batch 1810 - batch loss: 1.7276504039764404 - avg loss: 2.0630158653317205   (start: 18100, end: 18110)\n",
      "Batch 1811 - batch loss: 2.8068926334381104 - avg loss: 2.063426393349439   (start: 18110, end: 18120)\n",
      "Batch 1812 - batch loss: 1.7149572372436523 - avg loss: 2.063234187526987   (start: 18120, end: 18130)\n",
      "Batch 1813 - batch loss: 1.450981855392456 - avg loss: 2.062896672459658   (start: 18130, end: 18140)\n",
      "Batch 1814 - batch loss: 2.242164134979248 - avg loss: 2.0629954424114594   (start: 18140, end: 18150)\n",
      "Batch 1815 - batch loss: 1.3428428173065186 - avg loss: 2.062598882595873   (start: 18150, end: 18160)\n",
      "Batch 1816 - batch loss: 2.084916591644287 - avg loss: 2.06261116531962   (start: 18160, end: 18170)\n",
      "Batch 1817 - batch loss: 1.7915891408920288 - avg loss: 2.0624620882984828   (start: 18170, end: 18180)\n",
      "Batch 1818 - batch loss: 1.6544115543365479 - avg loss: 2.0622377614518848   (start: 18180, end: 18190)\n",
      "Batch 1819 - batch loss: 1.2647511959075928 - avg loss: 2.061799582020267   (start: 18190, end: 18200)\n",
      "Batch 1820 - batch loss: 2.547771692276001 - avg loss: 2.0620664530308415   (start: 18200, end: 18210)\n",
      "Batch 1821 - batch loss: 2.1471707820892334 - avg loss: 2.0621131623223112   (start: 18210, end: 18220)\n",
      "Batch 1822 - batch loss: 2.3699207305908203 - avg loss: 2.0622820090410543   (start: 18220, end: 18230)\n",
      "Batch 1823 - batch loss: 1.709777593612671 - avg loss: 2.062088750041368   (start: 18230, end: 18240)\n",
      "Batch 1824 - batch loss: 2.1509718894958496 - avg loss: 2.06213745313148   (start: 18240, end: 18250)\n",
      "Batch 1825 - batch loss: 1.8281373977661133 - avg loss: 2.062009304141685   (start: 18250, end: 18260)\n",
      "Batch 1826 - batch loss: 2.4052464962005615 - avg loss: 2.062197173431263   (start: 18260, end: 18270)\n",
      "Batch 1827 - batch loss: 3.030515193939209 - avg loss: 2.062726887884495   (start: 18270, end: 18280)\n",
      "Batch 1828 - batch loss: 1.9023716449737549 - avg loss: 2.062639214159557   (start: 18280, end: 18290)\n",
      "Batch 1829 - batch loss: 1.9721956253051758 - avg loss: 2.0625897914334073   (start: 18290, end: 18300)\n",
      "Batch 1830 - batch loss: 3.045807361602783 - avg loss: 2.063126775360316   (start: 18300, end: 18310)\n",
      "Batch 1831 - batch loss: 1.8278324604034424 - avg loss: 2.0629983395988765   (start: 18310, end: 18320)\n",
      "Batch 1832 - batch loss: 2.0299839973449707 - avg loss: 2.0629803285010837   (start: 18320, end: 18330)\n",
      "Batch 1833 - batch loss: 1.976518988609314 - avg loss: 2.0629331849133568   (start: 18330, end: 18340)\n",
      "Batch 1834 - batch loss: 1.0477540493011475 - avg loss: 2.0623799537767833   (start: 18340, end: 18350)\n",
      "Batch 1835 - batch loss: 2.749096155166626 - avg loss: 2.0627539822089127   (start: 18350, end: 18360)\n",
      "Batch 1836 - batch loss: 1.60160231590271 - avg loss: 2.062502947006786   (start: 18360, end: 18370)\n",
      "Batch 1837 - batch loss: 2.028533697128296 - avg loss: 2.0624844653692027   (start: 18370, end: 18380)\n",
      "Batch 1838 - batch loss: 2.2702693939208984 - avg loss: 2.0625974533673275   (start: 18380, end: 18390)\n",
      "Batch 1839 - batch loss: 1.574057936668396 - avg loss: 2.062331942760426   (start: 18390, end: 18400)\n",
      "Batch 1840 - batch loss: 2.1721577644348145 - avg loss: 2.062391598285507   (start: 18400, end: 18410)\n",
      "Batch 1841 - batch loss: 2.4242796897888184 - avg loss: 2.0625880630474525   (start: 18410, end: 18420)\n",
      "Batch 1842 - batch loss: 2.093533754348755 - avg loss: 2.0626048539814197   (start: 18420, end: 18430)\n",
      "Batch 1843 - batch loss: 2.5667366981506348 - avg loss: 2.062878244352444   (start: 18430, end: 18440)\n",
      "Batch 1844 - batch loss: 2.317018747329712 - avg loss: 2.0630159898825133   (start: 18440, end: 18450)\n",
      "Batch 1845 - batch loss: 3.205997943878174 - avg loss: 2.063635156704829   (start: 18450, end: 18460)\n",
      "Batch 1846 - batch loss: 2.415091037750244 - avg loss: 2.0638254414265647   (start: 18460, end: 18470)\n",
      "Batch 1847 - batch loss: 1.667004942893982 - avg loss: 2.0636107117195666   (start: 18470, end: 18480)\n",
      "Batch 1848 - batch loss: 1.3368558883666992 - avg loss: 2.0632176588134805   (start: 18480, end: 18490)\n",
      "Batch 1849 - batch loss: 2.313397169113159 - avg loss: 2.06335289098121   (start: 18490, end: 18500)\n",
      "Batch 1850 - batch loss: 2.462238311767578 - avg loss: 2.0635683882371727   (start: 18500, end: 18510)\n",
      "Batch 1851 - batch loss: 0.960638701915741 - avg loss: 2.06297285384931   (start: 18510, end: 18520)\n",
      "Batch 1852 - batch loss: 2.556658983230591 - avg loss: 2.0632392791754737   (start: 18520, end: 18530)\n",
      "Batch 1853 - batch loss: 2.2118589878082275 - avg loss: 2.0633194408306155   (start: 18530, end: 18540)\n",
      "Batch 1854 - batch loss: 1.738983154296875 - avg loss: 2.063144596471298   (start: 18540, end: 18550)\n",
      "Batch 1855 - batch loss: 2.4870386123657227 - avg loss: 2.063372987643655   (start: 18550, end: 18560)\n",
      "Batch 1856 - batch loss: 2.2069995403289795 - avg loss: 2.0634503309676644   (start: 18560, end: 18570)\n",
      "Batch 1857 - batch loss: 3.297729969024658 - avg loss: 2.064114636477921   (start: 18570, end: 18580)\n",
      "Batch 1858 - batch loss: 2.144235372543335 - avg loss: 2.0641577353138896   (start: 18580, end: 18590)\n",
      "Batch 1859 - batch loss: 2.3773176670074463 - avg loss: 2.0643261008685636   (start: 18590, end: 18600)\n",
      "Batch 1860 - batch loss: 1.3587557077407837 - avg loss: 2.063946965783594   (start: 18600, end: 18610)\n",
      "Batch 1861 - batch loss: 2.5387773513793945 - avg loss: 2.0642019767318196   (start: 18610, end: 18620)\n",
      "Batch 1862 - batch loss: 2.5908284187316895 - avg loss: 2.0644846532975736   (start: 18620, end: 18630)\n",
      "Batch 1863 - batch loss: 2.1759209632873535 - avg loss: 2.0645444367256798   (start: 18630, end: 18640)\n",
      "Batch 1864 - batch loss: 3.2948501110076904 - avg loss: 2.065204118052373   (start: 18640, end: 18650)\n",
      "Batch 1865 - batch loss: 1.960850477218628 - avg loss: 2.0651481943434584   (start: 18650, end: 18660)\n",
      "Batch 1866 - batch loss: 2.0981826782226562 - avg loss: 2.0651658882287713   (start: 18660, end: 18670)\n",
      "Batch 1867 - batch loss: 2.0313193798065186 - avg loss: 2.065147769112914   (start: 18670, end: 18680)\n",
      "Batch 1868 - batch loss: 2.8512063026428223 - avg loss: 2.0655683461774026   (start: 18680, end: 18690)\n",
      "Batch 1869 - batch loss: 2.4720470905303955 - avg loss: 2.065785714489891   (start: 18690, end: 18700)\n",
      "Batch 1870 - batch loss: 2.535712718963623 - avg loss: 2.0660368780411864   (start: 18700, end: 18710)\n",
      "Batch 1871 - batch loss: 2.8179078102111816 - avg loss: 2.0664385184964056   (start: 18710, end: 18720)\n",
      "Batch 1872 - batch loss: 1.7325499057769775 - avg loss: 2.066260254421275   (start: 18720, end: 18730)\n",
      "Batch 1873 - batch loss: 1.967755675315857 - avg loss: 2.06620769061172   (start: 18730, end: 18740)\n",
      "Batch 1874 - batch loss: 1.7216761112213135 - avg loss: 2.0660239404360454   (start: 18740, end: 18750)\n",
      "Batch 1875 - batch loss: 1.3258521556854248 - avg loss: 2.06562939257637   (start: 18750, end: 18760)\n",
      "Batch 1876 - batch loss: 2.6670069694519043 - avg loss: 2.0659497855315516   (start: 18760, end: 18770)\n",
      "Batch 1877 - batch loss: 2.4952831268310547 - avg loss: 2.0661783975343733   (start: 18770, end: 18780)\n",
      "Batch 1878 - batch loss: 1.9142717123031616 - avg loss: 2.0660975531037025   (start: 18780, end: 18790)\n",
      "Batch 1879 - batch loss: 1.9069416522979736 - avg loss: 2.066012895709657   (start: 18790, end: 18800)\n",
      "Batch 1880 - batch loss: 1.884305715560913 - avg loss: 2.0659162943379665   (start: 18800, end: 18810)\n",
      "Batch 1881 - batch loss: 2.532022714614868 - avg loss: 2.066163959811015   (start: 18810, end: 18820)\n",
      "Batch 1882 - batch loss: 2.1889615058898926 - avg loss: 2.066229173590133   (start: 18820, end: 18830)\n",
      "Batch 1883 - batch loss: 2.0543923377990723 - avg loss: 2.0662228907685876   (start: 18830, end: 18840)\n",
      "Batch 1884 - batch loss: 2.6842477321624756 - avg loss: 2.066550755405932   (start: 18840, end: 18850)\n",
      "Batch 1885 - batch loss: 1.964294672012329 - avg loss: 2.066496536909965   (start: 18850, end: 18860)\n",
      "Batch 1886 - batch loss: 2.2886767387390137 - avg loss: 2.0666142794652536   (start: 18860, end: 18870)\n",
      "Batch 1887 - batch loss: 2.3052725791931152 - avg loss: 2.0667406874629908   (start: 18870, end: 18880)\n",
      "Batch 1888 - batch loss: 1.0129625797271729 - avg loss: 2.0661828377500546   (start: 18880, end: 18890)\n",
      "Batch 1889 - batch loss: 2.6348674297332764 - avg loss: 2.0664837290685645   (start: 18890, end: 18900)\n",
      "Batch 1890 - batch loss: 2.07841157913208 - avg loss: 2.0664900367629397   (start: 18900, end: 18910)\n",
      "Batch 1891 - batch loss: 2.2308156490325928 - avg loss: 2.0665768896235472   (start: 18910, end: 18920)\n",
      "Batch 1892 - batch loss: 1.5168910026550293 - avg loss: 2.0662865114476525   (start: 18920, end: 18930)\n",
      "Batch 1893 - batch loss: 1.4400497674942017 - avg loss: 2.0659558690274027   (start: 18930, end: 18940)\n",
      "Batch 1894 - batch loss: 1.9726355075836182 - avg loss: 2.0659066234540813   (start: 18940, end: 18950)\n",
      "Batch 1895 - batch loss: 2.189077854156494 - avg loss: 2.0659715871833546   (start: 18950, end: 18960)\n",
      "Batch 1896 - batch loss: 2.7263336181640625 - avg loss: 2.0663196957922008   (start: 18960, end: 18970)\n",
      "Batch 1897 - batch loss: 3.0820133686065674 - avg loss: 2.0668548347135993   (start: 18970, end: 18980)\n",
      "Batch 1898 - batch loss: 1.8578497171401978 - avg loss: 2.066744774093497   (start: 18980, end: 18990)\n",
      "Batch 1899 - batch loss: 2.3004567623138428 - avg loss: 2.066867780403087   (start: 18990, end: 19000)\n",
      "Batch 1900 - batch loss: 1.856292486190796 - avg loss: 2.066757009601292   (start: 19000, end: 19010)\n",
      "Batch 1901 - batch loss: 2.492762327194214 - avg loss: 2.066980987160489   (start: 19010, end: 19020)\n",
      "Batch 1902 - batch loss: 1.8917949199676514 - avg loss: 2.066888929321712   (start: 19020, end: 19030)\n",
      "Batch 1903 - batch loss: 2.4066786766052246 - avg loss: 2.0670673903234364   (start: 19030, end: 19040)\n",
      "Batch 1904 - batch loss: 2.2423815727233887 - avg loss: 2.0671594187656415   (start: 19040, end: 19050)\n",
      "Batch 1905 - batch loss: 2.4802637100219727 - avg loss: 2.0673761576382836   (start: 19050, end: 19060)\n",
      "Batch 1906 - batch loss: 1.8389806747436523 - avg loss: 2.0672563907358743   (start: 19060, end: 19070)\n",
      "Batch 1907 - batch loss: 1.0396983623504639 - avg loss: 2.0667178383100957   (start: 19070, end: 19080)\n",
      "Batch 1908 - batch loss: 2.557297706604004 - avg loss: 2.066974820954566   (start: 19080, end: 19090)\n",
      "Batch 1909 - batch loss: 2.4531774520874023 - avg loss: 2.0671770212850022   (start: 19090, end: 19100)\n",
      "Batch 1910 - batch loss: 1.3962490558624268 - avg loss: 2.0668259339142945   (start: 19100, end: 19110)\n",
      "Batch 1911 - batch loss: 1.8042986392974854 - avg loss: 2.066688628843888   (start: 19110, end: 19120)\n",
      "Batch 1912 - batch loss: 3.410977840423584 - avg loss: 2.067391341447955   (start: 19120, end: 19130)\n",
      "Batch 1913 - batch loss: 1.9072154760360718 - avg loss: 2.0673076549978964   (start: 19130, end: 19140)\n",
      "Batch 1914 - batch loss: 1.3697478771209717 - avg loss: 2.0669433940172817   (start: 19140, end: 19150)\n",
      "Batch 1915 - batch loss: 1.3328090906143188 - avg loss: 2.066560234151205   (start: 19150, end: 19160)\n",
      "Batch 1916 - batch loss: 2.4034225940704346 - avg loss: 2.0667359578652995   (start: 19160, end: 19170)\n",
      "Batch 1917 - batch loss: 2.7215614318847656 - avg loss: 2.0670773684356956   (start: 19170, end: 19180)\n",
      "Batch 1918 - batch loss: 2.7381536960601807 - avg loss: 2.0674270694923003   (start: 19180, end: 19190)\n",
      "Batch 1919 - batch loss: 3.261573314666748 - avg loss: 2.068049020661662   (start: 19190, end: 19200)\n",
      "Batch 1920 - batch loss: 2.229607105255127 - avg loss: 2.0681331216947663   (start: 19200, end: 19210)\n",
      "Batch 1921 - batch loss: 2.269667387008667 - avg loss: 2.0682379782323905   (start: 19210, end: 19220)\n",
      "Batch 1922 - batch loss: 2.26611590385437 - avg loss: 2.0683408788697397   (start: 19220, end: 19230)\n",
      "Batch 1923 - batch loss: 1.857063889503479 - avg loss: 2.0682310675447053   (start: 19230, end: 19240)\n",
      "Batch 1924 - batch loss: 2.2864022254943848 - avg loss: 2.0683444032111726   (start: 19240, end: 19250)\n",
      "Batch 1925 - batch loss: 1.9080272912979126 - avg loss: 2.068261164835309   (start: 19250, end: 19260)\n",
      "Batch 1926 - batch loss: 1.3208948373794556 - avg loss: 2.0678733255372   (start: 19260, end: 19270)\n",
      "Batch 1927 - batch loss: 1.9546152353286743 - avg loss: 2.0678145817144777   (start: 19270, end: 19280)\n",
      "Batch 1928 - batch loss: 1.849513053894043 - avg loss: 2.067701413478179   (start: 19280, end: 19290)\n",
      "Batch 1929 - batch loss: 1.8887426853179932 - avg loss: 2.067608688748562   (start: 19290, end: 19300)\n",
      "Batch 1930 - batch loss: 2.5334203243255615 - avg loss: 2.067849916938918   (start: 19300, end: 19310)\n",
      "Batch 1931 - batch loss: 2.198647975921631 - avg loss: 2.0679176177976046   (start: 19310, end: 19320)\n",
      "Batch 1932 - batch loss: 1.7986834049224854 - avg loss: 2.0677783347076537   (start: 19320, end: 19330)\n",
      "Batch 1933 - batch loss: 2.1749749183654785 - avg loss: 2.067833762103547   (start: 19330, end: 19340)\n",
      "Batch 1934 - batch loss: 1.8604825735092163 - avg loss: 2.0677266038665474   (start: 19340, end: 19350)\n",
      "Batch 1935 - batch loss: 1.635022521018982 - avg loss: 2.067503099691523   (start: 19350, end: 19360)\n",
      "Batch 1936 - batch loss: 1.6582276821136475 - avg loss: 2.067291806238979   (start: 19360, end: 19370)\n",
      "Batch 1937 - batch loss: 2.544978141784668 - avg loss: 2.067538290416247   (start: 19370, end: 19380)\n",
      "Batch 1938 - batch loss: 2.5824615955352783 - avg loss: 2.0678038516875823   (start: 19380, end: 19390)\n",
      "Batch 1939 - batch loss: 2.3295648097991943 - avg loss: 2.067938780016506   (start: 19390, end: 19400)\n",
      "Batch 1940 - batch loss: 2.030251979827881 - avg loss: 2.0679193638391804   (start: 19400, end: 19410)\n",
      "Batch 1941 - batch loss: 3.489948272705078 - avg loss: 2.068651613534786   (start: 19410, end: 19420)\n",
      "Batch 1942 - batch loss: 1.2681306600570679 - avg loss: 2.068239610985389   (start: 19420, end: 19430)\n",
      "Batch 1943 - batch loss: 2.7523488998413086 - avg loss: 2.068591519055788   (start: 19430, end: 19440)\n",
      "Batch 1944 - batch loss: 1.4284640550613403 - avg loss: 2.0682624046784133   (start: 19440, end: 19450)\n",
      "Batch 1945 - batch loss: 2.253068447113037 - avg loss: 2.0683573718122443   (start: 19450, end: 19460)\n",
      "Batch 1946 - batch loss: 2.910249710083008 - avg loss: 2.068789776711202   (start: 19460, end: 19470)\n",
      "Batch 1947 - batch loss: 1.7983617782592773 - avg loss: 2.0686509533033726   (start: 19470, end: 19480)\n",
      "Batch 1948 - batch loss: 2.9908618927001953 - avg loss: 2.069124124642211   (start: 19480, end: 19490)\n",
      "Batch 1949 - batch loss: 1.8137584924697876 - avg loss: 2.0689931679077636   (start: 19490, end: 19500)\n",
      "Batch 1950 - batch loss: 0.807550311088562 - avg loss: 2.068346605705396   (start: 19500, end: 19510)\n",
      "Batch 1951 - batch loss: 1.9425461292266846 - avg loss: 2.068282158739987   (start: 19510, end: 19520)\n",
      "Batch 1952 - batch loss: 1.4726097583770752 - avg loss: 2.067977154950759   (start: 19520, end: 19530)\n",
      "Batch 1953 - batch loss: 1.9452260732650757 - avg loss: 2.0679143345404793   (start: 19530, end: 19540)\n",
      "Batch 1954 - batch loss: 1.6276975870132446 - avg loss: 2.0676891597335603   (start: 19540, end: 19550)\n",
      "Batch 1955 - batch loss: 2.2281100749969482 - avg loss: 2.0677711745164147   (start: 19550, end: 19560)\n",
      "Batch 1956 - batch loss: 2.0839781761169434 - avg loss: 2.0677794560706304   (start: 19560, end: 19570)\n",
      "Batch 1957 - batch loss: 2.123978853225708 - avg loss: 2.0678081585206587   (start: 19570, end: 19580)\n",
      "Batch 1958 - batch loss: 2.5428617000579834 - avg loss: 2.0680506565000036   (start: 19580, end: 19590)\n",
      "Batch 1959 - batch loss: 1.7140487432479858 - avg loss: 2.067870043278957   (start: 19590, end: 19600)\n",
      "Batch 1960 - batch loss: 2.652500629425049 - avg loss: 2.068168172083723   (start: 19600, end: 19610)\n",
      "Batch 1961 - batch loss: 1.5808392763137817 - avg loss: 2.067919788344798   (start: 19610, end: 19620)\n",
      "Batch 1962 - batch loss: 1.9755570888519287 - avg loss: 2.0678727365366   (start: 19620, end: 19630)\n",
      "Batch 1963 - batch loss: 2.1798629760742188 - avg loss: 2.067929758043493   (start: 19630, end: 19640)\n",
      "Batch 1964 - batch loss: 1.7777507305145264 - avg loss: 2.067782084238135   (start: 19640, end: 19650)\n",
      "Batch 1965 - batch loss: 1.993130087852478 - avg loss: 2.0677441127242053   (start: 19650, end: 19660)\n",
      "Batch 1966 - batch loss: 1.8328416347503662 - avg loss: 2.067624691027218   (start: 19660, end: 19670)\n",
      "Batch 1967 - batch loss: 1.6995187997817993 - avg loss: 2.0674376453507723   (start: 19670, end: 19680)\n",
      "Batch 1968 - batch loss: 2.3947997093200684 - avg loss: 2.0676039033822446   (start: 19680, end: 19690)\n",
      "Batch 1969 - batch loss: 2.351304054260254 - avg loss: 2.0677479136111168   (start: 19690, end: 19700)\n",
      "Batch 1970 - batch loss: 1.4176201820373535 - avg loss: 2.0674180669690196   (start: 19700, end: 19710)\n",
      "Batch 1971 - batch loss: 1.7437900304794312 - avg loss: 2.0672539553886495   (start: 19710, end: 19720)\n",
      "Batch 1972 - batch loss: 2.225609302520752 - avg loss: 2.0673342165884123   (start: 19720, end: 19730)\n",
      "Batch 1973 - batch loss: 2.736999034881592 - avg loss: 2.0676734591508708   (start: 19730, end: 19740)\n",
      "Batch 1974 - batch loss: 2.0343689918518066 - avg loss: 2.0676565961294537   (start: 19740, end: 19750)\n",
      "Batch 1975 - batch loss: 1.6659733057022095 - avg loss: 2.067453315112031   (start: 19750, end: 19760)\n",
      "Batch 1976 - batch loss: 2.2689335346221924 - avg loss: 2.0675552272109234   (start: 19760, end: 19770)\n",
      "Batch 1977 - batch loss: 1.4352680444717407 - avg loss: 2.0672355673612066   (start: 19770, end: 19780)\n",
      "Batch 1978 - batch loss: 1.9456138610839844 - avg loss: 2.0671741112185704   (start: 19780, end: 19790)\n",
      "Batch 1979 - batch loss: 2.3750996589660645 - avg loss: 2.0673296291719785   (start: 19790, end: 19800)\n",
      "Batch 1980 - batch loss: 0.9546778798103333 - avg loss: 2.0667679675115234   (start: 19800, end: 19810)\n",
      "Batch 1981 - batch loss: 1.758766770362854 - avg loss: 2.066612568320227   (start: 19810, end: 19820)\n",
      "Batch 1982 - batch loss: 2.131747007369995 - avg loss: 2.0666454147342717   (start: 19820, end: 19830)\n",
      "Batch 1983 - batch loss: 2.431314706802368 - avg loss: 2.0668292198209994   (start: 19830, end: 19840)\n",
      "Batch 1984 - batch loss: 1.8953711986541748 - avg loss: 2.0667428429841395   (start: 19840, end: 19850)\n",
      "Batch 1985 - batch loss: 2.192824602127075 - avg loss: 2.0668063282606464   (start: 19850, end: 19860)\n",
      "Batch 1986 - batch loss: 1.5161964893341064 - avg loss: 2.0665292221514737   (start: 19860, end: 19870)\n",
      "Batch 1987 - batch loss: 1.7747564315795898 - avg loss: 2.066382455154204   (start: 19870, end: 19880)\n",
      "Batch 1988 - batch loss: 1.8307812213897705 - avg loss: 2.066264003050753   (start: 19880, end: 19890)\n",
      "Batch 1989 - batch loss: 2.420424699783325 - avg loss: 2.066441973250116   (start: 19890, end: 19900)\n",
      "Batch 1990 - batch loss: 1.9559307098388672 - avg loss: 2.066386467844083   (start: 19900, end: 19910)\n",
      "Batch 1991 - batch loss: 1.938490629196167 - avg loss: 2.066322263105806   (start: 19910, end: 19920)\n",
      "Batch 1992 - batch loss: 2.136425256729126 - avg loss: 2.0663574377137457   (start: 19920, end: 19930)\n",
      "Batch 1993 - batch loss: 1.4932715892791748 - avg loss: 2.0660700325741095   (start: 19930, end: 19940)\n",
      "Batch 1994 - batch loss: 1.564479947090149 - avg loss: 2.065818608972363   (start: 19940, end: 19950)\n",
      "Batch 1995 - batch loss: 2.362701654434204 - avg loss: 2.0659673479730953   (start: 19950, end: 19960)\n",
      "Batch 1996 - batch loss: 2.3177692890167236 - avg loss: 2.066093438078776   (start: 19960, end: 19970)\n",
      "Batch 1997 - batch loss: 1.7641655206680298 - avg loss: 2.0659423230049967   (start: 19970, end: 19980)\n",
      "Batch 1998 - batch loss: 1.8782628774642944 - avg loss: 2.0658484363388934   (start: 19980, end: 19990)\n",
      "Batch 1999 - batch loss: 1.7674486637115479 - avg loss: 2.0656992364525797   (start: 19990, end: 20000)\n",
      "Batch 2000 - batch loss: 2.2472143173217773 - avg loss: 2.065789948636922   (start: 20000, end: 20010)\n",
      "Batch 2001 - batch loss: 2.1160950660705566 - avg loss: 2.0658150760682075   (start: 20010, end: 20020)\n",
      "Batch 2002 - batch loss: 2.9593400955200195 - avg loss: 2.0662611694378787   (start: 20020, end: 20030)\n",
      "Batch 2003 - batch loss: 1.7799255847930908 - avg loss: 2.066118287409613   (start: 20030, end: 20040)\n",
      "Batch 2004 - batch loss: 2.3017730712890625 - avg loss: 2.0662358209676577   (start: 20040, end: 20050)\n",
      "Batch 2005 - batch loss: 2.2070860862731934 - avg loss: 2.0663060354568428   (start: 20050, end: 20060)\n",
      "Batch 2006 - batch loss: 2.821439743041992 - avg loss: 2.0666822854357094   (start: 20060, end: 20070)\n",
      "Batch 2007 - batch loss: 1.9666564464569092 - avg loss: 2.0666324717708795   (start: 20070, end: 20080)\n",
      "Batch 2008 - batch loss: 2.6138792037963867 - avg loss: 2.066904869347796   (start: 20080, end: 20090)\n",
      "Batch 2009 - batch loss: 2.4106664657592773 - avg loss: 2.0670758950176524   (start: 20090, end: 20100)\n",
      "Batch 2010 - batch loss: 1.5404449701309204 - avg loss: 2.066814019868529   (start: 20100, end: 20110)\n",
      "Batch 2011 - batch loss: 1.788697600364685 - avg loss: 2.0666757910317974   (start: 20110, end: 20120)\n",
      "Batch 2012 - batch loss: 2.385450839996338 - avg loss: 2.0668341492280047   (start: 20120, end: 20130)\n",
      "Batch 2013 - batch loss: 1.3829965591430664 - avg loss: 2.066494607226969   (start: 20130, end: 20140)\n",
      "Batch 2014 - batch loss: 2.288062810897827 - avg loss: 2.0666045666332575   (start: 20140, end: 20150)\n",
      "Batch 2015 - batch loss: 2.017909288406372 - avg loss: 2.066580412229375   (start: 20150, end: 20160)\n",
      "Batch 2016 - batch loss: 2.131016254425049 - avg loss: 2.0666123586062697   (start: 20160, end: 20170)\n",
      "Batch 2017 - batch loss: 2.0223658084869385 - avg loss: 2.0665904326646842   (start: 20170, end: 20180)\n",
      "Batch 2018 - batch loss: 3.162773847579956 - avg loss: 2.0671333665006997   (start: 20180, end: 20190)\n",
      "Batch 2019 - batch loss: 2.4190425872802734 - avg loss: 2.067307578986234   (start: 20190, end: 20200)\n",
      "Batch 2020 - batch loss: 2.271021842956543 - avg loss: 2.067408377731395   (start: 20200, end: 20210)\n",
      "Batch 2021 - batch loss: 2.489252805709839 - avg loss: 2.0676170050449354   (start: 20210, end: 20220)\n",
      "Batch 2022 - batch loss: 2.2558083534240723 - avg loss: 2.067710030921544   (start: 20220, end: 20230)\n",
      "Batch 2023 - batch loss: 2.0882251262664795 - avg loss: 2.067720166838216   (start: 20230, end: 20240)\n",
      "Batch 2024 - batch loss: 2.7298760414123535 - avg loss: 2.0680471573935613   (start: 20240, end: 20250)\n",
      "Batch 2025 - batch loss: 2.04069447517395 - avg loss: 2.068033656563246   (start: 20250, end: 20260)\n",
      "Batch 2026 - batch loss: 1.8176496028900146 - avg loss: 2.067910132116441   (start: 20260, end: 20270)\n",
      "Batch 2027 - batch loss: 1.7931725978851318 - avg loss: 2.067774659959522   (start: 20270, end: 20280)\n",
      "Batch 2028 - batch loss: 1.604099988937378 - avg loss: 2.0675461362182594   (start: 20280, end: 20290)\n",
      "Batch 2029 - batch loss: 1.9562244415283203 - avg loss: 2.067491297945013   (start: 20290, end: 20300)\n",
      "Batch 2030 - batch loss: 1.8425142765045166 - avg loss: 2.067380526393344   (start: 20300, end: 20310)\n",
      "Batch 2031 - batch loss: 1.8916336297988892 - avg loss: 2.067294036778878   (start: 20310, end: 20320)\n",
      "Batch 2032 - batch loss: 3.5267481803894043 - avg loss: 2.068011918797378   (start: 20320, end: 20330)\n",
      "Batch 2033 - batch loss: 1.6924060583114624 - avg loss: 2.067827255149155   (start: 20330, end: 20340)\n",
      "Batch 2034 - batch loss: 2.1668455600738525 - avg loss: 2.0678759127928523   (start: 20340, end: 20350)\n",
      "Batch 2035 - batch loss: 2.2393007278442383 - avg loss: 2.0679601096568265   (start: 20350, end: 20360)\n",
      "Batch 2036 - batch loss: 2.2164206504821777 - avg loss: 2.068032991611086   (start: 20360, end: 20370)\n",
      "Batch 2037 - batch loss: 2.4732284545898438 - avg loss: 2.0682318117597505   (start: 20370, end: 20380)\n",
      "Batch 2038 - batch loss: 1.6939270496368408 - avg loss: 2.0680482390465955   (start: 20380, end: 20390)\n",
      "Batch 2039 - batch loss: 2.2806601524353027 - avg loss: 2.068152460572766   (start: 20390, end: 20400)\n",
      "Batch 2040 - batch loss: 2.629098415374756 - avg loss: 2.068427299355129   (start: 20400, end: 20410)\n",
      "Batch 2041 - batch loss: 2.4038608074188232 - avg loss: 2.0685915664991366   (start: 20410, end: 20420)\n",
      "Batch 2042 - batch loss: 1.9028257131576538 - avg loss: 2.068510428049141   (start: 20420, end: 20430)\n",
      "Batch 2043 - batch loss: 2.4463448524475098 - avg loss: 2.0686952785503143   (start: 20430, end: 20440)\n",
      "Batch 2044 - batch loss: 1.9458335638046265 - avg loss: 2.0686351994721988   (start: 20440, end: 20450)\n",
      "Batch 2045 - batch loss: 2.2887682914733887 - avg loss: 2.068742791403773   (start: 20450, end: 20460)\n",
      "Batch 2046 - batch loss: 2.022449493408203 - avg loss: 2.0687201762117873   (start: 20460, end: 20470)\n",
      "Batch 2047 - batch loss: 2.094681739807129 - avg loss: 2.0687328527565114   (start: 20470, end: 20480)\n",
      "Batch 2048 - batch loss: 1.888429880142212 - avg loss: 2.0686448571622633   (start: 20480, end: 20490)\n",
      "Batch 2049 - batch loss: 1.9690450429916382 - avg loss: 2.068596271887058   (start: 20490, end: 20500)\n",
      "Batch 2050 - batch loss: 2.9896042346954346 - avg loss: 2.069045325013732   (start: 20500, end: 20510)\n",
      "Batch 2051 - batch loss: 2.1005454063415527 - avg loss: 2.0690606759305585   (start: 20510, end: 20520)\n",
      "Batch 2052 - batch loss: 2.9491612911224365 - avg loss: 2.0694893659525713   (start: 20520, end: 20530)\n",
      "Batch 2053 - batch loss: 1.9812681674957275 - avg loss: 2.069446415028298   (start: 20530, end: 20540)\n",
      "Batch 2054 - batch loss: 2.5565953254699707 - avg loss: 2.0696834704591702   (start: 20540, end: 20550)\n",
      "Batch 2055 - batch loss: 1.7009292840957642 - avg loss: 2.0695041153101608   (start: 20550, end: 20560)\n",
      "Batch 2056 - batch loss: 2.268918037414551 - avg loss: 2.069601059365632   (start: 20560, end: 20570)\n",
      "Batch 2057 - batch loss: 2.1409876346588135 - avg loss: 2.0696357467200017   (start: 20570, end: 20580)\n",
      "Batch 2058 - batch loss: 2.1486198902130127 - avg loss: 2.0696741071588036   (start: 20580, end: 20590)\n",
      "Batch 2059 - batch loss: 2.2867236137390137 - avg loss: 2.0697794709969495   (start: 20590, end: 20600)\n",
      "Batch 2060 - batch loss: 1.5243068933486938 - avg loss: 2.069514806961215   (start: 20600, end: 20610)\n",
      "Batch 2061 - batch loss: 3.012662172317505 - avg loss: 2.069972201415801   (start: 20610, end: 20620)\n",
      "Batch 2062 - batch loss: 1.7008702754974365 - avg loss: 2.0697932862796313   (start: 20620, end: 20630)\n",
      "Batch 2063 - batch loss: 1.4034459590911865 - avg loss: 2.06947044358235   (start: 20630, end: 20640)\n",
      "Batch 2064 - batch loss: 2.367116689682007 - avg loss: 2.069614582200316   (start: 20640, end: 20650)\n",
      "Batch 2065 - batch loss: 1.7842963933944702 - avg loss: 2.0694764804632366   (start: 20650, end: 20660)\n",
      "Batch 2066 - batch loss: 1.8573280572891235 - avg loss: 2.069373844554589   (start: 20660, end: 20670)\n",
      "Batch 2067 - batch loss: 1.8841956853866577 - avg loss: 2.0692842999901946   (start: 20670, end: 20680)\n",
      "Batch 2068 - batch loss: 2.0319719314575195 - avg loss: 2.069266265979304   (start: 20680, end: 20690)\n",
      "Batch 2069 - batch loss: 1.2318763732910156 - avg loss: 2.0688617297992615   (start: 20690, end: 20700)\n",
      "Batch 2070 - batch loss: 2.1087734699249268 - avg loss: 2.068881001523127   (start: 20700, end: 20710)\n",
      "Batch 2071 - batch loss: 3.0456085205078125 - avg loss: 2.0693523951133708   (start: 20710, end: 20720)\n",
      "Batch 2072 - batch loss: 1.852333426475525 - avg loss: 2.0692477067541626   (start: 20720, end: 20730)\n",
      "Batch 2073 - batch loss: 1.2368175983428955 - avg loss: 2.068846342188873   (start: 20730, end: 20740)\n",
      "Batch 2074 - batch loss: 2.706454038619995 - avg loss: 2.06915362300643   (start: 20740, end: 20750)\n",
      "Batch 2075 - batch loss: 2.36470890045166 - avg loss: 2.0692959906737927   (start: 20750, end: 20760)\n",
      "Batch 2076 - batch loss: 1.9650646448135376 - avg loss: 2.069245807069623   (start: 20760, end: 20770)\n",
      "Batch 2077 - batch loss: 2.05255126953125 - avg loss: 2.069237773124706   (start: 20770, end: 20780)\n",
      "Batch 2078 - batch loss: 2.7066314220428467 - avg loss: 2.069544359776422   (start: 20780, end: 20790)\n",
      "Batch 2079 - batch loss: 2.2887110710144043 - avg loss: 2.0696497283875943   (start: 20790, end: 20800)\n",
      "Batch 2080 - batch loss: 1.336430549621582 - avg loss: 2.069297388561181   (start: 20800, end: 20810)\n",
      "Batch 2081 - batch loss: 2.308948040008545 - avg loss: 2.0694124945417034   (start: 20810, end: 20820)\n",
      "Batch 2082 - batch loss: 1.795162558555603 - avg loss: 2.069280833506664   (start: 20820, end: 20830)\n",
      "Batch 2083 - batch loss: 2.174833059310913 - avg loss: 2.069331482367415   (start: 20830, end: 20840)\n",
      "Batch 2084 - batch loss: 1.639737844467163 - avg loss: 2.069125442253314   (start: 20840, end: 20850)\n",
      "Batch 2085 - batch loss: 1.8171148300170898 - avg loss: 2.069004631796825   (start: 20850, end: 20860)\n",
      "Batch 2086 - batch loss: 2.1291146278381348 - avg loss: 2.0690334339032175   (start: 20860, end: 20870)\n",
      "Batch 2087 - batch loss: 1.625868558883667 - avg loss: 2.068821190189128   (start: 20870, end: 20880)\n",
      "Batch 2088 - batch loss: 1.8210017681121826 - avg loss: 2.068702559541891   (start: 20880, end: 20890)\n",
      "Batch 2089 - batch loss: 2.8663647174835205 - avg loss: 2.0690842160767917   (start: 20890, end: 20900)\n",
      "Batch 2090 - batch loss: 1.6285734176635742 - avg loss: 2.0688735461588514   (start: 20900, end: 20910)\n",
      "Batch 2091 - batch loss: 1.7154366970062256 - avg loss: 2.0687045992902315   (start: 20910, end: 20920)\n",
      "Batch 2092 - batch loss: 2.2560319900512695 - avg loss: 2.0687941011491713   (start: 20920, end: 20930)\n",
      "Batch 2093 - batch loss: 2.0082008838653564 - avg loss: 2.0687651645602103   (start: 20930, end: 20940)\n",
      "Batch 2094 - batch loss: 2.26924467086792 - avg loss: 2.068860858835298   (start: 20940, end: 20950)\n",
      "Batch 2095 - batch loss: 1.6944854259490967 - avg loss: 2.0686822446020505   (start: 20950, end: 20960)\n",
      "Batch 2096 - batch loss: 1.3120218515396118 - avg loss: 2.068321414657815   (start: 20960, end: 20970)\n",
      "Batch 2097 - batch loss: 2.307304859161377 - avg loss: 2.0684353247838887   (start: 20970, end: 20980)\n",
      "Batch 2098 - batch loss: 1.2342853546142578 - avg loss: 2.0680379212726123   (start: 20980, end: 20990)\n",
      "Batch 2099 - batch loss: 2.9148123264312744 - avg loss: 2.0684411471798305   (start: 20990, end: 21000)\n",
      "Batch 2100 - batch loss: 2.0518009662628174 - avg loss: 2.0684332270556434   (start: 21000, end: 21010)\n",
      "Batch 2101 - batch loss: 1.9697773456573486 - avg loss: 2.0683862927638272   (start: 21010, end: 21020)\n",
      "Batch 2102 - batch loss: 1.283078908920288 - avg loss: 2.068012870327382   (start: 21020, end: 21030)\n",
      "Batch 2103 - batch loss: 2.3160576820373535 - avg loss: 2.068130762348157   (start: 21030, end: 21040)\n",
      "Batch 2104 - batch loss: 2.5859951972961426 - avg loss: 2.068376778706802   (start: 21040, end: 21050)\n",
      "Batch 2105 - batch loss: 2.452224016189575 - avg loss: 2.0685590423523306   (start: 21050, end: 21060)\n",
      "Batch 2106 - batch loss: 2.0799412727355957 - avg loss: 2.0685644444550277   (start: 21060, end: 21070)\n",
      "Batch 2107 - batch loss: 1.7607824802398682 - avg loss: 2.0684184378306374   (start: 21070, end: 21080)\n",
      "Batch 2108 - batch loss: 2.3801207542419434 - avg loss: 2.0685662340925677   (start: 21080, end: 21090)\n",
      "Batch 2109 - batch loss: 1.7763395309448242 - avg loss: 2.068427738024725   (start: 21090, end: 21100)\n",
      "Batch 2110 - batch loss: 2.3617959022521973 - avg loss: 2.0685667092062636   (start: 21100, end: 21110)\n",
      "Batch 2111 - batch loss: 2.3497326374053955 - avg loss: 2.068699837013176   (start: 21110, end: 21120)\n",
      "Batch 2112 - batch loss: 1.4842146635055542 - avg loss: 2.0684232231118473   (start: 21120, end: 21130)\n",
      "Batch 2113 - batch loss: 2.074467182159424 - avg loss: 2.06842608212748   (start: 21130, end: 21140)\n",
      "Batch 2114 - batch loss: 2.016425609588623 - avg loss: 2.068401495615641   (start: 21140, end: 21150)\n",
      "Batch 2115 - batch loss: 1.50075364112854 - avg loss: 2.0681332310341256   (start: 21150, end: 21160)\n",
      "Batch 2116 - batch loss: 1.7714831829071045 - avg loss: 2.0679931034724217   (start: 21160, end: 21170)\n",
      "Batch 2117 - batch loss: 2.1543147563934326 - avg loss: 2.0680338596824885   (start: 21170, end: 21180)\n",
      "Batch 2118 - batch loss: 2.1348907947540283 - avg loss: 2.068065410855245   (start: 21180, end: 21190)\n",
      "Batch 2119 - batch loss: 1.0789133310317993 - avg loss: 2.067598829685517   (start: 21190, end: 21200)\n",
      "Batch 2120 - batch loss: 2.2410695552825928 - avg loss: 2.0676806169205935   (start: 21200, end: 21210)\n",
      "Batch 2121 - batch loss: 1.7216434478759766 - avg loss: 2.067517545681647   (start: 21210, end: 21220)\n",
      "Batch 2122 - batch loss: 2.1097564697265625 - avg loss: 2.0675374415478953   (start: 21220, end: 21230)\n",
      "Batch 2123 - batch loss: 1.3275455236434937 - avg loss: 2.0671890461063205   (start: 21230, end: 21240)\n",
      "Batch 2124 - batch loss: 2.1589956283569336 - avg loss: 2.06723224920385   (start: 21240, end: 21250)\n",
      "Batch 2125 - batch loss: 2.971221923828125 - avg loss: 2.0676574560122343   (start: 21250, end: 21260)\n",
      "Batch 2126 - batch loss: 2.9202029705047607 - avg loss: 2.068058276658446   (start: 21260, end: 21270)\n",
      "Batch 2127 - batch loss: 1.8578869104385376 - avg loss: 2.067959511918681   (start: 21270, end: 21280)\n",
      "Batch 2128 - batch loss: 2.976313352584839 - avg loss: 2.068386169429562   (start: 21280, end: 21290)\n",
      "Batch 2129 - batch loss: 1.3004001379013062 - avg loss: 2.0680256126072485   (start: 21290, end: 21300)\n",
      "Batch 2130 - batch loss: 2.535696506500244 - avg loss: 2.0682450733739746   (start: 21300, end: 21310)\n",
      "Batch 2131 - batch loss: 1.846808671951294 - avg loss: 2.0681412101462904   (start: 21310, end: 21320)\n",
      "Batch 2132 - batch loss: 1.6433597803115845 - avg loss: 2.067942062734272   (start: 21320, end: 21330)\n",
      "Batch 2133 - batch loss: 2.5948166847229004 - avg loss: 2.0681889580585406   (start: 21330, end: 21340)\n",
      "Batch 2134 - batch loss: 1.7988669872283936 - avg loss: 2.0680628119363718   (start: 21340, end: 21350)\n",
      "Batch 2135 - batch loss: 2.3163208961486816 - avg loss: 2.0681790376312277   (start: 21350, end: 21360)\n",
      "Batch 2136 - batch loss: 2.125487804412842 - avg loss: 2.068205855023264   (start: 21360, end: 21370)\n",
      "Batch 2137 - batch loss: 1.6755911111831665 - avg loss: 2.0680222185668375   (start: 21370, end: 21380)\n",
      "Batch 2138 - batch loss: 1.7953243255615234 - avg loss: 2.0678947300708086   (start: 21380, end: 21390)\n",
      "Batch 2139 - batch loss: 1.525301218032837 - avg loss: 2.0676411817006977   (start: 21390, end: 21400)\n",
      "Batch 2140 - batch loss: 1.7718292474746704 - avg loss: 2.0675030163881214   (start: 21400, end: 21410)\n",
      "Batch 2141 - batch loss: 2.233802318572998 - avg loss: 2.0675806537840993   (start: 21410, end: 21420)\n",
      "Batch 2142 - batch loss: 2.3180737495422363 - avg loss: 2.067697542769521   (start: 21420, end: 21430)\n",
      "Batch 2143 - batch loss: 2.0540618896484375 - avg loss: 2.0676911828566844   (start: 21430, end: 21440)\n",
      "Batch 2144 - batch loss: 2.1492807865142822 - avg loss: 2.0677292199679465   (start: 21440, end: 21450)\n",
      "Batch 2145 - batch loss: 2.875405788421631 - avg loss: 2.0681055836997517   (start: 21450, end: 21460)\n",
      "Batch 2146 - batch loss: 1.3956228494644165 - avg loss: 2.0677923639818965   (start: 21460, end: 21470)\n",
      "Batch 2147 - batch loss: 2.112807273864746 - avg loss: 2.067813320643853   (start: 21470, end: 21480)\n",
      "Batch 2148 - batch loss: 2.1719069480895996 - avg loss: 2.067861758813907   (start: 21480, end: 21490)\n",
      "Batch 2149 - batch loss: 2.010897159576416 - avg loss: 2.067835263651471   (start: 21490, end: 21500)\n",
      "Batch 2150 - batch loss: 2.1226863861083984 - avg loss: 2.067860763940851   (start: 21500, end: 21510)\n",
      "Batch 2151 - batch loss: 2.763378143310547 - avg loss: 2.0681839597491085   (start: 21510, end: 21520)\n",
      "Batch 2152 - batch loss: 2.5262951850891113 - avg loss: 2.0683967378379795   (start: 21520, end: 21530)\n",
      "Batch 2153 - batch loss: 2.276280641555786 - avg loss: 2.068493248471089   (start: 21530, end: 21540)\n",
      "Batch 2154 - batch loss: 1.2923634052276611 - avg loss: 2.0681330954115795   (start: 21540, end: 21550)\n",
      "Batch 2155 - batch loss: 3.0558650493621826 - avg loss: 2.0685912271156384   (start: 21550, end: 21560)\n",
      "Batch 2156 - batch loss: 3.243544340133667 - avg loss: 2.069135943440635   (start: 21560, end: 21570)\n",
      "Batch 2157 - batch loss: 2.444615602493286 - avg loss: 2.0693099377219384   (start: 21570, end: 21580)\n",
      "Batch 2158 - batch loss: 1.6694952249526978 - avg loss: 2.069124752584018   (start: 21580, end: 21590)\n",
      "Batch 2159 - batch loss: 1.6608788967132568 - avg loss: 2.068935749872967   (start: 21590, end: 21600)\n",
      "Batch 2160 - batch loss: 1.8867883682250977 - avg loss: 2.0688514614039026   (start: 21600, end: 21610)\n",
      "Batch 2161 - batch loss: 1.8823251724243164 - avg loss: 2.068765186524634   (start: 21610, end: 21620)\n",
      "Batch 2162 - batch loss: 1.6484674215316772 - avg loss: 2.0685708741043873   (start: 21620, end: 21630)\n",
      "Batch 2163 - batch loss: 2.1812427043914795 - avg loss: 2.0686229405694   (start: 21630, end: 21640)\n",
      "Batch 2164 - batch loss: 1.7940906286239624 - avg loss: 2.0684961358063765   (start: 21640, end: 21650)\n",
      "Batch 2165 - batch loss: 1.8135490417480469 - avg loss: 2.0683784317001632   (start: 21650, end: 21660)\n",
      "Batch 2166 - batch loss: 2.7360167503356934 - avg loss: 2.0686865250636313   (start: 21660, end: 21670)\n",
      "Batch 2167 - batch loss: 2.2197890281677246 - avg loss: 2.068756221790155   (start: 21670, end: 21680)\n",
      "Batch 2168 - batch loss: 2.346691846847534 - avg loss: 2.0688843617740456   (start: 21680, end: 21690)\n",
      "Batch 2169 - batch loss: 1.9070398807525635 - avg loss: 2.0688097790638973   (start: 21690, end: 21700)\n",
      "Batch 2170 - batch loss: 1.6567466259002686 - avg loss: 2.0686199756769033   (start: 21700, end: 21710)\n",
      "Batch 2171 - batch loss: 1.7646291255950928 - avg loss: 2.0684800167219852   (start: 21710, end: 21720)\n",
      "Batch 2172 - batch loss: 2.530427932739258 - avg loss: 2.068692602049191   (start: 21720, end: 21730)\n",
      "Batch 2173 - batch loss: 2.082794427871704 - avg loss: 2.068699088629606   (start: 21730, end: 21740)\n",
      "Batch 2174 - batch loss: 1.9428977966308594 - avg loss: 2.0686412489551236   (start: 21740, end: 21750)\n",
      "Batch 2175 - batch loss: 1.533513069152832 - avg loss: 2.0683953260783765   (start: 21750, end: 21760)\n",
      "Batch 2176 - batch loss: 1.592660665512085 - avg loss: 2.068176798443757   (start: 21760, end: 21770)\n",
      "Batch 2177 - batch loss: 2.250966787338257 - avg loss: 2.0682607240584927   (start: 21770, end: 21780)\n",
      "Batch 2178 - batch loss: 1.3334908485412598 - avg loss: 2.0679235189756486   (start: 21780, end: 21790)\n",
      "Batch 2179 - batch loss: 1.9438129663467407 - avg loss: 2.0678665875294886   (start: 21790, end: 21800)\n",
      "Batch 2180 - batch loss: 1.8331409692764282 - avg loss: 2.067758964595856   (start: 21800, end: 21810)\n",
      "Batch 2181 - batch loss: 2.1776092052459717 - avg loss: 2.0678093084275013   (start: 21810, end: 21820)\n",
      "Batch 2182 - batch loss: 1.5797884464263916 - avg loss: 2.0675857532914494   (start: 21820, end: 21830)\n",
      "Batch 2183 - batch loss: 2.413377285003662 - avg loss: 2.0677440827473617   (start: 21830, end: 21840)\n",
      "Batch 2184 - batch loss: 2.5719475746154785 - avg loss: 2.067974839494212   (start: 21840, end: 21850)\n",
      "Batch 2185 - batch loss: 1.5063813924789429 - avg loss: 2.067717934898139   (start: 21850, end: 21860)\n",
      "Batch 2186 - batch loss: 2.1430368423461914 - avg loss: 2.0677523742705435   (start: 21860, end: 21870)\n",
      "Batch 2187 - batch loss: 1.5761102437973022 - avg loss: 2.0675276749421734   (start: 21870, end: 21880)\n",
      "Batch 2188 - batch loss: 1.6360080242156982 - avg loss: 2.067330543991636   (start: 21880, end: 21890)\n",
      "Batch 2189 - batch loss: 2.5065910816192627 - avg loss: 2.067531119579594   (start: 21890, end: 21900)\n",
      "Batch 2190 - batch loss: 2.670323133468628 - avg loss: 2.067806241448096   (start: 21900, end: 21910)\n",
      "Batch 2191 - batch loss: 2.0220253467559814 - avg loss: 2.0677853560034376   (start: 21910, end: 21920)\n",
      "Batch 2192 - batch loss: 2.4848971366882324 - avg loss: 2.0679755574538183   (start: 21920, end: 21930)\n",
      "Batch 2193 - batch loss: 1.7144889831542969 - avg loss: 2.0678144423333538   (start: 21930, end: 21940)\n",
      "Batch 2194 - batch loss: 1.5377633571624756 - avg loss: 2.067572961201157   (start: 21940, end: 21950)\n",
      "Batch 2195 - batch loss: 2.4135799407958984 - avg loss: 2.067730523578022   (start: 21950, end: 21960)\n",
      "Batch 2196 - batch loss: 1.895745038986206 - avg loss: 2.0676522416096144   (start: 21960, end: 21970)\n",
      "Batch 2197 - batch loss: 2.317779064178467 - avg loss: 2.0677660390721115   (start: 21970, end: 21980)\n",
      "Batch 2198 - batch loss: 1.7518678903579712 - avg loss: 2.0676223837066208   (start: 21980, end: 21990)\n",
      "Batch 2199 - batch loss: 2.3221194744110107 - avg loss: 2.0677380642023953   (start: 21990, end: 22000)\n",
      "Batch 2200 - batch loss: 2.1166648864746094 - avg loss: 2.06776029356281   (start: 22000, end: 22010)\n",
      "Batch 2201 - batch loss: 2.1962790489196777 - avg loss: 2.0678186581201925   (start: 22010, end: 22020)\n",
      "Batch 2202 - batch loss: 2.3772964477539062 - avg loss: 2.0679591382789004   (start: 22020, end: 22030)\n",
      "Batch 2203 - batch loss: 1.6348333358764648 - avg loss: 2.0677626202197343   (start: 22030, end: 22040)\n",
      "Batch 2204 - batch loss: 2.1146724224090576 - avg loss: 2.0677838945064413   (start: 22040, end: 22050)\n",
      "Batch 2205 - batch loss: 2.4345011711120605 - avg loss: 2.0679501308059   (start: 22050, end: 22060)\n",
      "Batch 2206 - batch loss: 1.9901840686798096 - avg loss: 2.0679148947106913   (start: 22060, end: 22070)\n",
      "Batch 2207 - batch loss: 1.7745940685272217 - avg loss: 2.0677820501336153   (start: 22070, end: 22080)\n",
      "Batch 2208 - batch loss: 1.6509664058685303 - avg loss: 2.06759336038972   (start: 22080, end: 22090)\n",
      "Batch 2209 - batch loss: 2.613651752471924 - avg loss: 2.0678404456350057   (start: 22090, end: 22100)\n",
      "Batch 2210 - batch loss: 2.279944896697998 - avg loss: 2.0679363770918413   (start: 22100, end: 22110)\n",
      "Batch 2211 - batch loss: 1.2695679664611816 - avg loss: 2.067575451047252   (start: 22110, end: 22120)\n",
      "Batch 2212 - batch loss: 2.3594486713409424 - avg loss: 2.067707341341104   (start: 22120, end: 22130)\n",
      "Batch 2213 - batch loss: 1.7705411911010742 - avg loss: 2.067573119954365   (start: 22130, end: 22140)\n",
      "Batch 2214 - batch loss: 1.7837097644805908 - avg loss: 2.067444964940607   (start: 22140, end: 22150)\n",
      "Batch 2215 - batch loss: 1.5578330755233765 - avg loss: 2.0672149956764296   (start: 22150, end: 22160)\n",
      "Batch 2216 - batch loss: 2.3470547199249268 - avg loss: 2.067341220179925   (start: 22160, end: 22170)\n",
      "Batch 2217 - batch loss: 2.02498197555542 - avg loss: 2.067322122233746   (start: 22170, end: 22180)\n",
      "Batch 2218 - batch loss: 2.452610731124878 - avg loss: 2.067495753873625   (start: 22180, end: 22190)\n",
      "Batch 2219 - batch loss: 1.775323510169983 - avg loss: 2.067364144754839   (start: 22190, end: 22200)\n",
      "Batch 2220 - batch loss: 1.4174100160598755 - avg loss: 2.067071504444756   (start: 22200, end: 22210)\n",
      "Batch 2221 - batch loss: 1.9306180477142334 - avg loss: 2.0670100942482077   (start: 22210, end: 22220)\n",
      "Batch 2222 - batch loss: 1.605019211769104 - avg loss: 2.066802271089198   (start: 22220, end: 22230)\n",
      "Batch 2223 - batch loss: 2.253842353820801 - avg loss: 2.0668863718458215   (start: 22230, end: 22240)\n",
      "Batch 2224 - batch loss: 1.5180552005767822 - avg loss: 2.0666397061508692   (start: 22240, end: 22250)\n",
      "Batch 2225 - batch loss: 3.1450839042663574 - avg loss: 2.067124182430346   (start: 22250, end: 22260)\n",
      "Batch 2226 - batch loss: 2.9955639839172363 - avg loss: 2.067541084002635   (start: 22260, end: 22270)\n",
      "Batch 2227 - batch loss: 2.890542507171631 - avg loss: 2.0679104742284737   (start: 22270, end: 22280)\n",
      "Batch 2228 - batch loss: 1.850938081741333 - avg loss: 2.0678131335409513   (start: 22280, end: 22290)\n",
      "Batch 2229 - batch loss: 1.6389764547348022 - avg loss: 2.0676208300975407   (start: 22290, end: 22300)\n",
      "Batch 2230 - batch loss: 1.511664628982544 - avg loss: 2.0673716341311064   (start: 22300, end: 22310)\n",
      "Batch 2231 - batch loss: 1.2705066204071045 - avg loss: 2.067014615755782   (start: 22310, end: 22320)\n",
      "Batch 2232 - batch loss: 1.2667033672332764 - avg loss: 2.06665621394274   (start: 22320, end: 22330)\n",
      "Batch 2233 - batch loss: 1.559023141860962 - avg loss: 2.0664289833822735   (start: 22330, end: 22340)\n",
      "Batch 2234 - batch loss: 2.693021297454834 - avg loss: 2.0667093378852144   (start: 22340, end: 22350)\n",
      "Batch 2235 - batch loss: 1.9012796878814697 - avg loss: 2.0666353532474666   (start: 22350, end: 22360)\n",
      "Batch 2236 - batch loss: 2.545405149459839 - avg loss: 2.066849376401786   (start: 22360, end: 22370)\n",
      "Batch 2237 - batch loss: 1.9622329473495483 - avg loss: 2.066802630901763   (start: 22370, end: 22380)\n",
      "Batch 2238 - batch loss: 1.7014906406402588 - avg loss: 2.0666394723531867   (start: 22380, end: 22390)\n",
      "Batch 2239 - batch loss: 2.3478505611419678 - avg loss: 2.066765013017825   (start: 22390, end: 22400)\n",
      "Batch 2240 - batch loss: 2.053654193878174 - avg loss: 2.0667591625853663   (start: 22400, end: 22410)\n",
      "Batch 2241 - batch loss: 2.2989439964294434 - avg loss: 2.066862724063441   (start: 22410, end: 22420)\n",
      "Batch 2242 - batch loss: 1.757749319076538 - avg loss: 2.0667249115779365   (start: 22420, end: 22430)\n",
      "Batch 2243 - batch loss: 1.8234355449676514 - avg loss: 2.0666164938566305   (start: 22430, end: 22440)\n",
      "Batch 2244 - batch loss: 2.905062675476074 - avg loss: 2.066989966543321   (start: 22440, end: 22450)\n",
      "Batch 2245 - batch loss: 2.253056049346924 - avg loss: 2.067072809857125   (start: 22450, end: 22460)\n",
      "Batch 2246 - batch loss: 1.5820386409759521 - avg loss: 2.0668569513039956   (start: 22460, end: 22470)\n",
      "Batch 2247 - batch loss: 1.7615735530853271 - avg loss: 2.0667211490805886   (start: 22470, end: 22480)\n",
      "Batch 2248 - batch loss: 1.8388153314590454 - avg loss: 2.06661981256764   (start: 22480, end: 22490)\n",
      "Batch 2249 - batch loss: 2.948721170425415 - avg loss: 2.0670118576155767   (start: 22490, end: 22500)\n",
      "Batch 2250 - batch loss: 1.9016460180282593 - avg loss: 2.0669383943372175   (start: 22500, end: 22510)\n",
      "Batch 2251 - batch loss: 1.605403184890747 - avg loss: 2.0667334497504295   (start: 22510, end: 22520)\n",
      "Batch 2252 - batch loss: 1.7934751510620117 - avg loss: 2.0666121633329024   (start: 22520, end: 22530)\n",
      "Batch 2253 - batch loss: 2.083071231842041 - avg loss: 2.0666194654928445   (start: 22530, end: 22540)\n",
      "Batch 2254 - batch loss: 2.1361937522888184 - avg loss: 2.066650318835104   (start: 22540, end: 22550)\n",
      "Batch 2255 - batch loss: 1.8585665225982666 - avg loss: 2.066558083109822   (start: 22550, end: 22560)\n",
      "Batch 2256 - batch loss: 2.3301281929016113 - avg loss: 2.0666748620685245   (start: 22560, end: 22570)\n",
      "Batch 2257 - batch loss: 2.5389397144317627 - avg loss: 2.066884013907481   (start: 22570, end: 22580)\n",
      "Batch 2258 - batch loss: 1.9916226863861084 - avg loss: 2.0668506976934387   (start: 22580, end: 22590)\n",
      "Batch 2259 - batch loss: 2.607938766479492 - avg loss: 2.0670901171929015   (start: 22590, end: 22600)\n",
      "Batch 2260 - batch loss: 1.7321479320526123 - avg loss: 2.066941978234414   (start: 22600, end: 22610)\n",
      "Batch 2261 - batch loss: 2.9398257732391357 - avg loss: 2.067327868506299   (start: 22610, end: 22620)\n",
      "Batch 2262 - batch loss: 2.1040823459625244 - avg loss: 2.06734410998993   (start: 22620, end: 22630)\n",
      "Batch 2263 - batch loss: 1.4417537450790405 - avg loss: 2.067067789157372   (start: 22630, end: 22640)\n",
      "Batch 2264 - batch loss: 2.820918321609497 - avg loss: 2.067400614999514   (start: 22640, end: 22650)\n",
      "Batch 2265 - batch loss: 2.0087356567382812 - avg loss: 2.0673747257858066   (start: 22650, end: 22660)\n",
      "Batch 2266 - batch loss: 1.7649259567260742 - avg loss: 2.0672413121249953   (start: 22660, end: 22670)\n",
      "Batch 2267 - batch loss: 1.7474157810211182 - avg loss: 2.0671002955768896   (start: 22670, end: 22680)\n",
      "Batch 2268 - batch loss: 2.0703113079071045 - avg loss: 2.0671017107431875   (start: 22680, end: 22690)\n",
      "Batch 2269 - batch loss: 1.8762233257293701 - avg loss: 2.0670176233489084   (start: 22690, end: 22700)\n",
      "Batch 2270 - batch loss: 1.6768081188201904 - avg loss: 2.0668458005816124   (start: 22700, end: 22710)\n",
      "Batch 2271 - batch loss: 1.9220876693725586 - avg loss: 2.066782086615411   (start: 22710, end: 22720)\n",
      "Batch 2272 - batch loss: 2.079597234725952 - avg loss: 2.0667877246040214   (start: 22720, end: 22730)\n",
      "Batch 2273 - batch loss: 1.6944986581802368 - avg loss: 2.0666240090954795   (start: 22730, end: 22740)\n",
      "Batch 2274 - batch loss: 1.524167776107788 - avg loss: 2.0663855667952653   (start: 22740, end: 22750)\n",
      "Batch 2275 - batch loss: 1.6001970767974854 - avg loss: 2.0661807388119624   (start: 22750, end: 22760)\n",
      "Batch 2276 - batch loss: 2.7814419269561768 - avg loss: 2.066494863180932   (start: 22760, end: 22770)\n",
      "Batch 2277 - batch loss: 2.1972765922546387 - avg loss: 2.066552273948743   (start: 22770, end: 22780)\n",
      "Batch 2278 - batch loss: 1.9767277240753174 - avg loss: 2.066512859929492   (start: 22780, end: 22790)\n",
      "Batch 2279 - batch loss: 2.7700295448303223 - avg loss: 2.06682141987901   (start: 22790, end: 22800)\n",
      "Batch 2280 - batch loss: 1.7767114639282227 - avg loss: 2.0666942344533408   (start: 22800, end: 22810)\n",
      "Batch 2281 - batch loss: 1.2841955423355103 - avg loss: 2.066351334062404   (start: 22810, end: 22820)\n",
      "Batch 2282 - batch loss: 2.734563112258911 - avg loss: 2.0666440242850044   (start: 22820, end: 22830)\n",
      "Batch 2283 - batch loss: 2.0362415313720703 - avg loss: 2.0666307132110497   (start: 22830, end: 22840)\n",
      "Batch 2284 - batch loss: 2.0268492698669434 - avg loss: 2.0666133033890173   (start: 22840, end: 22850)\n",
      "Batch 2285 - batch loss: 2.638136386871338 - avg loss: 2.0668633134867784   (start: 22850, end: 22860)\n",
      "Batch 2286 - batch loss: 3.1217293739318848 - avg loss: 2.0673245579382193   (start: 22860, end: 22870)\n",
      "Batch 2287 - batch loss: 2.604344367980957 - avg loss: 2.0675592693936573   (start: 22870, end: 22880)\n",
      "Batch 2288 - batch loss: 1.5983842611312866 - avg loss: 2.0673542999710874   (start: 22880, end: 22890)\n",
      "Batch 2289 - batch loss: 1.9602057933807373 - avg loss: 2.0673075102302185   (start: 22890, end: 22900)\n",
      "Batch 2290 - batch loss: 1.255919337272644 - avg loss: 2.066953346907234   (start: 22900, end: 22910)\n",
      "Batch 2291 - batch loss: 1.5901645421981812 - avg loss: 2.0667453238685303   (start: 22910, end: 22920)\n",
      "Batch 2292 - batch loss: 2.4401373863220215 - avg loss: 2.0669081638434337   (start: 22920, end: 22930)\n",
      "Batch 2293 - batch loss: 2.0868239402770996 - avg loss: 2.0669168455245295   (start: 22930, end: 22940)\n",
      "Batch 2294 - batch loss: 1.2259994745254517 - avg loss: 2.0665504327267086   (start: 22940, end: 22950)\n",
      "Batch 2295 - batch loss: 1.572371244430542 - avg loss: 2.0663351978886   (start: 22950, end: 22960)\n",
      "Batch 2296 - batch loss: 1.8035074472427368 - avg loss: 2.0662207757072135   (start: 22960, end: 22970)\n",
      "Batch 2297 - batch loss: 1.9913911819458008 - avg loss: 2.066188212785646   (start: 22970, end: 22980)\n",
      "Batch 2298 - batch loss: 2.0727272033691406 - avg loss: 2.066191057061672   (start: 22980, end: 22990)\n",
      "Batch 2299 - batch loss: 2.5273520946502686 - avg loss: 2.0663915618606237   (start: 22990, end: 23000)\n",
      "Batch 2300 - batch loss: 2.167030096054077 - avg loss: 2.0664352987290258   (start: 23000, end: 23010)\n",
      "Batch 2301 - batch loss: 2.6564993858337402 - avg loss: 2.066691625439323   (start: 23010, end: 23020)\n",
      "Batch 2302 - batch loss: 1.9843342304229736 - avg loss: 2.066655864520949   (start: 23020, end: 23030)\n",
      "Batch 2303 - batch loss: 2.0849056243896484 - avg loss: 2.0666637854236694   (start: 23030, end: 23040)\n",
      "Batch 2304 - batch loss: 2.1737570762634277 - avg loss: 2.0667102467212137   (start: 23040, end: 23050)\n",
      "Batch 2305 - batch loss: 3.8493709564208984 - avg loss: 2.06748329993444   (start: 23050, end: 23060)\n",
      "Batch 2306 - batch loss: 2.692183494567871 - avg loss: 2.0677540845875106   (start: 23060, end: 23070)\n",
      "Batch 2307 - batch loss: 2.878657341003418 - avg loss: 2.068105429152682   (start: 23070, end: 23080)\n",
      "Batch 2308 - batch loss: 2.061638593673706 - avg loss: 2.0681026284443758   (start: 23080, end: 23090)\n",
      "Batch 2309 - batch loss: 1.939004898071289 - avg loss: 2.068046741981011   (start: 23090, end: 23100)\n",
      "Batch 2310 - batch loss: 2.3227269649505615 - avg loss: 2.068156945452655   (start: 23100, end: 23110)\n",
      "Batch 2311 - batch loss: 1.8225332498550415 - avg loss: 2.068050706829992   (start: 23110, end: 23120)\n",
      "Batch 2312 - batch loss: 1.851778268814087 - avg loss: 2.0679572038304173   (start: 23120, end: 23130)\n",
      "Batch 2313 - batch loss: 2.6534905433654785 - avg loss: 2.0682102433029907   (start: 23130, end: 23140)\n",
      "Batch 2314 - batch loss: 1.8555256128311157 - avg loss: 2.0681183708924196   (start: 23140, end: 23150)\n",
      "Batch 2315 - batch loss: 1.9470514059066772 - avg loss: 2.068066096727918   (start: 23150, end: 23160)\n",
      "Batch 2316 - batch loss: 2.045531988143921 - avg loss: 2.068056371173933   (start: 23160, end: 23170)\n",
      "Batch 2317 - batch loss: 2.0328497886657715 - avg loss: 2.068041182829451   (start: 23170, end: 23180)\n",
      "Batch 2318 - batch loss: 2.3324532508850098 - avg loss: 2.0681552026949346   (start: 23180, end: 23190)\n",
      "Batch 2319 - batch loss: 1.4984703063964844 - avg loss: 2.067909648860323   (start: 23190, end: 23200)\n",
      "Batch 2320 - batch loss: 1.7193095684051514 - avg loss: 2.0677594549437117   (start: 23200, end: 23210)\n",
      "Batch 2321 - batch loss: 1.769040822982788 - avg loss: 2.067630807815391   (start: 23210, end: 23220)\n",
      "Batch 2322 - batch loss: 2.7255043983459473 - avg loss: 2.0679140078113143   (start: 23220, end: 23230)\n",
      "Batch 2323 - batch loss: 1.9969263076782227 - avg loss: 2.067883462329329   (start: 23230, end: 23240)\n",
      "Batch 2324 - batch loss: 1.9294017553329468 - avg loss: 2.067823900304815   (start: 23240, end: 23250)\n",
      "Batch 2325 - batch loss: 1.901531457901001 - avg loss: 2.0677524074233   (start: 23250, end: 23260)\n",
      "Batch 2326 - batch loss: 1.4129582643508911 - avg loss: 2.067471017589577   (start: 23260, end: 23270)\n",
      "Batch 2327 - batch loss: 2.944777488708496 - avg loss: 2.067847867448305   (start: 23270, end: 23280)\n",
      "Batch 2328 - batch loss: 2.7081027030944824 - avg loss: 2.068122772916595   (start: 23280, end: 23290)\n",
      "Batch 2329 - batch loss: 1.686253309249878 - avg loss: 2.067958880442918   (start: 23290, end: 23300)\n",
      "Batch 2330 - batch loss: 1.5406782627105713 - avg loss: 2.0677326768317075   (start: 23300, end: 23310)\n",
      "Batch 2331 - batch loss: 2.066429376602173 - avg loss: 2.0677321179551083   (start: 23310, end: 23320)\n",
      "Batch 2332 - batch loss: 1.7158422470092773 - avg loss: 2.067581286463061   (start: 23320, end: 23330)\n",
      "Batch 2333 - batch loss: 1.4711382389068604 - avg loss: 2.06732574102709   (start: 23330, end: 23340)\n",
      "Batch 2334 - batch loss: 1.8759229183197021 - avg loss: 2.0672437697968085   (start: 23340, end: 23350)\n",
      "Batch 2335 - batch loss: 2.3563809394836426 - avg loss: 2.0673675442701334   (start: 23350, end: 23360)\n",
      "Batch 2336 - batch loss: 2.6377155780792236 - avg loss: 2.0676115956324823   (start: 23360, end: 23370)\n",
      "Batch 2337 - batch loss: 2.33933424949646 - avg loss: 2.067727815758172   (start: 23370, end: 23380)\n",
      "Batch 2338 - batch loss: 2.8143417835235596 - avg loss: 2.0680470179675634   (start: 23380, end: 23390)\n",
      "Batch 2339 - batch loss: 1.7602603435516357 - avg loss: 2.067915485200719   (start: 23390, end: 23400)\n",
      "Batch 2340 - batch loss: 2.8940460681915283 - avg loss: 2.0682683816479597   (start: 23400, end: 23410)\n",
      "Batch 2341 - batch loss: 1.8168758153915405 - avg loss: 2.0681610406717614   (start: 23410, end: 23420)\n",
      "Batch 2342 - batch loss: 1.3387291431427002 - avg loss: 2.0678497167718346   (start: 23420, end: 23430)\n",
      "Batch 2343 - batch loss: 2.007194757461548 - avg loss: 2.067823840082709   (start: 23430, end: 23440)\n",
      "Batch 2344 - batch loss: 2.351929187774658 - avg loss: 2.067944993749102   (start: 23440, end: 23450)\n",
      "Batch 2345 - batch loss: 2.670788526535034 - avg loss: 2.0682019603018666   (start: 23450, end: 23460)\n",
      "Batch 2346 - batch loss: 1.4962806701660156 - avg loss: 2.0679582784569006   (start: 23460, end: 23470)\n",
      "Batch 2347 - batch loss: 2.5533087253570557 - avg loss: 2.0681649864836893   (start: 23470, end: 23480)\n",
      "Batch 2348 - batch loss: 1.3254899978637695 - avg loss: 2.067848820034724   (start: 23480, end: 23490)\n",
      "Batch 2349 - batch loss: 2.2963852882385254 - avg loss: 2.0679460695956617   (start: 23490, end: 23500)\n",
      "Batch 2350 - batch loss: 1.5015453100204468 - avg loss: 2.0677051505146   (start: 23500, end: 23510)\n",
      "Batch 2351 - batch loss: 1.9938437938690186 - avg loss: 2.0676737468765705   (start: 23510, end: 23520)\n",
      "Batch 2352 - batch loss: 2.4172329902648926 - avg loss: 2.0678223058410365   (start: 23520, end: 23530)\n",
      "Batch 2353 - batch loss: 1.9245579242706299 - avg loss: 2.067761445865858   (start: 23530, end: 23540)\n",
      "Batch 2354 - batch loss: 2.0772197246551514 - avg loss: 2.067765462120121   (start: 23540, end: 23550)\n",
      "Batch 2355 - batch loss: 1.6709339618682861 - avg loss: 2.0675970276972637   (start: 23550, end: 23560)\n",
      "Batch 2356 - batch loss: 1.4437203407287598 - avg loss: 2.067332336697277   (start: 23560, end: 23570)\n",
      "Batch 2357 - batch loss: 1.6517622470855713 - avg loss: 2.0671560983216994   (start: 23570, end: 23580)\n",
      "Batch 2358 - batch loss: 1.746476173400879 - avg loss: 2.067020159396341   (start: 23580, end: 23590)\n",
      "Batch 2359 - batch loss: 1.6684846878051758 - avg loss: 2.0668512884338024   (start: 23590, end: 23600)\n",
      "Batch 2360 - batch loss: 2.369655132293701 - avg loss: 2.066979540803078   (start: 23600, end: 23610)\n",
      "Batch 2361 - batch loss: 2.209179401397705 - avg loss: 2.0670397439616703   (start: 23610, end: 23620)\n",
      "Batch 2362 - batch loss: 2.6272084712982178 - avg loss: 2.06727680224662   (start: 23620, end: 23630)\n",
      "Batch 2363 - batch loss: 1.9384839534759521 - avg loss: 2.067222321346125   (start: 23630, end: 23640)\n",
      "Batch 2364 - batch loss: 1.8881117105484009 - avg loss: 2.0671465874726374   (start: 23640, end: 23650)\n",
      "Batch 2365 - batch loss: 2.3825793266296387 - avg loss: 2.067279906466364   (start: 23650, end: 23660)\n",
      "Batch 2366 - batch loss: 3.200836181640625 - avg loss: 2.067758806455876   (start: 23660, end: 23670)\n",
      "Batch 2367 - batch loss: 2.571930408477783 - avg loss: 2.0679717167607836   (start: 23670, end: 23680)\n",
      "Batch 2368 - batch loss: 1.6947256326675415 - avg loss: 2.06781416248299   (start: 23680, end: 23690)\n",
      "Batch 2369 - batch loss: 1.621795654296875 - avg loss: 2.06762596901962   (start: 23690, end: 23700)\n",
      "Batch 2370 - batch loss: 2.3693361282348633 - avg loss: 2.067753219192212   (start: 23700, end: 23710)\n",
      "Batch 2371 - batch loss: 2.4005937576293945 - avg loss: 2.067893539823931   (start: 23710, end: 23720)\n",
      "Batch 2372 - batch loss: 2.415731906890869 - avg loss: 2.068040121520967   (start: 23720, end: 23730)\n",
      "Batch 2373 - batch loss: 2.041959524154663 - avg loss: 2.0680291355911584   (start: 23730, end: 23740)\n",
      "Batch 2374 - batch loss: 2.504845142364502 - avg loss: 2.068213058120326   (start: 23740, end: 23750)\n",
      "Batch 2375 - batch loss: 1.5820412635803223 - avg loss: 2.068008440361681   (start: 23750, end: 23760)\n",
      "Batch 2376 - batch loss: 1.353729009628296 - avg loss: 2.0677079441771067   (start: 23760, end: 23770)\n",
      "Batch 2377 - batch loss: 2.3480734825134277 - avg loss: 2.067825843898863   (start: 23770, end: 23780)\n",
      "Batch 2378 - batch loss: 2.878276824951172 - avg loss: 2.0681665126592885   (start: 23780, end: 23790)\n",
      "Batch 2379 - batch loss: 2.7627649307250977 - avg loss: 2.0684583607341063   (start: 23790, end: 23800)\n",
      "Batch 2380 - batch loss: 1.527137041091919 - avg loss: 2.0682310103268646   (start: 23800, end: 23810)\n",
      "Batch 2381 - batch loss: 3.2058849334716797 - avg loss: 2.0687086148286045   (start: 23810, end: 23820)\n",
      "Batch 2382 - batch loss: 2.250607967376709 - avg loss: 2.0687849469110837   (start: 23820, end: 23830)\n",
      "Batch 2383 - batch loss: 2.297004461288452 - avg loss: 2.068880676573155   (start: 23830, end: 23840)\n",
      "Batch 2384 - batch loss: 2.0278372764587402 - avg loss: 2.0688634676003606   (start: 23840, end: 23850)\n",
      "Batch 2385 - batch loss: 2.065441131591797 - avg loss: 2.0688620332600385   (start: 23850, end: 23860)\n",
      "Batch 2386 - batch loss: 2.3817296028137207 - avg loss: 2.0689931047177486   (start: 23860, end: 23870)\n",
      "Batch 2387 - batch loss: 3.479088544845581 - avg loss: 2.069583596945608   (start: 23870, end: 23880)\n",
      "Batch 2388 - batch loss: 1.936417818069458 - avg loss: 2.0695278557238095   (start: 23880, end: 23890)\n",
      "Batch 2389 - batch loss: 1.913667917251587 - avg loss: 2.069462642360432   (start: 23890, end: 23900)\n",
      "Batch 2390 - batch loss: 2.1276462078094482 - avg loss: 2.069486976766726   (start: 23900, end: 23910)\n",
      "Batch 2391 - batch loss: 2.195711612701416 - avg loss: 2.069539746263354   (start: 23910, end: 23920)\n",
      "Batch 2392 - batch loss: 2.582343578338623 - avg loss: 2.069754039548801   (start: 23920, end: 23930)\n",
      "Batch 2393 - batch loss: 1.893970251083374 - avg loss: 2.069680612736577   (start: 23930, end: 23940)\n",
      "Batch 2394 - batch loss: 1.4080373048782349 - avg loss: 2.0694043524827737   (start: 23940, end: 23950)\n",
      "Batch 2395 - batch loss: 2.0619168281555176 - avg loss: 2.0694012274726203   (start: 23950, end: 23960)\n",
      "Batch 2396 - batch loss: 2.8479819297790527 - avg loss: 2.069726042116887   (start: 23960, end: 23970)\n",
      "Batch 2397 - batch loss: 3.20363187789917 - avg loss: 2.070198896927472   (start: 23970, end: 23980)\n",
      "Batch 2398 - batch loss: 1.4808528423309326 - avg loss: 2.0699532337117166   (start: 23980, end: 23990)\n",
      "Batch 2399 - batch loss: 1.5727837085723877 - avg loss: 2.0697460797429086   (start: 23990, end: 24000)\n",
      "Batch 2400 - batch loss: 2.500563621520996 - avg loss: 2.069925512288422   (start: 24000, end: 24010)\n",
      "Batch 2401 - batch loss: 2.1765401363372803 - avg loss: 2.069969898060299   (start: 24010, end: 24020)\n",
      "Batch 2402 - batch loss: 1.86408269405365 - avg loss: 2.0698842188243414   (start: 24020, end: 24030)\n",
      "Batch 2403 - batch loss: 1.9862501621246338 - avg loss: 2.069849429283285   (start: 24030, end: 24040)\n",
      "Batch 2404 - batch loss: 1.8739242553710938 - avg loss: 2.0697679635145065   (start: 24040, end: 24050)\n",
      "Batch 2405 - batch loss: 2.0835509300231934 - avg loss: 2.0697736920957652   (start: 24050, end: 24060)\n",
      "Batch 2406 - batch loss: 1.6469764709472656 - avg loss: 2.0695980389087487   (start: 24060, end: 24070)\n",
      "Batch 2407 - batch loss: 2.649016857147217 - avg loss: 2.069838661341572   (start: 24070, end: 24080)\n",
      "Batch 2408 - batch loss: 2.4195754528045654 - avg loss: 2.069983840582528   (start: 24080, end: 24090)\n",
      "Batch 2409 - batch loss: 2.070662021636963 - avg loss: 2.0699841219854553   (start: 24090, end: 24100)\n",
      "Batch 2410 - batch loss: 2.0242977142333984 - avg loss: 2.0699651728325095   (start: 24100, end: 24110)\n",
      "Batch 2411 - batch loss: 2.123476028442383 - avg loss: 2.0699873580960295   (start: 24110, end: 24120)\n",
      "Batch 2412 - batch loss: 1.4498372077941895 - avg loss: 2.0697303543039443   (start: 24120, end: 24130)\n",
      "Batch 2413 - batch loss: 2.3686814308166504 - avg loss: 2.06985419484931   (start: 24130, end: 24140)\n",
      "Batch 2414 - batch loss: 1.8376811742782593 - avg loss: 2.069758056952593   (start: 24140, end: 24150)\n",
      "Batch 2415 - batch loss: 1.3317317962646484 - avg loss: 2.069452582506944   (start: 24150, end: 24160)\n",
      "Batch 2416 - batch loss: 1.732189416885376 - avg loss: 2.0693130445815733   (start: 24160, end: 24170)\n",
      "Batch 2417 - batch loss: 1.8651702404022217 - avg loss: 2.069228618277115   (start: 24170, end: 24180)\n",
      "Batch 2418 - batch loss: 2.744534969329834 - avg loss: 2.0695077858467936   (start: 24180, end: 24190)\n",
      "Batch 2419 - batch loss: 2.5508618354797363 - avg loss: 2.0697066924788734   (start: 24190, end: 24200)\n",
      "Batch 2420 - batch loss: 2.3360610008239746 - avg loss: 2.0698167107805445   (start: 24200, end: 24210)\n",
      "Batch 2421 - batch loss: 1.7514251470565796 - avg loss: 2.069685252661748   (start: 24210, end: 24220)\n",
      "Batch 2422 - batch loss: 2.014199733734131 - avg loss: 2.0696623531491904   (start: 24220, end: 24230)\n",
      "Batch 2423 - batch loss: 2.095982313156128 - avg loss: 2.0696732112185003   (start: 24230, end: 24240)\n",
      "Batch 2424 - batch loss: 2.2226593494415283 - avg loss: 2.0697362982858087   (start: 24240, end: 24250)\n",
      "Batch 2425 - batch loss: 2.7477028369903564 - avg loss: 2.070015756875547   (start: 24250, end: 24260)\n",
      "Batch 2426 - batch loss: 3.023503065109253 - avg loss: 2.070408623504403   (start: 24260, end: 24270)\n",
      "Batch 2427 - batch loss: 2.2774839401245117 - avg loss: 2.070493909878629   (start: 24270, end: 24280)\n",
      "Batch 2428 - batch loss: 1.8573709726333618 - avg loss: 2.0704061688587663   (start: 24280, end: 24290)\n",
      "Batch 2429 - batch loss: 1.467460036277771 - avg loss: 2.0701580428782806   (start: 24290, end: 24300)\n",
      "Batch 2430 - batch loss: 2.4309754371643066 - avg loss: 2.0703064663230712   (start: 24300, end: 24310)\n",
      "Batch 2431 - batch loss: 2.4029860496520996 - avg loss: 2.0704432589149007   (start: 24310, end: 24320)\n",
      "Batch 2432 - batch loss: 2.7973153591156006 - avg loss: 2.070742014402036   (start: 24320, end: 24330)\n",
      "Batch 2433 - batch loss: 1.319509744644165 - avg loss: 2.070433373370911   (start: 24330, end: 24340)\n",
      "Batch 2434 - batch loss: 2.095405101776123 - avg loss: 2.070443628700852   (start: 24340, end: 24350)\n",
      "Batch 2435 - batch loss: 2.6579174995422363 - avg loss: 2.070684792030425   (start: 24350, end: 24360)\n",
      "Batch 2436 - batch loss: 1.80845046043396 - avg loss: 2.0705771866419984   (start: 24360, end: 24370)\n",
      "Batch 2437 - batch loss: 3.0519628524780273 - avg loss: 2.0709797238306105   (start: 24370, end: 24380)\n",
      "Batch 2438 - batch loss: 1.622861623764038 - avg loss: 2.0707959935722804   (start: 24380, end: 24390)\n",
      "Batch 2439 - batch loss: 2.247774600982666 - avg loss: 2.0708685257884323   (start: 24390, end: 24400)\n",
      "Batch 2440 - batch loss: 1.7036759853363037 - avg loss: 2.070718098692794   (start: 24400, end: 24410)\n",
      "Batch 2441 - batch loss: 1.8107808828353882 - avg loss: 2.0706116542964565   (start: 24410, end: 24420)\n",
      "Batch 2442 - batch loss: 1.0893980264663696 - avg loss: 2.0702100113869886   (start: 24420, end: 24430)\n",
      "Batch 2443 - batch loss: 1.3977006673812866 - avg loss: 2.0699348438976246   (start: 24430, end: 24440)\n",
      "Batch 2444 - batch loss: 2.3242135047912598 - avg loss: 2.070038843349933   (start: 24440, end: 24450)\n",
      "Batch 2445 - batch loss: 2.3776769638061523 - avg loss: 2.0701646152716235   (start: 24450, end: 24460)\n",
      "Batch 2446 - batch loss: 2.5408923625946045 - avg loss: 2.070356984600321   (start: 24460, end: 24470)\n",
      "Batch 2447 - batch loss: 1.7436258792877197 - avg loss: 2.07022351601155   (start: 24470, end: 24480)\n",
      "Batch 2448 - batch loss: 1.7045505046844482 - avg loss: 2.0700742007762183   (start: 24480, end: 24490)\n",
      "Batch 2449 - batch loss: 2.112112522125244 - avg loss: 2.070091359274728   (start: 24490, end: 24500)\n",
      "Batch 2450 - batch loss: 2.8720431327819824 - avg loss: 2.0704185529807693   (start: 24500, end: 24510)\n",
      "Batch 2451 - batch loss: 1.9725691080093384 - avg loss: 2.0703786470081056   (start: 24510, end: 24520)\n",
      "Batch 2452 - batch loss: 2.6133501529693604 - avg loss: 2.0705999969901527   (start: 24520, end: 24530)\n",
      "Batch 2453 - batch loss: 2.32112455368042 - avg loss: 2.0707020852365625   (start: 24530, end: 24540)\n",
      "Batch 2454 - batch loss: 2.4675776958465576 - avg loss: 2.070863745363084   (start: 24540, end: 24550)\n",
      "Batch 2455 - batch loss: 1.1763545274734497 - avg loss: 2.0704995315121515   (start: 24550, end: 24560)\n",
      "Batch 2456 - batch loss: 1.447706937789917 - avg loss: 2.07024605467303   (start: 24560, end: 24570)\n",
      "Batch 2457 - batch loss: 2.672847270965576 - avg loss: 2.0704912138334417   (start: 24570, end: 24580)\n",
      "Batch 2458 - batch loss: 1.91940438747406 - avg loss: 2.070429771447773   (start: 24580, end: 24590)\n",
      "Batch 2459 - batch loss: 2.776409149169922 - avg loss: 2.070716754934652   (start: 24590, end: 24600)\n",
      "Batch 2460 - batch loss: 2.218113422393799 - avg loss: 2.0707766479324006   (start: 24600, end: 24610)\n",
      "Batch 2461 - batch loss: 1.525594711303711 - avg loss: 2.0705552092903905   (start: 24610, end: 24620)\n",
      "Batch 2462 - batch loss: 1.854015588760376 - avg loss: 2.070467292270281   (start: 24620, end: 24630)\n",
      "Batch 2463 - batch loss: 1.4828007221221924 - avg loss: 2.0702287912272013   (start: 24630, end: 24640)\n",
      "Batch 2464 - batch loss: 1.8987659215927124 - avg loss: 2.070159232253719   (start: 24640, end: 24650)\n",
      "Batch 2465 - batch loss: 2.468576669692993 - avg loss: 2.0703207965024775   (start: 24650, end: 24660)\n",
      "Batch 2466 - batch loss: 1.5323455333709717 - avg loss: 2.070102727891561   (start: 24660, end: 24670)\n",
      "Batch 2467 - batch loss: 1.9112670421600342 - avg loss: 2.0700383698341334   (start: 24670, end: 24680)\n",
      "Batch 2468 - batch loss: 2.3276076316833496 - avg loss: 2.070142691122853   (start: 24680, end: 24690)\n",
      "Batch 2469 - batch loss: 2.1292521953582764 - avg loss: 2.070166622096228   (start: 24690, end: 24700)\n",
      "Batch 2470 - batch loss: 1.7253602743148804 - avg loss: 2.070027080878995   (start: 24700, end: 24710)\n",
      "Batch 2471 - batch loss: 1.806451439857483 - avg loss: 2.069920456428744   (start: 24710, end: 24720)\n",
      "Batch 2472 - batch loss: 1.9891436100006104 - avg loss: 2.069887792924325   (start: 24720, end: 24730)\n",
      "Batch 2473 - batch loss: 1.1229968070983887 - avg loss: 2.069505056066675   (start: 24730, end: 24740)\n",
      "Batch 2474 - batch loss: 1.7755107879638672 - avg loss: 2.0693862705038053   (start: 24740, end: 24750)\n",
      "Batch 2475 - batch loss: 1.503522515296936 - avg loss: 2.069157731022704   (start: 24750, end: 24760)\n",
      "Batch 2476 - batch loss: 1.7330856323242188 - avg loss: 2.069022053954194   (start: 24760, end: 24770)\n",
      "Batch 2477 - batch loss: 2.1845617294311523 - avg loss: 2.069068680134774   (start: 24770, end: 24780)\n",
      "Batch 2478 - batch loss: 2.537527561187744 - avg loss: 2.0692576510428227   (start: 24780, end: 24790)\n",
      "Batch 2479 - batch loss: 1.5422751903533936 - avg loss: 2.0690451581151255   (start: 24790, end: 24800)\n",
      "Batch 2480 - batch loss: 1.5956555604934692 - avg loss: 2.0688543521507476   (start: 24800, end: 24810)\n",
      "Batch 2481 - batch loss: 1.6828575134277344 - avg loss: 2.0686988336822854   (start: 24810, end: 24820)\n",
      "Batch 2482 - batch loss: 1.2255456447601318 - avg loss: 2.0683592633283094   (start: 24820, end: 24830)\n",
      "Batch 2483 - batch loss: 2.2063283920288086 - avg loss: 2.068414806455806   (start: 24830, end: 24840)\n",
      "Batch 2484 - batch loss: 2.7775826454162598 - avg loss: 2.068700185867862   (start: 24840, end: 24850)\n",
      "Batch 2485 - batch loss: 1.3222880363464355 - avg loss: 2.0683999396291166   (start: 24850, end: 24860)\n",
      "Batch 2486 - batch loss: 1.508838415145874 - avg loss: 2.068174945047499   (start: 24860, end: 24870)\n",
      "Batch 2487 - batch loss: 1.9582713842391968 - avg loss: 2.0681307715905826   (start: 24870, end: 24880)\n",
      "Batch 2488 - batch loss: 1.168140172958374 - avg loss: 2.0677691843673474   (start: 24880, end: 24890)\n",
      "Batch 2489 - batch loss: 2.0235798358917236 - avg loss: 2.0677514376410517   (start: 24890, end: 24900)\n",
      "Batch 2490 - batch loss: 1.7713756561279297 - avg loss: 2.0676324590053583   (start: 24900, end: 24910)\n",
      "Batch 2491 - batch loss: 1.885358214378357 - avg loss: 2.067559315247482   (start: 24910, end: 24920)\n",
      "Batch 2492 - batch loss: 3.0944857597351074 - avg loss: 2.067971239212379   (start: 24920, end: 24930)\n",
      "Batch 2493 - batch loss: 2.6292002201080322 - avg loss: 2.0681962708807413   (start: 24930, end: 24940)\n",
      "Batch 2494 - batch loss: 3.061007022857666 - avg loss: 2.068594191021814   (start: 24940, end: 24950)\n",
      "Batch 2495 - batch loss: 2.6674327850341797 - avg loss: 2.0688341103303127   (start: 24950, end: 24960)\n",
      "Batch 2496 - batch loss: 2.0685269832611084 - avg loss: 2.068833987331887   (start: 24960, end: 24970)\n",
      "Batch 2497 - batch loss: 1.7228114604949951 - avg loss: 2.068695467505291   (start: 24970, end: 24980)\n",
      "Batch 2498 - batch loss: 1.5443555116653442 - avg loss: 2.068485647594991   (start: 24980, end: 24990)\n",
      "Batch 2499 - batch loss: 2.257206916809082 - avg loss: 2.0685611361026766   (start: 24990, end: 25000)\n",
      "Batch 2500 - batch loss: 1.8716561794281006 - avg loss: 2.068482405612203   (start: 25000, end: 25010)\n",
      "Batch 2501 - batch loss: 2.0874195098876953 - avg loss: 2.068489974398884   (start: 25010, end: 25020)\n",
      "Batch 2502 - batch loss: 2.16117525100708 - avg loss: 2.0685270040739168   (start: 25020, end: 25030)\n",
      "Batch 2503 - batch loss: 2.4165782928466797 - avg loss: 2.0686660021924363   (start: 25030, end: 25040)\n",
      "Batch 2504 - batch loss: 1.542946457862854 - avg loss: 2.0684561341108676   (start: 25040, end: 25050)\n",
      "Batch 2505 - batch loss: 1.3801071643829346 - avg loss: 2.0681814537558285   (start: 25050, end: 25060)\n",
      "Batch 2506 - batch loss: 2.558748722076416 - avg loss: 2.0683771327619396   (start: 25060, end: 25070)\n",
      "Batch 2507 - batch loss: 2.913283586502075 - avg loss: 2.068714017312873   (start: 25070, end: 25080)\n",
      "Batch 2508 - batch loss: 2.010871410369873 - avg loss: 2.068690963264669   (start: 25080, end: 25090)\n",
      "Batch 2509 - batch loss: 1.6521079540252686 - avg loss: 2.068524993938279   (start: 25090, end: 25100)\n",
      "Batch 2510 - batch loss: 2.5597379207611084 - avg loss: 2.0687206183615454   (start: 25100, end: 25110)\n",
      "Batch 2511 - batch loss: 1.2206026315689087 - avg loss: 2.0683829917744467   (start: 25110, end: 25120)\n",
      "Batch 2512 - batch loss: 1.624350905418396 - avg loss: 2.0682062977488376   (start: 25120, end: 25130)\n",
      "Batch 2513 - batch loss: 2.2564234733581543 - avg loss: 2.0682811653604563   (start: 25130, end: 25140)\n",
      "Batch 2514 - batch loss: 2.381748676300049 - avg loss: 2.068405804529816   (start: 25140, end: 25150)\n",
      "Batch 2515 - batch loss: 2.848991632461548 - avg loss: 2.0687160532690574   (start: 25150, end: 25160)\n",
      "Batch 2516 - batch loss: 2.777780771255493 - avg loss: 2.068997763526501   (start: 25160, end: 25170)\n",
      "Batch 2517 - batch loss: 2.241457462310791 - avg loss: 2.069066254272643   (start: 25170, end: 25180)\n",
      "Batch 2518 - batch loss: 2.3434386253356934 - avg loss: 2.0691751754203453   (start: 25180, end: 25190)\n",
      "Batch 2519 - batch loss: 2.285130262374878 - avg loss: 2.0692608718834227   (start: 25190, end: 25200)\n",
      "Batch 2520 - batch loss: 1.9502496719360352 - avg loss: 2.0692136639500838   (start: 25200, end: 25210)\n",
      "Batch 2521 - batch loss: 1.5082279443740845 - avg loss: 2.068991227106477   (start: 25210, end: 25220)\n",
      "Batch 2522 - batch loss: 1.8736587762832642 - avg loss: 2.068913806396678   (start: 25220, end: 25230)\n",
      "Batch 2523 - batch loss: 2.103269338607788 - avg loss: 2.0689274179387582   (start: 25230, end: 25240)\n",
      "Batch 2524 - batch loss: 1.719117522239685 - avg loss: 2.0687888793662044   (start: 25240, end: 25250)\n",
      "Batch 2525 - batch loss: 1.9404083490371704 - avg loss: 2.0687380557199933   (start: 25250, end: 25260)\n",
      "Batch 2526 - batch loss: 2.0186736583709717 - avg loss: 2.068718243928403   (start: 25260, end: 25270)\n",
      "Batch 2527 - batch loss: 2.380540370941162 - avg loss: 2.0688415912887717   (start: 25270, end: 25280)\n",
      "Batch 2528 - batch loss: 2.4632856845855713 - avg loss: 2.068997559692606   (start: 25280, end: 25290)\n",
      "Batch 2529 - batch loss: 2.4639103412628174 - avg loss: 2.0691536517011317   (start: 25290, end: 25300)\n",
      "Batch 2530 - batch loss: 2.1614437103271484 - avg loss: 2.0691901155725763   (start: 25300, end: 25310)\n",
      "Batch 2531 - batch loss: 2.468513250350952 - avg loss: 2.0693478261313354   (start: 25310, end: 25320)\n",
      "Batch 2532 - batch loss: 1.4047486782073975 - avg loss: 2.0690854498392217   (start: 25320, end: 25330)\n",
      "Batch 2533 - batch loss: 2.48176908493042 - avg loss: 2.069248308416606   (start: 25330, end: 25340)\n",
      "Batch 2534 - batch loss: 2.2199182510375977 - avg loss: 2.069307744291407   (start: 25340, end: 25350)\n",
      "Batch 2535 - batch loss: 1.8132545948028564 - avg loss: 2.0692067769611673   (start: 25350, end: 25360)\n",
      "Batch 2536 - batch loss: 2.2790727615356445 - avg loss: 2.0692894990678186   (start: 25360, end: 25370)\n",
      "Batch 2537 - batch loss: 3.3223795890808105 - avg loss: 2.0697832303877606   (start: 25370, end: 25380)\n",
      "Batch 2538 - batch loss: 2.323730945587158 - avg loss: 2.069883249180671   (start: 25380, end: 25390)\n",
      "Batch 2539 - batch loss: 1.7536062002182007 - avg loss: 2.0697587306574574   (start: 25390, end: 25400)\n",
      "Batch 2540 - batch loss: 2.7704830169677734 - avg loss: 2.0700344977909917   (start: 25400, end: 25410)\n",
      "Batch 2541 - batch loss: 2.1593942642211914 - avg loss: 2.0700696511216092   (start: 25410, end: 25420)\n",
      "Batch 2542 - batch loss: 1.8657286167144775 - avg loss: 2.069989296802141   (start: 25420, end: 25430)\n",
      "Batch 2543 - batch loss: 2.9245312213897705 - avg loss: 2.0703252016467117   (start: 25430, end: 25440)\n",
      "Batch 2544 - batch loss: 1.6017940044403076 - avg loss: 2.07014110294447   (start: 25440, end: 25450)\n",
      "Batch 2545 - batch loss: 2.5732901096343994 - avg loss: 2.070338726277812   (start: 25450, end: 25460)\n",
      "Batch 2546 - batch loss: 1.0053255558013916 - avg loss: 2.069920582119792   (start: 25460, end: 25470)\n",
      "Batch 2547 - batch loss: 1.9346723556518555 - avg loss: 2.069867501968117   (start: 25470, end: 25480)\n",
      "Batch 2548 - batch loss: 1.132962942123413 - avg loss: 2.0694999442749653   (start: 25480, end: 25490)\n",
      "Batch 2549 - batch loss: 2.174298048019409 - avg loss: 2.069541041570551   (start: 25490, end: 25500)\n",
      "Batch 2550 - batch loss: 1.574838638305664 - avg loss: 2.069347116677072   (start: 25500, end: 25510)\n",
      "Batch 2551 - batch loss: 2.140047788619995 - avg loss: 2.0693748207021283   (start: 25510, end: 25520)\n",
      "Batch 2552 - batch loss: 2.4097414016723633 - avg loss: 2.0695081409453597   (start: 25520, end: 25530)\n",
      "Batch 2553 - batch loss: 1.9261150360107422 - avg loss: 2.069451996425025   (start: 25530, end: 25540)\n",
      "Batch 2554 - batch loss: 2.2671093940734863 - avg loss: 2.0695293574417173   (start: 25540, end: 25550)\n",
      "Batch 2555 - batch loss: 2.3274588584899902 - avg loss: 2.069630268827104   (start: 25550, end: 25560)\n",
      "Batch 2556 - batch loss: 1.9177640676498413 - avg loss: 2.0695708764918765   (start: 25560, end: 25570)\n",
      "Batch 2557 - batch loss: 1.5144730806350708 - avg loss: 2.069353871880517   (start: 25570, end: 25580)\n",
      "Batch 2558 - batch loss: 2.336019992828369 - avg loss: 2.069458079039934   (start: 25580, end: 25590)\n",
      "Batch 2559 - batch loss: 1.9399254322052002 - avg loss: 2.0694074803497644   (start: 25590, end: 25600)\n",
      "Batch 2560 - batch loss: 2.262765407562256 - avg loss: 2.069482981297524   (start: 25600, end: 25610)\n",
      "Batch 2561 - batch loss: 1.7875888347625732 - avg loss: 2.0693729523566438   (start: 25610, end: 25620)\n",
      "Batch 2562 - batch loss: 2.6980624198913574 - avg loss: 2.069618246725561   (start: 25620, end: 25630)\n",
      "Batch 2563 - batch loss: 2.0256550312042236 - avg loss: 2.069601100385654   (start: 25630, end: 25640)\n",
      "Batch 2564 - batch loss: 2.4542932510375977 - avg loss: 2.069751077832302   (start: 25640, end: 25650)\n",
      "Batch 2565 - batch loss: 1.8121769428253174 - avg loss: 2.0696506982005767   (start: 25650, end: 25660)\n",
      "Batch 2566 - batch loss: 2.584807872772217 - avg loss: 2.069851382725147   (start: 25660, end: 25670)\n",
      "Batch 2567 - batch loss: 1.4214439392089844 - avg loss: 2.0695988876147435   (start: 25670, end: 25680)\n",
      "Batch 2568 - batch loss: 2.1116955280303955 - avg loss: 2.0696152740064973   (start: 25680, end: 25690)\n",
      "Batch 2569 - batch loss: 2.2758684158325195 - avg loss: 2.0696955281472857   (start: 25690, end: 25700)\n",
      "Batch 2570 - batch loss: 2.3752083778381348 - avg loss: 2.0698143585050026   (start: 25700, end: 25710)\n",
      "Batch 2571 - batch loss: 1.8271524906158447 - avg loss: 2.069720010966943   (start: 25710, end: 25720)\n",
      "Batch 2572 - batch loss: 2.4938340187072754 - avg loss: 2.0698848434612067   (start: 25720, end: 25730)\n",
      "Batch 2573 - batch loss: 2.6766018867492676 - avg loss: 2.070120553268234   (start: 25730, end: 25740)\n",
      "Batch 2574 - batch loss: 2.304168224334717 - avg loss: 2.0702114455676774   (start: 25740, end: 25750)\n",
      "Batch 2575 - batch loss: 1.7119802236557007 - avg loss: 2.0700723806523387   (start: 25750, end: 25760)\n",
      "Batch 2576 - batch loss: 2.018275737762451 - avg loss: 2.0700522810625483   (start: 25760, end: 25770)\n",
      "Batch 2577 - batch loss: 1.0143240690231323 - avg loss: 2.0696427666280877   (start: 25770, end: 25780)\n",
      "Batch 2578 - batch loss: 2.457185745239258 - avg loss: 2.0697930353285963   (start: 25780, end: 25790)\n",
      "Batch 2579 - batch loss: 1.4366883039474487 - avg loss: 2.069547645897828   (start: 25790, end: 25800)\n",
      "Batch 2580 - batch loss: 2.052323818206787 - avg loss: 2.069540972582179   (start: 25800, end: 25810)\n",
      "Batch 2581 - batch loss: 2.38202166557312 - avg loss: 2.069661995313779   (start: 25810, end: 25820)\n",
      "Batch 2582 - batch loss: 2.442268133163452 - avg loss: 2.0698062485611075   (start: 25820, end: 25830)\n",
      "Batch 2583 - batch loss: 2.63716721534729 - avg loss: 2.070025815498718   (start: 25830, end: 25840)\n",
      "Batch 2584 - batch loss: 3.1728968620300293 - avg loss: 2.07045245806991   (start: 25840, end: 25850)\n",
      "Batch 2585 - batch loss: 2.414783000946045 - avg loss: 2.070585609865299   (start: 25850, end: 25860)\n",
      "Batch 2586 - batch loss: 2.911184310913086 - avg loss: 2.0709105417172697   (start: 25860, end: 25870)\n",
      "Batch 2587 - batch loss: 2.2693028450012207 - avg loss: 2.0709872002579512   (start: 25870, end: 25880)\n",
      "Batch 2588 - batch loss: 1.9585727453231812 - avg loss: 2.070943780229008   (start: 25880, end: 25890)\n",
      "Batch 2589 - batch loss: 2.408973217010498 - avg loss: 2.0710742935250623   (start: 25890, end: 25900)\n",
      "Batch 2590 - batch loss: 1.4763323068618774 - avg loss: 2.0708447520404376   (start: 25900, end: 25910)\n",
      "Batch 2591 - batch loss: 2.7307522296905518 - avg loss: 2.0710993459747162   (start: 25910, end: 25920)\n",
      "Batch 2592 - batch loss: 1.945145606994629 - avg loss: 2.0710507714513917   (start: 25920, end: 25930)\n",
      "Batch 2593 - batch loss: 2.8797457218170166 - avg loss: 2.0713625274075853   (start: 25930, end: 25940)\n",
      "Batch 2594 - batch loss: 2.48356032371521 - avg loss: 2.071521370489014   (start: 25940, end: 25950)\n",
      "Batch 2595 - batch loss: 1.659788727760315 - avg loss: 2.0713627677760984   (start: 25950, end: 25960)\n",
      "Batch 2596 - batch loss: 1.4397459030151367 - avg loss: 2.071119557585586   (start: 25960, end: 25970)\n",
      "Batch 2597 - batch loss: 2.281691074371338 - avg loss: 2.0712006089777284   (start: 25970, end: 25980)\n",
      "Batch 2598 - batch loss: 2.261852979660034 - avg loss: 2.071273965026471   (start: 25980, end: 25990)\n",
      "Batch 2599 - batch loss: 2.934530735015869 - avg loss: 2.071605986861082   (start: 25990, end: 26000)\n",
      "Batch 2600 - batch loss: 2.176825761795044 - avg loss: 2.0716464404462163   (start: 26000, end: 26010)\n",
      "Batch 2601 - batch loss: 1.8780485391616821 - avg loss: 2.071572036948413   (start: 26010, end: 26020)\n",
      "Batch 2602 - batch loss: 1.3846380710601807 - avg loss: 2.071308136077922   (start: 26020, end: 26030)\n",
      "Batch 2603 - batch loss: 2.62532377243042 - avg loss: 2.071520891698641   (start: 26030, end: 26040)\n",
      "Batch 2604 - batch loss: 1.9945166110992432 - avg loss: 2.07149133151415   (start: 26040, end: 26050)\n",
      "Batch 2605 - batch loss: 2.6567392349243164 - avg loss: 2.071715908606786   (start: 26050, end: 26060)\n",
      "Batch 2606 - batch loss: 1.6507503986358643 - avg loss: 2.0715544335358347   (start: 26060, end: 26070)\n",
      "Batch 2607 - batch loss: 1.5832700729370117 - avg loss: 2.0713672079374454   (start: 26070, end: 26080)\n",
      "Batch 2608 - batch loss: 1.4290635585784912 - avg loss: 2.07112102026042   (start: 26080, end: 26090)\n",
      "Batch 2609 - batch loss: 2.75569486618042 - avg loss: 2.0713833090902747   (start: 26090, end: 26100)\n",
      "Batch 2610 - batch loss: 2.7671868801116943 - avg loss: 2.0716497983936146   (start: 26100, end: 26110)\n",
      "Batch 2611 - batch loss: 1.9011528491973877 - avg loss: 2.0715845239107678   (start: 26110, end: 26120)\n",
      "Batch 2612 - batch loss: 1.702528953552246 - avg loss: 2.0714432856519243   (start: 26120, end: 26130)\n",
      "Batch 2613 - batch loss: 2.609635829925537 - avg loss: 2.071649174153942   (start: 26130, end: 26140)\n",
      "Batch 2614 - batch loss: 2.4019789695739746 - avg loss: 2.0717754952994176   (start: 26140, end: 26150)\n",
      "Batch 2615 - batch loss: 1.6600961685180664 - avg loss: 2.071618125526183   (start: 26150, end: 26160)\n",
      "Batch 2616 - batch loss: 2.416516065597534 - avg loss: 2.071749916867441   (start: 26160, end: 26170)\n",
      "Batch 2617 - batch loss: 2.6928086280822754 - avg loss: 2.0719871432659187   (start: 26170, end: 26180)\n",
      "Batch 2618 - batch loss: 2.4546103477478027 - avg loss: 2.0721332384184508   (start: 26180, end: 26190)\n",
      "Batch 2619 - batch loss: 1.9238731861114502 - avg loss: 2.072076650612227   (start: 26190, end: 26200)\n",
      "Batch 2620 - batch loss: 2.074195384979248 - avg loss: 2.0720774589809285   (start: 26200, end: 26210)\n",
      "Batch 2621 - batch loss: 1.48415207862854 - avg loss: 2.071853231147079   (start: 26210, end: 26220)\n",
      "Batch 2622 - batch loss: 3.1605465412139893 - avg loss: 2.072268287689232   (start: 26220, end: 26230)\n",
      "Batch 2623 - batch loss: 1.8692147731781006 - avg loss: 2.0721909044901046   (start: 26230, end: 26240)\n",
      "Batch 2624 - batch loss: 1.9479668140411377 - avg loss: 2.0721435810270763   (start: 26240, end: 26250)\n",
      "Batch 2625 - batch loss: 2.150681495666504 - avg loss: 2.072173488839201   (start: 26250, end: 26260)\n",
      "Batch 2626 - batch loss: 2.1660878658294678 - avg loss: 2.072209238506879   (start: 26260, end: 26270)\n",
      "Batch 2627 - batch loss: 2.9439547061920166 - avg loss: 2.072540952916196   (start: 26270, end: 26280)\n",
      "Batch 2628 - batch loss: 2.185692310333252 - avg loss: 2.0725839926109155   (start: 26280, end: 26290)\n",
      "Batch 2629 - batch loss: 1.5003985166549683 - avg loss: 2.0723664315934416   (start: 26290, end: 26300)\n",
      "Batch 2630 - batch loss: 2.6849963665008545 - avg loss: 2.072599282195839   (start: 26300, end: 26310)\n",
      "Batch 2631 - batch loss: 2.0226709842681885 - avg loss: 2.0725803124777813   (start: 26310, end: 26320)\n",
      "Batch 2632 - batch loss: 1.2050755023956299 - avg loss: 2.0722508385658625   (start: 26320, end: 26330)\n",
      "Batch 2633 - batch loss: 2.4828860759735107 - avg loss: 2.0724067365299508   (start: 26330, end: 26340)\n",
      "Batch 2634 - batch loss: 1.4035584926605225 - avg loss: 2.072152904179336   (start: 26340, end: 26350)\n",
      "Batch 2635 - batch loss: 2.3519513607025146 - avg loss: 2.0722590492690642   (start: 26350, end: 26360)\n",
      "Batch 2636 - batch loss: 1.4814354181289673 - avg loss: 2.0720349978351846   (start: 26360, end: 26370)\n",
      "Batch 2637 - batch loss: 1.7198143005371094 - avg loss: 2.0719014797543287   (start: 26370, end: 26380)\n",
      "Batch 2638 - batch loss: 1.7291933298110962 - avg loss: 2.071771616870682   (start: 26380, end: 26390)\n",
      "Batch 2639 - batch loss: 1.882075548171997 - avg loss: 2.0716997622992053   (start: 26390, end: 26400)\n",
      "Batch 2640 - batch loss: 1.0507131814956665 - avg loss: 2.0713131713939408   (start: 26400, end: 26410)\n",
      "Batch 2641 - batch loss: 1.524285912513733 - avg loss: 2.0711061209553034   (start: 26410, end: 26420)\n",
      "Batch 2642 - batch loss: 2.528517723083496 - avg loss: 2.0712791862606865   (start: 26420, end: 26430)\n",
      "Batch 2643 - batch loss: 1.760650873184204 - avg loss: 2.0711617020272994   (start: 26430, end: 26440)\n",
      "Batch 2644 - batch loss: 2.9209132194519043 - avg loss: 2.0714829691416377   (start: 26440, end: 26450)\n",
      "Batch 2645 - batch loss: 2.064018964767456 - avg loss: 2.0714801482783063   (start: 26450, end: 26460)\n",
      "Batch 2646 - batch loss: 2.4253385066986084 - avg loss: 2.071613831073327   (start: 26460, end: 26470)\n",
      "Batch 2647 - batch loss: 1.8415565490722656 - avg loss: 2.0715269514351093   (start: 26470, end: 26480)\n",
      "Batch 2648 - batch loss: 2.0869522094726562 - avg loss: 2.071532774484576   (start: 26480, end: 26490)\n",
      "Batch 2649 - batch loss: 1.7135953903198242 - avg loss: 2.0713977037735707   (start: 26490, end: 26500)\n",
      "Batch 2650 - batch loss: 2.7319750785827637 - avg loss: 2.071646884224272   (start: 26500, end: 26510)\n",
      "Batch 2651 - batch loss: 2.180497646331787 - avg loss: 2.0716879290063637   (start: 26510, end: 26520)\n",
      "Batch 2652 - batch loss: 1.2544256448745728 - avg loss: 2.0713798768826805   (start: 26520, end: 26530)\n",
      "Batch 2653 - batch loss: 1.7088184356689453 - avg loss: 2.0712432674474077   (start: 26530, end: 26540)\n",
      "Batch 2654 - batch loss: 2.0490758419036865 - avg loss: 2.0712349181345853   (start: 26540, end: 26550)\n",
      "Batch 2655 - batch loss: 2.3787567615509033 - avg loss: 2.0713507019611725   (start: 26550, end: 26560)\n",
      "Batch 2656 - batch loss: 2.126500129699707 - avg loss: 2.0713714582380782   (start: 26560, end: 26570)\n",
      "Batch 2657 - batch loss: 1.83431875705719 - avg loss: 2.0712822736251435   (start: 26570, end: 26580)\n",
      "Batch 2658 - batch loss: 2.191019058227539 - avg loss: 2.0713273043827978   (start: 26580, end: 26590)\n",
      "Batch 2659 - batch loss: 2.1764943599700928 - avg loss: 2.0713668408698607   (start: 26590, end: 26600)\n",
      "Batch 2660 - batch loss: 1.7884365320205688 - avg loss: 2.071260516063829   (start: 26600, end: 26610)\n",
      "Batch 2661 - batch loss: 1.4310104846954346 - avg loss: 2.071020001401407   (start: 26610, end: 26620)\n",
      "Batch 2662 - batch loss: 1.1972105503082275 - avg loss: 2.0706918716788785   (start: 26620, end: 26630)\n",
      "Batch 2663 - batch loss: 2.3744988441467285 - avg loss: 2.07080591333521   (start: 26630, end: 26640)\n",
      "Batch 2664 - batch loss: 1.3139410018920898 - avg loss: 2.0705219114922673   (start: 26640, end: 26650)\n",
      "Batch 2665 - batch loss: 1.4360440969467163 - avg loss: 2.0702839228146432   (start: 26650, end: 26660)\n",
      "Batch 2666 - batch loss: 3.1852245330810547 - avg loss: 2.070701973287184   (start: 26660, end: 26670)\n",
      "Batch 2667 - batch loss: 1.8659932613372803 - avg loss: 2.0706252458839045   (start: 26670, end: 26680)\n",
      "Batch 2668 - batch loss: 1.647670030593872 - avg loss: 2.0704667763390225   (start: 26680, end: 26690)\n",
      "Batch 2669 - batch loss: 1.9167578220367432 - avg loss: 2.07040920744228   (start: 26690, end: 26700)\n",
      "Batch 2670 - batch loss: 1.4432123899459839 - avg loss: 2.0701743902137153   (start: 26700, end: 26710)\n",
      "Batch 2671 - batch loss: 1.5717989206314087 - avg loss: 2.069987872448153   (start: 26710, end: 26720)\n",
      "Batch 2672 - batch loss: 1.7400686740875244 - avg loss: 2.069864445886851   (start: 26720, end: 26730)\n",
      "Batch 2673 - batch loss: 2.50370717048645 - avg loss: 2.0700266907352427   (start: 26730, end: 26740)\n",
      "Batch 2674 - batch loss: 2.0669422149658203 - avg loss: 2.0700255376601886   (start: 26740, end: 26750)\n",
      "Batch 2675 - batch loss: 3.405540943145752 - avg loss: 2.0705246091869025   (start: 26750, end: 26760)\n",
      "Batch 2676 - batch loss: 1.322335124015808 - avg loss: 2.070245121146121   (start: 26760, end: 26770)\n",
      "Batch 2677 - batch loss: 1.903338074684143 - avg loss: 2.0701827958860535   (start: 26770, end: 26780)\n",
      "Batch 2678 - batch loss: 1.2441147565841675 - avg loss: 2.0698744464872845   (start: 26780, end: 26790)\n",
      "Batch 2679 - batch loss: 2.3023715019226074 - avg loss: 2.0699611991199096   (start: 26790, end: 26800)\n",
      "Batch 2680 - batch loss: 2.600468873977661 - avg loss: 2.070159075910233   (start: 26800, end: 26810)\n",
      "Batch 2681 - batch loss: 2.56054949760437 - avg loss: 2.070341920959336   (start: 26810, end: 26820)\n",
      "Batch 2682 - batch loss: 2.070923089981079 - avg loss: 2.070342137570973   (start: 26820, end: 26830)\n",
      "Batch 2683 - batch loss: 3.337958812713623 - avg loss: 2.070814423962606   (start: 26830, end: 26840)\n",
      "Batch 2684 - batch loss: 3.2970569133758545 - avg loss: 2.0712711250759814   (start: 26840, end: 26850)\n",
      "Batch 2685 - batch loss: 2.9585883617401123 - avg loss: 2.071601474009959   (start: 26850, end: 26860)\n",
      "Batch 2686 - batch loss: 1.2341023683547974 - avg loss: 2.0712897884477504   (start: 26860, end: 26870)\n",
      "Batch 2687 - batch loss: 1.9314159154891968 - avg loss: 2.071237752036679   (start: 26870, end: 26880)\n",
      "Batch 2688 - batch loss: 2.1912262439727783 - avg loss: 2.071282374012111   (start: 26880, end: 26890)\n",
      "Batch 2689 - batch loss: 2.418973207473755 - avg loss: 2.0714116271100522   (start: 26890, end: 26900)\n",
      "Batch 2690 - batch loss: 2.7480721473693848 - avg loss: 2.0716630802948384   (start: 26900, end: 26910)\n",
      "Batch 2691 - batch loss: 1.6612485647201538 - avg loss: 2.0715106231939564   (start: 26910, end: 26920)\n",
      "Batch 2692 - batch loss: 1.944451928138733 - avg loss: 2.0714634420966465   (start: 26920, end: 26930)\n",
      "Batch 2693 - batch loss: 2.058915138244629 - avg loss: 2.0714587842258774   (start: 26930, end: 26940)\n",
      "Batch 2694 - batch loss: 2.349785327911377 - avg loss: 2.071562059381234   (start: 26940, end: 26950)\n",
      "Batch 2695 - batch loss: 1.390521764755249 - avg loss: 2.071309447995987   (start: 26950, end: 26960)\n",
      "Batch 2696 - batch loss: 1.8611738681793213 - avg loss: 2.071231533431724   (start: 26960, end: 26970)\n",
      "Batch 2697 - batch loss: 2.3018288612365723 - avg loss: 2.0713170031603396   (start: 26970, end: 26980)\n",
      "Batch 2698 - batch loss: 1.6721982955932617 - avg loss: 2.0711691266477175   (start: 26980, end: 26990)\n",
      "Batch 2699 - batch loss: 2.372422695159912 - avg loss: 2.0712807020434627   (start: 26990, end: 27000)\n",
      "Batch 2700 - batch loss: 1.9948753118515015 - avg loss: 2.071252414227768   (start: 27000, end: 27010)\n",
      "Batch 2701 - batch loss: 1.4906243085861206 - avg loss: 2.0710375259577303   (start: 27010, end: 27020)\n",
      "Batch 2702 - batch loss: 1.458635687828064 - avg loss: 2.070810962199636   (start: 27020, end: 27030)\n",
      "Batch 2703 - batch loss: 1.6947587728500366 - avg loss: 2.0706718896444025   (start: 27030, end: 27040)\n",
      "Batch 2704 - batch loss: 2.0213563442230225 - avg loss: 2.0706536583891637   (start: 27040, end: 27050)\n",
      "Batch 2705 - batch loss: 2.0434398651123047 - avg loss: 2.070643601554989   (start: 27050, end: 27060)\n",
      "Batch 2706 - batch loss: 1.8922303915023804 - avg loss: 2.070577693461139   (start: 27060, end: 27070)\n",
      "Batch 2707 - batch loss: 2.344090700149536 - avg loss: 2.0706786953099896   (start: 27070, end: 27080)\n",
      "Batch 2708 - batch loss: 1.5848369598388672 - avg loss: 2.070499351738387   (start: 27080, end: 27090)\n",
      "Batch 2709 - batch loss: 1.7745025157928467 - avg loss: 2.0703901278136843   (start: 27090, end: 27100)\n",
      "Batch 2710 - batch loss: 1.8731228113174438 - avg loss: 2.0703173622967177   (start: 27100, end: 27110)\n",
      "Batch 2711 - batch loss: 2.3897154331207275 - avg loss: 2.0704351344467264   (start: 27110, end: 27120)\n",
      "Batch 2712 - batch loss: 1.9430363178253174 - avg loss: 2.070388175797032   (start: 27120, end: 27130)\n",
      "Batch 2713 - batch loss: 1.5620299577713013 - avg loss: 2.0702008662104343   (start: 27130, end: 27140)\n",
      "Batch 2714 - batch loss: 2.3716773986816406 - avg loss: 2.070311907290534   (start: 27140, end: 27150)\n",
      "Batch 2715 - batch loss: 1.9246318340301514 - avg loss: 2.070258269561057   (start: 27150, end: 27160)\n",
      "Batch 2716 - batch loss: 1.3399772644042969 - avg loss: 2.0699894874465348   (start: 27160, end: 27170)\n",
      "Batch 2717 - batch loss: 2.344985246658325 - avg loss: 2.0700906632225506   (start: 27170, end: 27180)\n",
      "Batch 2718 - batch loss: 2.038015842437744 - avg loss: 2.07007886667206   (start: 27180, end: 27190)\n",
      "Batch 2719 - batch loss: 2.0755553245544434 - avg loss: 2.070080880075693   (start: 27190, end: 27200)\n",
      "Batch 2720 - batch loss: 2.475222110748291 - avg loss: 2.070229774317028   (start: 27200, end: 27210)\n",
      "Batch 2721 - batch loss: 2.0336809158325195 - avg loss: 2.0702163471096497   (start: 27210, end: 27220)\n",
      "Batch 2722 - batch loss: 1.9237661361694336 - avg loss: 2.070162564439455   (start: 27220, end: 27230)\n",
      "Batch 2723 - batch loss: 2.3242812156677246 - avg loss: 2.070255853224781   (start: 27230, end: 27240)\n",
      "Batch 2724 - batch loss: 1.7120258808135986 - avg loss: 2.0701243926844466   (start: 27240, end: 27250)\n",
      "Batch 2725 - batch loss: 2.2813804149627686 - avg loss: 2.0702018893910785   (start: 27250, end: 27260)\n",
      "Batch 2726 - batch loss: 1.926576018333435 - avg loss: 2.0701492213048818   (start: 27260, end: 27270)\n",
      "Batch 2727 - batch loss: 2.6134305000305176 - avg loss: 2.0703483713337403   (start: 27270, end: 27280)\n",
      "Batch 2728 - batch loss: 2.1521058082580566 - avg loss: 2.0703783300867356   (start: 27280, end: 27290)\n",
      "Batch 2729 - batch loss: 2.6198184490203857 - avg loss: 2.070579590203561   (start: 27290, end: 27300)\n",
      "Batch 2730 - batch loss: 2.756014108657837 - avg loss: 2.0708305731835885   (start: 27300, end: 27310)\n",
      "Batch 2731 - batch loss: 1.903465986251831 - avg loss: 2.0707693123538182   (start: 27310, end: 27320)\n",
      "Batch 2732 - batch loss: 2.4608209133148193 - avg loss: 2.0709120315638296   (start: 27320, end: 27330)\n",
      "Batch 2733 - batch loss: 2.379451036453247 - avg loss: 2.071024884162546   (start: 27330, end: 27340)\n",
      "Batch 2734 - batch loss: 2.4729809761047363 - avg loss: 2.0711718516550293   (start: 27340, end: 27350)\n",
      "Batch 2735 - batch loss: 2.052867889404297 - avg loss: 2.0711651616103466   (start: 27350, end: 27360)\n",
      "Batch 2736 - batch loss: 1.9456628561019897 - avg loss: 2.07111930764414   (start: 27360, end: 27370)\n",
      "Batch 2737 - batch loss: 1.237034797668457 - avg loss: 2.0708146748793568   (start: 27370, end: 27380)\n",
      "Batch 2738 - batch loss: 2.0519020557403564 - avg loss: 2.0708077699435634   (start: 27380, end: 27390)\n",
      "Batch 2739 - batch loss: 2.8148884773254395 - avg loss: 2.071079332245527   (start: 27390, end: 27400)\n",
      "Batch 2740 - batch loss: 3.076592445373535 - avg loss: 2.0714461739504264   (start: 27400, end: 27410)\n",
      "Batch 2741 - batch loss: 1.0732179880142212 - avg loss: 2.071082122824994   (start: 27410, end: 27420)\n",
      "Batch 2742 - batch loss: 1.1388475894927979 - avg loss: 2.0707422633523973   (start: 27420, end: 27430)\n",
      "Batch 2743 - batch loss: 2.3450846672058105 - avg loss: 2.070842242362548   (start: 27430, end: 27440)\n",
      "Batch 2744 - batch loss: 2.485488176345825 - avg loss: 2.0709932973476057   (start: 27440, end: 27450)\n",
      "Batch 2745 - batch loss: 1.846131682395935 - avg loss: 2.070911410379306   (start: 27450, end: 27460)\n",
      "Batch 2746 - batch loss: 2.178732395172119 - avg loss: 2.0709506608288115   (start: 27460, end: 27470)\n",
      "Batch 2747 - batch loss: 1.9004024267196655 - avg loss: 2.070888598152644   (start: 27470, end: 27480)\n",
      "Batch 2748 - batch loss: 1.7761646509170532 - avg loss: 2.070781386822256   (start: 27480, end: 27490)\n",
      "Batch 2749 - batch loss: 1.8834247589111328 - avg loss: 2.070713257139379   (start: 27490, end: 27500)\n",
      "Batch 2750 - batch loss: 1.4425183534622192 - avg loss: 2.0704849056658508   (start: 27500, end: 27510)\n",
      "Batch 2751 - batch loss: 2.3093502521514893 - avg loss: 2.070571702666754   (start: 27510, end: 27520)\n",
      "Batch 2752 - batch loss: 2.0810513496398926 - avg loss: 2.070575509294786   (start: 27520, end: 27530)\n",
      "Batch 2753 - batch loss: 1.3874621391296387 - avg loss: 2.0703274652242833   (start: 27530, end: 27540)\n",
      "Batch 2754 - batch loss: 1.3603713512420654 - avg loss: 2.0700697679052333   (start: 27540, end: 27550)\n",
      "Batch 2755 - batch loss: 2.030557155609131 - avg loss: 2.070055430963181   (start: 27550, end: 27560)\n",
      "Batch 2756 - batch loss: 2.1202938556671143 - avg loss: 2.0700736530976402   (start: 27560, end: 27570)\n",
      "Batch 2757 - batch loss: 2.527987003326416 - avg loss: 2.0702396840440613   (start: 27570, end: 27580)\n",
      "Batch 2758 - batch loss: 1.8353378772735596 - avg loss: 2.0701545438458844   (start: 27580, end: 27590)\n",
      "Batch 2759 - batch loss: 2.7824835777282715 - avg loss: 2.0704126340755518   (start: 27590, end: 27600)\n",
      "Batch 2760 - batch loss: 1.2885410785675049 - avg loss: 2.070129449883046   (start: 27600, end: 27610)\n",
      "Batch 2761 - batch loss: 2.19215726852417 - avg loss: 2.0701736308456242   (start: 27610, end: 27620)\n",
      "Batch 2762 - batch loss: 3.074673891067505 - avg loss: 2.0705371850476593   (start: 27620, end: 27630)\n",
      "Batch 2763 - batch loss: 1.4546115398406982 - avg loss: 2.0703143465363687   (start: 27630, end: 27640)\n",
      "Batch 2764 - batch loss: 2.2294957637786865 - avg loss: 2.070371916669187   (start: 27640, end: 27650)\n",
      "Batch 2765 - batch loss: 1.4518356323242188 - avg loss: 2.0701482954528654   (start: 27650, end: 27660)\n",
      "Batch 2766 - batch loss: 2.0803475379943848 - avg loss: 2.070151981481973   (start: 27660, end: 27670)\n",
      "Batch 2767 - batch loss: 1.6938104629516602 - avg loss: 2.0700160199507125   (start: 27670, end: 27680)\n",
      "Batch 2768 - batch loss: 2.112288236618042 - avg loss: 2.0700312861900287   (start: 27680, end: 27690)\n",
      "Batch 2769 - batch loss: 1.9084234237670898 - avg loss: 2.0699729440014285   (start: 27690, end: 27700)\n",
      "Batch 2770 - batch loss: 1.294350266456604 - avg loss: 2.069693036864097   (start: 27700, end: 27710)\n",
      "Batch 2771 - batch loss: 1.6367969512939453 - avg loss: 2.06953686944506   (start: 27710, end: 27720)\n",
      "Batch 2772 - batch loss: 2.188491106033325 - avg loss: 2.069579766753603   (start: 27720, end: 27730)\n",
      "Batch 2773 - batch loss: 1.7432539463043213 - avg loss: 2.0694621294715376   (start: 27730, end: 27740)\n",
      "Batch 2774 - batch loss: 2.1309497356414795 - avg loss: 2.069484287167455   (start: 27740, end: 27750)\n",
      "Batch 2775 - batch loss: 2.2558021545410156 - avg loss: 2.069551404554837   (start: 27750, end: 27760)\n",
      "Batch 2776 - batch loss: 1.8282508850097656 - avg loss: 2.069464512037896   (start: 27760, end: 27770)\n",
      "Batch 2777 - batch loss: 2.087195873260498 - avg loss: 2.069470894817314   (start: 27770, end: 27780)\n",
      "Batch 2778 - batch loss: 1.8776893615722656 - avg loss: 2.0694018838301798   (start: 27780, end: 27790)\n",
      "Batch 2779 - batch loss: 2.756096601486206 - avg loss: 2.0696488963185455   (start: 27790, end: 27800)\n",
      "Batch 2780 - batch loss: 1.8008146286010742 - avg loss: 2.0695522281172805   (start: 27800, end: 27810)\n",
      "Batch 2781 - batch loss: 2.6657936573028564 - avg loss: 2.069766549263645   (start: 27810, end: 27820)\n",
      "Batch 2782 - batch loss: 1.6918227672576904 - avg loss: 2.069630744814487   (start: 27820, end: 27830)\n",
      "Batch 2783 - batch loss: 2.134446859359741 - avg loss: 2.0696540264648267   (start: 27830, end: 27840)\n",
      "Batch 2784 - batch loss: 2.002828598022461 - avg loss: 2.069630031696984   (start: 27840, end: 27850)\n",
      "Batch 2785 - batch loss: 1.422327995300293 - avg loss: 2.0693976906932523   (start: 27850, end: 27860)\n",
      "Batch 2786 - batch loss: 2.5390353202819824 - avg loss: 2.0695662007863946   (start: 27860, end: 27870)\n",
      "Batch 2787 - batch loss: 3.6706595420837402 - avg loss: 2.070140481037936   (start: 27870, end: 27880)\n",
      "Batch 2788 - batch loss: 2.381795883178711 - avg loss: 2.070252225534939   (start: 27880, end: 27890)\n",
      "Batch 2789 - batch loss: 2.8376517295837402 - avg loss: 2.0705272791206197   (start: 27890, end: 27900)\n",
      "Batch 2790 - batch loss: 2.098365306854248 - avg loss: 2.070537253333351   (start: 27900, end: 27910)\n",
      "Batch 2791 - batch loss: 1.9884216785430908 - avg loss: 2.0705078423108616   (start: 27910, end: 27920)\n",
      "Batch 2792 - batch loss: 1.8107099533081055 - avg loss: 2.0704148248067433   (start: 27920, end: 27930)\n",
      "Batch 2793 - batch loss: 2.339442014694214 - avg loss: 2.0705111122762805   (start: 27930, end: 27940)\n",
      "Batch 2794 - batch loss: 1.7817522287368774 - avg loss: 2.070407799616696   (start: 27940, end: 27950)\n",
      "Batch 2795 - batch loss: 2.5992634296417236 - avg loss: 2.070596946837735   (start: 27950, end: 27960)\n",
      "Batch 2796 - batch loss: 1.3563694953918457 - avg loss: 2.070341592010618   (start: 27960, end: 27970)\n",
      "Batch 2797 - batch loss: 2.4995551109313965 - avg loss: 2.0704949921246   (start: 27970, end: 27980)\n",
      "Batch 2798 - batch loss: 2.9810614585876465 - avg loss: 2.0708203106192276   (start: 27980, end: 27990)\n",
      "Batch 2799 - batch loss: 1.0315486192703247 - avg loss: 2.0704491421580316   (start: 27990, end: 28000)\n",
      "Batch 2800 - batch loss: 2.232452869415283 - avg loss: 2.0705069799756886   (start: 28000, end: 28010)\n",
      "Batch 2801 - batch loss: 1.2710399627685547 - avg loss: 2.0702216598410677   (start: 28010, end: 28020)\n",
      "Batch 2802 - batch loss: 1.8600342273712158 - avg loss: 2.070146673243683   (start: 28020, end: 28030)\n",
      "Batch 2803 - batch loss: 1.8394653797149658 - avg loss: 2.070064404594065   (start: 28030, end: 28040)\n",
      "Batch 2804 - batch loss: 1.5700087547302246 - avg loss: 2.0698861316351116   (start: 28040, end: 28050)\n",
      "Batch 2805 - batch loss: 1.6411679983139038 - avg loss: 2.0697333454151114   (start: 28050, end: 28060)\n",
      "Batch 2806 - batch loss: 2.6581432819366455 - avg loss: 2.0699429677651366   (start: 28060, end: 28070)\n",
      "Batch 2807 - batch loss: 1.6713616847991943 - avg loss: 2.0698010228637957   (start: 28070, end: 28080)\n",
      "Batch 2808 - batch loss: 2.230086088180542 - avg loss: 2.0698580841188035   (start: 28080, end: 28090)\n",
      "Batch 2809 - batch loss: 1.105659008026123 - avg loss: 2.0695149527749983   (start: 28090, end: 28100)\n",
      "Batch 2810 - batch loss: 2.421297788619995 - avg loss: 2.0696400978606775   (start: 28100, end: 28110)\n",
      "Batch 2811 - batch loss: 1.964583158493042 - avg loss: 2.0696027376404187   (start: 28110, end: 28120)\n",
      "Batch 2812 - batch loss: 2.380070209503174 - avg loss: 2.0697131064537366   (start: 28120, end: 28130)\n",
      "Batch 2813 - batch loss: 1.219641923904419 - avg loss: 2.06941102003492   (start: 28130, end: 28140)\n",
      "Batch 2814 - batch loss: 1.548462152481079 - avg loss: 2.0692259582702475   (start: 28140, end: 28150)\n",
      "Batch 2815 - batch loss: 1.6022218465805054 - avg loss: 2.0690601187419486   (start: 28150, end: 28160)\n",
      "Batch 2816 - batch loss: 1.451827883720398 - avg loss: 2.068841008967358   (start: 28160, end: 28170)\n",
      "Batch 2817 - batch loss: 2.570345401763916 - avg loss: 2.069018973620586   (start: 28170, end: 28180)\n",
      "Batch 2818 - batch loss: 2.171485662460327 - avg loss: 2.069055322215421   (start: 28180, end: 28190)\n",
      "Batch 2819 - batch loss: 2.4311435222625732 - avg loss: 2.06918372228636   (start: 28190, end: 28200)\n",
      "Batch 2820 - batch loss: 2.1967990398406982 - avg loss: 2.069228959903359   (start: 28200, end: 28210)\n",
      "Batch 2821 - batch loss: 2.1172595024108887 - avg loss: 2.069245979939683   (start: 28210, end: 28220)\n",
      "Batch 2822 - batch loss: 1.4688547849655151 - avg loss: 2.069033301514258   (start: 28220, end: 28230)\n",
      "Batch 2823 - batch loss: 2.0479817390441895 - avg loss: 2.06902584699497   (start: 28230, end: 28240)\n",
      "Batch 2824 - batch loss: 2.303535223007202 - avg loss: 2.06910885916347   (start: 28240, end: 28250)\n",
      "Batch 2825 - batch loss: 1.5157448053359985 - avg loss: 2.068913047396369   (start: 28250, end: 28260)\n",
      "Batch 2826 - batch loss: 2.038341999053955 - avg loss: 2.068902233442233   (start: 28260, end: 28270)\n",
      "Batch 2827 - batch loss: 1.9789215326309204 - avg loss: 2.0688704156555247   (start: 28270, end: 28280)\n",
      "Batch 2828 - batch loss: 2.133080005645752 - avg loss: 2.0688931125766947   (start: 28280, end: 28290)\n",
      "Batch 2829 - batch loss: 1.6054799556732178 - avg loss: 2.0687293623445733   (start: 28290, end: 28300)\n",
      "Batch 2830 - batch loss: 2.640430212020874 - avg loss: 2.0689313054211103   (start: 28300, end: 28310)\n",
      "Batch 2831 - batch loss: 2.0193514823913574 - avg loss: 2.0689137984214527   (start: 28310, end: 28320)\n",
      "Batch 2832 - batch loss: 1.3971773386001587 - avg loss: 2.0686766870695923   (start: 28320, end: 28330)\n",
      "Batch 2833 - batch loss: 2.134329319000244 - avg loss: 2.068699853135905   (start: 28330, end: 28340)\n",
      "Batch 2834 - batch loss: 2.980987787246704 - avg loss: 2.0690216478216583   (start: 28340, end: 28350)\n",
      "Batch 2835 - batch loss: 2.8026649951934814 - avg loss: 2.069280337295344   (start: 28350, end: 28360)\n",
      "Batch 2836 - batch loss: 1.572323203086853 - avg loss: 2.069105167350258   (start: 28360, end: 28370)\n",
      "Batch 2837 - batch loss: 2.145587205886841 - avg loss: 2.069132116623879   (start: 28370, end: 28380)\n",
      "Batch 2838 - batch loss: 2.874478578567505 - avg loss: 2.0694157892064586   (start: 28380, end: 28390)\n",
      "Batch 2839 - batch loss: 2.116337299346924 - avg loss: 2.069432310864959   (start: 28390, end: 28400)\n",
      "Batch 2840 - batch loss: 1.5479443073272705 - avg loss: 2.0692487529615664   (start: 28400, end: 28410)\n",
      "Batch 2841 - batch loss: 1.9462149143218994 - avg loss: 2.06920546167422   (start: 28410, end: 28420)\n",
      "Batch 2842 - batch loss: 2.058014392852783 - avg loss: 2.069201525315155   (start: 28420, end: 28430)\n",
      "Batch 2843 - batch loss: 2.4729950428009033 - avg loss: 2.0693435061581527   (start: 28430, end: 28440)\n",
      "Batch 2844 - batch loss: 2.049956798553467 - avg loss: 2.06933669184968   (start: 28440, end: 28450)\n",
      "Batch 2845 - batch loss: 2.327359676361084 - avg loss: 2.0694273534745964   (start: 28450, end: 28460)\n",
      "Batch 2846 - batch loss: 2.754894971847534 - avg loss: 2.069668121868826   (start: 28460, end: 28470)\n",
      "Batch 2847 - batch loss: 1.9537057876586914 - avg loss: 2.069627404757095   (start: 28470, end: 28480)\n",
      "Batch 2848 - batch loss: 3.6563198566436768 - avg loss: 2.070184334364637   (start: 28480, end: 28490)\n",
      "Batch 2849 - batch loss: 2.740837812423706 - avg loss: 2.070419651374482   (start: 28490, end: 28500)\n",
      "Batch 2850 - batch loss: 1.8492238521575928 - avg loss: 2.0703420660362792   (start: 28500, end: 28510)\n",
      "Batch 2851 - batch loss: 2.0279152393341064 - avg loss: 2.070327189869834   (start: 28510, end: 28520)\n",
      "Batch 2852 - batch loss: 1.4120116233825684 - avg loss: 2.0700964448412718   (start: 28520, end: 28530)\n",
      "Batch 2853 - batch loss: 2.0144340991973877 - avg loss: 2.0700769415666946   (start: 28530, end: 28540)\n",
      "Batch 2854 - batch loss: 2.2811713218688965 - avg loss: 2.0701508800536654   (start: 28540, end: 28550)\n",
      "Batch 2855 - batch loss: 1.538703441619873 - avg loss: 2.069964799017799   (start: 28550, end: 28560)\n",
      "Batch 2856 - batch loss: 2.2332606315612793 - avg loss: 2.0700219554170096   (start: 28560, end: 28570)\n",
      "Batch 2857 - batch loss: 1.6248130798339844 - avg loss: 2.0698661790434674   (start: 28570, end: 28580)\n",
      "Batch 2858 - batch loss: 2.119457960128784 - avg loss: 2.0698835248920457   (start: 28580, end: 28590)\n",
      "Batch 2859 - batch loss: 1.6564815044403076 - avg loss: 2.0697389787310487   (start: 28590, end: 28600)\n",
      "Batch 2860 - batch loss: 2.104588031768799 - avg loss: 2.0697511594556337   (start: 28600, end: 28610)\n",
      "Batch 2861 - batch loss: 2.0189526081085205 - avg loss: 2.0697334101365046   (start: 28610, end: 28620)\n",
      "Batch 2862 - batch loss: 2.7271151542663574 - avg loss: 2.069963023040497   (start: 28620, end: 28630)\n",
      "Batch 2863 - batch loss: 3.2631676197052 - avg loss: 2.0703796447572094   (start: 28630, end: 28640)\n",
      "Batch 2864 - batch loss: 2.6291980743408203 - avg loss: 2.0705746948198915   (start: 28640, end: 28650)\n",
      "Batch 2865 - batch loss: 2.7611818313598633 - avg loss: 2.070815660324616   (start: 28650, end: 28660)\n",
      "Batch 2866 - batch loss: 2.385747194290161 - avg loss: 2.070925507389131   (start: 28660, end: 28670)\n",
      "Batch 2867 - batch loss: 3.0389535427093506 - avg loss: 2.071263034598099   (start: 28670, end: 28680)\n",
      "Batch 2868 - batch loss: 1.9048264026641846 - avg loss: 2.071205022527017   (start: 28680, end: 28690)\n",
      "Batch 2869 - batch loss: 1.6095988750457764 - avg loss: 2.071044184148104   (start: 28690, end: 28700)\n",
      "Batch 2870 - batch loss: 1.7628768682479858 - avg loss: 2.0709368461767004   (start: 28700, end: 28710)\n",
      "Batch 2871 - batch loss: 1.5222580432891846 - avg loss: 2.070745802025277   (start: 28710, end: 28720)\n",
      "Batch 2872 - batch loss: 2.5401840209960938 - avg loss: 2.070909198551198   (start: 28720, end: 28730)\n",
      "Batch 2873 - batch loss: 1.8980906009674072 - avg loss: 2.070849066819262   (start: 28730, end: 28740)\n",
      "Batch 2874 - batch loss: 1.7915538549423218 - avg loss: 2.0707519206586094   (start: 28740, end: 28750)\n",
      "Batch 2875 - batch loss: 2.4679362773895264 - avg loss: 2.0708900237033694   (start: 28750, end: 28760)\n",
      "Batch 2876 - batch loss: 3.01141095161438 - avg loss: 2.071216934001566   (start: 28760, end: 28770)\n",
      "Batch 2877 - batch loss: 2.7058053016662598 - avg loss: 2.0714374303072174   (start: 28770, end: 28780)\n",
      "Batch 2878 - batch loss: 1.7053499221801758 - avg loss: 2.0713102724370795   (start: 28780, end: 28790)\n",
      "Batch 2879 - batch loss: 1.866624116897583 - avg loss: 2.071239200855295   (start: 28790, end: 28800)\n",
      "Batch 2880 - batch loss: 2.453366756439209 - avg loss: 2.07137183797976   (start: 28800, end: 28810)\n",
      "Batch 2881 - batch loss: 1.9644830226898193 - avg loss: 2.0713347495636287   (start: 28810, end: 28820)\n",
      "Batch 2882 - batch loss: 2.191885232925415 - avg loss: 2.071376563813841   (start: 28820, end: 28830)\n",
      "Batch 2883 - batch loss: 3.028003215789795 - avg loss: 2.0717082651494776   (start: 28830, end: 28840)\n",
      "Batch 2884 - batch loss: 1.8692615032196045 - avg loss: 2.0716380929616336   (start: 28840, end: 28850)\n",
      "Batch 2885 - batch loss: 2.4337267875671387 - avg loss: 2.071763556819778   (start: 28850, end: 28860)\n",
      "Batch 2886 - batch loss: 1.7100515365600586 - avg loss: 2.0716382668924282   (start: 28860, end: 28870)\n",
      "Batch 2887 - batch loss: 2.3106980323791504 - avg loss: 2.0717210438195357   (start: 28870, end: 28880)\n",
      "Batch 2888 - batch loss: 2.175375461578369 - avg loss: 2.0717569228149526   (start: 28880, end: 28890)\n",
      "Batch 2889 - batch loss: 2.1078121662139893 - avg loss: 2.0717693986777204   (start: 28890, end: 28900)\n",
      "Batch 2890 - batch loss: 1.9790923595428467 - avg loss: 2.0717373415905067   (start: 28900, end: 28910)\n",
      "Batch 2891 - batch loss: 2.555323362350464 - avg loss: 2.071904556673757   (start: 28910, end: 28920)\n",
      "Batch 2892 - batch loss: 2.167466163635254 - avg loss: 2.071937588684459   (start: 28920, end: 28930)\n",
      "Batch 2893 - batch loss: 1.7075765132904053 - avg loss: 2.071811686446935   (start: 28930, end: 28940)\n",
      "Batch 2894 - batch loss: 3.2091774940490723 - avg loss: 2.0722045589193367   (start: 28940, end: 28950)\n",
      "Batch 2895 - batch loss: 1.7564332485198975 - avg loss: 2.072095521864641   (start: 28950, end: 28960)\n",
      "Batch 2896 - batch loss: 2.242950677871704 - avg loss: 2.072154498445934   (start: 28960, end: 28970)\n",
      "Batch 2897 - batch loss: 1.7881437540054321 - avg loss: 2.0720564961186603   (start: 28970, end: 28980)\n",
      "Batch 2898 - batch loss: 2.9421932697296143 - avg loss: 2.072356646782203   (start: 28980, end: 28990)\n",
      "Batch 2899 - batch loss: 2.709317207336426 - avg loss: 2.072576288354808   (start: 28990, end: 29000)\n",
      "Batch 2900 - batch loss: 2.2710556983947754 - avg loss: 2.0726447059384134   (start: 29000, end: 29010)\n",
      "Batch 2901 - batch loss: 2.206333875656128 - avg loss: 2.0726907738811144   (start: 29010, end: 29020)\n",
      "Batch 2902 - batch loss: 1.3848135471343994 - avg loss: 2.0724538199621523   (start: 29020, end: 29030)\n",
      "Batch 2903 - batch loss: 2.3167223930358887 - avg loss: 2.0725379344845605   (start: 29030, end: 29040)\n",
      "Batch 2904 - batch loss: 1.4735307693481445 - avg loss: 2.0723317358046516   (start: 29040, end: 29050)\n",
      "Batch 2905 - batch loss: 2.1615333557128906 - avg loss: 2.0723624314756455   (start: 29050, end: 29060)\n",
      "Batch 2906 - batch loss: 1.5242853164672852 - avg loss: 2.0721738944563786   (start: 29060, end: 29070)\n",
      "Batch 2907 - batch loss: 1.7145721912384033 - avg loss: 2.07205092275651   (start: 29070, end: 29080)\n",
      "Batch 2908 - batch loss: 1.5905455350875854 - avg loss: 2.071885400106916   (start: 29080, end: 29090)\n",
      "Batch 2909 - batch loss: 1.358666181564331 - avg loss: 2.0716403075919527   (start: 29090, end: 29100)\n",
      "Batch 2910 - batch loss: 2.441713809967041 - avg loss: 2.0717674369297665   (start: 29100, end: 29110)\n",
      "Batch 2911 - batch loss: 1.9966579675674438 - avg loss: 2.07174164384276   (start: 29110, end: 29120)\n",
      "Batch 2912 - batch loss: 2.074702024459839 - avg loss: 2.071742660107991   (start: 29120, end: 29130)\n",
      "Batch 2913 - batch loss: 2.8784422874450684 - avg loss: 2.072019495944414   (start: 29130, end: 29140)\n",
      "Batch 2914 - batch loss: 2.9727160930633545 - avg loss: 2.0723284827701836   (start: 29140, end: 29150)\n",
      "Batch 2915 - batch loss: 1.933168649673462 - avg loss: 2.0722807599193276   (start: 29150, end: 29160)\n",
      "Batch 2916 - batch loss: 1.8914283514022827 - avg loss: 2.0722187604649163   (start: 29160, end: 29170)\n",
      "Batch 2917 - batch loss: 2.0934908390045166 - avg loss: 2.072226050416438   (start: 29170, end: 29180)\n",
      "Batch 2918 - batch loss: 1.7442671060562134 - avg loss: 2.0721136972323473   (start: 29180, end: 29190)\n",
      "Batch 2919 - batch loss: 2.336787700653076 - avg loss: 2.0722043390143408   (start: 29190, end: 29200)\n",
      "Batch 2920 - batch loss: 1.944488525390625 - avg loss: 2.072160615695743   (start: 29200, end: 29210)\n",
      "Batch 2921 - batch loss: 2.674701690673828 - avg loss: 2.072366824140294   (start: 29210, end: 29220)\n",
      "Batch 2922 - batch loss: 2.5332117080688477 - avg loss: 2.0725244857495753   (start: 29220, end: 29230)\n",
      "Batch 2923 - batch loss: 1.5617731809616089 - avg loss: 2.072349810200742   (start: 29230, end: 29240)\n",
      "Batch 2924 - batch loss: 1.3356040716171265 - avg loss: 2.0720979313157564   (start: 29240, end: 29250)\n",
      "Batch 2925 - batch loss: 1.0943869352340698 - avg loss: 2.071763785384081   (start: 29250, end: 29260)\n",
      "Batch 2926 - batch loss: 2.1050498485565186 - avg loss: 2.0717751574589607   (start: 29260, end: 29270)\n",
      "Batch 2927 - batch loss: 2.5955605506896973 - avg loss: 2.071954045912933   (start: 29270, end: 29280)\n",
      "Batch 2928 - batch loss: 2.1649813652038574 - avg loss: 2.071985806691113   (start: 29280, end: 29290)\n",
      "Batch 2929 - batch loss: 1.1671793460845947 - avg loss: 2.0716769990253776   (start: 29290, end: 29300)\n",
      "Batch 2930 - batch loss: 1.49686598777771 - avg loss: 2.0714808847260775   (start: 29300, end: 29310)\n",
      "Batch 2931 - batch loss: 1.088462233543396 - avg loss: 2.071145612334815   (start: 29310, end: 29320)\n",
      "Batch 2932 - batch loss: 2.0734102725982666 - avg loss: 2.0711463844658287   (start: 29320, end: 29330)\n",
      "Batch 2933 - batch loss: 1.886794090270996 - avg loss: 2.071083551373056   (start: 29330, end: 29340)\n",
      "Batch 2934 - batch loss: 1.6712768077850342 - avg loss: 2.0709473310174893   (start: 29340, end: 29350)\n",
      "Batch 2935 - batch loss: 2.32977032661438 - avg loss: 2.071035485988742   (start: 29350, end: 29360)\n",
      "Batch 2936 - batch loss: 1.5981719493865967 - avg loss: 2.0708744837631365   (start: 29360, end: 29370)\n",
      "Batch 2937 - batch loss: 2.160834550857544 - avg loss: 2.070905103255   (start: 29370, end: 29380)\n",
      "Batch 2938 - batch loss: 2.6280534267425537 - avg loss: 2.0710946739673126   (start: 29380, end: 29390)\n",
      "Batch 2939 - batch loss: 1.687450647354126 - avg loss: 2.070964182801798   (start: 29390, end: 29400)\n",
      "Batch 2940 - batch loss: 1.0859832763671875 - avg loss: 2.070629269198794   (start: 29400, end: 29410)\n",
      "Batch 2941 - batch loss: 2.4767425060272217 - avg loss: 2.070767309048158   (start: 29410, end: 29420)\n",
      "Batch 2942 - batch loss: 2.444479465484619 - avg loss: 2.0708942924516363   (start: 29420, end: 29430)\n",
      "Batch 2943 - batch loss: 2.4706778526306152 - avg loss: 2.0710300884978925   (start: 29430, end: 29440)\n",
      "Batch 2944 - batch loss: 1.5211777687072754 - avg loss: 2.0708433814283542   (start: 29440, end: 29450)\n",
      "Batch 2945 - batch loss: 2.575003147125244 - avg loss: 2.071014515089487   (start: 29450, end: 29460)\n",
      "Batch 2946 - batch loss: 2.4521126747131348 - avg loss: 2.0711438324154536   (start: 29460, end: 29470)\n",
      "Batch 2947 - batch loss: 2.26405930519104 - avg loss: 2.0712092718566937   (start: 29470, end: 29480)\n",
      "Batch 2948 - batch loss: 3.043409824371338 - avg loss: 2.071538943118991   (start: 29480, end: 29490)\n",
      "Batch 2949 - batch loss: 2.342107057571411 - avg loss: 2.07163066112389   (start: 29490, end: 29500)\n",
      "Batch 2950 - batch loss: 1.4219462871551514 - avg loss: 2.071410503762328   (start: 29500, end: 29510)\n",
      "Batch 2951 - batch loss: 1.607487678527832 - avg loss: 2.071253348333726   (start: 29510, end: 29520)\n",
      "Batch 2952 - batch loss: 1.091062307357788 - avg loss: 2.0709214177407778   (start: 29520, end: 29530)\n",
      "Batch 2953 - batch loss: 2.3321797847747803 - avg loss: 2.0710098599774174   (start: 29530, end: 29540)\n",
      "Batch 2954 - batch loss: 1.2279912233352661 - avg loss: 2.070724574482784   (start: 29540, end: 29550)\n",
      "Batch 2955 - batch loss: 1.6935036182403564 - avg loss: 2.070596962521944   (start: 29550, end: 29560)\n",
      "Batch 2956 - batch loss: 1.9971199035644531 - avg loss: 2.0705721140069095   (start: 29560, end: 29570)\n",
      "Batch 2957 - batch loss: 2.5163562297821045 - avg loss: 2.070722818576137   (start: 29570, end: 29580)\n",
      "Batch 2958 - batch loss: 1.9854873418807983 - avg loss: 2.070694013075395   (start: 29580, end: 29590)\n",
      "Batch 2959 - batch loss: 1.8254362344741821 - avg loss: 2.0706111557177596   (start: 29590, end: 29600)\n",
      "Batch 2960 - batch loss: 2.217703342437744 - avg loss: 2.0706608322414746   (start: 29600, end: 29610)\n",
      "Batch 2961 - batch loss: 1.8168977499008179 - avg loss: 2.0705751593574973   (start: 29610, end: 29620)\n",
      "Batch 2962 - batch loss: 2.4701225757598877 - avg loss: 2.0707100049249636   (start: 29620, end: 29630)\n",
      "Batch 2963 - batch loss: 1.8762624263763428 - avg loss: 2.0706444018282872   (start: 29630, end: 29640)\n",
      "Batch 2964 - batch loss: 2.0829269886016846 - avg loss: 2.0706485443533373   (start: 29640, end: 29650)\n",
      "Batch 2965 - batch loss: 1.3346463441848755 - avg loss: 2.0704003979608325   (start: 29650, end: 29660)\n",
      "Batch 2966 - batch loss: 2.459315061569214 - avg loss: 2.070531478063161   (start: 29660, end: 29670)\n",
      "Batch 2967 - batch loss: 2.4241902828216553 - avg loss: 2.070650635342392   (start: 29670, end: 29680)\n",
      "Batch 2968 - batch loss: 2.400221824645996 - avg loss: 2.0707616394479174   (start: 29680, end: 29690)\n",
      "Batch 2969 - batch loss: 2.806151866912842 - avg loss: 2.071009245585111   (start: 29690, end: 29700)\n",
      "Batch 2970 - batch loss: 1.4620788097381592 - avg loss: 2.0708042875117862   (start: 29700, end: 29710)\n",
      "Batch 2971 - batch loss: 2.4026429653167725 - avg loss: 2.0709159425177774   (start: 29710, end: 29720)\n",
      "Batch 2972 - batch loss: 2.204362392425537 - avg loss: 2.0709608286428725   (start: 29720, end: 29730)\n",
      "Batch 2973 - batch loss: 2.266744613647461 - avg loss: 2.0710266604468415   (start: 29730, end: 29740)\n",
      "Batch 2974 - batch loss: 2.4768290519714355 - avg loss: 2.07116306461206   (start: 29740, end: 29750)\n",
      "Batch 2975 - batch loss: 2.253183603286743 - avg loss: 2.071224227427475   (start: 29750, end: 29760)\n",
      "Batch 2976 - batch loss: 1.9701601266860962 - avg loss: 2.0711902791235643   (start: 29760, end: 29770)\n",
      "Batch 2977 - batch loss: 2.005889415740967 - avg loss: 2.071168351365545   (start: 29770, end: 29780)\n",
      "Batch 2978 - batch loss: 2.3582799434661865 - avg loss: 2.0712647298791738   (start: 29780, end: 29790)\n",
      "Batch 2979 - batch loss: 2.3879494667053223 - avg loss: 2.0713709999250884   (start: 29790, end: 29800)\n",
      "Batch 2980 - batch loss: 2.2529666423797607 - avg loss: 2.0714319176179616   (start: 29800, end: 29810)\n",
      "Batch 2981 - batch loss: 2.090987205505371 - avg loss: 2.0714384753939132   (start: 29810, end: 29820)\n",
      "Batch 2982 - batch loss: 2.2321977615356445 - avg loss: 2.0714923672095824   (start: 29820, end: 29830)\n",
      "Batch 2983 - batch loss: 3.2940659523010254 - avg loss: 2.0719020768560608   (start: 29830, end: 29840)\n",
      "Batch 2984 - batch loss: 2.0342776775360107 - avg loss: 2.0718894723671766   (start: 29840, end: 29850)\n",
      "Batch 2985 - batch loss: 2.451127767562866 - avg loss: 2.0720164778243753   (start: 29850, end: 29860)\n",
      "Batch 2986 - batch loss: 1.9300527572631836 - avg loss: 2.0719689506330257   (start: 29860, end: 29870)\n",
      "Batch 2987 - batch loss: 1.99493408203125 - avg loss: 2.071943169217831   (start: 29870, end: 29880)\n",
      "Batch 2988 - batch loss: 2.822129726409912 - avg loss: 2.072194151672562   (start: 29880, end: 29890)\n",
      "Batch 2989 - batch loss: 2.6528561115264893 - avg loss: 2.0723883529969283   (start: 29890, end: 29900)\n",
      "Batch 2990 - batch loss: 1.847646713256836 - avg loss: 2.0723132136991214   (start: 29900, end: 29910)\n",
      "Batch 2991 - batch loss: 1.8000177145004272 - avg loss: 2.072222205845111   (start: 29910, end: 29920)\n",
      "Batch 2992 - batch loss: 1.6544275283813477 - avg loss: 2.0720826152412144   (start: 29920, end: 29930)\n",
      "Batch 2993 - batch loss: 1.6965450048446655 - avg loss: 2.0719571851776215   (start: 29930, end: 29940)\n",
      "Batch 2994 - batch loss: 2.4565823078155518 - avg loss: 2.072085607589187   (start: 29940, end: 29950)\n",
      "Batch 2995 - batch loss: 1.7482776641845703 - avg loss: 2.071977527501268   (start: 29950, end: 29960)\n",
      "Batch 2996 - batch loss: 1.8979190587997437 - avg loss: 2.071919449934134   (start: 29960, end: 29970)\n",
      "Batch 2997 - batch loss: 1.9040172100067139 - avg loss: 2.0718634451843245   (start: 29970, end: 29980)\n",
      "Batch 2998 - batch loss: 2.888972282409668 - avg loss: 2.0721359056168773   (start: 29980, end: 29990)\n",
      "Batch 2999 - batch loss: 2.471252679824829 - avg loss: 2.072268944541613   (start: 29990, end: 30000)\n",
      "Batch 3000 - batch loss: 1.7712914943695068 - avg loss: 2.0721686521556846   (start: 30000, end: 30010)\n",
      "Batch 3001 - batch loss: 1.6732141971588135 - avg loss: 2.072035755934833   (start: 30010, end: 30020)\n",
      "Batch 3002 - batch loss: 2.711669921875 - avg loss: 2.072248754325089   (start: 30020, end: 30030)\n",
      "Batch 3003 - batch loss: 1.7590986490249634 - avg loss: 2.0721445099491573   (start: 30030, end: 30040)\n",
      "Batch 3004 - batch loss: 1.6840835809707642 - avg loss: 2.0720153715368514   (start: 30040, end: 30050)\n",
      "Batch 3005 - batch loss: 2.1581814289093018 - avg loss: 2.0720440362265964   (start: 30050, end: 30060)\n",
      "Batch 3006 - batch loss: 2.2584354877471924 - avg loss: 2.072106022076786   (start: 30060, end: 30070)\n",
      "Batch 3007 - batch loss: 2.698218584060669 - avg loss: 2.072314171199786   (start: 30070, end: 30080)\n",
      "Batch 3008 - batch loss: 1.7645593881607056 - avg loss: 2.0722118931063864   (start: 30080, end: 30090)\n",
      "Batch 3009 - batch loss: 2.33723783493042 - avg loss: 2.0722999415920422   (start: 30090, end: 30100)\n",
      "Batch 3010 - batch loss: 3.3307127952575684 - avg loss: 2.0727178801020605   (start: 30100, end: 30110)\n",
      "Batch 3011 - batch loss: 2.2715306282043457 - avg loss: 2.072783886990541   (start: 30110, end: 30120)\n",
      "Batch 3012 - batch loss: 1.7617437839508057 - avg loss: 2.0726806542978626   (start: 30120, end: 30130)\n",
      "Batch 3013 - batch loss: 4.025367736816406 - avg loss: 2.0733285265880146   (start: 30130, end: 30140)\n",
      "Batch 3014 - batch loss: 1.8072378635406494 - avg loss: 2.0732402709783804   (start: 30140, end: 30150)\n",
      "Batch 3015 - batch loss: 1.8362715244293213 - avg loss: 2.0731617004390737   (start: 30150, end: 30160)\n",
      "Batch 3016 - batch loss: 1.852378487586975 - avg loss: 2.0730885207198653   (start: 30160, end: 30170)\n",
      "Batch 3017 - batch loss: 2.6141347885131836 - avg loss: 2.073267793837093   (start: 30170, end: 30180)\n",
      "Batch 3018 - batch loss: 2.9413580894470215 - avg loss: 2.0735553361675367   (start: 30180, end: 30190)\n",
      "Batch 3019 - batch loss: 2.0517096519470215 - avg loss: 2.073548102497265   (start: 30190, end: 30200)\n",
      "Batch 3020 - batch loss: 1.343904733657837 - avg loss: 2.0733065787075136   (start: 30200, end: 30210)\n",
      "Batch 3021 - batch loss: 2.218595504760742 - avg loss: 2.0733546557843017   (start: 30210, end: 30220)\n",
      "Batch 3022 - batch loss: 2.067007064819336 - avg loss: 2.073352556018848   (start: 30220, end: 30230)\n",
      "Batch 3023 - batch loss: 1.8779306411743164 - avg loss: 2.0732879323697593   (start: 30230, end: 30240)\n",
      "Batch 3024 - batch loss: 2.096935510635376 - avg loss: 2.0732957497510043   (start: 30240, end: 30250)\n",
      "Batch 3025 - batch loss: 2.240410804748535 - avg loss: 2.0733509761406266   (start: 30250, end: 30260)\n",
      "Batch 3026 - batch loss: 2.225216865539551 - avg loss: 2.0734011465698963   (start: 30260, end: 30270)\n",
      "Batch 3027 - batch loss: 2.3801023960113525 - avg loss: 2.0735024349613895   (start: 30270, end: 30280)\n",
      "Batch 3028 - batch loss: 1.8640388250350952 - avg loss: 2.0734332822344412   (start: 30280, end: 30290)\n",
      "Batch 3029 - batch loss: 2.2019011974334717 - avg loss: 2.073475680886322   (start: 30290, end: 30300)\n",
      "Batch 3030 - batch loss: 1.5716280937194824 - avg loss: 2.07331010926403   (start: 30300, end: 30310)\n",
      "Batch 3031 - batch loss: 1.253670573234558 - avg loss: 2.0730397796017512   (start: 30310, end: 30320)\n",
      "Batch 3032 - batch loss: 1.276806354522705 - avg loss: 2.0727772562172873   (start: 30320, end: 30330)\n",
      "Batch 3033 - batch loss: 1.5021005868911743 - avg loss: 2.072589162390878   (start: 30330, end: 30340)\n",
      "Batch 3034 - batch loss: 1.4648025035858154 - avg loss: 2.072388903195226   (start: 30340, end: 30350)\n",
      "Batch 3035 - batch loss: 2.519977331161499 - avg loss: 2.07253633021366   (start: 30350, end: 30360)\n",
      "Batch 3036 - batch loss: 1.6393136978149414 - avg loss: 2.072393681997526   (start: 30360, end: 30370)\n",
      "Batch 3037 - batch loss: 1.6022517681121826 - avg loss: 2.0722389282404867   (start: 30370, end: 30380)\n",
      "Batch 3038 - batch loss: 1.7166717052459717 - avg loss: 2.072121926850887   (start: 30380, end: 30390)\n",
      "Batch 3039 - batch loss: 2.2891852855682373 - avg loss: 2.072193329271517   (start: 30390, end: 30400)\n",
      "Batch 3040 - batch loss: 1.285213589668274 - avg loss: 2.0719345394853934   (start: 30400, end: 30410)\n",
      "Batch 3041 - batch loss: 1.8657209873199463 - avg loss: 2.0718667506779753   (start: 30410, end: 30420)\n",
      "Batch 3042 - batch loss: 1.545301914215088 - avg loss: 2.071693709325211   (start: 30420, end: 30430)\n",
      "Batch 3043 - batch loss: 1.9018275737762451 - avg loss: 2.0716379057327177   (start: 30430, end: 30440)\n",
      "Batch 3044 - batch loss: 1.1877195835113525 - avg loss: 2.0713476205694263   (start: 30440, end: 30450)\n",
      "Batch 3045 - batch loss: 2.187344551086426 - avg loss: 2.071385702293168   (start: 30450, end: 30460)\n",
      "Batch 3046 - batch loss: 2.1338319778442383 - avg loss: 2.0714061966402477   (start: 30460, end: 30470)\n",
      "Batch 3047 - batch loss: 1.811193823814392 - avg loss: 2.071320825126853   (start: 30470, end: 30480)\n",
      "Batch 3048 - batch loss: 0.9740584492683411 - avg loss: 2.0709609489786542   (start: 30480, end: 30490)\n",
      "Batch 3049 - batch loss: 1.650371789932251 - avg loss: 2.070823050893721   (start: 30490, end: 30500)\n",
      "Batch 3050 - batch loss: 1.4745705127716064 - avg loss: 2.070627622333209   (start: 30500, end: 30510)\n",
      "Batch 3051 - batch loss: 1.840867280960083 - avg loss: 2.070552340438919   (start: 30510, end: 30520)\n",
      "Batch 3052 - batch loss: 1.9991897344589233 - avg loss: 2.0705289658545825   (start: 30520, end: 30530)\n",
      "Batch 3053 - batch loss: 1.9674783945083618 - avg loss: 2.070495223034888   (start: 30530, end: 30540)\n",
      "Batch 3054 - batch loss: 1.8389370441436768 - avg loss: 2.070419426576986   (start: 30540, end: 30550)\n",
      "Batch 3055 - batch loss: 2.0590548515319824 - avg loss: 2.0704157078024292   (start: 30550, end: 30560)\n",
      "Batch 3056 - batch loss: 2.3512797355651855 - avg loss: 2.070507583506637   (start: 30560, end: 30570)\n",
      "Batch 3057 - batch loss: 2.2380197048187256 - avg loss: 2.070562361832769   (start: 30570, end: 30580)\n",
      "Batch 3058 - batch loss: 2.0705113410949707 - avg loss: 2.0705623451538746   (start: 30580, end: 30590)\n",
      "Batch 3059 - batch loss: 1.8022491931915283 - avg loss: 2.0704746611172857   (start: 30590, end: 30600)\n",
      "Batch 3060 - batch loss: 2.0609288215637207 - avg loss: 2.0704715425810054   (start: 30600, end: 30610)\n",
      "Batch 3061 - batch loss: 2.059417724609375 - avg loss: 2.070467932581668   (start: 30610, end: 30620)\n",
      "Batch 3062 - batch loss: 2.662388563156128 - avg loss: 2.070661181236769   (start: 30620, end: 30630)\n",
      "Batch 3063 - batch loss: 1.3291264772415161 - avg loss: 2.070419165993951   (start: 30630, end: 30640)\n",
      "Batch 3064 - batch loss: 2.217050075531006 - avg loss: 2.070467006421206   (start: 30640, end: 30650)\n",
      "Batch 3065 - batch loss: 2.0719361305236816 - avg loss: 2.0704674855875798   (start: 30650, end: 30660)\n",
      "Batch 3066 - batch loss: 2.262578248977661 - avg loss: 2.07053012359325   (start: 30660, end: 30670)\n",
      "Batch 3067 - batch loss: 3.4020912647247314 - avg loss: 2.070964139610568   (start: 30670, end: 30680)\n",
      "Batch 3068 - batch loss: 1.5930297374725342 - avg loss: 2.070808409925935   (start: 30680, end: 30690)\n",
      "Batch 3069 - batch loss: 1.9180752038955688 - avg loss: 2.0707586596959575   (start: 30690, end: 30700)\n",
      "Batch 3070 - batch loss: 2.3835747241973877 - avg loss: 2.070860520999931   (start: 30700, end: 30710)\n",
      "Batch 3071 - batch loss: 3.2801311016082764 - avg loss: 2.071254163767056   (start: 30710, end: 30720)\n",
      "Batch 3072 - batch loss: 2.168093681335449 - avg loss: 2.071285676789369   (start: 30720, end: 30730)\n",
      "Batch 3073 - batch loss: 1.6054818630218506 - avg loss: 2.071134146596211   (start: 30730, end: 30740)\n",
      "Batch 3074 - batch loss: 2.616894245147705 - avg loss: 2.0713116295550895   (start: 30740, end: 30750)\n",
      "Batch 3075 - batch loss: 2.130178928375244 - avg loss: 2.07133076716849   (start: 30750, end: 30760)\n",
      "Batch 3076 - batch loss: 2.4751393795013428 - avg loss: 2.0714620016866356   (start: 30760, end: 30770)\n",
      "Batch 3077 - batch loss: 2.4785969257354736 - avg loss: 2.071594274241557   (start: 30770, end: 30780)\n",
      "Batch 3078 - batch loss: 1.9910738468170166 - avg loss: 2.0715681227548974   (start: 30780, end: 30790)\n",
      "Batch 3079 - batch loss: 2.1712028980255127 - avg loss: 2.0716004717079075   (start: 30790, end: 30800)\n",
      "Batch 3080 - batch loss: 1.7053406238555908 - avg loss: 2.071481594769299   (start: 30800, end: 30810)\n",
      "Batch 3081 - batch loss: 2.143947124481201 - avg loss: 2.071505107270828   (start: 30810, end: 30820)\n",
      "Batch 3082 - batch loss: 2.294227123260498 - avg loss: 2.0715773492481198   (start: 30820, end: 30830)\n",
      "Batch 3083 - batch loss: 1.2535197734832764 - avg loss: 2.071312090630816   (start: 30830, end: 30840)\n",
      "Batch 3084 - batch loss: 1.6367391347885132 - avg loss: 2.071171224194562   (start: 30840, end: 30850)\n",
      "Batch 3085 - batch loss: 2.1974425315856934 - avg loss: 2.0712121416629325   (start: 30850, end: 30860)\n",
      "Batch 3086 - batch loss: 2.3519675731658936 - avg loss: 2.071303089324579   (start: 30860, end: 30870)\n",
      "Batch 3087 - batch loss: 1.4352906942367554 - avg loss: 2.071097126761403   (start: 30870, end: 30880)\n",
      "Batch 3088 - batch loss: 1.6379451751708984 - avg loss: 2.070956902756356   (start: 30880, end: 30890)\n",
      "Batch 3089 - batch loss: 1.9991964101791382 - avg loss: 2.070933679295975   (start: 30890, end: 30900)\n",
      "Batch 3090 - batch loss: 1.6078293323516846 - avg loss: 2.0707838558255953   (start: 30900, end: 30910)\n",
      "Batch 3091 - batch loss: 1.5786008834838867 - avg loss: 2.070624676339068   (start: 30910, end: 30920)\n",
      "Batch 3092 - batch loss: 1.8088527917861938 - avg loss: 2.0705400426874183   (start: 30920, end: 30930)\n",
      "Batch 3093 - batch loss: 2.6013286113739014 - avg loss: 2.0707115968466576   (start: 30930, end: 30940)\n",
      "Batch 3094 - batch loss: 2.257336378097534 - avg loss: 2.070771895645123   (start: 30940, end: 30950)\n",
      "Batch 3095 - batch loss: 2.0049328804016113 - avg loss: 2.0707506298133262   (start: 30950, end: 30960)\n",
      "Batch 3096 - batch loss: 1.7693321704864502 - avg loss: 2.070653303865852   (start: 30960, end: 30970)\n",
      "Batch 3097 - batch loss: 2.6430532932281494 - avg loss: 2.070838068226524   (start: 30970, end: 30980)\n",
      "Batch 3098 - batch loss: 1.8628027439117432 - avg loss: 2.0707709384026085   (start: 30980, end: 30990)\n",
      "Batch 3099 - batch loss: 1.7332063913345337 - avg loss: 2.070662046613232   (start: 30990, end: 31000)\n",
      "Batch 3100 - batch loss: 2.7047524452209473 - avg loss: 2.0708665259420314   (start: 31000, end: 31010)\n",
      "Batch 3101 - batch loss: 2.494293451309204 - avg loss: 2.0710030272074627   (start: 31010, end: 31020)\n",
      "Batch 3102 - batch loss: 2.542213201522827 - avg loss: 2.0711548835317664   (start: 31020, end: 31030)\n",
      "Batch 3103 - batch loss: 2.1789910793304443 - avg loss: 2.0711896245742274   (start: 31030, end: 31040)\n",
      "Batch 3104 - batch loss: 1.7745532989501953 - avg loss: 2.0710940895257175   (start: 31040, end: 31050)\n",
      "Batch 3105 - batch loss: 1.2390682697296143 - avg loss: 2.0708262125715007   (start: 31050, end: 31060)\n",
      "Batch 3106 - batch loss: 1.3387318849563599 - avg loss: 2.0705905851728477   (start: 31060, end: 31070)\n",
      "Batch 3107 - batch loss: 1.8770538568496704 - avg loss: 2.0705283146682394   (start: 31070, end: 31080)\n",
      "Batch 3108 - batch loss: 2.212207317352295 - avg loss: 2.070573885270582   (start: 31080, end: 31090)\n",
      "Batch 3109 - batch loss: 1.7152639627456665 - avg loss: 2.0704596377070694   (start: 31090, end: 31100)\n",
      "Batch 3110 - batch loss: 1.59718656539917 - avg loss: 2.070307508786366   (start: 31100, end: 31110)\n",
      "Batch 3111 - batch loss: 1.7664079666137695 - avg loss: 2.0702098546918375   (start: 31110, end: 31120)\n",
      "Batch 3112 - batch loss: 2.388638973236084 - avg loss: 2.0703121448038018   (start: 31120, end: 31130)\n",
      "Batch 3113 - batch loss: 1.5674036741256714 - avg loss: 2.0701506456160437   (start: 31130, end: 31140)\n",
      "Batch 3114 - batch loss: 2.9295201301574707 - avg loss: 2.070426526670471   (start: 31140, end: 31150)\n",
      "Batch 3115 - batch loss: 1.9275176525115967 - avg loss: 2.0703806637455164   (start: 31150, end: 31160)\n",
      "Batch 3116 - batch loss: 2.510720729827881 - avg loss: 2.0705219342190753   (start: 31160, end: 31170)\n",
      "Batch 3117 - batch loss: 1.947028398513794 - avg loss: 2.0704823275687527   (start: 31170, end: 31180)\n",
      "Batch 3118 - batch loss: 2.138667345046997 - avg loss: 2.0705041887478095   (start: 31180, end: 31190)\n",
      "Batch 3119 - batch loss: 2.4226484298706055 - avg loss: 2.070617055491759   (start: 31190, end: 31200)\n",
      "Batch 3120 - batch loss: 1.8713268041610718 - avg loss: 2.070553200877427   (start: 31200, end: 31210)\n",
      "Batch 3121 - batch loss: 2.1023144721984863 - avg loss: 2.070563374250688   (start: 31210, end: 31220)\n",
      "Batch 3122 - batch loss: 1.5761317014694214 - avg loss: 2.070405054790944   (start: 31220, end: 31230)\n",
      "Batch 3123 - batch loss: 2.306252956390381 - avg loss: 2.0704805502780115   (start: 31230, end: 31240)\n",
      "Batch 3124 - batch loss: 2.054898977279663 - avg loss: 2.070475564174652   (start: 31240, end: 31250)\n",
      "Batch 3125 - batch loss: 1.8394060134887695 - avg loss: 2.0704016455723853   (start: 31250, end: 31260)\n",
      "Batch 3126 - batch loss: 2.0124189853668213 - avg loss: 2.0703831029883735   (start: 31260, end: 31270)\n",
      "Batch 3127 - batch loss: 2.001175880432129 - avg loss: 2.0703609779172236   (start: 31270, end: 31280)\n",
      "Batch 3128 - batch loss: 1.2881598472595215 - avg loss: 2.070110993535422   (start: 31280, end: 31290)\n",
      "Batch 3129 - batch loss: 3.002978563308716 - avg loss: 2.070409034292538   (start: 31290, end: 31300)\n",
      "Batch 3130 - batch loss: 3.436927080154419 - avg loss: 2.0708454820874476   (start: 31300, end: 31310)\n",
      "Batch 3131 - batch loss: 2.842175245285034 - avg loss: 2.0710917559582   (start: 31310, end: 31320)\n",
      "Batch 3132 - batch loss: 2.1176083087921143 - avg loss: 2.0711066032460503   (start: 31320, end: 31330)\n",
      "Batch 3133 - batch loss: 1.9072479009628296 - avg loss: 2.0710543190398334   (start: 31330, end: 31340)\n",
      "Batch 3134 - batch loss: 2.8815927505493164 - avg loss: 2.071312863994063   (start: 31340, end: 31350)\n",
      "Batch 3135 - batch loss: 2.245253562927246 - avg loss: 2.0713683297781618   (start: 31350, end: 31360)\n",
      "Batch 3136 - batch loss: 2.2061703205108643 - avg loss: 2.071411301404152   (start: 31360, end: 31370)\n",
      "Batch 3137 - batch loss: 2.340027332305908 - avg loss: 2.0714969024337577   (start: 31370, end: 31380)\n",
      "Batch 3138 - batch loss: 2.22347092628479 - avg loss: 2.0715453172231335   (start: 31380, end: 31390)\n",
      "Batch 3139 - batch loss: 1.9312318563461304 - avg loss: 2.0715006314075675   (start: 31390, end: 31400)\n",
      "Batch 3140 - batch loss: 1.5087382793426514 - avg loss: 2.071321464787999   (start: 31400, end: 31410)\n",
      "Batch 3141 - batch loss: 2.4745383262634277 - avg loss: 2.0714497960615432   (start: 31410, end: 31420)\n",
      "Batch 3142 - batch loss: 2.056957483291626 - avg loss: 2.0714451850807065   (start: 31420, end: 31430)\n",
      "Batch 3143 - batch loss: 2.0290865898132324 - avg loss: 2.0714317122450616   (start: 31430, end: 31440)\n",
      "Batch 3144 - batch loss: 2.137967824935913 - avg loss: 2.07145286840172   (start: 31440, end: 31450)\n",
      "Batch 3145 - batch loss: 2.1738619804382324 - avg loss: 2.071485420567021   (start: 31450, end: 31460)\n",
      "Batch 3146 - batch loss: 2.9546468257904053 - avg loss: 2.0717660565394462   (start: 31460, end: 31470)\n",
      "Batch 3147 - batch loss: 1.5760576725006104 - avg loss: 2.0716085888189766   (start: 31470, end: 31480)\n",
      "Batch 3148 - batch loss: 1.8473237752914429 - avg loss: 2.071537364680035   (start: 31480, end: 31490)\n",
      "Batch 3149 - batch loss: 2.370516300201416 - avg loss: 2.0716322786278196   (start: 31490, end: 31500)\n",
      "Batch 3150 - batch loss: 2.3266067504882812 - avg loss: 2.0717131972161598   (start: 31500, end: 31510)\n",
      "Batch 3151 - batch loss: 2.1506965160369873 - avg loss: 2.071738255375684   (start: 31510, end: 31520)\n",
      "Batch 3152 - batch loss: 2.3427610397338867 - avg loss: 2.071824212490926   (start: 31520, end: 31530)\n",
      "Batch 3153 - batch loss: 1.4819053411483765 - avg loss: 2.0716371741677357   (start: 31530, end: 31540)\n",
      "Batch 3154 - batch loss: 1.3150668144226074 - avg loss: 2.071397373736755   (start: 31540, end: 31550)\n",
      "Batch 3155 - batch loss: 1.663226842880249 - avg loss: 2.0712680421363565   (start: 31550, end: 31560)\n",
      "Batch 3156 - batch loss: 1.903329610824585 - avg loss: 2.0712148465610283   (start: 31560, end: 31570)\n",
      "Batch 3157 - batch loss: 2.6446139812469482 - avg loss: 2.0713964169013344   (start: 31570, end: 31580)\n",
      "Batch 3158 - batch loss: 1.2080540657043457 - avg loss: 2.0711231208104204   (start: 31580, end: 31590)\n",
      "Batch 3159 - batch loss: 2.2547407150268555 - avg loss: 2.071181227644033   (start: 31590, end: 31600)\n",
      "Batch 3160 - batch loss: 1.6122316122055054 - avg loss: 2.0710360363705633   (start: 31600, end: 31610)\n",
      "Batch 3161 - batch loss: 1.5611884593963623 - avg loss: 2.070874794252608   (start: 31610, end: 31620)\n",
      "Batch 3162 - batch loss: 2.2270278930664062 - avg loss: 2.070924162921218   (start: 31620, end: 31630)\n",
      "Batch 3163 - batch loss: 2.0631401538848877 - avg loss: 2.070921702741371   (start: 31630, end: 31640)\n",
      "Batch 3164 - batch loss: 2.0519180297851562 - avg loss: 2.0709156984213215   (start: 31640, end: 31650)\n",
      "Batch 3165 - batch loss: 2.009524345397949 - avg loss: 2.0708963075959828   (start: 31650, end: 31660)\n",
      "Batch 3166 - batch loss: 2.6486730575561523 - avg loss: 2.0710787442079055   (start: 31660, end: 31670)\n",
      "Batch 3167 - batch loss: 1.5483739376068115 - avg loss: 2.0709137490038017   (start: 31670, end: 31680)\n",
      "Batch 3168 - batch loss: 2.948622226715088 - avg loss: 2.071190716021066   (start: 31680, end: 31690)\n",
      "Batch 3169 - batch loss: 1.0803556442260742 - avg loss: 2.070878149752361   (start: 31690, end: 31700)\n",
      "Batch 3170 - batch loss: 2.2157821655273438 - avg loss: 2.070923846383006   (start: 31700, end: 31710)\n",
      "Batch 3171 - batch loss: 2.2945656776428223 - avg loss: 2.0709943513739453   (start: 31710, end: 31720)\n",
      "Batch 3172 - batch loss: 2.0240254402160645 - avg loss: 2.070979548691576   (start: 31720, end: 31730)\n",
      "Batch 3173 - batch loss: 2.045510768890381 - avg loss: 2.0709715245013425   (start: 31730, end: 31740)\n",
      "Batch 3174 - batch loss: 1.4816371202468872 - avg loss: 2.0707859073661443   (start: 31740, end: 31750)\n",
      "Batch 3175 - batch loss: 1.5043985843658447 - avg loss: 2.0706075738261567   (start: 31750, end: 31760)\n",
      "Batch 3176 - batch loss: 2.633453845977783 - avg loss: 2.070784736643957   (start: 31760, end: 31770)\n",
      "Batch 3177 - batch loss: 2.020177125930786 - avg loss: 2.070768812285646   (start: 31770, end: 31780)\n",
      "Batch 3178 - batch loss: 2.0926880836486816 - avg loss: 2.0707757073065216   (start: 31780, end: 31790)\n",
      "Batch 3179 - batch loss: 2.5317704677581787 - avg loss: 2.070920674212324   (start: 31790, end: 31800)\n",
      "Batch 3180 - batch loss: 0.9274977445602417 - avg loss: 2.0705612202891386   (start: 31800, end: 31810)\n",
      "Batch 3181 - batch loss: 1.8459361791610718 - avg loss: 2.070490627881493   (start: 31810, end: 31820)\n",
      "Batch 3182 - batch loss: 2.1753475666046143 - avg loss: 2.0705235706834797   (start: 31820, end: 31830)\n",
      "Batch 3183 - batch loss: 1.7639080286026 - avg loss: 2.0704272718323233   (start: 31830, end: 31840)\n",
      "Batch 3184 - batch loss: 1.8438901901245117 - avg loss: 2.0703561455900292   (start: 31840, end: 31850)\n",
      "Batch 3185 - batch loss: 1.0623257160186768 - avg loss: 2.070039751858211   (start: 31850, end: 31860)\n",
      "Batch 3186 - batch loss: 2.252610206604004 - avg loss: 2.0700970378496595   (start: 31860, end: 31870)\n",
      "Batch 3187 - batch loss: 1.6212568283081055 - avg loss: 2.0699562473196904   (start: 31870, end: 31880)\n",
      "Batch 3188 - batch loss: 1.1213650703430176 - avg loss: 2.069658790067581   (start: 31880, end: 31890)\n",
      "Batch 3189 - batch loss: 2.735787868499756 - avg loss: 2.0698676079605067   (start: 31890, end: 31900)\n",
      "Batch 3190 - batch loss: 1.3021469116210938 - avg loss: 2.0696270185852828   (start: 31900, end: 31910)\n",
      "Batch 3191 - batch loss: 1.8091930150985718 - avg loss: 2.069545428985193   (start: 31910, end: 31920)\n",
      "Batch 3192 - batch loss: 2.1246440410614014 - avg loss: 2.0695626850491067   (start: 31920, end: 31930)\n",
      "Batch 3193 - batch loss: 1.8780122995376587 - avg loss: 2.0695027131062416   (start: 31930, end: 31940)\n",
      "Batch 3194 - batch loss: 2.8708081245422363 - avg loss: 2.0697535129220275   (start: 31940, end: 31950)\n",
      "Batch 3195 - batch loss: 2.0406229496002197 - avg loss: 2.069744398227621   (start: 31950, end: 31960)\n",
      "Batch 3196 - batch loss: 1.9195687770843506 - avg loss: 2.0696974243079644   (start: 31960, end: 31970)\n",
      "Batch 3197 - batch loss: 2.6347556114196777 - avg loss: 2.0698741154233837   (start: 31970, end: 31980)\n",
      "Batch 3198 - batch loss: 1.7805383205413818 - avg loss: 2.069783669723202   (start: 31980, end: 31990)\n",
      "Batch 3199 - batch loss: 2.3555216789245605 - avg loss: 2.0698729628510772   (start: 31990, end: 32000)\n",
      "Batch 3200 - batch loss: 2.4437780380249023 - avg loss: 2.0699897716843085   (start: 32000, end: 32010)\n",
      "Batch 3201 - batch loss: 1.751611351966858 - avg loss: 2.069890340572592   (start: 32010, end: 32020)\n",
      "Batch 3202 - batch loss: 1.723707914352417 - avg loss: 2.069782259890038   (start: 32020, end: 32030)\n",
      "Batch 3203 - batch loss: 2.8078742027282715 - avg loss: 2.0700126256649565   (start: 32030, end: 32040)\n",
      "Batch 3204 - batch loss: 2.1753344535827637 - avg loss: 2.0700454873897356   (start: 32040, end: 32050)\n",
      "Batch 3205 - batch loss: 1.39454185962677 - avg loss: 2.069834787568225   (start: 32050, end: 32060)\n",
      "Batch 3206 - batch loss: 2.8187901973724365 - avg loss: 2.070068325270066   (start: 32060, end: 32070)\n",
      "Batch 3207 - batch loss: 1.2764827013015747 - avg loss: 2.069820948205238   (start: 32070, end: 32080)\n",
      "Batch 3208 - batch loss: 2.1984760761260986 - avg loss: 2.069861040174051   (start: 32080, end: 32090)\n",
      "Batch 3209 - batch loss: 2.143946886062622 - avg loss: 2.06988411987682   (start: 32090, end: 32100)\n",
      "Batch 3210 - batch loss: 1.7538559436798096 - avg loss: 2.0697856993921744   (start: 32100, end: 32110)\n",
      "Batch 3211 - batch loss: 2.618880271911621 - avg loss: 2.0699566503798827   (start: 32110, end: 32120)\n",
      "Batch 3212 - batch loss: 2.038846254348755 - avg loss: 2.069946967716941   (start: 32120, end: 32130)\n",
      "Batch 3213 - batch loss: 1.3992211818695068 - avg loss: 2.0697382789223404   (start: 32130, end: 32140)\n",
      "Batch 3214 - batch loss: 2.0497090816497803 - avg loss: 2.069732049000949   (start: 32140, end: 32150)\n",
      "Batch 3215 - batch loss: 1.6587774753570557 - avg loss: 2.0696042646185973   (start: 32150, end: 32160)\n",
      "Batch 3216 - batch loss: 2.2617440223693848 - avg loss: 2.0696639909965118   (start: 32160, end: 32170)\n",
      "Batch 3217 - batch loss: 2.029698371887207 - avg loss: 2.0696515715996475   (start: 32170, end: 32180)\n",
      "Batch 3218 - batch loss: 1.2764519453048706 - avg loss: 2.0694051597865704   (start: 32180, end: 32190)\n",
      "Batch 3219 - batch loss: 1.499107003211975 - avg loss: 2.0692280485578207   (start: 32190, end: 32200)\n",
      "Batch 3220 - batch loss: 1.4709463119506836 - avg loss: 2.0690423044607678   (start: 32200, end: 32210)\n",
      "Batch 3221 - batch loss: 2.526146173477173 - avg loss: 2.0691841740662973   (start: 32210, end: 32220)\n",
      "Batch 3222 - batch loss: 1.1034255027770996 - avg loss: 2.068884528186282   (start: 32220, end: 32230)\n",
      "Batch 3223 - batch loss: 1.8516509532928467 - avg loss: 2.068817148045186   (start: 32230, end: 32240)\n",
      "Batch 3224 - batch loss: 2.2234280109405518 - avg loss: 2.068865089398022   (start: 32240, end: 32250)\n",
      "Batch 3225 - batch loss: 2.391639232635498 - avg loss: 2.0689651433791867   (start: 32250, end: 32260)\n",
      "Batch 3226 - batch loss: 2.056856155395508 - avg loss: 2.0689613909812987   (start: 32260, end: 32270)\n",
      "Batch 3227 - batch loss: 1.5773342847824097 - avg loss: 2.0688090901429472   (start: 32270, end: 32280)\n",
      "Batch 3228 - batch loss: 2.151099443435669 - avg loss: 2.068834574922536   (start: 32280, end: 32290)\n",
      "Batch 3229 - batch loss: 2.7268850803375244 - avg loss: 2.0690383057291664   (start: 32290, end: 32300)\n",
      "Batch 3230 - batch loss: 2.4467873573303223 - avg loss: 2.0691552197036636   (start: 32300, end: 32310)\n",
      "Batch 3231 - batch loss: 1.7243423461914062 - avg loss: 2.069048532552206   (start: 32310, end: 32320)\n",
      "Batch 3232 - batch loss: 1.3232033252716064 - avg loss: 2.0688178349935047   (start: 32320, end: 32330)\n",
      "Batch 3233 - batch loss: 2.4429068565368652 - avg loss: 2.0689335087787684   (start: 32330, end: 32340)\n",
      "Batch 3234 - batch loss: 2.4968440532684326 - avg loss: 2.069065784063   (start: 32340, end: 32350)\n",
      "Batch 3235 - batch loss: 1.5504051446914673 - avg loss: 2.068905505744282   (start: 32350, end: 32360)\n",
      "Batch 3236 - batch loss: 3.208805561065674 - avg loss: 2.06925765281111   (start: 32360, end: 32370)\n",
      "Batch 3237 - batch loss: 2.308929443359375 - avg loss: 2.069331671276381   (start: 32370, end: 32380)\n",
      "Batch 3238 - batch loss: 2.1094040870666504 - avg loss: 2.069344043124418   (start: 32380, end: 32390)\n",
      "Batch 3239 - batch loss: 1.6486698389053345 - avg loss: 2.069214205407066   (start: 32390, end: 32400)\n",
      "Batch 3240 - batch loss: 1.9576936960220337 - avg loss: 2.0691797961169134   (start: 32400, end: 32410)\n",
      "Batch 3241 - batch loss: 1.5860183238983154 - avg loss: 2.0690307642007446   (start: 32410, end: 32420)\n",
      "Batch 3242 - batch loss: 1.7045907974243164 - avg loss: 2.0689183867826824   (start: 32420, end: 32430)\n",
      "Batch 3243 - batch loss: 2.070230007171631 - avg loss: 2.068918791104627   (start: 32430, end: 32440)\n",
      "Batch 3244 - batch loss: 2.4751806259155273 - avg loss: 2.0690439873557245   (start: 32440, end: 32450)\n",
      "Batch 3245 - batch loss: 1.9344415664672852 - avg loss: 2.0690025201897084   (start: 32450, end: 32460)\n",
      "Batch 3246 - batch loss: 1.501778483390808 - avg loss: 2.0688278284629456   (start: 32460, end: 32470)\n",
      "Batch 3247 - batch loss: 3.040306806564331 - avg loss: 2.0691269291335432   (start: 32470, end: 32480)\n",
      "Batch 3248 - batch loss: 1.4097509384155273 - avg loss: 2.068923981767979   (start: 32480, end: 32490)\n",
      "Batch 3249 - batch loss: 1.6235573291778564 - avg loss: 2.068786945874874   (start: 32490, end: 32500)\n",
      "Batch 3250 - batch loss: 2.132841110229492 - avg loss: 2.068806648786088   (start: 32500, end: 32510)\n",
      "Batch 3251 - batch loss: 2.286353349685669 - avg loss: 2.068873545065577   (start: 32510, end: 32520)\n",
      "Batch 3252 - batch loss: 2.5008814334869385 - avg loss: 2.069006347982399   (start: 32520, end: 32530)\n",
      "Batch 3253 - batch loss: 1.8993908166885376 - avg loss: 2.0689542227422963   (start: 32530, end: 32540)\n",
      "Batch 3254 - batch loss: 2.248601198196411 - avg loss: 2.0690094138253854   (start: 32540, end: 32550)\n",
      "Batch 3255 - batch loss: 0.9160265922546387 - avg loss: 2.0686553036222   (start: 32550, end: 32560)\n",
      "Batch 3256 - batch loss: 2.4135451316833496 - avg loss: 2.0687611954944938   (start: 32560, end: 32570)\n",
      "Batch 3257 - batch loss: 2.3529298305511475 - avg loss: 2.068848417297765   (start: 32570, end: 32580)\n",
      "Batch 3258 - batch loss: 2.028193950653076 - avg loss: 2.0688359427759346   (start: 32580, end: 32590)\n",
      "Batch 3259 - batch loss: 2.223865032196045 - avg loss: 2.0688834977113397   (start: 32590, end: 32600)\n",
      "Batch 3260 - batch loss: 2.244608163833618 - avg loss: 2.0689373844534806   (start: 32600, end: 32610)\n",
      "Batch 3261 - batch loss: 2.379213809967041 - avg loss: 2.069032502916238   (start: 32610, end: 32620)\n",
      "Batch 3262 - batch loss: 1.972320318222046 - avg loss: 2.069002863877104   (start: 32620, end: 32630)\n",
      "Batch 3263 - batch loss: 2.4729344844818115 - avg loss: 2.069126617437338   (start: 32630, end: 32640)\n",
      "Batch 3264 - batch loss: 1.9946037530899048 - avg loss: 2.06910379267031   (start: 32640, end: 32650)\n",
      "Batch 3265 - batch loss: 1.4192938804626465 - avg loss: 2.0689048306641227   (start: 32650, end: 32660)\n",
      "Batch 3266 - batch loss: 2.709057569503784 - avg loss: 2.069100775793856   (start: 32660, end: 32670)\n",
      "Batch 3267 - batch loss: 1.3127949237823486 - avg loss: 2.0688693480545624   (start: 32670, end: 32680)\n",
      "Batch 3268 - batch loss: 3.074744701385498 - avg loss: 2.0691770492944923   (start: 32680, end: 32690)\n",
      "Batch 3269 - batch loss: 1.966068983078003 - avg loss: 2.069145517775772   (start: 32690, end: 32700)\n",
      "Batch 3270 - batch loss: 2.4033803939819336 - avg loss: 2.0692476990280513   (start: 32700, end: 32710)\n",
      "Batch 3271 - batch loss: 2.461928606033325 - avg loss: 2.0693677115301923   (start: 32710, end: 32720)\n",
      "Batch 3272 - batch loss: 0.8897817730903625 - avg loss: 2.069007312526697   (start: 32720, end: 32730)\n",
      "Batch 3273 - batch loss: 2.6319522857666016 - avg loss: 2.0691792566235936   (start: 32730, end: 32740)\n",
      "Batch 3274 - batch loss: 2.843743324279785 - avg loss: 2.0694157647358553   (start: 32740, end: 32750)\n",
      "Batch 3275 - batch loss: 1.5996743440628052 - avg loss: 2.069272376023806   (start: 32750, end: 32760)\n",
      "Batch 3276 - batch loss: 1.458580732345581 - avg loss: 2.069086019098668   (start: 32760, end: 32770)\n",
      "Batch 3277 - batch loss: 2.669419527053833 - avg loss: 2.069269159278032   (start: 32770, end: 32780)\n",
      "Batch 3278 - batch loss: 1.8313140869140625 - avg loss: 2.069196589875054   (start: 32780, end: 32790)\n",
      "Batch 3279 - batch loss: 2.25787615776062 - avg loss: 2.069254114133556   (start: 32790, end: 32800)\n",
      "Batch 3280 - batch loss: 2.5546040534973145 - avg loss: 2.0694020415762147   (start: 32800, end: 32810)\n",
      "Batch 3281 - batch loss: 1.55574631690979 - avg loss: 2.0692455346521847   (start: 32810, end: 32820)\n",
      "Batch 3282 - batch loss: 2.625319004058838 - avg loss: 2.0694149143260825   (start: 32820, end: 32830)\n",
      "Batch 3283 - batch loss: 1.8672685623168945 - avg loss: 2.0693533594076876   (start: 32830, end: 32840)\n",
      "Batch 3284 - batch loss: 1.9120347499847412 - avg loss: 2.0693054694200397   (start: 32840, end: 32850)\n",
      "Batch 3285 - batch loss: 1.9610522985458374 - avg loss: 2.069272525667491   (start: 32850, end: 32860)\n",
      "Batch 3286 - batch loss: 2.1051688194274902 - avg loss: 2.06928344635315   (start: 32860, end: 32870)\n",
      "Batch 3287 - batch loss: 2.7630739212036133 - avg loss: 2.069494453188567   (start: 32870, end: 32880)\n",
      "Batch 3288 - batch loss: 2.566305637359619 - avg loss: 2.0696455055400933   (start: 32880, end: 32890)\n",
      "Batch 3289 - batch loss: 3.2064902782440186 - avg loss: 2.0699910510637114   (start: 32890, end: 32900)\n",
      "Batch 3290 - batch loss: 2.167619466781616 - avg loss: 2.0700207163374027   (start: 32900, end: 32910)\n",
      "Batch 3291 - batch loss: 1.3043596744537354 - avg loss: 2.069788134003902   (start: 32910, end: 32920)\n",
      "Batch 3292 - batch loss: 1.6187556982040405 - avg loss: 2.0696511669720774   (start: 32920, end: 32930)\n",
      "Batch 3293 - batch loss: 1.468178391456604 - avg loss: 2.0694685705010647   (start: 32930, end: 32940)\n",
      "Batch 3294 - batch loss: 2.316819667816162 - avg loss: 2.06954363911937   (start: 32940, end: 32950)\n",
      "Batch 3295 - batch loss: 1.771451711654663 - avg loss: 2.069453198607396   (start: 32950, end: 32960)\n",
      "Batch 3296 - batch loss: 2.0430891513824463 - avg loss: 2.0694452022327448   (start: 32960, end: 32970)\n",
      "Batch 3297 - batch loss: 2.0419552326202393 - avg loss: 2.0694368668871985   (start: 32970, end: 32980)\n",
      "Batch 3298 - batch loss: 1.9160354137420654 - avg loss: 2.0693903675076455   (start: 32980, end: 32990)\n",
      "Batch 3299 - batch loss: 1.4471328258514404 - avg loss: 2.0692018046162346   (start: 32990, end: 33000)\n",
      "Batch 3300 - batch loss: 2.3242650032043457 - avg loss: 2.0692790730799087   (start: 33000, end: 33010)\n",
      "Batch 3301 - batch loss: 2.2326419353485107 - avg loss: 2.069328546993376   (start: 33010, end: 33020)\n",
      "Batch 3302 - batch loss: 2.6941614151000977 - avg loss: 2.069517718312815   (start: 33020, end: 33030)\n",
      "Batch 3303 - batch loss: 2.2340943813323975 - avg loss: 2.069567529651501   (start: 33030, end: 33040)\n",
      "Batch 3304 - batch loss: 2.5884132385253906 - avg loss: 2.069724517763112   (start: 33040, end: 33050)\n",
      "Batch 3305 - batch loss: 2.3581695556640625 - avg loss: 2.069811766715895   (start: 33050, end: 33060)\n",
      "Batch 3306 - batch loss: 1.896759271621704 - avg loss: 2.0697594375670914   (start: 33060, end: 33070)\n",
      "Batch 3307 - batch loss: 1.6745176315307617 - avg loss: 2.0696399569727633   (start: 33070, end: 33080)\n",
      "Batch 3308 - batch loss: 1.9768327474594116 - avg loss: 2.069611910067501   (start: 33080, end: 33090)\n",
      "Batch 3309 - batch loss: 1.888527512550354 - avg loss: 2.0695572017903054   (start: 33090, end: 33100)\n",
      "Batch 3310 - batch loss: 1.5607013702392578 - avg loss: 2.069403515341634   (start: 33100, end: 33110)\n",
      "Batch 3311 - batch loss: 2.509681463241577 - avg loss: 2.069536449504647   (start: 33110, end: 33120)\n",
      "Batch 3312 - batch loss: 2.5469651222229004 - avg loss: 2.06968055716318   (start: 33120, end: 33130)\n",
      "Batch 3313 - batch loss: 1.66543710231781 - avg loss: 2.06955857663969   (start: 33130, end: 33140)\n",
      "Batch 3314 - batch loss: 1.9983978271484375 - avg loss: 2.0695371103502507   (start: 33140, end: 33150)\n",
      "Batch 3315 - batch loss: 1.789947509765625 - avg loss: 2.0694527950304122   (start: 33150, end: 33160)\n",
      "Batch 3316 - batch loss: 1.7355775833129883 - avg loss: 2.0693521392535903   (start: 33160, end: 33170)\n",
      "Batch 3317 - batch loss: 2.5416688919067383 - avg loss: 2.0694944890886275   (start: 33170, end: 33180)\n",
      "Batch 3318 - batch loss: 2.1868577003479004 - avg loss: 2.069529850104373   (start: 33180, end: 33190)\n",
      "Batch 3319 - batch loss: 1.2846367359161377 - avg loss: 2.0692934365157623   (start: 33190, end: 33200)\n",
      "Batch 3320 - batch loss: 1.898075819015503 - avg loss: 2.0692418804731543   (start: 33200, end: 33210)\n",
      "Batch 3321 - batch loss: 2.456092596054077 - avg loss: 2.069358331621734   (start: 33210, end: 33220)\n",
      "Batch 3322 - batch loss: 2.747558355331421 - avg loss: 2.0695624243161994   (start: 33220, end: 33230)\n",
      "Batch 3323 - batch loss: 2.118335247039795 - avg loss: 2.0695770972472234   (start: 33230, end: 33240)\n",
      "Batch 3324 - batch loss: 2.1849565505981445 - avg loss: 2.0696117978346975   (start: 33240, end: 33250)\n",
      "Batch 3325 - batch loss: 2.398176670074463 - avg loss: 2.0697105846273134   (start: 33250, end: 33260)\n",
      "Batch 3326 - batch loss: 3.357531785964966 - avg loss: 2.0700976664431647   (start: 33260, end: 33270)\n",
      "Batch 3327 - batch loss: 2.148831605911255 - avg loss: 2.0701213244778605   (start: 33270, end: 33280)\n",
      "Batch 3328 - batch loss: 1.4122483730316162 - avg loss: 2.0699237056879998   (start: 33280, end: 33290)\n",
      "Batch 3329 - batch loss: 2.263122081756592 - avg loss: 2.069981723218351   (start: 33290, end: 33300)\n",
      "Batch 3330 - batch loss: 1.7091995477676392 - avg loss: 2.0698734127483864   (start: 33300, end: 33310)\n",
      "Batch 3331 - batch loss: 2.447218179702759 - avg loss: 2.069986661477965   (start: 33310, end: 33320)\n",
      "Batch 3332 - batch loss: 2.174466133117676 - avg loss: 2.0700180084541544   (start: 33320, end: 33330)\n",
      "Batch 3333 - batch loss: 2.2149250507354736 - avg loss: 2.070061471874155   (start: 33330, end: 33340)\n",
      "Batch 3334 - batch loss: 1.4640623331069946 - avg loss: 2.069879762986968   (start: 33340, end: 33350)\n",
      "Batch 3335 - batch loss: 1.893572449684143 - avg loss: 2.0698269130729083   (start: 33350, end: 33360)\n",
      "Batch 3336 - batch loss: 2.334521770477295 - avg loss: 2.0699062342768055   (start: 33360, end: 33370)\n",
      "Batch 3337 - batch loss: 2.1357836723327637 - avg loss: 2.0699259698783803   (start: 33370, end: 33380)\n",
      "Batch 3338 - batch loss: 2.0963778495788574 - avg loss: 2.0699338919747263   (start: 33380, end: 33390)\n",
      "Batch 3339 - batch loss: 1.1372605562210083 - avg loss: 2.069654648461028   (start: 33390, end: 33400)\n",
      "Batch 3340 - batch loss: 1.6667009592056274 - avg loss: 2.0695340397542767   (start: 33400, end: 33410)\n",
      "Batch 3341 - batch loss: 1.2709169387817383 - avg loss: 2.069295075929928   (start: 33410, end: 33420)\n",
      "Batch 3342 - batch loss: 2.4615702629089355 - avg loss: 2.069412418193458   (start: 33420, end: 33430)\n",
      "Batch 3343 - batch loss: 1.9949729442596436 - avg loss: 2.069390157585224   (start: 33430, end: 33440)\n",
      "Batch 3344 - batch loss: 2.102544069290161 - avg loss: 2.0694000690685437   (start: 33440, end: 33450)\n",
      "Batch 3345 - batch loss: 1.9278547763824463 - avg loss: 2.069357766231519   (start: 33450, end: 33460)\n",
      "Batch 3346 - batch loss: 1.9041951894760132 - avg loss: 2.0693084197789475   (start: 33460, end: 33470)\n",
      "Batch 3347 - batch loss: 2.997607707977295 - avg loss: 2.06958568957829   (start: 33470, end: 33480)\n",
      "Batch 3348 - batch loss: 1.3787498474121094 - avg loss: 2.0693794083474253   (start: 33480, end: 33490)\n",
      "Batch 3349 - batch loss: 1.8831897974014282 - avg loss: 2.069323829359083   (start: 33490, end: 33500)\n",
      "Batch 3350 - batch loss: 2.2216949462890625 - avg loss: 2.069369299701348   (start: 33500, end: 33510)\n",
      "Batch 3351 - batch loss: 2.656714916229248 - avg loss: 2.0695445221406463   (start: 33510, end: 33520)\n",
      "Batch 3352 - batch loss: 3.1796460151672363 - avg loss: 2.0698755992337055   (start: 33520, end: 33530)\n",
      "Batch 3353 - batch loss: 1.5870673656463623 - avg loss: 2.0697316492535065   (start: 33530, end: 33540)\n",
      "Batch 3354 - batch loss: 2.2900795936584473 - avg loss: 2.0697973267332097   (start: 33540, end: 33550)\n",
      "Batch 3355 - batch loss: 2.1132118701934814 - avg loss: 2.0698102631287583   (start: 33550, end: 33560)\n",
      "Batch 3356 - batch loss: 1.7416397333145142 - avg loss: 2.0697125060451076   (start: 33560, end: 33570)\n",
      "Batch 3357 - batch loss: 1.7363243103027344 - avg loss: 2.0696132242715097   (start: 33570, end: 33580)\n",
      "Batch 3358 - batch loss: 1.925368070602417 - avg loss: 2.06957028138563   (start: 33580, end: 33590)\n",
      "Batch 3359 - batch loss: 2.7570903301239014 - avg loss: 2.069774900447755   (start: 33590, end: 33600)\n",
      "Batch 3360 - batch loss: 2.031813621520996 - avg loss: 2.0697636058095736   (start: 33600, end: 33610)\n",
      "Batch 3361 - batch loss: 1.7891504764556885 - avg loss: 2.069680139679486   (start: 33610, end: 33620)\n",
      "Batch 3362 - batch loss: 2.6913347244262695 - avg loss: 2.0698649908792324   (start: 33620, end: 33630)\n",
      "Batch 3363 - batch loss: 1.109154224395752 - avg loss: 2.0695794050390175   (start: 33630, end: 33640)\n",
      "Batch 3364 - batch loss: 2.2514286041259766 - avg loss: 2.0696334464057595   (start: 33640, end: 33650)\n",
      "Batch 3365 - batch loss: 2.2803094387054443 - avg loss: 2.06969603582712   (start: 33650, end: 33660)\n",
      "Batch 3366 - batch loss: 1.1796531677246094 - avg loss: 2.0694316928309506   (start: 33660, end: 33670)\n",
      "Batch 3367 - batch loss: 1.67855703830719 - avg loss: 2.0693156374109614   (start: 33670, end: 33680)\n",
      "Batch 3368 - batch loss: 2.3987154960632324 - avg loss: 2.0694134111891307   (start: 33680, end: 33690)\n",
      "Batch 3369 - batch loss: 1.6894899606704712 - avg loss: 2.069300674260193   (start: 33690, end: 33700)\n",
      "Batch 3370 - batch loss: 1.5854012966156006 - avg loss: 2.0691571265361812   (start: 33700, end: 33710)\n",
      "Batch 3371 - batch loss: 1.4775069952011108 - avg loss: 2.0689816668293797   (start: 33710, end: 33720)\n",
      "Batch 3372 - batch loss: 2.449368476867676 - avg loss: 2.06909444086141   (start: 33720, end: 33730)\n",
      "Batch 3373 - batch loss: 1.9376474618911743 - avg loss: 2.0690554820650346   (start: 33730, end: 33740)\n",
      "Batch 3374 - batch loss: 1.8917226791381836 - avg loss: 2.0690029390123157   (start: 33740, end: 33750)\n",
      "Batch 3375 - batch loss: 2.276028633117676 - avg loss: 2.0690642617890056   (start: 33750, end: 33760)\n",
      "Batch 3376 - batch loss: 1.7913026809692383 - avg loss: 2.0689820108026806   (start: 33760, end: 33770)\n",
      "Batch 3377 - batch loss: 2.4907426834106445 - avg loss: 2.0691068659455483   (start: 33770, end: 33780)\n",
      "Batch 3378 - batch loss: 2.4652175903320312 - avg loss: 2.069224093150161   (start: 33780, end: 33790)\n",
      "Batch 3379 - batch loss: 1.9347347021102905 - avg loss: 2.0691843033894983   (start: 33790, end: 33800)\n",
      "Batch 3380 - batch loss: 1.5495197772979736 - avg loss: 2.069030601962083   (start: 33800, end: 33810)\n",
      "Batch 3381 - batch loss: 1.4624416828155518 - avg loss: 2.0688512439138433   (start: 33810, end: 33820)\n",
      "Batch 3382 - batch loss: 2.12825345993042 - avg loss: 2.0688688029490243   (start: 33820, end: 33830)\n",
      "Batch 3383 - batch loss: 1.9471060037612915 - avg loss: 2.0688328210343707   (start: 33830, end: 33840)\n",
      "Batch 3384 - batch loss: 2.2657618522644043 - avg loss: 2.0688909980007604   (start: 33840, end: 33850)\n",
      "Batch 3385 - batch loss: 2.176827907562256 - avg loss: 2.0689228754105544   (start: 33850, end: 33860)\n",
      "Batch 3386 - batch loss: 2.2362723350524902 - avg loss: 2.0689722847579537   (start: 33860, end: 33870)\n",
      "Batch 3387 - batch loss: 2.2965645790100098 - avg loss: 2.0690394607597993   (start: 33870, end: 33880)\n",
      "Batch 3388 - batch loss: 1.7289994955062866 - avg loss: 2.0689391243876383   (start: 33880, end: 33890)\n",
      "Batch 3389 - batch loss: 1.5301897525787354 - avg loss: 2.0687802012691106   (start: 33890, end: 33900)\n",
      "Batch 3390 - batch loss: 2.6579055786132812 - avg loss: 2.06895393331787   (start: 33900, end: 33910)\n",
      "Batch 3391 - batch loss: 3.204303026199341 - avg loss: 2.06928864708346   (start: 33910, end: 33920)\n",
      "Batch 3392 - batch loss: 1.8450918197631836 - avg loss: 2.0692225708007252   (start: 33920, end: 33930)\n",
      "Batch 3393 - batch loss: 1.266669511795044 - avg loss: 2.068986108496952   (start: 33930, end: 33940)\n",
      "Batch 3394 - batch loss: 1.0702216625213623 - avg loss: 2.068691921620376   (start: 33940, end: 33950)\n",
      "Batch 3395 - batch loss: 2.126582622528076 - avg loss: 2.068708968352092   (start: 33950, end: 33960)\n",
      "Batch 3396 - batch loss: 1.5215083360671997 - avg loss: 2.0685478848571597   (start: 33960, end: 33970)\n",
      "Batch 3397 - batch loss: 2.7263104915618896 - avg loss: 2.0687414583141064   (start: 33970, end: 33980)\n",
      "Batch 3398 - batch loss: 2.4961190223693848 - avg loss: 2.0688671945789063   (start: 33980, end: 33990)\n",
      "Batch 3399 - batch loss: 2.751779317855835 - avg loss: 2.0690680510857526   (start: 33990, end: 34000)\n",
      "Batch 3400 - batch loss: 2.147841453552246 - avg loss: 2.0690912129212324   (start: 34000, end: 34010)\n",
      "Batch 3401 - batch loss: 1.9368776082992554 - avg loss: 2.0690523494278104   (start: 34010, end: 34020)\n",
      "Batch 3402 - batch loss: 2.7337844371795654 - avg loss: 2.069247686509136   (start: 34020, end: 34030)\n",
      "Batch 3403 - batch loss: 1.3716508150100708 - avg loss: 2.069042752058049   (start: 34030, end: 34040)\n",
      "Batch 3404 - batch loss: 2.3144121170043945 - avg loss: 2.069114813545552   (start: 34040, end: 34050)\n",
      "Batch 3405 - batch loss: 1.9235652685165405 - avg loss: 2.069072080267505   (start: 34050, end: 34060)\n",
      "Batch 3406 - batch loss: 2.7748541831970215 - avg loss: 2.069279236740334   (start: 34060, end: 34070)\n",
      "Batch 3407 - batch loss: 1.4070148468017578 - avg loss: 2.0690849103348357   (start: 34070, end: 34080)\n",
      "Batch 3408 - batch loss: 1.7450144290924072 - avg loss: 2.068989847125319   (start: 34080, end: 34090)\n",
      "Batch 3409 - batch loss: 2.3101212978363037 - avg loss: 2.069060560160718   (start: 34090, end: 34100)\n",
      "Batch 3410 - batch loss: 2.296928882598877 - avg loss: 2.06912736412508   (start: 34100, end: 34110)\n",
      "Batch 3411 - batch loss: 1.84284245967865 - avg loss: 2.0690610438131083   (start: 34110, end: 34120)\n",
      "Batch 3412 - batch loss: 2.106884479522705 - avg loss: 2.069072125980032   (start: 34120, end: 34130)\n",
      "Batch 3413 - batch loss: 2.0758566856384277 - avg loss: 2.0690741132558546   (start: 34130, end: 34140)\n",
      "Batch 3414 - batch loss: 3.075997829437256 - avg loss: 2.0693689664670347   (start: 34140, end: 34150)\n",
      "Batch 3415 - batch loss: 1.5920730829238892 - avg loss: 2.069229242847731   (start: 34150, end: 34160)\n",
      "Batch 3416 - batch loss: 1.9716752767562866 - avg loss: 2.069200693252738   (start: 34160, end: 34170)\n",
      "Batch 3417 - batch loss: 1.7446590662002563 - avg loss: 2.069105742513401   (start: 34170, end: 34180)\n",
      "Batch 3418 - batch loss: 2.5266153812408447 - avg loss: 2.069239556388431   (start: 34180, end: 34190)\n",
      "Batch 3419 - batch loss: 1.42354416847229 - avg loss: 2.069050756567403   (start: 34190, end: 34200)\n",
      "Batch 3420 - batch loss: 1.662520170211792 - avg loss: 2.0689319227216396   (start: 34200, end: 34210)\n",
      "Batch 3421 - batch loss: 2.3028249740600586 - avg loss: 2.0690002725320835   (start: 34210, end: 34220)\n",
      "Batch 3422 - batch loss: 2.810037136077881 - avg loss: 2.0692167600762104   (start: 34220, end: 34230)\n",
      "Batch 3423 - batch loss: 1.5546138286590576 - avg loss: 2.069066467163997   (start: 34230, end: 34240)\n",
      "Batch 3424 - batch loss: 3.03037691116333 - avg loss: 2.069347141746187   (start: 34240, end: 34250)\n",
      "Batch 3425 - batch loss: 2.514116048812866 - avg loss: 2.069476963376971   (start: 34250, end: 34260)\n",
      "Batch 3426 - batch loss: 1.999634027481079 - avg loss: 2.0694565831797442   (start: 34260, end: 34270)\n",
      "Batch 3427 - batch loss: 3.065420627593994 - avg loss: 2.0697471211156877   (start: 34270, end: 34280)\n",
      "Batch 3428 - batch loss: 2.424297332763672 - avg loss: 2.069850518669391   (start: 34280, end: 34290)\n",
      "Batch 3429 - batch loss: 2.560525894165039 - avg loss: 2.069993572714725   (start: 34290, end: 34300)\n",
      "Batch 3430 - batch loss: 1.5206689834594727 - avg loss: 2.069833466451462   (start: 34300, end: 34310)\n",
      "Batch 3431 - batch loss: 1.6553434133529663 - avg loss: 2.0697126942914683   (start: 34310, end: 34320)\n",
      "Batch 3432 - batch loss: 2.9240479469299316 - avg loss: 2.0699615539630787   (start: 34320, end: 34330)\n",
      "Batch 3433 - batch loss: 2.7554821968078613 - avg loss: 2.070161181407122   (start: 34330, end: 34340)\n",
      "Batch 3434 - batch loss: 2.0817739963531494 - avg loss: 2.0701645621392752   (start: 34340, end: 34350)\n",
      "Batch 3435 - batch loss: 2.4663326740264893 - avg loss: 2.070279861356937   (start: 34350, end: 34360)\n",
      "Batch 3436 - batch loss: 1.871991753578186 - avg loss: 2.070222169152172   (start: 34360, end: 34370)\n",
      "Batch 3437 - batch loss: 2.4336955547332764 - avg loss: 2.0703278914865466   (start: 34370, end: 34380)\n",
      "Batch 3438 - batch loss: 2.557298183441162 - avg loss: 2.070469493781387   (start: 34380, end: 34390)\n",
      "Batch 3439 - batch loss: 2.264742374420166 - avg loss: 2.070525968455991   (start: 34390, end: 34400)\n",
      "Batch 3440 - batch loss: 1.6584558486938477 - avg loss: 2.0704062154424014   (start: 34400, end: 34410)\n",
      "Batch 3441 - batch loss: 1.4917054176330566 - avg loss: 2.0702380862158445   (start: 34410, end: 34420)\n",
      "Batch 3442 - batch loss: 1.841123342514038 - avg loss: 2.0701715411261836   (start: 34420, end: 34430)\n",
      "Batch 3443 - batch loss: 2.1525282859802246 - avg loss: 2.0701954542344456   (start: 34430, end: 34440)\n",
      "Batch 3444 - batch loss: 2.50630259513855 - avg loss: 2.0703220455670737   (start: 34440, end: 34450)\n",
      "Batch 3445 - batch loss: 1.6920678615570068 - avg loss: 2.0702122794080458   (start: 34450, end: 34460)\n",
      "Batch 3446 - batch loss: 1.486348032951355 - avg loss: 2.0700428961047512   (start: 34460, end: 34470)\n",
      "Batch 3447 - batch loss: 1.5142790079116821 - avg loss: 2.0698817116824215   (start: 34470, end: 34480)\n",
      "Batch 3448 - batch loss: 2.0149519443511963 - avg loss: 2.069865785394416   (start: 34480, end: 34490)\n",
      "Batch 3449 - batch loss: 1.6331106424331665 - avg loss: 2.0697391897008037   (start: 34490, end: 34500)\n",
      "Batch 3450 - batch loss: 3.1039421558380127 - avg loss: 2.0700388718121157   (start: 34500, end: 34510)\n",
      "Batch 3451 - batch loss: 1.5375701189041138 - avg loss: 2.0698846224630696   (start: 34510, end: 34520)\n",
      "Batch 3452 - batch loss: 2.5510191917419434 - avg loss: 2.0700239605949196   (start: 34520, end: 34530)\n",
      "Batch 3453 - batch loss: 2.199063777923584 - avg loss: 2.070061320125125   (start: 34530, end: 34540)\n",
      "Batch 3454 - batch loss: 2.508667230606079 - avg loss: 2.0701882682902424   (start: 34540, end: 34550)\n",
      "Batch 3455 - batch loss: 1.301271915435791 - avg loss: 2.069965780919625   (start: 34550, end: 34560)\n",
      "Batch 3456 - batch loss: 2.023812770843506 - avg loss: 2.0699524303237102   (start: 34560, end: 34570)\n",
      "Batch 3457 - batch loss: 2.330183506011963 - avg loss: 2.070027685117142   (start: 34570, end: 34580)\n",
      "Batch 3458 - batch loss: 2.069427967071533 - avg loss: 2.070027511738118   (start: 34580, end: 34590)\n",
      "Batch 3459 - batch loss: 1.7348906993865967 - avg loss: 2.069930651387727   (start: 34590, end: 34600)\n",
      "Batch 3460 - batch loss: 2.4188320636749268 - avg loss: 2.0700314608105206   (start: 34600, end: 34610)\n",
      "Batch 3461 - batch loss: 2.5905487537384033 - avg loss: 2.07018181242604   (start: 34610, end: 34620)\n",
      "Batch 3462 - batch loss: 1.7911052703857422 - avg loss: 2.0701012243399757   (start: 34620, end: 34630)\n",
      "Batch 3463 - batch loss: 2.584652900695801 - avg loss: 2.070249766971718   (start: 34630, end: 34640)\n",
      "Batch 3464 - batch loss: 2.3328969478607178 - avg loss: 2.070325567023923   (start: 34640, end: 34650)\n",
      "Batch 3465 - batch loss: 1.570682406425476 - avg loss: 2.0701814114669124   (start: 34650, end: 34660)\n",
      "Batch 3466 - batch loss: 2.707064151763916 - avg loss: 2.0703651099786793   (start: 34660, end: 34670)\n",
      "Batch 3467 - batch loss: 1.7835931777954102 - avg loss: 2.0702824191101143   (start: 34670, end: 34680)\n",
      "Batch 3468 - batch loss: 2.0919277667999268 - avg loss: 2.070288658760645   (start: 34680, end: 34690)\n",
      "Batch 3469 - batch loss: 1.728839635848999 - avg loss: 2.070190258465858   (start: 34690, end: 34700)\n",
      "Batch 3470 - batch loss: 2.833158016204834 - avg loss: 2.070410070553941   (start: 34700, end: 34710)\n",
      "Batch 3471 - batch loss: 1.70166015625 - avg loss: 2.0703038637813886   (start: 34710, end: 34720)\n",
      "Batch 3472 - batch loss: 2.482518196105957 - avg loss: 2.0704225549222826   (start: 34720, end: 34730)\n",
      "Batch 3473 - batch loss: 2.530036211013794 - avg loss: 2.0705548559171274   (start: 34730, end: 34740)\n",
      "Batch 3474 - batch loss: 1.665804147720337 - avg loss: 2.0704383808931857   (start: 34740, end: 34750)\n",
      "Batch 3475 - batch loss: 1.7278963327407837 - avg loss: 2.0703398360001617   (start: 34750, end: 34760)\n",
      "Batch 3476 - batch loss: 2.0272915363311768 - avg loss: 2.07032745512594   (start: 34760, end: 34770)\n",
      "Batch 3477 - batch loss: 2.1583924293518066 - avg loss: 2.0703527757050733   (start: 34770, end: 34780)\n",
      "Batch 3478 - batch loss: 1.944024682044983 - avg loss: 2.070316464094363   (start: 34780, end: 34790)\n",
      "Batch 3479 - batch loss: 2.3679094314575195 - avg loss: 2.07040197931487   (start: 34790, end: 34800)\n",
      "Batch 3480 - batch loss: 2.3079898357391357 - avg loss: 2.0704702320745434   (start: 34800, end: 34810)\n",
      "Batch 3481 - batch loss: 2.5118050575256348 - avg loss: 2.070596979583289   (start: 34810, end: 34820)\n",
      "Batch 3482 - batch loss: 1.8172671794891357 - avg loss: 2.070524246364772   (start: 34820, end: 34830)\n",
      "Batch 3483 - batch loss: 1.6109358072280884 - avg loss: 2.07039233234665   (start: 34830, end: 34840)\n",
      "Batch 3484 - batch loss: 1.6364701986312866 - avg loss: 2.0702678209739913   (start: 34840, end: 34850)\n",
      "Batch 3485 - batch loss: 2.1175084114074707 - avg loss: 2.0702813724916145   (start: 34850, end: 34860)\n",
      "Batch 3486 - batch loss: 1.5924670696258545 - avg loss: 2.0701443451607093   (start: 34860, end: 34870)\n",
      "Batch 3487 - batch loss: 1.8856855630874634 - avg loss: 2.070091461335574   (start: 34870, end: 34880)\n",
      "Batch 3488 - batch loss: 2.4915547370910645 - avg loss: 2.0702122590643657   (start: 34880, end: 34890)\n",
      "Batch 3489 - batch loss: 1.6778558492660522 - avg loss: 2.070099836024309   (start: 34890, end: 34900)\n",
      "Batch 3490 - batch loss: 2.5475881099700928 - avg loss: 2.0702366129575505   (start: 34900, end: 34910)\n",
      "Batch 3491 - batch loss: 1.7811863422393799 - avg loss: 2.0701538379659357   (start: 34910, end: 34920)\n",
      "Batch 3492 - batch loss: 2.237576961517334 - avg loss: 2.070201769006174   (start: 34920, end: 34930)\n",
      "Batch 3493 - batch loss: 1.7355257272720337 - avg loss: 2.0701059830755115   (start: 34930, end: 34940)\n",
      "Batch 3494 - batch loss: 1.5095293521881104 - avg loss: 2.069945589189707   (start: 34940, end: 34950)\n",
      "Batch 3495 - batch loss: 2.9582769870758057 - avg loss: 2.0701996885598115   (start: 34950, end: 34960)\n",
      "Batch 3496 - batch loss: 1.5509483814239502 - avg loss: 2.0700512037708108   (start: 34960, end: 34970)\n",
      "Batch 3497 - batch loss: 1.8518342971801758 - avg loss: 2.069988820435593   (start: 34970, end: 34980)\n",
      "Batch 3498 - batch loss: 2.7578139305114746 - avg loss: 2.0701853980606506   (start: 34980, end: 34990)\n",
      "Batch 3499 - batch loss: 1.9391227960586548 - avg loss: 2.070147951602936   (start: 34990, end: 35000)\n",
      "Batch 3500 - batch loss: 1.756212830543518 - avg loss: 2.0700582814740986   (start: 35000, end: 35010)\n",
      "Batch 3501 - batch loss: 1.856036901473999 - avg loss: 2.0699971674306945   (start: 35010, end: 35020)\n",
      "Batch 3502 - batch loss: 2.3684725761413574 - avg loss: 2.070082373085479   (start: 35020, end: 35030)\n",
      "Batch 3503 - batch loss: 1.541465163230896 - avg loss: 2.0699315120096076   (start: 35030, end: 35040)\n",
      "Batch 3504 - batch loss: 1.8645570278167725 - avg loss: 2.0698729172922916   (start: 35040, end: 35050)\n",
      "Batch 3505 - batch loss: 1.645745038986206 - avg loss: 2.069751945279084   (start: 35050, end: 35060)\n",
      "Batch 3506 - batch loss: 1.3745477199554443 - avg loss: 2.069553711967044   (start: 35060, end: 35070)\n",
      "Batch 3507 - batch loss: 2.067265272140503 - avg loss: 2.0695530596181766   (start: 35070, end: 35080)\n",
      "Batch 3508 - batch loss: 2.5300686359405518 - avg loss: 2.069684298026932   (start: 35080, end: 35090)\n",
      "Batch 3509 - batch loss: 2.125579595565796 - avg loss: 2.0697002226131254   (start: 35090, end: 35100)\n",
      "Batch 3510 - batch loss: 1.7868874073028564 - avg loss: 2.0696196721103313   (start: 35100, end: 35110)\n",
      "Batch 3511 - batch loss: 2.145918369293213 - avg loss: 2.0696413972518983   (start: 35110, end: 35120)\n",
      "Batch 3512 - batch loss: 1.7836898565292358 - avg loss: 2.069559999147508   (start: 35120, end: 35130)\n",
      "Batch 3513 - batch loss: 2.496187210083008 - avg loss: 2.069681407004917   (start: 35130, end: 35140)\n",
      "Batch 3514 - batch loss: 2.662964105606079 - avg loss: 2.0698501929789144   (start: 35140, end: 35150)\n",
      "Batch 3515 - batch loss: 1.7382701635360718 - avg loss: 2.0697558869409614   (start: 35150, end: 35160)\n",
      "Batch 3516 - batch loss: 1.442259669303894 - avg loss: 2.0695774689092192   (start: 35160, end: 35170)\n",
      "Batch 3517 - batch loss: 2.6800942420959473 - avg loss: 2.0697510097770953   (start: 35170, end: 35180)\n",
      "Batch 3518 - batch loss: 2.2556278705596924 - avg loss: 2.0698038307094007   (start: 35180, end: 35190)\n",
      "Batch 3519 - batch loss: 2.6393485069274902 - avg loss: 2.069965633174235   (start: 35190, end: 35200)\n",
      "Batch 3520 - batch loss: 1.777153730392456 - avg loss: 2.0698824716000286   (start: 35200, end: 35210)\n",
      "Batch 3521 - batch loss: 2.3802130222320557 - avg loss: 2.069970583624626   (start: 35210, end: 35220)\n",
      "Batch 3522 - batch loss: 2.6513946056365967 - avg loss: 2.070135620247394   (start: 35220, end: 35230)\n",
      "Batch 3523 - batch loss: 1.9305875301361084 - avg loss: 2.0700960209028674   (start: 35230, end: 35240)\n",
      "Batch 3524 - batch loss: 1.9263864755630493 - avg loss: 2.070055252237523   (start: 35240, end: 35250)\n",
      "Batch 3525 - batch loss: 1.6176741123199463 - avg loss: 2.069926953559157   (start: 35250, end: 35260)\n",
      "Batch 3526 - batch loss: 2.0636446475982666 - avg loss: 2.0699251723553123   (start: 35260, end: 35270)\n",
      "Batch 3527 - batch loss: 1.097583532333374 - avg loss: 2.0696495653144895   (start: 35270, end: 35280)\n",
      "Batch 3528 - batch loss: 2.3167948722839355 - avg loss: 2.06971959798861   (start: 35280, end: 35290)\n",
      "Batch 3529 - batch loss: 1.7449661493301392 - avg loss: 2.0696275998445137   (start: 35290, end: 35300)\n",
      "Batch 3530 - batch loss: 1.9045019149780273 - avg loss: 2.0695808352778564   (start: 35300, end: 35310)\n",
      "Batch 3531 - batch loss: 2.0001583099365234 - avg loss: 2.069561179976231   (start: 35310, end: 35320)\n",
      "Batch 3532 - batch loss: 1.8007606267929077 - avg loss: 2.0694850971703485   (start: 35320, end: 35330)\n",
      "Batch 3533 - batch loss: 1.713547706604004 - avg loss: 2.0693843791764133   (start: 35330, end: 35340)\n",
      "Batch 3534 - batch loss: 1.4009697437286377 - avg loss: 2.0691952944139107   (start: 35340, end: 35350)\n",
      "Batch 3535 - batch loss: 2.9007043838500977 - avg loss: 2.0694304496993845   (start: 35350, end: 35360)\n",
      "Batch 3536 - batch loss: 2.303083896636963 - avg loss: 2.06949650948082   (start: 35360, end: 35370)\n",
      "Batch 3537 - batch loss: 1.9858604669570923 - avg loss: 2.069472870124539   (start: 35370, end: 35380)\n",
      "Batch 3538 - batch loss: 2.495607376098633 - avg loss: 2.069593281118032   (start: 35380, end: 35390)\n",
      "Batch 3539 - batch loss: 1.504992961883545 - avg loss: 2.0694337895024293   (start: 35390, end: 35400)\n",
      "Batch 3540 - batch loss: 1.512802004814148 - avg loss: 2.0692765932909953   (start: 35400, end: 35410)\n",
      "Batch 3541 - batch loss: 2.3751914501190186 - avg loss: 2.069362961121833   (start: 35410, end: 35420)\n",
      "Batch 3542 - batch loss: 2.9437499046325684 - avg loss: 2.0696097539368234   (start: 35420, end: 35430)\n",
      "Batch 3543 - batch loss: 1.7209646701812744 - avg loss: 2.069511377784522   (start: 35430, end: 35440)\n",
      "Batch 3544 - batch loss: 2.145118236541748 - avg loss: 2.069532705530293   (start: 35440, end: 35450)\n",
      "Batch 3545 - batch loss: 1.6127312183380127 - avg loss: 2.0694038839038993   (start: 35450, end: 35460)\n",
      "Batch 3546 - batch loss: 0.993421733379364 - avg loss: 2.06910053398833   (start: 35460, end: 35470)\n",
      "Batch 3547 - batch loss: 2.1118392944335938 - avg loss: 2.069112579862187   (start: 35470, end: 35480)\n",
      "Batch 3548 - batch loss: 2.4238648414611816 - avg loss: 2.06921253823401   (start: 35480, end: 35490)\n",
      "Batch 3549 - batch loss: 1.8275474309921265 - avg loss: 2.0691444635559137   (start: 35490, end: 35500)\n",
      "Batch 3550 - batch loss: 1.2960851192474365 - avg loss: 2.0689267616848044   (start: 35500, end: 35510)\n",
      "Batch 3551 - batch loss: 2.602663040161133 - avg loss: 2.0690770252767177   (start: 35510, end: 35520)\n",
      "Batch 3552 - batch loss: 2.72969126701355 - avg loss: 2.0692629566703955   (start: 35520, end: 35530)\n",
      "Batch 3553 - batch loss: 1.4215397834777832 - avg loss: 2.0690807047927384   (start: 35530, end: 35540)\n",
      "Batch 3554 - batch loss: 2.6364336013793945 - avg loss: 2.0692402977313002   (start: 35540, end: 35550)\n",
      "Batch 3555 - batch loss: 1.2262592315673828 - avg loss: 2.069003238938791   (start: 35550, end: 35560)\n",
      "Batch 3556 - batch loss: 2.664872646331787 - avg loss: 2.0691707591545323   (start: 35560, end: 35570)\n",
      "Batch 3557 - batch loss: 2.1552982330322266 - avg loss: 2.0691949658644475   (start: 35570, end: 35580)\n",
      "Batch 3558 - batch loss: 2.2596347332000732 - avg loss: 2.0692484752118303   (start: 35580, end: 35590)\n",
      "Batch 3559 - batch loss: 1.8138256072998047 - avg loss: 2.069176727215226   (start: 35590, end: 35600)\n",
      "Batch 3560 - batch loss: 1.706189751625061 - avg loss: 2.0690747932147793   (start: 35600, end: 35610)\n",
      "Batch 3561 - batch loss: 1.6165584325790405 - avg loss: 2.0689477532482896   (start: 35610, end: 35620)\n",
      "Batch 3562 - batch loss: 2.1709446907043457 - avg loss: 2.068976379949793   (start: 35620, end: 35630)\n",
      "Batch 3563 - batch loss: 2.0501513481140137 - avg loss: 2.0689710979543285   (start: 35630, end: 35640)\n",
      "Batch 3564 - batch loss: 2.14009428024292 - avg loss: 2.0689910483560925   (start: 35640, end: 35650)\n",
      "Batch 3565 - batch loss: 2.5726640224456787 - avg loss: 2.0691322914783834   (start: 35650, end: 35660)\n",
      "Batch 3566 - batch loss: 2.294471263885498 - avg loss: 2.0691954647254835   (start: 35660, end: 35670)\n",
      "Batch 3567 - batch loss: 1.2189074754714966 - avg loss: 2.068957155311455   (start: 35670, end: 35680)\n",
      "Batch 3568 - batch loss: 2.109734296798706 - avg loss: 2.0689685806803224   (start: 35680, end: 35690)\n",
      "Batch 3569 - batch loss: 1.992310881614685 - avg loss: 2.068947107935486   (start: 35690, end: 35700)\n",
      "Batch 3570 - batch loss: 1.8157176971435547 - avg loss: 2.068876195190935   (start: 35700, end: 35710)\n",
      "Batch 3571 - batch loss: 1.6760327816009521 - avg loss: 2.0687662166316994   (start: 35710, end: 35720)\n",
      "Batch 3572 - batch loss: 2.476419448852539 - avg loss: 2.068880309335931   (start: 35720, end: 35730)\n",
      "Batch 3573 - batch loss: 1.7899268865585327 - avg loss: 2.068802258574102   (start: 35730, end: 35740)\n",
      "Batch 3574 - batch loss: 2.227280616760254 - avg loss: 2.0688465881847837   (start: 35740, end: 35750)\n",
      "Batch 3575 - batch loss: 3.6570708751678467 - avg loss: 2.069290722493224   (start: 35750, end: 35760)\n",
      "Batch 3576 - batch loss: 1.9959036111831665 - avg loss: 2.0692702061076185   (start: 35760, end: 35770)\n",
      "Batch 3577 - batch loss: 3.159672498703003 - avg loss: 2.069574958006052   (start: 35770, end: 35780)\n",
      "Batch 3578 - batch loss: 1.6981557607650757 - avg loss: 2.0694711806388435   (start: 35780, end: 35790)\n",
      "Batch 3579 - batch loss: 1.6239221096038818 - avg loss: 2.069346725591068   (start: 35790, end: 35800)\n",
      "Batch 3580 - batch loss: 1.4140079021453857 - avg loss: 2.0691637211723455   (start: 35800, end: 35810)\n",
      "Batch 3581 - batch loss: 2.1724424362182617 - avg loss: 2.06919255386778   (start: 35810, end: 35820)\n",
      "Batch 3582 - batch loss: 2.2995450496673584 - avg loss: 2.0692568442657144   (start: 35820, end: 35830)\n",
      "Batch 3583 - batch loss: 2.0905747413635254 - avg loss: 2.0692627923396816   (start: 35830, end: 35840)\n",
      "Batch 3584 - batch loss: 2.1095499992370605 - avg loss: 2.0692740300542973   (start: 35840, end: 35850)\n",
      "Batch 3585 - batch loss: 2.466845750808716 - avg loss: 2.0693848977957234   (start: 35850, end: 35860)\n",
      "Batch 3586 - batch loss: 1.9830173254013062 - avg loss: 2.0693608198552735   (start: 35860, end: 35870)\n",
      "Batch 3587 - batch loss: 1.1053613424301147 - avg loss: 2.0690921466452887   (start: 35870, end: 35880)\n",
      "Batch 3588 - batch loss: 2.256284236907959 - avg loss: 2.069144303817276   (start: 35880, end: 35890)\n",
      "Batch 3589 - batch loss: 1.7775157690048218 - avg loss: 2.0690630702421196   (start: 35890, end: 35900)\n",
      "Batch 3590 - batch loss: 2.080517292022705 - avg loss: 2.069066259944648   (start: 35900, end: 35910)\n",
      "Batch 3591 - batch loss: 2.005237102508545 - avg loss: 2.0690484901346715   (start: 35910, end: 35920)\n",
      "Batch 3592 - batch loss: 1.7720651626586914 - avg loss: 2.068965834045755   (start: 35920, end: 35930)\n",
      "Batch 3593 - batch loss: 2.7848987579345703 - avg loss: 2.0691650363061584   (start: 35930, end: 35940)\n",
      "Batch 3594 - batch loss: 1.8121535778045654 - avg loss: 2.0690935449407895   (start: 35940, end: 35950)\n",
      "Batch 3595 - batch loss: 2.0206680297851562 - avg loss: 2.0690800784460297   (start: 35950, end: 35960)\n",
      "Batch 3596 - batch loss: 1.6918041706085205 - avg loss: 2.068975192177518   (start: 35960, end: 35970)\n",
      "Batch 3597 - batch loss: 1.5435881614685059 - avg loss: 2.0688291702123403   (start: 35970, end: 35980)\n",
      "Batch 3598 - batch loss: 2.2441821098327637 - avg loss: 2.0688778928963134   (start: 35980, end: 35990)\n",
      "Batch 3599 - batch loss: 2.1921603679656982 - avg loss: 2.0689121380282773   (start: 35990, end: 36000)\n",
      "Batch 3600 - batch loss: 1.3797094821929932 - avg loss: 2.06872074601055   (start: 36000, end: 36010)\n",
      "Batch 3601 - batch loss: 1.6504873037338257 - avg loss: 2.068604634560723   (start: 36010, end: 36020)\n",
      "Batch 3602 - batch loss: 1.8867906332015991 - avg loss: 2.068554172722988   (start: 36020, end: 36030)\n",
      "Batch 3603 - batch loss: 1.9244344234466553 - avg loss: 2.0685141838913355   (start: 36030, end: 36040)\n",
      "Batch 3604 - batch loss: 2.4339656829833984 - avg loss: 2.068615557400099   (start: 36040, end: 36050)\n",
      "Batch 3605 - batch loss: 1.7474905252456665 - avg loss: 2.068526504423905   (start: 36050, end: 36060)\n",
      "Batch 3606 - batch loss: 1.779756784439087 - avg loss: 2.068446446281409   (start: 36060, end: 36070)\n",
      "Batch 3607 - batch loss: 1.4498698711395264 - avg loss: 2.0682750004457264   (start: 36070, end: 36080)\n",
      "Batch 3608 - batch loss: 1.938970923423767 - avg loss: 2.068239172217125   (start: 36080, end: 36090)\n",
      "Batch 3609 - batch loss: 2.838334083557129 - avg loss: 2.068452494907247   (start: 36090, end: 36100)\n",
      "Batch 3610 - batch loss: 1.666426420211792 - avg loss: 2.068341161183986   (start: 36100, end: 36110)\n",
      "Batch 3611 - batch loss: 2.395874500274658 - avg loss: 2.0684318404030035   (start: 36110, end: 36120)\n",
      "Batch 3612 - batch loss: 2.7378411293029785 - avg loss: 2.0686171183683784   (start: 36120, end: 36130)\n",
      "Batch 3613 - batch loss: 1.2890584468841553 - avg loss: 2.0684014131466064   (start: 36130, end: 36140)\n",
      "Batch 3614 - batch loss: 1.7079427242279053 - avg loss: 2.068301701199464   (start: 36140, end: 36150)\n",
      "Batch 3615 - batch loss: 2.6163909435272217 - avg loss: 2.068453274551878   (start: 36150, end: 36160)\n",
      "Batch 3616 - batch loss: 1.0386810302734375 - avg loss: 2.068168571139028   (start: 36160, end: 36170)\n",
      "Batch 3617 - batch loss: 1.6789947748184204 - avg loss: 2.068061005136728   (start: 36170, end: 36180)\n",
      "Batch 3618 - batch loss: 1.7122348546981812 - avg loss: 2.067962683459348   (start: 36180, end: 36190)\n",
      "Batch 3619 - batch loss: 2.585742235183716 - avg loss: 2.0681057164846863   (start: 36190, end: 36200)\n",
      "Batch 3620 - batch loss: 2.2032899856567383 - avg loss: 2.068143049892356   (start: 36200, end: 36210)\n",
      "Batch 3621 - batch loss: 2.4723525047302246 - avg loss: 2.068254648306171   (start: 36210, end: 36220)\n",
      "Batch 3622 - batch loss: 2.328530788421631 - avg loss: 2.0683264882565204   (start: 36220, end: 36230)\n",
      "Batch 3623 - batch loss: 1.5427429676055908 - avg loss: 2.0681814596912194   (start: 36230, end: 36240)\n",
      "Batch 3624 - batch loss: 1.9623925685882568 - avg loss: 2.068152276548846   (start: 36240, end: 36250)\n",
      "Batch 3625 - batch loss: 2.4469714164733887 - avg loss: 2.0682567495604083   (start: 36250, end: 36260)\n",
      "Batch 3626 - batch loss: 1.3831760883331299 - avg loss: 2.0680678660034113   (start: 36260, end: 36270)\n",
      "Batch 3627 - batch loss: 2.22067928314209 - avg loss: 2.068109930892369   (start: 36270, end: 36280)\n",
      "Batch 3628 - batch loss: 1.718520164489746 - avg loss: 2.068013598633785   (start: 36280, end: 36290)\n",
      "Batch 3629 - batch loss: 1.9468581676483154 - avg loss: 2.067980222481998   (start: 36290, end: 36300)\n",
      "Batch 3630 - batch loss: 2.542741060256958 - avg loss: 2.0681109745717188   (start: 36300, end: 36310)\n",
      "Batch 3631 - batch loss: 2.161334991455078 - avg loss: 2.0681366419772482   (start: 36310, end: 36320)\n",
      "Batch 3632 - batch loss: 1.2071460485458374 - avg loss: 2.0678996503467966   (start: 36320, end: 36330)\n",
      "Batch 3633 - batch loss: 2.6484503746032715 - avg loss: 2.0680594056369057   (start: 36330, end: 36340)\n",
      "Batch 3634 - batch loss: 3.5106723308563232 - avg loss: 2.0684562730166083   (start: 36340, end: 36350)\n",
      "Batch 3635 - batch loss: 1.775272011756897 - avg loss: 2.0683756392813883   (start: 36350, end: 36360)\n",
      "Batch 3636 - batch loss: 2.4547629356384277 - avg loss: 2.0684818771962514   (start: 36360, end: 36370)\n",
      "Batch 3637 - batch loss: 1.2353801727294922 - avg loss: 2.0682528772774864   (start: 36370, end: 36380)\n",
      "Batch 3638 - batch loss: 2.0116565227508545 - avg loss: 2.068237324555715   (start: 36380, end: 36390)\n",
      "Batch 3639 - batch loss: 1.7626793384552002 - avg loss: 2.068153380054039   (start: 36390, end: 36400)\n",
      "Batch 3640 - batch loss: 2.7063965797424316 - avg loss: 2.0683286734348925   (start: 36400, end: 36410)\n",
      "Batch 3641 - batch loss: 2.2749860286712646 - avg loss: 2.0683854162562096   (start: 36410, end: 36420)\n",
      "Batch 3642 - batch loss: 2.091549873352051 - avg loss: 2.068391774877427   (start: 36420, end: 36430)\n",
      "Batch 3643 - batch loss: 1.333513855934143 - avg loss: 2.0681901069523607   (start: 36430, end: 36440)\n",
      "Batch 3644 - batch loss: 2.6372668743133545 - avg loss: 2.0683462322657653   (start: 36440, end: 36450)\n",
      "Batch 3645 - batch loss: 2.0912528038024902 - avg loss: 2.068352514923894   (start: 36450, end: 36460)\n",
      "Batch 3646 - batch loss: 2.2148056030273438 - avg loss: 2.068392672063489   (start: 36460, end: 36470)\n",
      "Batch 3647 - batch loss: 1.9478975534439087 - avg loss: 2.068359641603341   (start: 36470, end: 36480)\n",
      "Batch 3648 - batch loss: 2.5293779373168945 - avg loss: 2.0684859825997   (start: 36480, end: 36490)\n",
      "Batch 3649 - batch loss: 2.4980833530426025 - avg loss: 2.0686036805094106   (start: 36490, end: 36500)\n",
      "Batch 3650 - batch loss: 2.389453411102295 - avg loss: 2.0686915604684883   (start: 36500, end: 36510)\n",
      "Batch 3651 - batch loss: 3.0564427375793457 - avg loss: 2.0689620290273902   (start: 36510, end: 36520)\n",
      "Batch 3652 - batch loss: 2.219014883041382 - avg loss: 2.0690031056367566   (start: 36520, end: 36530)\n",
      "Batch 3653 - batch loss: 1.8480221033096313 - avg loss: 2.068942629171971   (start: 36530, end: 36540)\n",
      "Batch 3654 - batch loss: 2.3905603885650635 - avg loss: 2.0690306230869893   (start: 36540, end: 36550)\n",
      "Batch 3655 - batch loss: 2.2839903831481934 - avg loss: 2.0690894195202665   (start: 36550, end: 36560)\n",
      "Batch 3656 - batch loss: 1.9019628763198853 - avg loss: 2.0690437190709363   (start: 36560, end: 36570)\n",
      "Batch 3657 - batch loss: 1.709893822669983 - avg loss: 2.0689455370325542   (start: 36570, end: 36580)\n",
      "Batch 3658 - batch loss: 1.8903791904449463 - avg loss: 2.0688967350794014   (start: 36580, end: 36590)\n",
      "Batch 3659 - batch loss: 1.5009137392044067 - avg loss: 2.0687415484685063   (start: 36590, end: 36600)\n",
      "Batch 3660 - batch loss: 1.4301536083221436 - avg loss: 2.06856711854768   (start: 36600, end: 36610)\n",
      "Batch 3661 - batch loss: 2.8052191734313965 - avg loss: 2.0687682796768123   (start: 36610, end: 36620)\n",
      "Batch 3662 - batch loss: 2.0412509441375732 - avg loss: 2.068760767436698   (start: 36620, end: 36630)\n",
      "Batch 3663 - batch loss: 1.7002241611480713 - avg loss: 2.0686601843017938   (start: 36630, end: 36640)\n",
      "Batch 3664 - batch loss: 1.9414598941802979 - avg loss: 2.0686254775377773   (start: 36640, end: 36650)\n",
      "Batch 3665 - batch loss: 1.1253621578216553 - avg loss: 2.068368177123234   (start: 36650, end: 36660)\n",
      "Batch 3666 - batch loss: 1.7092773914337158 - avg loss: 2.0682702521748593   (start: 36660, end: 36670)\n",
      "Batch 3667 - batch loss: 1.7833551168441772 - avg loss: 2.068192576292817   (start: 36670, end: 36680)\n",
      "Batch 3668 - batch loss: 1.5773948431015015 - avg loss: 2.068058807491184   (start: 36680, end: 36690)\n",
      "Batch 3669 - batch loss: 2.395261526107788 - avg loss: 2.068147963545303   (start: 36690, end: 36700)\n",
      "Batch 3670 - batch loss: 2.025506019592285 - avg loss: 2.0681363476520986   (start: 36700, end: 36710)\n",
      "Batch 3671 - batch loss: 1.1034860610961914 - avg loss: 2.0678736433256946   (start: 36710, end: 36720)\n",
      "Batch 3672 - batch loss: 2.509000062942505 - avg loss: 2.067993743086004   (start: 36720, end: 36730)\n",
      "Batch 3673 - batch loss: 1.3537997007369995 - avg loss: 2.067799351675457   (start: 36730, end: 36740)\n",
      "Batch 3674 - batch loss: 2.109895944595337 - avg loss: 2.0678108065306735   (start: 36740, end: 36750)\n",
      "Batch 3675 - batch loss: 3.2899603843688965 - avg loss: 2.0681432737716525   (start: 36750, end: 36760)\n",
      "Batch 3676 - batch loss: 1.9989433288574219 - avg loss: 2.068124454096669   (start: 36760, end: 36770)\n",
      "Batch 3677 - batch loss: 1.4496748447418213 - avg loss: 2.0679563057526353   (start: 36770, end: 36780)\n",
      "Batch 3678 - batch loss: 2.4526114463806152 - avg loss: 2.068060860017552   (start: 36780, end: 36790)\n",
      "Batch 3679 - batch loss: 2.0355794429779053 - avg loss: 2.0680520335455306   (start: 36790, end: 36800)\n",
      "Batch 3680 - batch loss: 2.1661458015441895 - avg loss: 2.06807868221926   (start: 36800, end: 36810)\n",
      "Batch 3681 - batch loss: 2.3032431602478027 - avg loss: 2.0681425508987896   (start: 36810, end: 36820)\n",
      "Batch 3682 - batch loss: 2.534642457962036 - avg loss: 2.068269213919985   (start: 36820, end: 36830)\n",
      "Batch 3683 - batch loss: 2.218282699584961 - avg loss: 2.068309934192967   (start: 36830, end: 36840)\n",
      "Batch 3684 - batch loss: 2.706509590148926 - avg loss: 2.068483122702046   (start: 36840, end: 36850)\n",
      "Batch 3685 - batch loss: 2.4736709594726562 - avg loss: 2.0685930488650333   (start: 36850, end: 36860)\n",
      "Batch 3686 - batch loss: 1.742432951927185 - avg loss: 2.06850458667438   (start: 36860, end: 36870)\n",
      "Batch 3687 - batch loss: 1.5110973119735718 - avg loss: 2.068353445873214   (start: 36870, end: 36880)\n",
      "Batch 3688 - batch loss: 1.914973258972168 - avg loss: 2.0683118681592263   (start: 36880, end: 36890)\n",
      "Batch 3689 - batch loss: 1.858997106552124 - avg loss: 2.068255143291582   (start: 36890, end: 36900)\n",
      "Batch 3690 - batch loss: 2.5658745765686035 - avg loss: 2.068389962970064   (start: 36900, end: 36910)\n",
      "Batch 3691 - batch loss: 1.8179261684417725 - avg loss: 2.0683221233724125   (start: 36910, end: 36920)\n",
      "Batch 3692 - batch loss: 1.5661537647247314 - avg loss: 2.068186144937902   (start: 36920, end: 36930)\n",
      "Batch 3693 - batch loss: 1.570237398147583 - avg loss: 2.068051345602009   (start: 36930, end: 36940)\n",
      "Batch 3694 - batch loss: 1.5846402645111084 - avg loss: 2.0679205171632833   (start: 36940, end: 36950)\n",
      "Batch 3695 - batch loss: 1.0998010635375977 - avg loss: 2.0676585800816745   (start: 36950, end: 36960)\n",
      "Batch 3696 - batch loss: 2.142195463180542 - avg loss: 2.0676787415323368   (start: 36960, end: 36970)\n",
      "Batch 3697 - batch loss: 2.166147470474243 - avg loss: 2.0677053690955987   (start: 36970, end: 36980)\n",
      "Batch 3698 - batch loss: 2.2512409687042236 - avg loss: 2.0677549867218783   (start: 36980, end: 36990)\n",
      "Batch 3699 - batch loss: 1.50406813621521 - avg loss: 2.067602638924444   (start: 36990, end: 37000)\n",
      "Batch 3700 - batch loss: 2.115919589996338 - avg loss: 2.0676156940314616   (start: 37000, end: 37010)\n",
      "Batch 3701 - batch loss: 2.490645408630371 - avg loss: 2.0677299646188736   (start: 37010, end: 37020)\n",
      "Batch 3702 - batch loss: 1.4646856784820557 - avg loss: 2.0675671117195655   (start: 37020, end: 37030)\n",
      "Batch 3703 - batch loss: 1.9132534265518188 - avg loss: 2.067525450357479   (start: 37030, end: 37040)\n",
      "Batch 3704 - batch loss: 2.3353779315948486 - avg loss: 2.06759774522421   (start: 37040, end: 37050)\n",
      "Batch 3705 - batch loss: 2.2616636753082275 - avg loss: 2.067650110558825   (start: 37050, end: 37060)\n",
      "Batch 3706 - batch loss: 2.042105197906494 - avg loss: 2.0676432195653933   (start: 37060, end: 37070)\n",
      "Batch 3707 - batch loss: 2.1165356636047363 - avg loss: 2.067656405229913   (start: 37070, end: 37080)\n",
      "Batch 3708 - batch loss: 2.6022629737854004 - avg loss: 2.067800542886574   (start: 37080, end: 37090)\n",
      "Batch 3709 - batch loss: 2.2009634971618652 - avg loss: 2.067836435866163   (start: 37090, end: 37100)\n",
      "Batch 3710 - batch loss: 1.8478567600250244 - avg loss: 2.0677771581308244   (start: 37100, end: 37110)\n",
      "Batch 3711 - batch loss: 1.4561004638671875 - avg loss: 2.06761237453862   (start: 37110, end: 37120)\n",
      "Batch 3712 - batch loss: 2.7685647010803223 - avg loss: 2.0678011578207482   (start: 37120, end: 37130)\n",
      "Batch 3713 - batch loss: 2.3173861503601074 - avg loss: 2.0678683589495956   (start: 37130, end: 37140)\n",
      "Batch 3714 - batch loss: 1.8343909978866577 - avg loss: 2.067805511746079   (start: 37140, end: 37150)\n",
      "Batch 3715 - batch loss: 1.8819983005523682 - avg loss: 2.0677555098054996   (start: 37150, end: 37160)\n",
      "Batch 3716 - batch loss: 1.4726382493972778 - avg loss: 2.0675954029288763   (start: 37160, end: 37170)\n",
      "Batch 3717 - batch loss: 2.1056466102600098 - avg loss: 2.067605637250375   (start: 37170, end: 37180)\n",
      "Batch 3718 - batch loss: 1.72372567653656 - avg loss: 2.067513171544348   (start: 37180, end: 37190)\n",
      "Batch 3719 - batch loss: 1.3653554916381836 - avg loss: 2.0673244194798572   (start: 37190, end: 37200)\n",
      "Batch 3720 - batch loss: 2.5352299213409424 - avg loss: 2.0674501667257217   (start: 37200, end: 37210)\n",
      "Batch 3721 - batch loss: 3.2664382457733154 - avg loss: 2.067772302158029   (start: 37210, end: 37220)\n",
      "Batch 3722 - batch loss: 2.7686188220977783 - avg loss: 2.0679605499474296   (start: 37220, end: 37230)\n",
      "Batch 3723 - batch loss: 2.033149242401123 - avg loss: 2.0679512021204838   (start: 37230, end: 37240)\n",
      "Batch 3724 - batch loss: 1.1934417486190796 - avg loss: 2.06771643448196   (start: 37240, end: 37250)\n",
      "Batch 3725 - batch loss: 2.2674551010131836 - avg loss: 2.067770041209424   (start: 37250, end: 37260)\n",
      "Batch 3726 - batch loss: 2.833494186401367 - avg loss: 2.067975494427882   (start: 37260, end: 37270)\n",
      "Batch 3727 - batch loss: 1.5432305335998535 - avg loss: 2.067834736659419   (start: 37270, end: 37280)\n",
      "Batch 3728 - batch loss: 1.4750452041625977 - avg loss: 2.0676757692331664   (start: 37280, end: 37290)\n",
      "Batch 3729 - batch loss: 2.561800003051758 - avg loss: 2.0678082422181046   (start: 37290, end: 37300)\n",
      "Batch 3730 - batch loss: 1.8066174983978271 - avg loss: 2.0677382366582493   (start: 37300, end: 37310)\n",
      "Batch 3731 - batch loss: 2.390873432159424 - avg loss: 2.067824821651685   (start: 37310, end: 37320)\n",
      "Batch 3732 - batch loss: 1.7381788492202759 - avg loss: 2.0677365157388983   (start: 37320, end: 37330)\n",
      "Batch 3733 - batch loss: 2.377437114715576 - avg loss: 2.067819456445641   (start: 37330, end: 37340)\n",
      "Batch 3734 - batch loss: 2.3905375003814697 - avg loss: 2.0679058602057308   (start: 37340, end: 37350)\n",
      "Batch 3735 - batch loss: 1.0688986778259277 - avg loss: 2.0676384599963145   (start: 37350, end: 37360)\n",
      "Batch 3736 - batch loss: 3.0111114978790283 - avg loss: 2.0678909280289295   (start: 37360, end: 37370)\n",
      "Batch 3737 - batch loss: 2.510702610015869 - avg loss: 2.0680093902231476   (start: 37370, end: 37380)\n",
      "Batch 3738 - batch loss: 1.9177812337875366 - avg loss: 2.0679692115239137   (start: 37380, end: 37390)\n",
      "Batch 3739 - batch loss: 2.423633098602295 - avg loss: 2.068064308819924   (start: 37390, end: 37400)\n",
      "Batch 3740 - batch loss: 1.9419912099838257 - avg loss: 2.0680306084460036   (start: 37400, end: 37410)\n",
      "Batch 3741 - batch loss: 1.980002760887146 - avg loss: 2.0680070841681952   (start: 37410, end: 37420)\n",
      "Batch 3742 - batch loss: 1.8201795816421509 - avg loss: 2.0679408732404565   (start: 37420, end: 37430)\n",
      "Batch 3743 - batch loss: 3.028107166290283 - avg loss: 2.0681973279127455   (start: 37430, end: 37440)\n",
      "Batch 3744 - batch loss: 1.9090560674667358 - avg loss: 2.068154833584188   (start: 37440, end: 37450)\n",
      "Batch 3745 - batch loss: 2.1037962436676025 - avg loss: 2.068164348109037   (start: 37450, end: 37460)\n",
      "Batch 3746 - batch loss: 1.996548056602478 - avg loss: 2.0681452351409275   (start: 37460, end: 37470)\n",
      "Batch 3747 - batch loss: 2.4886295795440674 - avg loss: 2.0682574241335643   (start: 37470, end: 37480)\n",
      "Batch 3748 - batch loss: 3.3120415210723877 - avg loss: 2.0685891883632093   (start: 37480, end: 37490)\n",
      "Batch 3749 - batch loss: 3.0124592781066895 - avg loss: 2.0688408870538075   (start: 37490, end: 37500)\n",
      "Batch 3750 - batch loss: 3.1023507118225098 - avg loss: 2.069116416199307   (start: 37500, end: 37510)\n",
      "Batch 3751 - batch loss: 2.0774667263031006 - avg loss: 2.0691186417617016   (start: 37510, end: 37520)\n",
      "Batch 3752 - batch loss: 2.1900651454925537 - avg loss: 2.0691508683814006   (start: 37520, end: 37530)\n",
      "Batch 3753 - batch loss: 1.185408353805542 - avg loss: 2.0689154548186472   (start: 37530, end: 37540)\n",
      "Batch 3754 - batch loss: 2.8353095054626465 - avg loss: 2.069119554432667   (start: 37540, end: 37550)\n",
      "Batch 3755 - batch loss: 2.551753282546997 - avg loss: 2.069248051165392   (start: 37550, end: 37560)\n",
      "Batch 3756 - batch loss: 1.9965238571166992 - avg loss: 2.069228694180018   (start: 37560, end: 37570)\n",
      "Batch 3757 - batch loss: 2.3699631690979004 - avg loss: 2.0693087193196984   (start: 37570, end: 37580)\n",
      "Batch 3758 - batch loss: 1.4202029705047607 - avg loss: 2.0691360388863878   (start: 37580, end: 37590)\n",
      "Batch 3759 - batch loss: 1.575756311416626 - avg loss: 2.069004820873763   (start: 37590, end: 37600)\n",
      "Batch 3760 - batch loss: 1.1973670721054077 - avg loss: 2.0687730639610353   (start: 37600, end: 37610)\n",
      "Batch 3761 - batch loss: 1.6591930389404297 - avg loss: 2.0686641910144585   (start: 37610, end: 37620)\n",
      "Batch 3762 - batch loss: 2.4408135414123535 - avg loss: 2.068763087998354   (start: 37620, end: 37630)\n",
      "Batch 3763 - batch loss: 2.4457919597625732 - avg loss: 2.068863255073743   (start: 37630, end: 37640)\n",
      "Batch 3764 - batch loss: 2.198917865753174 - avg loss: 2.068897798131028   (start: 37640, end: 37650)\n",
      "Batch 3765 - batch loss: 1.506142020225525 - avg loss: 2.0687483674943037   (start: 37650, end: 37660)\n",
      "Batch 3766 - batch loss: 2.1603968143463135 - avg loss: 2.068772696787336   (start: 37660, end: 37670)\n",
      "Batch 3767 - batch loss: 2.6799302101135254 - avg loss: 2.0689348935796197   (start: 37670, end: 37680)\n",
      "Batch 3768 - batch loss: 1.7052128314971924 - avg loss: 2.068838389981296   (start: 37680, end: 37690)\n",
      "Batch 3769 - batch loss: 2.5001702308654785 - avg loss: 2.0689528016101777   (start: 37690, end: 37700)\n",
      "Batch 3770 - batch loss: 2.2226223945617676 - avg loss: 2.0689935519663036   (start: 37700, end: 37710)\n",
      "Batch 3771 - batch loss: 2.828463077545166 - avg loss: 2.0691948959550572   (start: 37710, end: 37720)\n",
      "Batch 3772 - batch loss: 2.151737689971924 - avg loss: 2.069216773186443   (start: 37720, end: 37730)\n",
      "Batch 3773 - batch loss: 1.5740740299224854 - avg loss: 2.0690855747913015   (start: 37730, end: 37740)\n",
      "Batch 3774 - batch loss: 2.1858348846435547 - avg loss: 2.0691165017607984   (start: 37740, end: 37750)\n",
      "Batch 3775 - batch loss: 1.3601953983306885 - avg loss: 2.068928757824509   (start: 37750, end: 37760)\n",
      "Batch 3776 - batch loss: 1.7393051385879517 - avg loss: 2.068841486545918   (start: 37760, end: 37770)\n",
      "Batch 3777 - batch loss: 3.2710464000701904 - avg loss: 2.069159698539969   (start: 37770, end: 37780)\n",
      "Batch 3778 - batch loss: 2.468736410140991 - avg loss: 2.0692654346372437   (start: 37780, end: 37790)\n",
      "Batch 3779 - batch loss: 2.5464046001434326 - avg loss: 2.0693916619297057   (start: 37790, end: 37800)\n",
      "Batch 3780 - batch loss: 2.123915195465088 - avg loss: 2.0694060823300062   (start: 37800, end: 37810)\n",
      "Batch 3781 - batch loss: 1.9461424350738525 - avg loss: 2.069373490144058   (start: 37810, end: 37820)\n",
      "Batch 3782 - batch loss: 2.4429092407226562 - avg loss: 2.069472230760124   (start: 37820, end: 37830)\n",
      "Batch 3783 - batch loss: 2.575835704803467 - avg loss: 2.069606047745865   (start: 37830, end: 37840)\n",
      "Batch 3784 - batch loss: 1.996664047241211 - avg loss: 2.0695867764115174   (start: 37840, end: 37850)\n",
      "Batch 3785 - batch loss: 2.124624013900757 - avg loss: 2.069601313452587   (start: 37850, end: 37860)\n",
      "Batch 3786 - batch loss: 1.607212781906128 - avg loss: 2.069479214553314   (start: 37860, end: 37870)\n",
      "Batch 3787 - batch loss: 2.8894333839416504 - avg loss: 2.0696956755272815   (start: 37870, end: 37880)\n",
      "Batch 3788 - batch loss: 2.6190147399902344 - avg loss: 2.069840652847013   (start: 37880, end: 37890)\n",
      "Batch 3789 - batch loss: 2.1805267333984375 - avg loss: 2.0698698576176073   (start: 37890, end: 37900)\n",
      "Batch 3790 - batch loss: 1.5044620037078857 - avg loss: 2.069720712839472   (start: 37900, end: 37910)\n",
      "Batch 3791 - batch loss: 2.2479376792907715 - avg loss: 2.0697677109846335   (start: 37910, end: 37920)\n",
      "Batch 3792 - batch loss: 1.8574488162994385 - avg loss: 2.0697117344766753   (start: 37920, end: 37930)\n",
      "Batch 3793 - batch loss: 3.1374309062957764 - avg loss: 2.0699931575583355   (start: 37930, end: 37940)\n",
      "Batch 3794 - batch loss: 2.366074323654175 - avg loss: 2.07007117631093   (start: 37940, end: 37950)\n",
      "Batch 3795 - batch loss: 1.912289023399353 - avg loss: 2.0700296109387195   (start: 37950, end: 37960)\n",
      "Batch 3796 - batch loss: 2.1869540214538574 - avg loss: 2.070060404831402   (start: 37960, end: 37970)\n",
      "Batch 3797 - batch loss: 1.9781303405761719 - avg loss: 2.0700361999698287   (start: 37970, end: 37980)\n",
      "Batch 3798 - batch loss: 1.7246917486190796 - avg loss: 2.069945295928936   (start: 37980, end: 37990)\n",
      "Batch 3799 - batch loss: 2.794778347015381 - avg loss: 2.070136041468696   (start: 37990, end: 38000)\n",
      "Batch 3800 - batch loss: 2.834620952606201 - avg loss: 2.0703371687802288   (start: 38000, end: 38010)\n",
      "Batch 3801 - batch loss: 1.5051863193511963 - avg loss: 2.070188523107049   (start: 38010, end: 38020)\n",
      "Batch 3802 - batch loss: 1.1701016426086426 - avg loss: 2.0699518449896424   (start: 38020, end: 38030)\n",
      "Batch 3803 - batch loss: 2.817115545272827 - avg loss: 2.070148260263113   (start: 38030, end: 38040)\n",
      "Batch 3804 - batch loss: 2.3958327770233154 - avg loss: 2.0702338540914336   (start: 38040, end: 38050)\n",
      "Batch 3805 - batch loss: 2.6701579093933105 - avg loss: 2.070391479959879   (start: 38050, end: 38060)\n",
      "Batch 3806 - batch loss: 1.9245868921279907 - avg loss: 2.0703531808824343   (start: 38060, end: 38070)\n",
      "Batch 3807 - batch loss: 1.9971660375595093 - avg loss: 2.070333961569587   (start: 38070, end: 38080)\n",
      "Batch 3808 - batch loss: 1.8044315576553345 - avg loss: 2.0702641525898247   (start: 38080, end: 38090)\n",
      "Batch 3809 - batch loss: 3.5907814502716064 - avg loss: 2.0706632384947277   (start: 38090, end: 38100)\n",
      "Batch 3810 - batch loss: 1.7472118139266968 - avg loss: 2.070578365384109   (start: 38100, end: 38110)\n",
      "Batch 3811 - batch loss: 1.986922025680542 - avg loss: 2.070556419859528   (start: 38110, end: 38120)\n",
      "Batch 3812 - batch loss: 2.287868022918701 - avg loss: 2.070613412149866   (start: 38120, end: 38130)\n",
      "Batch 3813 - batch loss: 1.6044944524765015 - avg loss: 2.0704911995227886   (start: 38130, end: 38140)\n",
      "Batch 3814 - batch loss: 1.3230394124984741 - avg loss: 2.0702952750700954   (start: 38140, end: 38150)\n",
      "Batch 3815 - batch loss: 2.334801435470581 - avg loss: 2.0703645901016468   (start: 38150, end: 38160)\n",
      "Batch 3816 - batch loss: 1.5701512098312378 - avg loss: 2.070233541272653   (start: 38160, end: 38170)\n",
      "Batch 3817 - batch loss: 2.287414073944092 - avg loss: 2.0702904245970823   (start: 38170, end: 38180)\n",
      "Batch 3818 - batch loss: 1.3528848886489868 - avg loss: 2.0701025729249305   (start: 38180, end: 38190)\n",
      "Batch 3819 - batch loss: 1.4677307605743408 - avg loss: 2.0699448839688177   (start: 38190, end: 38200)\n",
      "Batch 3820 - batch loss: 1.4628901481628418 - avg loss: 2.0697860107063715   (start: 38200, end: 38210)\n",
      "Batch 3821 - batch loss: 1.092178463935852 - avg loss: 2.0695302264188857   (start: 38210, end: 38220)\n",
      "Batch 3822 - batch loss: 2.403522253036499 - avg loss: 2.0696175902762275   (start: 38220, end: 38230)\n",
      "Batch 3823 - batch loss: 2.731337547302246 - avg loss: 2.069790634198044   (start: 38230, end: 38240)\n",
      "Batch 3824 - batch loss: 2.8053107261657715 - avg loss: 2.0699829270325454   (start: 38240, end: 38250)\n",
      "Batch 3825 - batch loss: 3.158278465270996 - avg loss: 2.07026737437657   (start: 38250, end: 38260)\n",
      "Batch 3826 - batch loss: 1.35909104347229 - avg loss: 2.0700815430907316   (start: 38260, end: 38270)\n",
      "Batch 3827 - batch loss: 1.7644726037979126 - avg loss: 2.070001707944626   (start: 38270, end: 38280)\n",
      "Batch 3828 - batch loss: 2.266087055206299 - avg loss: 2.070052918534143   (start: 38280, end: 38290)\n",
      "Batch 3829 - batch loss: 1.0706040859222412 - avg loss: 2.069791965836333   (start: 38290, end: 38300)\n",
      "Batch 3830 - batch loss: 2.416774272918701 - avg loss: 2.0698825380908574   (start: 38300, end: 38310)\n",
      "Batch 3831 - batch loss: 1.1983007192611694 - avg loss: 2.0696550898082817   (start: 38310, end: 38320)\n",
      "Batch 3832 - batch loss: 1.9622783660888672 - avg loss: 2.069627076053072   (start: 38320, end: 38330)\n",
      "Batch 3833 - batch loss: 1.8445971012115479 - avg loss: 2.069568382788898   (start: 38330, end: 38340)\n",
      "Batch 3834 - batch loss: 3.542844772338867 - avg loss: 2.0699525487314148   (start: 38340, end: 38350)\n",
      "Batch 3835 - batch loss: 2.770662784576416 - avg loss: 2.0701352156333557   (start: 38350, end: 38360)\n",
      "Batch 3836 - batch loss: 1.9162511825561523 - avg loss: 2.0700951103341434   (start: 38360, end: 38370)\n",
      "Batch 3837 - batch loss: 2.292401075363159 - avg loss: 2.070153032680425   (start: 38370, end: 38380)\n",
      "Batch 3838 - batch loss: 2.0197110176086426 - avg loss: 2.070139893317291   (start: 38380, end: 38390)\n",
      "Batch 3839 - batch loss: 2.5874059200286865 - avg loss: 2.070274598011747   (start: 38390, end: 38400)\n",
      "Batch 3840 - batch loss: 2.1732046604156494 - avg loss: 2.070301395736924   (start: 38400, end: 38410)\n",
      "Batch 3841 - batch loss: 1.996352195739746 - avg loss: 2.070282148157539   (start: 38410, end: 38420)\n",
      "Batch 3842 - batch loss: 2.5061938762664795 - avg loss: 2.070395578219498   (start: 38420, end: 38430)\n",
      "Batch 3843 - batch loss: 1.6790956258773804 - avg loss: 2.0702937832266928   (start: 38430, end: 38440)\n",
      "Batch 3844 - batch loss: 1.852586030960083 - avg loss: 2.070237162224803   (start: 38440, end: 38450)\n",
      "Batch 3845 - batch loss: 2.7130417823791504 - avg loss: 2.0704042981114785   (start: 38450, end: 38460)\n",
      "Batch 3846 - batch loss: 2.051403045654297 - avg loss: 2.0703993588724723   (start: 38460, end: 38470)\n",
      "Batch 3847 - batch loss: 1.2590758800506592 - avg loss: 2.070188515972571   (start: 38470, end: 38480)\n",
      "Batch 3848 - batch loss: 1.224698781967163 - avg loss: 2.0699688511936656   (start: 38480, end: 38490)\n",
      "Batch 3849 - batch loss: 2.1515228748321533 - avg loss: 2.0699900340569486   (start: 38490, end: 38500)\n",
      "Batch 3850 - batch loss: 2.691603183746338 - avg loss: 2.070151450091664   (start: 38500, end: 38510)\n",
      "Batch 3851 - batch loss: 2.5018577575683594 - avg loss: 2.070263523380209   (start: 38510, end: 38520)\n",
      "Batch 3852 - batch loss: 2.020533561706543 - avg loss: 2.0702506165643064   (start: 38520, end: 38530)\n",
      "Batch 3853 - batch loss: 1.1750633716583252 - avg loss: 2.0700183417213105   (start: 38530, end: 38540)\n",
      "Batch 3854 - batch loss: 1.7933404445648193 - avg loss: 2.069946570541763   (start: 38540, end: 38550)\n",
      "Batch 3855 - batch loss: 2.483144998550415 - avg loss: 2.0700537278104374   (start: 38550, end: 38560)\n",
      "Batch 3856 - batch loss: 2.9023327827453613 - avg loss: 2.070269511853718   (start: 38560, end: 38570)\n",
      "Batch 3857 - batch loss: 2.595612049102783 - avg loss: 2.070405681510859   (start: 38570, end: 38580)\n",
      "Batch 3858 - batch loss: 1.5879008769989014 - avg loss: 2.070280647874033   (start: 38580, end: 38590)\n",
      "Batch 3859 - batch loss: 1.380123496055603 - avg loss: 2.0701018506844426   (start: 38590, end: 38600)\n",
      "Batch 3860 - batch loss: 2.303718090057373 - avg loss: 2.070162357350947   (start: 38600, end: 38610)\n",
      "Batch 3861 - batch loss: 1.8684114217758179 - avg loss: 2.0701101173365566   (start: 38610, end: 38620)\n",
      "Batch 3862 - batch loss: 2.4764673709869385 - avg loss: 2.0702153094809135   (start: 38620, end: 38630)\n",
      "Batch 3863 - batch loss: 2.377002716064453 - avg loss: 2.0702947058076693   (start: 38630, end: 38640)\n",
      "Batch 3864 - batch loss: 2.1272947788238525 - avg loss: 2.070309453562654   (start: 38640, end: 38650)\n",
      "Batch 3865 - batch loss: 1.7196645736694336 - avg loss: 2.0702187539041197   (start: 38650, end: 38660)\n",
      "Batch 3866 - batch loss: 2.214402675628662 - avg loss: 2.070256039635106   (start: 38660, end: 38670)\n",
      "Batch 3867 - batch loss: 2.7134270668029785 - avg loss: 2.0704223196317884   (start: 38670, end: 38680)\n",
      "Batch 3868 - batch loss: 1.9015814065933228 - avg loss: 2.070378680212549   (start: 38680, end: 38690)\n",
      "Batch 3869 - batch loss: 2.085059642791748 - avg loss: 2.070382473742931   (start: 38690, end: 38700)\n",
      "Batch 3870 - batch loss: 2.6766715049743652 - avg loss: 2.0705390971041377   (start: 38700, end: 38710)\n",
      "Batch 3871 - batch loss: 1.976034164428711 - avg loss: 2.0705146898384674   (start: 38710, end: 38720)\n",
      "Batch 3872 - batch loss: 1.7462329864501953 - avg loss: 2.0704309610227205   (start: 38720, end: 38730)\n",
      "Batch 3873 - batch loss: 1.8023306131362915 - avg loss: 2.0703617559768026   (start: 38730, end: 38740)\n",
      "Batch 3874 - batch loss: 2.287243127822876 - avg loss: 2.0704177253630855   (start: 38740, end: 38750)\n",
      "Batch 3875 - batch loss: 2.5034046173095703 - avg loss: 2.0705294350875296   (start: 38750, end: 38760)\n",
      "Batch 3876 - batch loss: 2.312685012817383 - avg loss: 2.0705918946123503   (start: 38760, end: 38770)\n",
      "Batch 3877 - batch loss: 2.5906410217285156 - avg loss: 2.0707259970174863   (start: 38770, end: 38780)\n",
      "Batch 3878 - batch loss: 2.013476848602295 - avg loss: 2.0707112382785287   (start: 38780, end: 38790)\n",
      "Batch 3879 - batch loss: 2.543856382369995 - avg loss: 2.0708331829032947   (start: 38790, end: 38800)\n",
      "Batch 3880 - batch loss: 2.5303797721862793 - avg loss: 2.070951592228026   (start: 38800, end: 38810)\n",
      "Batch 3881 - batch loss: 1.5017555952072144 - avg loss: 2.070804967808392   (start: 38810, end: 38820)\n",
      "Batch 3882 - batch loss: 1.0913269519805908 - avg loss: 2.070552720057728   (start: 38820, end: 38830)\n",
      "Batch 3883 - batch loss: 2.3150556087493896 - avg loss: 2.0706156713678956   (start: 38830, end: 38840)\n",
      "Batch 3884 - batch loss: 2.371676206588745 - avg loss: 2.0706931644271545   (start: 38840, end: 38850)\n",
      "Batch 3885 - batch loss: 1.7789573669433594 - avg loss: 2.0706180908817395   (start: 38850, end: 38860)\n",
      "Batch 3886 - batch loss: 1.986201286315918 - avg loss: 2.070596373154812   (start: 38860, end: 38870)\n",
      "Batch 3887 - batch loss: 1.5618889331817627 - avg loss: 2.0704655327638726   (start: 38870, end: 38880)\n",
      "Batch 3888 - batch loss: 2.3528409004211426 - avg loss: 2.0705381414981634   (start: 38880, end: 38890)\n",
      "Batch 3889 - batch loss: 2.092109203338623 - avg loss: 2.0705436867582767   (start: 38890, end: 38900)\n",
      "Batch 3890 - batch loss: 1.586525559425354 - avg loss: 2.0704192924824265   (start: 38900, end: 38910)\n",
      "Batch 3891 - batch loss: 1.5613054037094116 - avg loss: 2.0702884821307377   (start: 38910, end: 38920)\n",
      "Batch 3892 - batch loss: 1.8811336755752563 - avg loss: 2.0702398936882624   (start: 38920, end: 38930)\n",
      "Batch 3893 - batch loss: 2.774064302444458 - avg loss: 2.0704206395559455   (start: 38930, end: 38940)\n",
      "Batch 3894 - batch loss: 2.7867088317871094 - avg loss: 2.07060453896345   (start: 38940, end: 38950)\n",
      "Batch 3895 - batch loss: 2.9096572399139404 - avg loss: 2.0708199015663635   (start: 38950, end: 38960)\n",
      "Batch 3896 - batch loss: 2.303187847137451 - avg loss: 2.0708795289580935   (start: 38960, end: 38970)\n",
      "Batch 3897 - batch loss: 1.4025747776031494 - avg loss: 2.070708080843328   (start: 38970, end: 38980)\n",
      "Batch 3898 - batch loss: 2.208862781524658 - avg loss: 2.070743514211033   (start: 38980, end: 38990)\n",
      "Batch 3899 - batch loss: 1.9171593189239502 - avg loss: 2.070704133648139   (start: 38990, end: 39000)\n",
      "Batch 3900 - batch loss: 2.054088592529297 - avg loss: 2.070699874345109   (start: 39000, end: 39010)\n",
      "Batch 3901 - batch loss: 1.8025634288787842 - avg loss: 2.070631156650218   (start: 39010, end: 39020)\n",
      "Batch 3902 - batch loss: 1.5196208953857422 - avg loss: 2.070489980564831   (start: 39020, end: 39030)\n",
      "Batch 3903 - batch loss: 2.6793301105499268 - avg loss: 2.0706459334669787   (start: 39030, end: 39040)\n",
      "Batch 3904 - batch loss: 1.5842573642730713 - avg loss: 2.070521378135559   (start: 39040, end: 39050)\n",
      "Batch 3905 - batch loss: 1.6769895553588867 - avg loss: 2.0704206275408903   (start: 39050, end: 39060)\n",
      "Batch 3906 - batch loss: 1.6214373111724854 - avg loss: 2.0703057098760915   (start: 39060, end: 39070)\n",
      "Batch 3907 - batch loss: 1.7828209400177002 - avg loss: 2.0702321467312967   (start: 39070, end: 39080)\n",
      "Batch 3908 - batch loss: 2.0294189453125 - avg loss: 2.0702217059020773   (start: 39080, end: 39090)\n",
      "Batch 3909 - batch loss: 2.4108643531799316 - avg loss: 2.0703088267837337   (start: 39090, end: 39100)\n",
      "Batch 3910 - batch loss: 2.411565065383911 - avg loss: 2.070396082278134   (start: 39100, end: 39110)\n",
      "Batch 3911 - batch loss: 1.2675336599349976 - avg loss: 2.070190851597576   (start: 39110, end: 39120)\n",
      "Batch 3912 - batch loss: 2.873137950897217 - avg loss: 2.070396051469618   (start: 39120, end: 39130)\n",
      "Batch 3913 - batch loss: 1.4730714559555054 - avg loss: 2.0702434391559965   (start: 39130, end: 39140)\n",
      "Batch 3914 - batch loss: 2.808720350265503 - avg loss: 2.0704320667194986   (start: 39140, end: 39150)\n",
      "Batch 3915 - batch loss: 1.396249532699585 - avg loss: 2.0702599057046824   (start: 39150, end: 39160)\n",
      "Batch 3916 - batch loss: 2.2119975090026855 - avg loss: 2.0702960909493333   (start: 39160, end: 39170)\n",
      "Batch 3917 - batch loss: 1.8262574672698975 - avg loss: 2.070233804419553   (start: 39170, end: 39180)\n",
      "Batch 3918 - batch loss: 2.258916139602661 - avg loss: 2.0702819499503473   (start: 39180, end: 39190)\n",
      "Batch 3919 - batch loss: 2.309450149536133 - avg loss: 2.07034296224616   (start: 39190, end: 39200)\n",
      "Batch 3920 - batch loss: 2.537074565887451 - avg loss: 2.0704619960649926   (start: 39200, end: 39210)\n",
      "Batch 3921 - batch loss: 3.0853164196014404 - avg loss: 2.0707207554794587   (start: 39210, end: 39220)\n",
      "Batch 3922 - batch loss: 1.7970726490020752 - avg loss: 2.0706510006728114   (start: 39220, end: 39230)\n",
      "Batch 3923 - batch loss: 1.7567377090454102 - avg loss: 2.0705710023823864   (start: 39230, end: 39240)\n",
      "Batch 3924 - batch loss: 1.8000891208648682 - avg loss: 2.070502089801108   (start: 39240, end: 39250)\n",
      "Batch 3925 - batch loss: 2.574256181716919 - avg loss: 2.070630402101647   (start: 39250, end: 39260)\n",
      "Batch 3926 - batch loss: 3.7974612712860107 - avg loss: 2.0710701349433034   (start: 39260, end: 39270)\n",
      "Batch 3927 - batch loss: 2.0096256732940674 - avg loss: 2.071054492259584   (start: 39270, end: 39280)\n",
      "Batch 3928 - batch loss: 1.013988733291626 - avg loss: 2.0707854503255123   (start: 39280, end: 39290)\n",
      "Batch 3929 - batch loss: 2.645684242248535 - avg loss: 2.070931735005391   (start: 39290, end: 39300)\n",
      "Batch 3930 - batch loss: 1.795538306236267 - avg loss: 2.0708616781677494   (start: 39300, end: 39310)\n",
      "Batch 3931 - batch loss: 1.2100155353546143 - avg loss: 2.0706427447641853   (start: 39310, end: 39320)\n",
      "Batch 3932 - batch loss: 3.4790070056915283 - avg loss: 2.0710008338211208   (start: 39320, end: 39330)\n",
      "Batch 3933 - batch loss: 1.5082875490188599 - avg loss: 2.070857795365401   (start: 39330, end: 39340)\n",
      "Batch 3934 - batch loss: 1.7662222385406494 - avg loss: 2.0707803784513414   (start: 39340, end: 39350)\n",
      "Batch 3935 - batch loss: 2.0720529556274414 - avg loss: 2.0707807017687134   (start: 39350, end: 39360)\n",
      "Batch 3936 - batch loss: 1.8146674633026123 - avg loss: 2.070715648876037   (start: 39360, end: 39370)\n",
      "Batch 3937 - batch loss: 1.5036221742630005 - avg loss: 2.0705716434228596   (start: 39370, end: 39380)\n",
      "Batch 3938 - batch loss: 1.8087961673736572 - avg loss: 2.070505186079359   (start: 39380, end: 39390)\n",
      "Batch 3939 - batch loss: 2.1370327472686768 - avg loss: 2.0705220712471735   (start: 39390, end: 39400)\n",
      "Batch 3940 - batch loss: 1.7032444477081299 - avg loss: 2.0704288772295283   (start: 39400, end: 39410)\n",
      "Batch 3941 - batch loss: 0.8411202430725098 - avg loss: 2.070117028260945   (start: 39410, end: 39420)\n",
      "Batch 3942 - batch loss: 2.223802089691162 - avg loss: 2.070156004944036   (start: 39420, end: 39430)\n",
      "Batch 3943 - batch loss: 2.002983570098877 - avg loss: 2.070138973393619   (start: 39430, end: 39440)\n",
      "Batch 3944 - batch loss: 2.4077415466308594 - avg loss: 2.0702245507252384   (start: 39440, end: 39450)\n",
      "Batch 3945 - batch loss: 2.1762208938598633 - avg loss: 2.0702514124442284   (start: 39450, end: 39460)\n",
      "Batch 3946 - batch loss: 1.733379602432251 - avg loss: 2.070166063619802   (start: 39460, end: 39470)\n",
      "Batch 3947 - batch loss: 2.129394292831421 - avg loss: 2.070181065704202   (start: 39470, end: 39480)\n",
      "Batch 3948 - batch loss: 2.7838218212127686 - avg loss: 2.0703617800003546   (start: 39480, end: 39490)\n",
      "Batch 3949 - batch loss: 1.5769232511520386 - avg loss: 2.070236858853811   (start: 39490, end: 39500)\n",
      "Batch 3950 - batch loss: 2.6241798400878906 - avg loss: 2.0703770620887476   (start: 39500, end: 39510)\n",
      "Batch 3951 - batch loss: 2.5261313915252686 - avg loss: 2.070492384540528   (start: 39510, end: 39520)\n",
      "Batch 3952 - batch loss: 1.7052339315414429 - avg loss: 2.070399984223554   (start: 39520, end: 39530)\n",
      "Batch 3953 - batch loss: 2.6802446842193604 - avg loss: 2.0705542190996273   (start: 39530, end: 39540)\n",
      "Batch 3954 - batch loss: 1.3420782089233398 - avg loss: 2.070370027946612   (start: 39540, end: 39550)\n",
      "Batch 3955 - batch loss: 3.0970523357391357 - avg loss: 2.070629553302475   (start: 39550, end: 39560)\n",
      "Batch 3956 - batch loss: 1.7937114238739014 - avg loss: 2.0705595714653686   (start: 39560, end: 39570)\n",
      "Batch 3957 - batch loss: 2.5617849826812744 - avg loss: 2.07068368096795   (start: 39570, end: 39580)\n",
      "Batch 3958 - batch loss: 2.872087240219116 - avg loss: 2.070886106721739   (start: 39580, end: 39590)\n",
      "Batch 3959 - batch loss: 1.2110774517059326 - avg loss: 2.0706689833240075   (start: 39590, end: 39600)\n",
      "Batch 3960 - batch loss: 1.7853329181671143 - avg loss: 2.070596946953102   (start: 39600, end: 39610)\n",
      "Batch 3961 - batch loss: 2.132843017578125 - avg loss: 2.070612657723073   (start: 39610, end: 39620)\n",
      "Batch 3962 - batch loss: 1.634655237197876 - avg loss: 2.07050265080394   (start: 39620, end: 39630)\n",
      "Batch 3963 - batch loss: 2.1969754695892334 - avg loss: 2.070534556156812   (start: 39630, end: 39640)\n",
      "Batch 3964 - batch loss: 1.8793230056762695 - avg loss: 2.0704863313017094   (start: 39640, end: 39650)\n",
      "Batch 3965 - batch loss: 1.331954836845398 - avg loss: 2.070300115594585   (start: 39650, end: 39660)\n",
      "Batch 3966 - batch loss: 1.8010259866714478 - avg loss: 2.070232237064481   (start: 39660, end: 39670)\n",
      "Batch 3967 - batch loss: 1.7985169887542725 - avg loss: 2.0701637604394025   (start: 39670, end: 39680)\n",
      "Batch 3968 - batch loss: 2.223775863647461 - avg loss: 2.070202463413252   (start: 39680, end: 39690)\n",
      "Batch 3969 - batch loss: 2.632917881011963 - avg loss: 2.0703442053320424   (start: 39690, end: 39700)\n",
      "Batch 3970 - batch loss: 1.5661872625350952 - avg loss: 2.0702172456385655   (start: 39700, end: 39710)\n",
      "Batch 3971 - batch loss: 2.623490333557129 - avg loss: 2.070356538963822   (start: 39710, end: 39720)\n",
      "Batch 3972 - batch loss: 3.6911633014678955 - avg loss: 2.070764494353327   (start: 39720, end: 39730)\n",
      "Batch 3973 - batch loss: 1.7802940607070923 - avg loss: 2.070691401642294   (start: 39730, end: 39740)\n",
      "Batch 3974 - batch loss: 2.3121304512023926 - avg loss: 2.070752141025831   (start: 39740, end: 39750)\n",
      "Batch 3975 - batch loss: 2.5050570964813232 - avg loss: 2.0708613726544667   (start: 39750, end: 39760)\n",
      "Batch 3976 - batch loss: 1.6337401866912842 - avg loss: 2.070751460362296   (start: 39760, end: 39770)\n",
      "Batch 3977 - batch loss: 2.3509669303894043 - avg loss: 2.0708219016569234   (start: 39770, end: 39780)\n",
      "Batch 3978 - batch loss: 2.2789108753204346 - avg loss: 2.0708741984585477   (start: 39780, end: 39790)\n",
      "Batch 3979 - batch loss: 2.402017831802368 - avg loss: 2.0709574003764732   (start: 39790, end: 39800)\n",
      "Batch 3980 - batch loss: 2.1250357627868652 - avg loss: 2.070970984491623   (start: 39800, end: 39810)\n",
      "Batch 3981 - batch loss: 1.756111741065979 - avg loss: 2.0708919138629374   (start: 39810, end: 39820)\n",
      "Batch 3982 - batch loss: 1.7055435180664062 - avg loss: 2.0708001869244996   (start: 39820, end: 39830)\n",
      "Batch 3983 - batch loss: 2.279618263244629 - avg loss: 2.070852601100283   (start: 39830, end: 39840)\n",
      "Batch 3984 - batch loss: 2.697378635406494 - avg loss: 2.0710098221879383   (start: 39840, end: 39850)\n",
      "Batch 3985 - batch loss: 2.6308445930480957 - avg loss: 2.0711502724565936   (start: 39850, end: 39860)\n",
      "Batch 3986 - batch loss: 2.0163416862487793 - avg loss: 2.0711365256328644   (start: 39860, end: 39870)\n",
      "Batch 3987 - batch loss: 2.645793914794922 - avg loss: 2.071280622270067   (start: 39870, end: 39880)\n",
      "Batch 3988 - batch loss: 2.5392050743103027 - avg loss: 2.0713979259682467   (start: 39880, end: 39890)\n",
      "Batch 3989 - batch loss: 1.5662322044372559 - avg loss: 2.0712713180179882   (start: 39890, end: 39900)\n",
      "Batch 3990 - batch loss: 2.336921453475952 - avg loss: 2.0713378803170257   (start: 39900, end: 39910)\n",
      "Batch 3991 - batch loss: 2.628469467163086 - avg loss: 2.0714774423377786   (start: 39910, end: 39920)\n",
      "Batch 3992 - batch loss: 1.6034657955169678 - avg loss: 2.0713602343120283   (start: 39920, end: 39930)\n",
      "Batch 3993 - batch loss: 1.9894462823867798 - avg loss: 2.0713397250601693   (start: 39930, end: 39940)\n",
      "Batch 3994 - batch loss: 2.2971456050872803 - avg loss: 2.0713962471828293   (start: 39940, end: 39950)\n",
      "Batch 3995 - batch loss: 1.6453838348388672 - avg loss: 2.0712896374700307   (start: 39950, end: 39960)\n",
      "Batch 3996 - batch loss: 2.4509596824645996 - avg loss: 2.071384626222844   (start: 39960, end: 39970)\n",
      "Batch 3997 - batch loss: 1.4591928720474243 - avg loss: 2.0712315017220497   (start: 39970, end: 39980)\n",
      "Batch 3998 - batch loss: 1.485709309577942 - avg loss: 2.0710850845697255   (start: 39980, end: 39990)\n",
      "Batch 3999 - batch loss: 1.9533907175064087 - avg loss: 2.07105566097796   (start: 39990, end: 40000)\n",
      "Batch 4000 - batch loss: 1.3944038152694702 - avg loss: 2.0708865402967027   (start: 40000, end: 40010)\n",
      "Batch 4001 - batch loss: 1.0314358472824097 - avg loss: 2.0706268074898526   (start: 40010, end: 40020)\n",
      "Batch 4002 - batch loss: 2.16706919670105 - avg loss: 2.0706509000177595   (start: 40020, end: 40030)\n",
      "Batch 4003 - batch loss: 2.6103389263153076 - avg loss: 2.0707856872371146   (start: 40030, end: 40040)\n",
      "Batch 4004 - batch loss: 1.7730863094329834 - avg loss: 2.0707113553075756   (start: 40040, end: 40050)\n",
      "Batch 4005 - batch loss: 2.087923526763916 - avg loss: 2.0707156519055427   (start: 40050, end: 40060)\n",
      "Batch 4006 - batch loss: 1.5708982944488525 - avg loss: 2.070590915854268   (start: 40060, end: 40070)\n",
      "Batch 4007 - batch loss: 2.2353854179382324 - avg loss: 2.0706320322470035   (start: 40070, end: 40080)\n",
      "Batch 4008 - batch loss: 1.4958100318908691 - avg loss: 2.070488649358414   (start: 40080, end: 40090)\n",
      "Batch 4009 - batch loss: 2.2380423545837402 - avg loss: 2.0705304333248042   (start: 40090, end: 40100)\n",
      "Batch 4010 - batch loss: 2.777637004852295 - avg loss: 2.070706725165125   (start: 40100, end: 40110)\n",
      "Batch 4011 - batch loss: 1.2697789669036865 - avg loss: 2.0705070921246813   (start: 40110, end: 40120)\n",
      "Batch 4012 - batch loss: 1.6340458393096924 - avg loss: 2.0703983302874485   (start: 40120, end: 40130)\n",
      "Batch 4013 - batch loss: 1.5383180379867554 - avg loss: 2.070265774160817   (start: 40130, end: 40140)\n",
      "Batch 4014 - batch loss: 1.39798903465271 - avg loss: 2.0700983328807396   (start: 40140, end: 40150)\n",
      "Batch 4015 - batch loss: 2.2914764881134033 - avg loss: 2.0701534569233773   (start: 40150, end: 40160)\n",
      "Batch 4016 - batch loss: 1.830080270767212 - avg loss: 2.070093692625106   (start: 40160, end: 40170)\n",
      "Batch 4017 - batch loss: 2.6985666751861572 - avg loss: 2.070250107006032   (start: 40170, end: 40180)\n",
      "Batch 4018 - batch loss: 1.440913438796997 - avg loss: 2.0700935166432033   (start: 40180, end: 40190)\n",
      "Batch 4019 - batch loss: 2.2117137908935547 - avg loss: 2.070128745567146   (start: 40190, end: 40200)\n",
      "Batch 4020 - batch loss: 2.3411073684692383 - avg loss: 2.0701961364208894   (start: 40200, end: 40210)\n",
      "Batch 4021 - batch loss: 1.4170222282409668 - avg loss: 2.0700337361453602   (start: 40210, end: 40220)\n",
      "Batch 4022 - batch loss: 1.9125945568084717 - avg loss: 2.069994601375453   (start: 40220, end: 40230)\n",
      "Batch 4023 - batch loss: 1.7654399871826172 - avg loss: 2.0699189168291823   (start: 40230, end: 40240)\n",
      "Batch 4024 - batch loss: 1.927987813949585 - avg loss: 2.0698836544433736   (start: 40240, end: 40250)\n",
      "Batch 4025 - batch loss: 1.766972303390503 - avg loss: 2.0698084156577172   (start: 40250, end: 40260)\n",
      "Batch 4026 - batch loss: 2.1301181316375732 - avg loss: 2.069823391996426   (start: 40260, end: 40270)\n",
      "Batch 4027 - batch loss: 2.632874011993408 - avg loss: 2.0699631761622643   (start: 40270, end: 40280)\n",
      "Batch 4028 - batch loss: 1.7063812017440796 - avg loss: 2.069872934917683   (start: 40280, end: 40290)\n",
      "Batch 4029 - batch loss: 2.906010389328003 - avg loss: 2.0700804131942117   (start: 40290, end: 40300)\n",
      "Batch 4030 - batch loss: 1.1871483325958252 - avg loss: 2.069861377699149   (start: 40300, end: 40310)\n",
      "Batch 4031 - batch loss: 1.9770084619522095 - avg loss: 2.0698383487021874   (start: 40310, end: 40320)\n",
      "Batch 4032 - batch loss: 1.4719228744506836 - avg loss: 2.069690092943633   (start: 40320, end: 40330)\n",
      "Batch 4033 - batch loss: 2.304692029953003 - avg loss: 2.0697483482577153   (start: 40330, end: 40340)\n",
      "Batch 4034 - batch loss: 0.8743119239807129 - avg loss: 2.0694520814858994   (start: 40340, end: 40350)\n",
      "Batch 4035 - batch loss: 0.7789342403411865 - avg loss: 2.0691323297908686   (start: 40350, end: 40360)\n",
      "Batch 4036 - batch loss: 1.5779186487197876 - avg loss: 2.069010651891173   (start: 40360, end: 40370)\n",
      "Batch 4037 - batch loss: 2.002312183380127 - avg loss: 2.068994134192186   (start: 40370, end: 40380)\n",
      "Batch 4038 - batch loss: 1.5321279764175415 - avg loss: 2.0688612136282405   (start: 40380, end: 40390)\n",
      "Batch 4039 - batch loss: 2.099196672439575 - avg loss: 2.068868722405174   (start: 40390, end: 40400)\n",
      "Batch 4040 - batch loss: 1.4257731437683105 - avg loss: 2.068709579723007   (start: 40400, end: 40410)\n",
      "Batch 4041 - batch loss: 1.9285533428192139 - avg loss: 2.068674904750987   (start: 40410, end: 40420)\n",
      "Batch 4042 - batch loss: 1.8033647537231445 - avg loss: 2.068609282650807   (start: 40420, end: 40430)\n",
      "Batch 4043 - batch loss: 2.6157402992248535 - avg loss: 2.0687445771652913   (start: 40430, end: 40440)\n",
      "Batch 4044 - batch loss: 1.8413254022598267 - avg loss: 2.0686883548723607   (start: 40440, end: 40450)\n",
      "Batch 4045 - batch loss: 1.5933154821395874 - avg loss: 2.0685708628128614   (start: 40450, end: 40460)\n",
      "Batch 4046 - batch loss: 2.464360475540161 - avg loss: 2.0686686610863303   (start: 40460, end: 40470)\n",
      "Batch 4047 - batch loss: 2.4866318702697754 - avg loss: 2.068771912867255   (start: 40470, end: 40480)\n",
      "Batch 4048 - batch loss: 2.303145170211792 - avg loss: 2.068829797099743   (start: 40480, end: 40490)\n",
      "Batch 4049 - batch loss: 2.8805172443389893 - avg loss: 2.0690302137533823   (start: 40490, end: 40500)\n",
      "Batch 4050 - batch loss: 2.038088321685791 - avg loss: 2.0690225756659797   (start: 40500, end: 40510)\n",
      "Batch 4051 - batch loss: 2.6471047401428223 - avg loss: 2.069165241550599   (start: 40510, end: 40520)\n",
      "Batch 4052 - batch loss: 2.84102201461792 - avg loss: 2.069355682402577   (start: 40520, end: 40530)\n",
      "Batch 4053 - batch loss: 2.3970565795898438 - avg loss: 2.069436516368336   (start: 40530, end: 40540)\n",
      "Batch 4054 - batch loss: 2.8523497581481934 - avg loss: 2.0696295899174806   (start: 40540, end: 40550)\n",
      "Batch 4055 - batch loss: 2.1410346031188965 - avg loss: 2.069647194703773   (start: 40550, end: 40560)\n",
      "Batch 4056 - batch loss: 2.316725969314575 - avg loss: 2.0697080965461714   (start: 40560, end: 40570)\n",
      "Batch 4057 - batch loss: 2.1304023265838623 - avg loss: 2.06972305323174   (start: 40570, end: 40580)\n",
      "Batch 4058 - batch loss: 2.1893913745880127 - avg loss: 2.069752535449369   (start: 40580, end: 40590)\n",
      "Batch 4059 - batch loss: 1.6636511087417603 - avg loss: 2.0696525104674213   (start: 40590, end: 40600)\n",
      "Batch 4060 - batch loss: 2.4468135833740234 - avg loss: 2.069745384408053   (start: 40600, end: 40610)\n",
      "Batch 4061 - batch loss: 2.1680760383605957 - avg loss: 2.0697695918560965   (start: 40610, end: 40620)\n",
      "Batch 4062 - batch loss: 1.809260368347168 - avg loss: 2.0697054744001506   (start: 40620, end: 40630)\n",
      "Batch 4063 - batch loss: 2.2578375339508057 - avg loss: 2.0697517667376384   (start: 40630, end: 40640)\n",
      "Batch 4064 - batch loss: 2.1025137901306152 - avg loss: 2.0697598262759884   (start: 40640, end: 40650)\n",
      "Batch 4065 - batch loss: 2.3425519466400146 - avg loss: 2.0698269173041153   (start: 40650, end: 40660)\n",
      "Batch 4066 - batch loss: 1.8281898498535156 - avg loss: 2.0697675032231095   (start: 40660, end: 40670)\n",
      "Batch 4067 - batch loss: 2.310112476348877 - avg loss: 2.0698265850749107   (start: 40670, end: 40680)\n",
      "Batch 4068 - batch loss: 3.240720272064209 - avg loss: 2.0701143446440895   (start: 40680, end: 40690)\n",
      "Batch 4069 - batch loss: 2.155543804168701 - avg loss: 2.0701353346832847   (start: 40690, end: 40700)\n",
      "Batch 4070 - batch loss: 2.0376269817352295 - avg loss: 2.0701273493349803   (start: 40700, end: 40710)\n",
      "Batch 4071 - batch loss: 3.038939952850342 - avg loss: 2.070365269915411   (start: 40710, end: 40720)\n",
      "Batch 4072 - batch loss: 1.580899953842163 - avg loss: 2.0702450967467216   (start: 40720, end: 40730)\n",
      "Batch 4073 - batch loss: 2.3179404735565186 - avg loss: 2.070305895808285   (start: 40730, end: 40740)\n",
      "Batch 4074 - batch loss: 2.37115216255188 - avg loss: 2.070379723113007   (start: 40740, end: 40750)\n",
      "Batch 4075 - batch loss: 2.0265231132507324 - avg loss: 2.0703689633951803   (start: 40750, end: 40760)\n",
      "Batch 4076 - batch loss: 2.9267213344573975 - avg loss: 2.0705790081268614   (start: 40760, end: 40770)\n",
      "Batch 4077 - batch loss: 2.3046584129333496 - avg loss: 2.0706364086675197   (start: 40770, end: 40780)\n",
      "Batch 4078 - batch loss: 1.5609184503555298 - avg loss: 2.070511447167566   (start: 40780, end: 40790)\n",
      "Batch 4079 - batch loss: 1.8591772317886353 - avg loss: 2.0704596495657577   (start: 40790, end: 40800)\n",
      "Batch 4080 - batch loss: 2.1330244541168213 - avg loss: 2.070474980319139   (start: 40800, end: 40810)\n",
      "Batch 4081 - batch loss: 2.1244709491729736 - avg loss: 2.0704882081410045   (start: 40810, end: 40820)\n",
      "Batch 4082 - batch loss: 2.1044085025787354 - avg loss: 2.070496515830066   (start: 40820, end: 40830)\n",
      "Batch 4083 - batch loss: 2.150634527206421 - avg loss: 2.070516138261843   (start: 40830, end: 40840)\n",
      "Batch 4084 - batch loss: 2.2230422496795654 - avg loss: 2.070553476355213   (start: 40840, end: 40850)\n",
      "Batch 4085 - batch loss: 1.8841874599456787 - avg loss: 2.070507865484824   (start: 40850, end: 40860)\n",
      "Batch 4086 - batch loss: 2.3742589950561523 - avg loss: 2.070582186779067   (start: 40860, end: 40870)\n",
      "Batch 4087 - batch loss: 1.8048118352890015 - avg loss: 2.070517174462166   (start: 40870, end: 40880)\n",
      "Batch 4088 - batch loss: 2.0982139110565186 - avg loss: 2.0705239479365107   (start: 40880, end: 40890)\n",
      "Batch 4089 - batch loss: 1.8030054569244385 - avg loss: 2.070458539992498   (start: 40890, end: 40900)\n",
      "Batch 4090 - batch loss: 1.4662634134292603 - avg loss: 2.0703108511324237   (start: 40900, end: 40910)\n",
      "Batch 4091 - batch loss: 2.35758900642395 - avg loss: 2.070381055960208   (start: 40910, end: 40920)\n",
      "Batch 4092 - batch loss: 2.390648365020752 - avg loss: 2.070459303531442   (start: 40920, end: 40930)\n",
      "Batch 4093 - batch loss: 2.6823313236236572 - avg loss: 2.070608759325309   (start: 40930, end: 40940)\n",
      "Batch 4094 - batch loss: 2.5802464485168457 - avg loss: 2.070733212973463   (start: 40940, end: 40950)\n",
      "Batch 4095 - batch loss: 1.729079008102417 - avg loss: 2.070649801302352   (start: 40950, end: 40960)\n",
      "Batch 4096 - batch loss: 1.6071666479110718 - avg loss: 2.070536673854612   (start: 40960, end: 40970)\n",
      "Batch 4097 - batch loss: 2.0358707904815674 - avg loss: 2.0705282146346575   (start: 40970, end: 40980)\n",
      "Batch 4098 - batch loss: 1.4688434600830078 - avg loss: 2.070381426453503   (start: 40980, end: 40990)\n",
      "Batch 4099 - batch loss: 2.514030694961548 - avg loss: 2.0704896335921634   (start: 40990, end: 41000)\n",
      "Batch 4100 - batch loss: 1.6717075109481812 - avg loss: 2.0703923933769373   (start: 41000, end: 41010)\n",
      "Batch 4101 - batch loss: 1.9435455799102783 - avg loss: 2.0703614702142197   (start: 41010, end: 41020)\n",
      "Batch 4102 - batch loss: 1.7870686054229736 - avg loss: 2.07029242491449   (start: 41020, end: 41030)\n",
      "Batch 4103 - batch loss: 1.637855887413025 - avg loss: 2.0701870553878083   (start: 41030, end: 41040)\n",
      "Batch 4104 - batch loss: 1.916501760482788 - avg loss: 2.070149616826321   (start: 41040, end: 41050)\n",
      "Batch 4105 - batch loss: 2.206735134124756 - avg loss: 2.0701828816868417   (start: 41050, end: 41060)\n",
      "Batch 4106 - batch loss: 2.1301822662353516 - avg loss: 2.070197490740786   (start: 41060, end: 41070)\n",
      "Batch 4107 - batch loss: 1.7352030277252197 - avg loss: 2.0701159438900034   (start: 41070, end: 41080)\n",
      "Batch 4108 - batch loss: 2.880683422088623 - avg loss: 2.07031321025121   (start: 41080, end: 41090)\n",
      "Batch 4109 - batch loss: 2.6766180992126465 - avg loss: 2.0704607296889135   (start: 41090, end: 41100)\n",
      "Batch 4110 - batch loss: 2.3566806316375732 - avg loss: 2.0705303526278453   (start: 41100, end: 41110)\n",
      "Batch 4111 - batch loss: 1.8032176494598389 - avg loss: 2.0704653446747403   (start: 41110, end: 41120)\n",
      "Batch 4112 - batch loss: 1.4723306894302368 - avg loss: 2.0703199192783766   (start: 41120, end: 41130)\n",
      "Batch 4113 - batch loss: 2.3092033863067627 - avg loss: 2.0703779852645283   (start: 41130, end: 41140)\n",
      "Batch 4114 - batch loss: 2.7766051292419434 - avg loss: 2.0705496078997596   (start: 41140, end: 41150)\n",
      "Batch 4115 - batch loss: 2.8915834426879883 - avg loss: 2.0707490816205536   (start: 41150, end: 41160)\n",
      "Batch 4116 - batch loss: 1.9558079242706299 - avg loss: 2.070721162952264   (start: 41160, end: 41170)\n",
      "Batch 4117 - batch loss: 1.8390874862670898 - avg loss: 2.0706649138807034   (start: 41170, end: 41180)\n",
      "Batch 4118 - batch loss: 1.819422721862793 - avg loss: 2.0706039179613014   (start: 41180, end: 41190)\n",
      "Batch 4119 - batch loss: 2.57505202293396 - avg loss: 2.0707263568217313   (start: 41190, end: 41200)\n",
      "Batch 4120 - batch loss: 1.8659532070159912 - avg loss: 2.0706766666616234   (start: 41200, end: 41210)\n",
      "Batch 4121 - batch loss: 1.9438623189926147 - avg loss: 2.070645901414736   (start: 41210, end: 41220)\n",
      "Batch 4122 - batch loss: 2.8266587257385254 - avg loss: 2.0708292661550525   (start: 41220, end: 41230)\n",
      "Batch 4123 - batch loss: 2.240027666091919 - avg loss: 2.070870293895095   (start: 41230, end: 41240)\n",
      "Batch 4124 - batch loss: 2.6694138050079346 - avg loss: 2.0710153953523345   (start: 41240, end: 41250)\n",
      "Batch 4125 - batch loss: 2.196612596511841 - avg loss: 2.071045835779179   (start: 41250, end: 41260)\n",
      "Batch 4126 - batch loss: 1.7680895328521729 - avg loss: 2.0709724274188863   (start: 41260, end: 41270)\n",
      "Batch 4127 - batch loss: 1.8087959289550781 - avg loss: 2.070908915670228   (start: 41270, end: 41280)\n",
      "Batch 4128 - batch loss: 1.7301181554794312 - avg loss: 2.070826379763182   (start: 41280, end: 41290)\n",
      "Batch 4129 - batch loss: 2.40726900100708 - avg loss: 2.0709078428675998   (start: 41290, end: 41300)\n",
      "Batch 4130 - batch loss: 1.4515925645828247 - avg loss: 2.0707579238944005   (start: 41300, end: 41310)\n",
      "Batch 4131 - batch loss: 1.9124723672866821 - avg loss: 2.0707196166444954   (start: 41310, end: 41320)\n",
      "Batch 4132 - batch loss: 2.2826220989227295 - avg loss: 2.070770887508826   (start: 41320, end: 41330)\n",
      "Batch 4133 - batch loss: 2.478732109069824 - avg loss: 2.07086957188753   (start: 41330, end: 41340)\n",
      "Batch 4134 - batch loss: 2.1430630683898926 - avg loss: 2.0708870310160674   (start: 41340, end: 41350)\n",
      "Batch 4135 - batch loss: 1.5725687742233276 - avg loss: 2.0707665478785446   (start: 41350, end: 41360)\n",
      "Batch 4136 - batch loss: 1.9903450012207031 - avg loss: 2.07074710829753   (start: 41360, end: 41370)\n",
      "Batch 4137 - batch loss: 1.5003728866577148 - avg loss: 2.070609270157936   (start: 41370, end: 41380)\n",
      "Batch 4138 - batch loss: 3.0347447395324707 - avg loss: 2.0708422093870675   (start: 41380, end: 41390)\n",
      "Batch 4139 - batch loss: 2.4593796730041504 - avg loss: 2.0709360590159607   (start: 41390, end: 41400)\n",
      "Batch 4140 - batch loss: 2.2491400241851807 - avg loss: 2.0709790930572956   (start: 41400, end: 41410)\n",
      "Batch 4141 - batch loss: 2.1601996421813965 - avg loss: 2.071000633508557   (start: 41410, end: 41420)\n",
      "Batch 4142 - batch loss: 1.8854577541351318 - avg loss: 2.0709558488405935   (start: 41420, end: 41430)\n",
      "Batch 4143 - batch loss: 2.4197659492492676 - avg loss: 2.0710400211621205   (start: 41430, end: 41440)\n",
      "Batch 4144 - batch loss: 1.7203891277313232 - avg loss: 2.070955425047903   (start: 41440, end: 41450)\n",
      "Batch 4145 - batch loss: 1.8622453212738037 - avg loss: 2.0709050849360424   (start: 41450, end: 41460)\n",
      "Batch 4146 - batch loss: 2.4537558555603027 - avg loss: 2.0709974048710857   (start: 41460, end: 41470)\n",
      "Batch 4147 - batch loss: 1.2995949983596802 - avg loss: 2.070811435149169   (start: 41470, end: 41480)\n",
      "Batch 4148 - batch loss: 2.4158730506896973 - avg loss: 2.070894602566749   (start: 41480, end: 41490)\n",
      "Batch 4149 - batch loss: 1.6152942180633545 - avg loss: 2.0707848193415677   (start: 41490, end: 41500)\n",
      "Batch 4150 - batch loss: 2.4459187984466553 - avg loss: 2.070875191295098   (start: 41500, end: 41510)\n",
      "Batch 4151 - batch loss: 2.0078978538513184 - avg loss: 2.0708600233429197   (start: 41510, end: 41520)\n",
      "Batch 4152 - batch loss: 1.6971395015716553 - avg loss: 2.070770035256772   (start: 41520, end: 41530)\n",
      "Batch 4153 - batch loss: 1.2456190586090088 - avg loss: 2.0705713951564717   (start: 41530, end: 41540)\n",
      "Batch 4154 - batch loss: 2.999204158782959 - avg loss: 2.070794892813181   (start: 41540, end: 41550)\n",
      "Batch 4155 - batch loss: 1.9712070226669312 - avg loss: 2.0707709303805184   (start: 41550, end: 41560)\n",
      "Batch 4156 - batch loss: 1.6202068328857422 - avg loss: 2.070662543539649   (start: 41560, end: 41570)\n",
      "Batch 4157 - batch loss: 2.9933993816375732 - avg loss: 2.0708844619711297   (start: 41570, end: 41580)\n",
      "Batch 4158 - batch loss: 1.8901827335357666 - avg loss: 2.0708410136113233   (start: 41580, end: 41590)\n",
      "Batch 4159 - batch loss: 1.9695504903793335 - avg loss: 2.070816664927854   (start: 41590, end: 41600)\n",
      "Batch 4160 - batch loss: 1.793649673461914 - avg loss: 2.070750054259393   (start: 41600, end: 41610)\n",
      "Batch 4161 - batch loss: 1.9902641773223877 - avg loss: 2.0707307159900665   (start: 41610, end: 41620)\n",
      "Batch 4162 - batch loss: 1.3668382167816162 - avg loss: 2.0705616329972227   (start: 41620, end: 41630)\n",
      "Batch 4163 - batch loss: 1.4191052913665771 - avg loss: 2.0704051833474555   (start: 41630, end: 41640)\n",
      "Batch 4164 - batch loss: 2.21458101272583 - avg loss: 2.0704397993929247   (start: 41640, end: 41650)\n",
      "Batch 4165 - batch loss: 2.2315869331359863 - avg loss: 2.0704784808940633   (start: 41650, end: 41660)\n",
      "Batch 4166 - batch loss: 2.8011085987091064 - avg loss: 2.0706538180953626   (start: 41660, end: 41670)\n",
      "Batch 4167 - batch loss: 1.7441438436508179 - avg loss: 2.07057548076944   (start: 41670, end: 41680)\n",
      "Batch 4168 - batch loss: 2.3514530658721924 - avg loss: 2.0706428536610457   (start: 41680, end: 41690)\n",
      "Batch 4169 - batch loss: 1.9834048748016357 - avg loss: 2.070621933282422   (start: 41690, end: 41700)\n",
      "Batch 4170 - batch loss: 2.5502963066101074 - avg loss: 2.0707369355296836   (start: 41700, end: 41710)\n",
      "Batch 4171 - batch loss: 2.603860378265381 - avg loss: 2.0708647215897833   (start: 41710, end: 41720)\n",
      "Batch 4172 - batch loss: 1.6368194818496704 - avg loss: 2.070760708831638   (start: 41720, end: 41730)\n",
      "Batch 4173 - batch loss: 1.6404306888580322 - avg loss: 2.0706576110788895   (start: 41730, end: 41740)\n",
      "Batch 4174 - batch loss: 2.1646018028259277 - avg loss: 2.070680112681703   (start: 41740, end: 41750)\n",
      "Batch 4175 - batch loss: 2.456080198287964 - avg loss: 2.0707724019742333   (start: 41750, end: 41760)\n",
      "Batch 4176 - batch loss: 1.9938627481460571 - avg loss: 2.070753989320695   (start: 41760, end: 41770)\n",
      "Batch 4177 - batch loss: 1.439867615699768 - avg loss: 2.0706029873164775   (start: 41770, end: 41780)\n",
      "Batch 4178 - batch loss: 1.868682861328125 - avg loss: 2.0705546695069565   (start: 41780, end: 41790)\n",
      "Batch 4179 - batch loss: 2.368264675140381 - avg loss: 2.0706258919963427   (start: 41790, end: 41800)\n",
      "Batch 4180 - batch loss: 1.680696725845337 - avg loss: 2.070532629818359   (start: 41800, end: 41810)\n",
      "Batch 4181 - batch loss: 2.3328840732574463 - avg loss: 2.070595363305551   (start: 41810, end: 41820)\n",
      "Batch 4182 - batch loss: 2.482191801071167 - avg loss: 2.0706937607327003   (start: 41820, end: 41830)\n",
      "Batch 4183 - batch loss: 2.179880142211914 - avg loss: 2.0707198569041823   (start: 41830, end: 41840)\n",
      "Batch 4184 - batch loss: 2.2504897117614746 - avg loss: 2.0707628126640047   (start: 41840, end: 41850)\n",
      "Batch 4185 - batch loss: 1.8820974826812744 - avg loss: 2.070717742112169   (start: 41850, end: 41860)\n",
      "Batch 4186 - batch loss: 2.542279005050659 - avg loss: 2.0708303672048225   (start: 41860, end: 41870)\n",
      "Batch 4187 - batch loss: 1.9642518758773804 - avg loss: 2.070804918663436   (start: 41870, end: 41880)\n",
      "Batch 4188 - batch loss: 2.0576627254486084 - avg loss: 2.070801781353048   (start: 41880, end: 41890)\n",
      "Batch 4189 - batch loss: 1.298525094985962 - avg loss: 2.0706174671080917   (start: 41890, end: 41900)\n",
      "Batch 4190 - batch loss: 2.251735210418701 - avg loss: 2.0706606829857606   (start: 41900, end: 41910)\n",
      "Batch 4191 - batch loss: 2.280661106109619 - avg loss: 2.0707107785065437   (start: 41910, end: 41920)\n",
      "Batch 4192 - batch loss: 1.6509771347045898 - avg loss: 2.070610675085651   (start: 41920, end: 41930)\n",
      "Batch 4193 - batch loss: 2.556187868118286 - avg loss: 2.070726454101634   (start: 41930, end: 41940)\n",
      "Batch 4194 - batch loss: 1.6284503936767578 - avg loss: 2.0706210247666106   (start: 41940, end: 41950)\n",
      "Batch 4195 - batch loss: 2.9007415771484375 - avg loss: 2.0708188609325737   (start: 41950, end: 41960)\n",
      "Batch 4196 - batch loss: 2.358341693878174 - avg loss: 2.0708873676833353   (start: 41960, end: 41970)\n",
      "Batch 4197 - batch loss: 1.3804244995117188 - avg loss: 2.0707228934412742   (start: 41970, end: 41980)\n",
      "Batch 4198 - batch loss: 1.5648491382598877 - avg loss: 2.070602418624608   (start: 41980, end: 41990)\n",
      "Batch 4199 - batch loss: 2.299635648727417 - avg loss: 2.070656950346061   (start: 41990, end: 42000)\n",
      "Batch 4200 - batch loss: 2.462606906890869 - avg loss: 2.0707502495501897   (start: 42000, end: 42010)\n",
      "Batch 4201 - batch loss: 1.683656930923462 - avg loss: 2.0706581283415684   (start: 42010, end: 42020)\n",
      "Batch 4202 - batch loss: 2.2637972831726074 - avg loss: 2.070704081031274   (start: 42020, end: 42030)\n",
      "Batch 4203 - batch loss: 1.5546910762786865 - avg loss: 2.070581337690467   (start: 42030, end: 42040)\n",
      "Batch 4204 - batch loss: 1.7656275033950806 - avg loss: 2.0705088159700638   (start: 42040, end: 42050)\n",
      "Batch 4205 - batch loss: 1.6085195541381836 - avg loss: 2.070398975441811   (start: 42050, end: 42060)\n",
      "Batch 4206 - batch loss: 2.2336654663085938 - avg loss: 2.0704377837353376   (start: 42060, end: 42070)\n",
      "Batch 4207 - batch loss: 3.1056559085845947 - avg loss: 2.070683795647136   (start: 42070, end: 42080)\n",
      "Batch 4208 - batch loss: 2.1487631797790527 - avg loss: 2.070702346225452   (start: 42080, end: 42090)\n",
      "Batch 4209 - batch loss: 2.4400908946990967 - avg loss: 2.0707900869733082   (start: 42090, end: 42100)\n",
      "Batch 4210 - batch loss: 2.38462495803833 - avg loss: 2.070864614370854   (start: 42100, end: 42110)\n",
      "Batch 4211 - batch loss: 2.5941290855407715 - avg loss: 2.0709888462016157   (start: 42110, end: 42120)\n",
      "Batch 4212 - batch loss: 1.8638919591903687 - avg loss: 2.0709396895704715   (start: 42120, end: 42130)\n",
      "Batch 4213 - batch loss: 2.465705394744873 - avg loss: 2.0710333691398057   (start: 42130, end: 42140)\n",
      "Batch 4214 - batch loss: 2.07759690284729 - avg loss: 2.0710349263245527   (start: 42140, end: 42150)\n",
      "Batch 4215 - batch loss: 2.1262948513031006 - avg loss: 2.071048033517384   (start: 42150, end: 42160)\n",
      "Batch 4216 - batch loss: 2.341646909713745 - avg loss: 2.0711122020912986   (start: 42160, end: 42170)\n",
      "Batch 4217 - batch loss: 1.8851064443588257 - avg loss: 2.071068103997953   (start: 42170, end: 42180)\n",
      "Batch 4218 - batch loss: 2.3750596046447754 - avg loss: 2.0711401569727443   (start: 42180, end: 42190)\n",
      "Batch 4219 - batch loss: 2.2036995887756348 - avg loss: 2.0711715691603754   (start: 42190, end: 42200)\n",
      "Batch 4220 - batch loss: 1.4505465030670166 - avg loss: 2.0710245364510427   (start: 42200, end: 42210)\n",
      "Batch 4221 - batch loss: 1.9983446598052979 - avg loss: 2.0710073218900185   (start: 42210, end: 42220)\n",
      "Batch 4222 - batch loss: 2.484238862991333 - avg loss: 2.0711051744926943   (start: 42220, end: 42230)\n",
      "Batch 4223 - batch loss: 1.7910524606704712 - avg loss: 2.0710388741343086   (start: 42230, end: 42240)\n",
      "Batch 4224 - batch loss: 2.3690340518951416 - avg loss: 2.071109405537329   (start: 42240, end: 42250)\n",
      "Batch 4225 - batch loss: 2.620537519454956 - avg loss: 2.0712394169225434   (start: 42250, end: 42260)\n",
      "Batch 4226 - batch loss: 1.6088424921035767 - avg loss: 2.0711300256462675   (start: 42260, end: 42270)\n",
      "Batch 4227 - batch loss: 2.7318689823150635 - avg loss: 2.071286302599122   (start: 42270, end: 42280)\n",
      "Batch 4228 - batch loss: 1.9391816854476929 - avg loss: 2.0712550648083554   (start: 42280, end: 42290)\n",
      "Batch 4229 - batch loss: 2.314300298690796 - avg loss: 2.071312522310455   (start: 42290, end: 42300)\n",
      "Batch 4230 - batch loss: 2.234816312789917 - avg loss: 2.0713511665530646   (start: 42300, end: 42310)\n",
      "Batch 4231 - batch loss: 1.6561429500579834 - avg loss: 2.071253054970717   (start: 42310, end: 42320)\n",
      "Batch 4232 - batch loss: 2.215477466583252 - avg loss: 2.071287126412156   (start: 42320, end: 42330)\n",
      "Batch 4233 - batch loss: 2.4611897468566895 - avg loss: 2.071379214891241   (start: 42330, end: 42340)\n",
      "Batch 4234 - batch loss: 3.240737199783325 - avg loss: 2.071655332479173   (start: 42340, end: 42350)\n",
      "Batch 4235 - batch loss: 1.7122023105621338 - avg loss: 2.071570475769561   (start: 42350, end: 42360)\n",
      "Batch 4236 - batch loss: 2.6659390926361084 - avg loss: 2.0717107563022177   (start: 42360, end: 42370)\n",
      "Batch 4237 - batch loss: 1.7908458709716797 - avg loss: 2.0716444833231398   (start: 42370, end: 42380)\n",
      "Batch 4238 - batch loss: 2.043430805206299 - avg loss: 2.071637827584023   (start: 42380, end: 42390)\n",
      "Batch 4239 - batch loss: 1.6106090545654297 - avg loss: 2.0715290943828393   (start: 42390, end: 42400)\n",
      "Batch 4240 - batch loss: 1.7129251956939697 - avg loss: 2.0714445379341977   (start: 42400, end: 42410)\n",
      "Batch 4241 - batch loss: 1.8015098571777344 - avg loss: 2.0713809041103515   (start: 42410, end: 42420)\n",
      "Batch 4242 - batch loss: 1.4939374923706055 - avg loss: 2.071244810918803   (start: 42420, end: 42430)\n",
      "Batch 4243 - batch loss: 0.988886833190918 - avg loss: 2.070989778407557   (start: 42430, end: 42440)\n",
      "Batch 4244 - batch loss: 1.9603641033172607 - avg loss: 2.070963718177854   (start: 42440, end: 42450)\n",
      "Batch 4245 - batch loss: 1.613581895828247 - avg loss: 2.070855997541408   (start: 42450, end: 42460)\n",
      "Batch 4246 - batch loss: 1.934842824935913 - avg loss: 2.0708239718355905   (start: 42460, end: 42470)\n",
      "Batch 4247 - batch loss: 1.8548758029937744 - avg loss: 2.070773136579272   (start: 42470, end: 42480)\n",
      "Batch 4248 - batch loss: 2.015373706817627 - avg loss: 2.07076009835151   (start: 42480, end: 42490)\n",
      "Batch 4249 - batch loss: 2.5044195652008057 - avg loss: 2.070862135873121   (start: 42490, end: 42500)\n",
      "Batch 4250 - batch loss: 2.3390307426452637 - avg loss: 2.0709252195256203   (start: 42500, end: 42510)\n",
      "Batch 4251 - batch loss: 2.065528631210327 - avg loss: 2.070923950337399   (start: 42510, end: 42520)\n",
      "Batch 4252 - batch loss: 3.4618263244628906 - avg loss: 2.0712509906322794   (start: 42520, end: 42530)\n",
      "Batch 4253 - batch loss: 1.6413637399673462 - avg loss: 2.0711499358013756   (start: 42530, end: 42540)\n",
      "Batch 4254 - batch loss: 2.073375940322876 - avg loss: 2.0711504589516743   (start: 42540, end: 42550)\n",
      "Batch 4255 - batch loss: 2.5210373401641846 - avg loss: 2.071256165455719   (start: 42550, end: 42560)\n",
      "Batch 4256 - batch loss: 2.400182008743286 - avg loss: 2.0713334325084056   (start: 42560, end: 42570)\n",
      "Batch 4257 - batch loss: 1.8912776708602905 - avg loss: 2.07129114604489   (start: 42570, end: 42580)\n",
      "Batch 4258 - batch loss: 2.109935760498047 - avg loss: 2.071300219680592   (start: 42580, end: 42590)\n",
      "Batch 4259 - batch loss: 1.6949243545532227 - avg loss: 2.071211868538543   (start: 42590, end: 42600)\n",
      "Batch 4260 - batch loss: 1.4471441507339478 - avg loss: 2.0710654081494786   (start: 42600, end: 42610)\n",
      "Batch 4261 - batch loss: 2.613862991333008 - avg loss: 2.0711927656302818   (start: 42610, end: 42620)\n",
      "Batch 4262 - batch loss: 1.828956961631775 - avg loss: 2.071135942781584   (start: 42620, end: 42630)\n",
      "Batch 4263 - batch loss: 2.0576701164245605 - avg loss: 2.0711327847547647   (start: 42630, end: 42640)\n",
      "Batch 4264 - batch loss: 2.911473035812378 - avg loss: 2.0713298164666187   (start: 42640, end: 42650)\n",
      "Batch 4265 - batch loss: 2.8744072914123535 - avg loss: 2.071518067163981   (start: 42650, end: 42660)\n",
      "Batch 4266 - batch loss: 1.9260940551757812 - avg loss: 2.071483986073756   (start: 42660, end: 42670)\n",
      "Batch 4267 - batch loss: 1.9124925136566162 - avg loss: 2.0714467340886538   (start: 42670, end: 42680)\n",
      "Batch 4268 - batch loss: 1.9494441747665405 - avg loss: 2.0714181553678004   (start: 42680, end: 42690)\n",
      "Batch 4269 - batch loss: 1.9067519903182983 - avg loss: 2.071379591863105   (start: 42690, end: 42700)\n",
      "Batch 4270 - batch loss: 1.4605633020401 - avg loss: 2.071236577044603   (start: 42700, end: 42710)\n",
      "Batch 4271 - batch loss: 1.752113938331604 - avg loss: 2.0711618760523947   (start: 42710, end: 42720)\n",
      "Batch 4272 - batch loss: 2.346060276031494 - avg loss: 2.0712262098693803   (start: 42720, end: 42730)\n",
      "Batch 4273 - batch loss: 2.3376708030700684 - avg loss: 2.071288550672656   (start: 42730, end: 42740)\n",
      "Batch 4274 - batch loss: 1.6911014318466187 - avg loss: 2.0711996180132815   (start: 42740, end: 42750)\n",
      "Batch 4275 - batch loss: 1.994653344154358 - avg loss: 2.0711817166396007   (start: 42750, end: 42760)\n",
      "Batch 4276 - batch loss: 2.31343412399292 - avg loss: 2.0712383573708033   (start: 42760, end: 42770)\n",
      "Batch 4277 - batch loss: 2.107496738433838 - avg loss: 2.071246832915699   (start: 42770, end: 42780)\n",
      "Batch 4278 - batch loss: 2.2232508659362793 - avg loss: 2.0712823561765124   (start: 42780, end: 42790)\n",
      "Batch 4279 - batch loss: 1.3573787212371826 - avg loss: 2.0711155562618067   (start: 42790, end: 42800)\n",
      "Batch 4280 - batch loss: 1.7319543361663818 - avg loss: 2.071036331496543   (start: 42800, end: 42810)\n",
      "Batch 4281 - batch loss: 2.626803398132324 - avg loss: 2.071166122964697   (start: 42810, end: 42820)\n",
      "Batch 4282 - batch loss: 2.258729934692383 - avg loss: 2.071209915589429   (start: 42820, end: 42830)\n",
      "Batch 4283 - batch loss: 2.4018959999084473 - avg loss: 2.071287106552155   (start: 42830, end: 42840)\n",
      "Batch 4284 - batch loss: 1.0369127988815308 - avg loss: 2.071045712314659   (start: 42840, end: 42850)\n",
      "Batch 4285 - batch loss: 1.1342368125915527 - avg loss: 2.070827138143002   (start: 42850, end: 42860)\n",
      "Batch 4286 - batch loss: 2.535362720489502 - avg loss: 2.0709354972711442   (start: 42860, end: 42870)\n",
      "Batch 4287 - batch loss: 1.9353139400482178 - avg loss: 2.0709038691094785   (start: 42870, end: 42880)\n",
      "Batch 4288 - batch loss: 2.7009518146514893 - avg loss: 2.071050767674538   (start: 42880, end: 42890)\n",
      "Batch 4289 - batch loss: 2.2567968368530273 - avg loss: 2.0710940651265615   (start: 42890, end: 42900)\n",
      "Batch 4290 - batch loss: 1.9278255701065063 - avg loss: 2.0710606769897586   (start: 42900, end: 42910)\n",
      "Batch 4291 - batch loss: 1.5741002559661865 - avg loss: 2.070944889380014   (start: 42910, end: 42920)\n",
      "Batch 4292 - batch loss: 1.3134106397628784 - avg loss: 2.0707684313670587   (start: 42920, end: 42930)\n",
      "Batch 4293 - batch loss: 2.1258859634399414 - avg loss: 2.0707812673083894   (start: 42930, end: 42940)\n",
      "Batch 4294 - batch loss: 1.7071634531021118 - avg loss: 2.0706966065833123   (start: 42940, end: 42950)\n",
      "Batch 4295 - batch loss: 1.3909986019134521 - avg loss: 2.0705383901017784   (start: 42950, end: 42960)\n",
      "Batch 4296 - batch loss: 2.4336764812469482 - avg loss: 2.0706228997808904   (start: 42960, end: 42970)\n",
      "Batch 4297 - batch loss: 2.639446258544922 - avg loss: 2.070755245839235   (start: 42970, end: 42980)\n",
      "Batch 4298 - batch loss: 2.030155658721924 - avg loss: 2.0707458018785188   (start: 42980, end: 42990)\n",
      "Batch 4299 - batch loss: 2.489717721939087 - avg loss: 2.0708432372087655   (start: 42990, end: 43000)\n",
      "Batch 4300 - batch loss: 1.971124291419983 - avg loss: 2.070820052148131   (start: 43000, end: 43010)\n",
      "Batch 4301 - batch loss: 2.4839110374450684 - avg loss: 2.0709160751572657   (start: 43010, end: 43020)\n",
      "Batch 4302 - batch loss: 1.7986770868301392 - avg loss: 2.070852807904575   (start: 43020, end: 43030)\n",
      "Batch 4303 - batch loss: 3.000413417816162 - avg loss: 2.0710687838827146   (start: 43030, end: 43040)\n",
      "Batch 4304 - batch loss: 2.6687426567077637 - avg loss: 2.0712076163734987   (start: 43040, end: 43050)\n",
      "Batch 4305 - batch loss: 1.5642887353897095 - avg loss: 2.0710898925274734   (start: 43050, end: 43060)\n",
      "Batch 4306 - batch loss: 1.672662377357483 - avg loss: 2.070997385558546   (start: 43060, end: 43070)\n",
      "Batch 4307 - batch loss: 3.125864028930664 - avg loss: 2.0712422478248813   (start: 43070, end: 43080)\n",
      "Batch 4308 - batch loss: 2.2707014083862305 - avg loss: 2.0712885367922893   (start: 43080, end: 43090)\n",
      "Batch 4309 - batch loss: 1.7515876293182373 - avg loss: 2.071214360247632   (start: 43090, end: 43100)\n",
      "Batch 4310 - batch loss: 1.2734073400497437 - avg loss: 2.071029297148537   (start: 43100, end: 43110)\n",
      "Batch 4311 - batch loss: 1.7093929052352905 - avg loss: 2.0709454297107093   (start: 43110, end: 43120)\n",
      "Batch 4312 - batch loss: 1.9571006298065186 - avg loss: 2.0709190339769035   (start: 43120, end: 43130)\n",
      "Batch 4313 - batch loss: 1.2501574754714966 - avg loss: 2.070728778631863   (start: 43130, end: 43140)\n",
      "Batch 4314 - batch loss: 1.5369187593460083 - avg loss: 2.0706050683145314   (start: 43140, end: 43150)\n",
      "Batch 4315 - batch loss: 2.4072818756103516 - avg loss: 2.070683074989067   (start: 43150, end: 43160)\n",
      "Batch 4316 - batch loss: 1.4691661596298218 - avg loss: 2.070543738200705   (start: 43160, end: 43170)\n",
      "Batch 4317 - batch loss: 2.3305511474609375 - avg loss: 2.0706039529782085   (start: 43170, end: 43180)\n",
      "Batch 4318 - batch loss: 2.787804365158081 - avg loss: 2.0707700100312714   (start: 43180, end: 43190)\n",
      "Batch 4319 - batch loss: 2.2817342281341553 - avg loss: 2.0708188443410176   (start: 43190, end: 43200)\n",
      "Batch 4320 - batch loss: 2.3029606342315674 - avg loss: 2.0708725684303233   (start: 43200, end: 43210)\n",
      "Batch 4321 - batch loss: 1.7398569583892822 - avg loss: 2.0707959799041684   (start: 43210, end: 43220)\n",
      "Batch 4322 - batch loss: 1.7517627477645874 - avg loss: 2.0707221808682816   (start: 43220, end: 43230)\n",
      "Batch 4323 - batch loss: 2.403852701187134 - avg loss: 2.070799223079271   (start: 43230, end: 43240)\n",
      "Batch 4324 - batch loss: 2.420790433883667 - avg loss: 2.0708801459025787   (start: 43240, end: 43250)\n",
      "Batch 4325 - batch loss: 1.7482084035873413 - avg loss: 2.0708055569653814   (start: 43250, end: 43260)\n",
      "Batch 4326 - batch loss: 1.6776883602142334 - avg loss: 2.0707147048283923   (start: 43260, end: 43270)\n",
      "Batch 4327 - batch loss: 1.926117181777954 - avg loss: 2.070681295049499   (start: 43270, end: 43280)\n",
      "Batch 4328 - batch loss: 2.3247599601745605 - avg loss: 2.0707399872798353   (start: 43280, end: 43290)\n",
      "Batch 4329 - batch loss: 2.231088161468506 - avg loss: 2.070777019190733   (start: 43290, end: 43300)\n",
      "Batch 4330 - batch loss: 2.1382718086242676 - avg loss: 2.0707926033028166   (start: 43300, end: 43310)\n",
      "Batch 4331 - batch loss: 2.167332172393799 - avg loss: 2.070814888521905   (start: 43310, end: 43320)\n",
      "Batch 4332 - batch loss: 1.6806856393814087 - avg loss: 2.0707248517692762   (start: 43320, end: 43330)\n",
      "Batch 4333 - batch loss: 2.2894022464752197 - avg loss: 2.070775308020939   (start: 43330, end: 43340)\n",
      "Batch 4334 - batch loss: 2.0546464920043945 - avg loss: 2.070771587417475   (start: 43340, end: 43350)\n",
      "Batch 4335 - batch loss: 1.9734833240509033 - avg loss: 2.0707491500873627   (start: 43350, end: 43360)\n",
      "Batch 4336 - batch loss: 2.027505397796631 - avg loss: 2.0707391791968184   (start: 43360, end: 43370)\n",
      "Batch 4337 - batch loss: 1.5582730770111084 - avg loss: 2.0706210450100535   (start: 43370, end: 43380)\n",
      "Batch 4338 - batch loss: 2.095073699951172 - avg loss: 2.070626680560858   (start: 43380, end: 43390)\n",
      "Batch 4339 - batch loss: 1.8779157400131226 - avg loss: 2.0705822771183358   (start: 43390, end: 43400)\n",
      "Batch 4340 - batch loss: 1.8245325088500977 - avg loss: 2.070525596683351   (start: 43400, end: 43410)\n",
      "Batch 4341 - batch loss: 2.986849069595337 - avg loss: 2.0707366338719537   (start: 43410, end: 43420)\n",
      "Batch 4342 - batch loss: 2.8374898433685303 - avg loss: 2.0709131830797585   (start: 43420, end: 43430)\n",
      "Batch 4343 - batch loss: 1.6702690124511719 - avg loss: 2.0708209537587114   (start: 43430, end: 43440)\n",
      "Batch 4344 - batch loss: 2.131594181060791 - avg loss: 2.0708349406924977   (start: 43440, end: 43450)\n",
      "Batch 4345 - batch loss: 2.1061336994171143 - avg loss: 2.070843062818297   (start: 43450, end: 43460)\n",
      "Batch 4346 - batch loss: 2.0665812492370605 - avg loss: 2.070842082414897   (start: 43460, end: 43470)\n",
      "Batch 4347 - batch loss: 1.5506364107131958 - avg loss: 2.070722439896106   (start: 43470, end: 43480)\n",
      "Batch 4348 - batch loss: 2.221707820892334 - avg loss: 2.0707571571600742   (start: 43480, end: 43490)\n",
      "Batch 4349 - batch loss: 1.747118353843689 - avg loss: 2.070682757435174   (start: 43490, end: 43500)\n",
      "Batch 4350 - batch loss: 2.080162525177002 - avg loss: 2.0706849361912627   (start: 43500, end: 43510)\n",
      "Batch 4351 - batch loss: 2.253483295440674 - avg loss: 2.070726939490722   (start: 43510, end: 43520)\n",
      "Batch 4352 - batch loss: 1.8420231342315674 - avg loss: 2.070674400137343   (start: 43520, end: 43530)\n",
      "Batch 4353 - batch loss: 2.6238157749176025 - avg loss: 2.0708014422537375   (start: 43530, end: 43540)\n",
      "Batch 4354 - batch loss: 1.9539201259613037 - avg loss: 2.070774603834382   (start: 43540, end: 43550)\n",
      "Batch 4355 - batch loss: 1.6215965747833252 - avg loss: 2.0706714867478233   (start: 43550, end: 43560)\n",
      "Batch 4356 - batch loss: 1.8637876510620117 - avg loss: 2.0706240036549413   (start: 43560, end: 43570)\n",
      "Batch 4357 - batch loss: 2.0842819213867188 - avg loss: 2.070627137642489   (start: 43570, end: 43580)\n",
      "Batch 4358 - batch loss: 1.1341278553009033 - avg loss: 2.0704122949532615   (start: 43580, end: 43590)\n",
      "Batch 4359 - batch loss: 2.174711227416992 - avg loss: 2.0704362167267623   (start: 43590, end: 43600)\n",
      "Batch 4360 - batch loss: 2.10929799079895 - avg loss: 2.0704451279338416   (start: 43600, end: 43610)\n",
      "Batch 4361 - batch loss: 1.5587222576141357 - avg loss: 2.070327814116712   (start: 43610, end: 43620)\n",
      "Batch 4362 - batch loss: 2.122873544692993 - avg loss: 2.0703398576029772   (start: 43620, end: 43630)\n",
      "Batch 4363 - batch loss: 2.8925082683563232 - avg loss: 2.0705282554972837   (start: 43630, end: 43640)\n",
      "Batch 4364 - batch loss: 1.8683593273162842 - avg loss: 2.0704819395916294   (start: 43640, end: 43650)\n",
      "Batch 4365 - batch loss: 2.1269896030426025 - avg loss: 2.070494882253895   (start: 43650, end: 43660)\n",
      "Batch 4366 - batch loss: 1.1790387630462646 - avg loss: 2.0702907475803873   (start: 43660, end: 43670)\n",
      "Batch 4367 - batch loss: 2.1270663738250732 - avg loss: 2.070303745663319   (start: 43670, end: 43680)\n",
      "Batch 4368 - batch loss: 1.8486064672470093 - avg loss: 2.070253002408932   (start: 43680, end: 43690)\n",
      "Batch 4369 - batch loss: 1.8102009296417236 - avg loss: 2.070193493925461   (start: 43690, end: 43700)\n",
      "Batch 4370 - batch loss: 1.780202865600586 - avg loss: 2.0701271496956912   (start: 43700, end: 43710)\n",
      "Batch 4371 - batch loss: 3.129878520965576 - avg loss: 2.070369544794335   (start: 43710, end: 43720)\n",
      "Batch 4372 - batch loss: 2.060046672821045 - avg loss: 2.0703671842016127   (start: 43720, end: 43730)\n",
      "Batch 4373 - batch loss: 3.587578296661377 - avg loss: 2.070714054597694   (start: 43730, end: 43740)\n",
      "Batch 4374 - batch loss: 1.2407640218734741 - avg loss: 2.0705243517330714   (start: 43740, end: 43750)\n",
      "Batch 4375 - batch loss: 1.6820478439331055 - avg loss: 2.0704355773939946   (start: 43750, end: 43760)\n",
      "Batch 4376 - batch loss: 1.4346669912338257 - avg loss: 2.07029032526099   (start: 43760, end: 43770)\n",
      "Batch 4377 - batch loss: 3.171034336090088 - avg loss: 2.070541751485483   (start: 43770, end: 43780)\n",
      "Batch 4378 - batch loss: 1.619969129562378 - avg loss: 2.0704388575320865   (start: 43780, end: 43790)\n",
      "Batch 4379 - batch loss: 2.7186427116394043 - avg loss: 2.0705868492795996   (start: 43790, end: 43800)\n",
      "Batch 4380 - batch loss: 1.7342262268066406 - avg loss: 2.0705100721459604   (start: 43800, end: 43810)\n",
      "Batch 4381 - batch loss: 1.4109275341033936 - avg loss: 2.0703595512564026   (start: 43810, end: 43820)\n",
      "Batch 4382 - batch loss: 1.221693992614746 - avg loss: 2.070165924617424   (start: 43820, end: 43830)\n",
      "Batch 4383 - batch loss: 2.2425484657287598 - avg loss: 2.070205245452532   (start: 43830, end: 43840)\n",
      "Batch 4384 - batch loss: 1.732168197631836 - avg loss: 2.0701281560459592   (start: 43840, end: 43850)\n",
      "Batch 4385 - batch loss: 2.8404266834259033 - avg loss: 2.0703037827051887   (start: 43850, end: 43860)\n",
      "Batch 4386 - batch loss: 1.8403724431991577 - avg loss: 2.070251370729008   (start: 43860, end: 43870)\n",
      "Batch 4387 - batch loss: 2.3942558765411377 - avg loss: 2.070325209495145   (start: 43870, end: 43880)\n",
      "Batch 4388 - batch loss: 1.5758377313613892 - avg loss: 2.070212544314436   (start: 43880, end: 43890)\n",
      "Batch 4389 - batch loss: 2.7212648391723633 - avg loss: 2.070360847798458   (start: 43890, end: 43900)\n",
      "Batch 4390 - batch loss: 2.362628936767578 - avg loss: 2.070427408511045   (start: 43900, end: 43910)\n",
      "Batch 4391 - batch loss: 1.9765228033065796 - avg loss: 2.0704060276810807   (start: 43910, end: 43920)\n",
      "Batch 4392 - batch loss: 1.8872013092041016 - avg loss: 2.0703643238981355   (start: 43920, end: 43930)\n",
      "Batch 4393 - batch loss: 2.0328240394592285 - avg loss: 2.0703557803650363   (start: 43930, end: 43940)\n",
      "Batch 4394 - batch loss: 2.757199764251709 - avg loss: 2.0705120588596633   (start: 43940, end: 43950)\n",
      "Batch 4395 - batch loss: 1.4970800876617432 - avg loss: 2.070381614826179   (start: 43950, end: 43960)\n",
      "Batch 4396 - batch loss: 2.125659465789795 - avg loss: 2.070394186545752   (start: 43960, end: 43970)\n",
      "Batch 4397 - batch loss: 2.1121392250061035 - avg loss: 2.070403678368958   (start: 43970, end: 43980)\n",
      "Batch 4398 - batch loss: 2.085843086242676 - avg loss: 2.0704071881229646   (start: 43980, end: 43990)\n",
      "Batch 4399 - batch loss: 2.019653081893921 - avg loss: 2.0703956530988217   (start: 43990, end: 44000)\n",
      "Batch 4400 - batch loss: 2.3526878356933594 - avg loss: 2.070459795835153   (start: 44000, end: 44010)\n",
      "Batch 4401 - batch loss: 1.8372156620025635 - avg loss: 2.0704068098892576   (start: 44010, end: 44020)\n",
      "Batch 4402 - batch loss: 1.3956329822540283 - avg loss: 2.070253556691975   (start: 44020, end: 44030)\n",
      "Batch 4403 - batch loss: 1.493610143661499 - avg loss: 2.070122620403821   (start: 44030, end: 44040)\n",
      "Batch 4404 - batch loss: 1.7827861309051514 - avg loss: 2.070057390780779   (start: 44040, end: 44050)\n",
      "Batch 4405 - batch loss: 1.732016921043396 - avg loss: 2.0699806680232355   (start: 44050, end: 44060)\n",
      "Batch 4406 - batch loss: 1.8294565677642822 - avg loss: 2.0699260902832175   (start: 44060, end: 44070)\n",
      "Batch 4407 - batch loss: 1.9716676473617554 - avg loss: 2.0699037993478906   (start: 44070, end: 44080)\n",
      "Batch 4408 - batch loss: 1.6812999248504639 - avg loss: 2.06981566056937   (start: 44080, end: 44090)\n",
      "Batch 4409 - batch loss: 2.133812427520752 - avg loss: 2.0698301723079076   (start: 44090, end: 44100)\n",
      "Batch 4410 - batch loss: 1.9281269311904907 - avg loss: 2.0697980473382596   (start: 44100, end: 44110)\n",
      "Batch 4411 - batch loss: 1.8626199960708618 - avg loss: 2.069751089484391   (start: 44110, end: 44120)\n",
      "Batch 4412 - batch loss: 2.7220447063446045 - avg loss: 2.0698989013168996   (start: 44120, end: 44130)\n",
      "Batch 4413 - batch loss: 1.6774708032608032 - avg loss: 2.069809995993371   (start: 44130, end: 44140)\n",
      "Batch 4414 - batch loss: 1.7046664953231812 - avg loss: 2.069727290783706   (start: 44140, end: 44150)\n",
      "Batch 4415 - batch loss: 2.2446789741516113 - avg loss: 2.069766908465628   (start: 44150, end: 44160)\n",
      "Batch 4416 - batch loss: 2.00303316116333 - avg loss: 2.0697518000781927   (start: 44160, end: 44170)\n",
      "Batch 4417 - batch loss: 1.5884500741958618 - avg loss: 2.069642858990397   (start: 44170, end: 44180)\n",
      "Batch 4418 - batch loss: 2.321380615234375 - avg loss: 2.0696998261223825   (start: 44180, end: 44190)\n",
      "Batch 4419 - batch loss: 2.312312602996826 - avg loss: 2.0697547158909058   (start: 44190, end: 44200)\n",
      "Batch 4420 - batch loss: 1.468538761138916 - avg loss: 2.0696187249488673   (start: 44200, end: 44210)\n",
      "Batch 4421 - batch loss: 2.972620725631714 - avg loss: 2.069822931642826   (start: 44210, end: 44220)\n",
      "Batch 4422 - batch loss: 1.6259193420410156 - avg loss: 2.069722569085828   (start: 44220, end: 44230)\n",
      "Batch 4423 - batch loss: 1.6852878332138062 - avg loss: 2.069635671541553   (start: 44230, end: 44240)\n",
      "Batch 4424 - batch loss: 1.464141845703125 - avg loss: 2.0694988367786515   (start: 44240, end: 44250)\n",
      "Batch 4425 - batch loss: 3.038036823272705 - avg loss: 2.0697176659667433   (start: 44250, end: 44260)\n",
      "Batch 4426 - batch loss: 2.2522342205047607 - avg loss: 2.0697588940115903   (start: 44260, end: 44270)\n",
      "Batch 4427 - batch loss: 3.0572662353515625 - avg loss: 2.0699819083163193   (start: 44270, end: 44280)\n",
      "Batch 4428 - batch loss: 1.7308273315429688 - avg loss: 2.069905332435359   (start: 44280, end: 44290)\n",
      "Batch 4429 - batch loss: 2.4065585136413574 - avg loss: 2.069981326381455   (start: 44290, end: 44300)\n",
      "Batch 4430 - batch loss: 1.5748306512832642 - avg loss: 2.0698695794450757   (start: 44300, end: 44310)\n",
      "Batch 4431 - batch loss: 2.268921136856079 - avg loss: 2.069914491800087   (start: 44310, end: 44320)\n",
      "Batch 4432 - batch loss: 2.3759474754333496 - avg loss: 2.069983526987011   (start: 44320, end: 44330)\n",
      "Batch 4433 - batch loss: 1.46800696849823 - avg loss: 2.06984776321649   (start: 44330, end: 44340)\n",
      "Batch 4434 - batch loss: 1.9594204425811768 - avg loss: 2.0698228641588496   (start: 44340, end: 44350)\n",
      "Batch 4435 - batch loss: 2.5204126834869385 - avg loss: 2.0699244398620347   (start: 44350, end: 44360)\n",
      "Batch 4436 - batch loss: 2.5452077388763428 - avg loss: 2.0700315580272397   (start: 44360, end: 44370)\n",
      "Batch 4437 - batch loss: 2.8673481941223145 - avg loss: 2.070211214772642   (start: 44370, end: 44380)\n",
      "Batch 4438 - batch loss: 2.791053533554077 - avg loss: 2.070373603220216   (start: 44380, end: 44390)\n",
      "Batch 4439 - batch loss: 2.437312602996826 - avg loss: 2.0704562471390844   (start: 44390, end: 44400)\n",
      "Batch 4440 - batch loss: 1.4708173274993896 - avg loss: 2.0703212237390303   (start: 44400, end: 44410)\n",
      "Batch 4441 - batch loss: 3.446972370147705 - avg loss: 2.070631140701302   (start: 44410, end: 44420)\n",
      "Batch 4442 - batch loss: 1.7337138652801514 - avg loss: 2.0705553096692464   (start: 44420, end: 44430)\n",
      "Batch 4443 - batch loss: 1.2628118991851807 - avg loss: 2.070373549225843   (start: 44430, end: 44440)\n",
      "Batch 4444 - batch loss: 1.8871625661849976 - avg loss: 2.070332331906824   (start: 44440, end: 44450)\n",
      "Batch 4445 - batch loss: 1.831969976425171 - avg loss: 2.0702787191413083   (start: 44450, end: 44460)\n",
      "Batch 4446 - batch loss: 2.3868346214294434 - avg loss: 2.070349903288439   (start: 44460, end: 44470)\n",
      "Batch 4447 - batch loss: 2.796886444091797 - avg loss: 2.07051324333808   (start: 44470, end: 44480)\n",
      "Batch 4448 - batch loss: 1.8571128845214844 - avg loss: 2.0704652774224095   (start: 44480, end: 44490)\n",
      "Batch 4449 - batch loss: 1.9056810140609741 - avg loss: 2.0704282472508675   (start: 44490, end: 44500)\n",
      "Batch 4450 - batch loss: 2.0607051849365234 - avg loss: 2.0704260627839357   (start: 44500, end: 44510)\n",
      "Batch 4451 - batch loss: 1.6150083541870117 - avg loss: 2.0703237677011423   (start: 44510, end: 44520)\n",
      "Batch 4452 - batch loss: 1.424857497215271 - avg loss: 2.070178816820728   (start: 44520, end: 44530)\n",
      "Batch 4453 - batch loss: 2.074368476867676 - avg loss: 2.070179757471838   (start: 44530, end: 44540)\n",
      "Batch 4454 - batch loss: 2.0044703483581543 - avg loss: 2.0701650078850564   (start: 44540, end: 44550)\n",
      "Batch 4455 - batch loss: 2.826864719390869 - avg loss: 2.0703348237987695   (start: 44550, end: 44560)\n",
      "Batch 4456 - batch loss: 1.9654042720794678 - avg loss: 2.0703112809332276   (start: 44560, end: 44570)\n",
      "Batch 4457 - batch loss: 2.0249722003936768 - avg loss: 2.0703011106594413   (start: 44570, end: 44580)\n",
      "Batch 4458 - batch loss: 1.2837255001068115 - avg loss: 2.0701247088629504   (start: 44580, end: 44590)\n",
      "Batch 4459 - batch loss: 2.6449532508850098 - avg loss: 2.070253594186274   (start: 44590, end: 44600)\n",
      "Batch 4460 - batch loss: 1.6622941493988037 - avg loss: 2.0701621439632776   (start: 44600, end: 44610)\n",
      "Batch 4461 - batch loss: 1.746141791343689 - avg loss: 2.070089526224008   (start: 44610, end: 44620)\n",
      "Batch 4462 - batch loss: 1.764389991760254 - avg loss: 2.0700210298013184   (start: 44620, end: 44630)\n",
      "Batch 4463 - batch loss: 2.0145697593688965 - avg loss: 2.0700086079217415   (start: 44630, end: 44640)\n",
      "Batch 4464 - batch loss: 2.7953553199768066 - avg loss: 2.0701710595929743   (start: 44640, end: 44650)\n",
      "Batch 4465 - batch loss: 1.4543523788452148 - avg loss: 2.0700331691584135   (start: 44650, end: 44660)\n",
      "Batch 4466 - batch loss: 2.1105775833129883 - avg loss: 2.0700422455887146   (start: 44660, end: 44670)\n",
      "Batch 4467 - batch loss: 1.6836702823638916 - avg loss: 2.069955770216462   (start: 44670, end: 44680)\n",
      "Batch 4468 - batch loss: 2.3952548503875732 - avg loss: 2.0700285603440456   (start: 44680, end: 44690)\n",
      "Batch 4469 - batch loss: 1.5629929304122925 - avg loss: 2.0699151295543516   (start: 44690, end: 44700)\n",
      "Batch 4470 - batch loss: 2.3692758083343506 - avg loss: 2.069982085644439   (start: 44700, end: 44710)\n",
      "Batch 4471 - batch loss: 1.5952041149139404 - avg loss: 2.069875918835242   (start: 44710, end: 44720)\n",
      "Batch 4472 - batch loss: 2.4463613033294678 - avg loss: 2.0699600872645942   (start: 44720, end: 44730)\n",
      "Batch 4473 - batch loss: 1.916503667831421 - avg loss: 2.069925787662575   (start: 44730, end: 44740)\n",
      "Batch 4474 - batch loss: 1.7160295248031616 - avg loss: 2.069846704698808   (start: 44740, end: 44750)\n",
      "Batch 4475 - batch loss: 1.3772650957107544 - avg loss: 2.0696919724358525   (start: 44750, end: 44760)\n",
      "Batch 4476 - batch loss: 3.029958724975586 - avg loss: 2.0699064613240674   (start: 44760, end: 44770)\n",
      "Batch 4477 - batch loss: 2.216041088104248 - avg loss: 2.06993909522911   (start: 44770, end: 44780)\n",
      "Batch 4478 - batch loss: 2.03358793258667 - avg loss: 2.069930979318719   (start: 44780, end: 44790)\n",
      "Batch 4479 - batch loss: 2.3739662170410156 - avg loss: 2.069998844327139   (start: 44790, end: 44800)\n",
      "Batch 4480 - batch loss: 2.1328907012939453 - avg loss: 2.0700128795552057   (start: 44800, end: 44810)\n",
      "Batch 4481 - batch loss: 2.823173999786377 - avg loss: 2.070180920858247   (start: 44810, end: 44820)\n",
      "Batch 4482 - batch loss: 2.219339609146118 - avg loss: 2.0702141929279074   (start: 44820, end: 44830)\n",
      "Batch 4483 - batch loss: 2.156278133392334 - avg loss: 2.0702333864917932   (start: 44830, end: 44840)\n",
      "Batch 4484 - batch loss: 1.7209678888320923 - avg loss: 2.0701555123563065   (start: 44840, end: 44850)\n",
      "Batch 4485 - batch loss: 2.2815747261047363 - avg loss: 2.0702026410263348   (start: 44850, end: 44860)\n",
      "Batch 4486 - batch loss: 2.716843366622925 - avg loss: 2.0703467552954673   (start: 44860, end: 44870)\n",
      "Batch 4487 - batch loss: 2.0695114135742188 - avg loss: 2.070346569167633   (start: 44870, end: 44880)\n",
      "Batch 4488 - batch loss: 2.250141143798828 - avg loss: 2.070386621423064   (start: 44880, end: 44890)\n",
      "Batch 4489 - batch loss: 2.453117609024048 - avg loss: 2.070471862177541   (start: 44890, end: 44900)\n",
      "Batch 4490 - batch loss: 1.4959489107131958 - avg loss: 2.070343934555304   (start: 44900, end: 44910)\n",
      "Batch 4491 - batch loss: 1.3281868696212769 - avg loss: 2.070178717043075   (start: 44910, end: 44920)\n",
      "Batch 4492 - batch loss: 2.6368954181671143 - avg loss: 2.07030485029505   (start: 44920, end: 44930)\n",
      "Batch 4493 - batch loss: 2.0448689460754395 - avg loss: 2.070299190325264   (start: 44930, end: 44940)\n",
      "Batch 4494 - batch loss: 2.921553611755371 - avg loss: 2.0704885683945475   (start: 44940, end: 44950)\n",
      "Batch 4495 - batch loss: 2.003533124923706 - avg loss: 2.070473676169576   (start: 44950, end: 44960)\n",
      "Batch 4496 - batch loss: 2.622713327407837 - avg loss: 2.070596477959934   (start: 44960, end: 44970)\n",
      "Batch 4497 - batch loss: 1.8149359226226807 - avg loss: 2.0705396392415394   (start: 44970, end: 44980)\n",
      "Batch 4498 - batch loss: 3.224196195602417 - avg loss: 2.0707960643485324   (start: 44980, end: 44990)\n",
      "Batch 4499 - batch loss: 1.8780263662338257 - avg loss: 2.0707532266378403   (start: 44990, end: 45000)\n",
      "Batch 4500 - batch loss: 1.374983549118042 - avg loss: 2.0705986455053096   (start: 45000, end: 45010)\n",
      "Batch 4501 - batch loss: 2.160508632659912 - avg loss: 2.0706186166264016   (start: 45010, end: 45020)\n",
      "Batch 4502 - batch loss: 1.3371331691741943 - avg loss: 2.070455728452417   (start: 45020, end: 45030)\n",
      "Batch 4503 - batch loss: 1.9594504833221436 - avg loss: 2.0704310825276546   (start: 45030, end: 45040)\n",
      "Batch 4504 - batch loss: 1.5992529392242432 - avg loss: 2.070326492484746   (start: 45040, end: 45050)\n",
      "Batch 4505 - batch loss: 2.0642569065093994 - avg loss: 2.0703251454838636   (start: 45050, end: 45060)\n",
      "Batch 4506 - batch loss: 0.9810171127319336 - avg loss: 2.0700834529982295   (start: 45060, end: 45070)\n",
      "Batch 4507 - batch loss: 3.2315070629119873 - avg loss: 2.0703410891140046   (start: 45070, end: 45080)\n",
      "Batch 4508 - batch loss: 2.0387234687805176 - avg loss: 2.07033407700038   (start: 45080, end: 45090)\n",
      "Batch 4509 - batch loss: 1.6877963542938232 - avg loss: 2.07024925710621   (start: 45090, end: 45100)\n",
      "Batch 4510 - batch loss: 2.0462708473205566 - avg loss: 2.0702439415642493   (start: 45100, end: 45110)\n",
      "Batch 4511 - batch loss: 2.439563274383545 - avg loss: 2.0703257942532605   (start: 45110, end: 45120)\n",
      "Batch 4512 - batch loss: 1.7596772909164429 - avg loss: 2.07025696010672   (start: 45120, end: 45130)\n",
      "Batch 4513 - batch loss: 1.8143091201782227 - avg loss: 2.0702002592117426   (start: 45130, end: 45140)\n",
      "Batch 4514 - batch loss: 1.8874486684799194 - avg loss: 2.070159782668945   (start: 45140, end: 45150)\n",
      "Batch 4515 - batch loss: 1.7561298608779907 - avg loss: 2.0700902454852   (start: 45150, end: 45160)\n",
      "Batch 4516 - batch loss: 1.4415380954742432 - avg loss: 2.0699510929171216   (start: 45160, end: 45170)\n",
      "Batch 4517 - batch loss: 1.7018200159072876 - avg loss: 2.0698696119350477   (start: 45170, end: 45180)\n",
      "Batch 4518 - batch loss: 1.861725091934204 - avg loss: 2.069823552072246   (start: 45180, end: 45190)\n",
      "Batch 4519 - batch loss: 1.4622600078582764 - avg loss: 2.0696891353589244   (start: 45190, end: 45200)\n",
      "Batch 4520 - batch loss: 2.5579934120178223 - avg loss: 2.0697971433829587   (start: 45200, end: 45210)\n",
      "Batch 4521 - batch loss: 1.7489368915557861 - avg loss: 2.069726187997769   (start: 45210, end: 45220)\n",
      "Batch 4522 - batch loss: 2.828310012817383 - avg loss: 2.069893904961028   (start: 45220, end: 45230)\n",
      "Batch 4523 - batch loss: 1.7862615585327148 - avg loss: 2.06983120992424   (start: 45230, end: 45240)\n",
      "Batch 4524 - batch loss: 2.1113250255584717 - avg loss: 2.0698403798282476   (start: 45240, end: 45250)\n",
      "Batch 4525 - batch loss: 1.9142405986785889 - avg loss: 2.0698060007338706   (start: 45250, end: 45260)\n",
      "Batch 4526 - batch loss: 2.2796013355255127 - avg loss: 2.0698523438606196   (start: 45260, end: 45270)\n",
      "Batch 4527 - batch loss: 1.5070146322250366 - avg loss: 2.069728042245859   (start: 45270, end: 45280)\n",
      "Batch 4528 - batch loss: 1.9130992889404297 - avg loss: 2.069693458727796   (start: 45280, end: 45290)\n",
      "Batch 4529 - batch loss: 1.3479669094085693 - avg loss: 2.06953413719373   (start: 45290, end: 45300)\n",
      "Batch 4530 - batch loss: 1.742895483970642 - avg loss: 2.069462047444619   (start: 45300, end: 45310)\n",
      "Batch 4531 - batch loss: 2.0897650718688965 - avg loss: 2.0694665273705732   (start: 45310, end: 45320)\n",
      "Batch 4532 - batch loss: 2.2160959243774414 - avg loss: 2.069498874468964   (start: 45320, end: 45330)\n",
      "Batch 4533 - batch loss: 2.3475677967071533 - avg loss: 2.0695602041827357   (start: 45330, end: 45340)\n",
      "Batch 4534 - batch loss: 2.288048267364502 - avg loss: 2.069608382366458   (start: 45340, end: 45350)\n",
      "Batch 4535 - batch loss: 1.747911810874939 - avg loss: 2.069537461605547   (start: 45350, end: 45360)\n",
      "Batch 4536 - batch loss: 1.530576467514038 - avg loss: 2.069418669233034   (start: 45360, end: 45370)\n",
      "Batch 4537 - batch loss: 1.3706629276275635 - avg loss: 2.069264690444668   (start: 45370, end: 45380)\n",
      "Batch 4538 - batch loss: 1.518884301185608 - avg loss: 2.069143434575697   (start: 45380, end: 45390)\n",
      "Batch 4539 - batch loss: 2.0621395111083984 - avg loss: 2.069141891861277   (start: 45390, end: 45400)\n",
      "Batch 4540 - batch loss: 0.9053019285202026 - avg loss: 2.068885595899299   (start: 45400, end: 45410)\n",
      "Batch 4541 - batch loss: 2.4385228157043457 - avg loss: 2.0689669779380058   (start: 45410, end: 45420)\n",
      "Batch 4542 - batch loss: 2.05981183052063 - avg loss: 2.068964962717355   (start: 45420, end: 45430)\n",
      "Batch 4543 - batch loss: 2.221248149871826 - avg loss: 2.068998475742697   (start: 45430, end: 45440)\n",
      "Batch 4544 - batch loss: 1.4379888772964478 - avg loss: 2.0688596397474392   (start: 45440, end: 45450)\n",
      "Batch 4545 - batch loss: 1.5679428577423096 - avg loss: 2.0687494512780145   (start: 45450, end: 45460)\n",
      "Batch 4546 - batch loss: 3.084336042404175 - avg loss: 2.0689728043880047   (start: 45460, end: 45470)\n",
      "Batch 4547 - batch loss: 1.7291841506958008 - avg loss: 2.068898092722725   (start: 45470, end: 45480)\n",
      "Batch 4548 - batch loss: 1.8968989849090576 - avg loss: 2.068860282411049   (start: 45480, end: 45490)\n",
      "Batch 4549 - batch loss: 2.3416152000427246 - avg loss: 2.0689202285467925   (start: 45490, end: 45500)\n",
      "Batch 4550 - batch loss: 2.4028615951538086 - avg loss: 2.0689936061267984   (start: 45500, end: 45510)\n",
      "Batch 4551 - batch loss: 1.5717854499816895 - avg loss: 2.068884377621494   (start: 45510, end: 45520)\n",
      "Batch 4552 - batch loss: 2.3253579139709473 - avg loss: 2.06894070829058   (start: 45520, end: 45530)\n",
      "Batch 4553 - batch loss: 3.159271717071533 - avg loss: 2.069180130997822   (start: 45530, end: 45540)\n",
      "Batch 4554 - batch loss: 1.499374270439148 - avg loss: 2.06905503640714   (start: 45540, end: 45550)\n",
      "Batch 4555 - batch loss: 1.8729616403579712 - avg loss: 2.0690119957144164   (start: 45550, end: 45560)\n",
      "Batch 4556 - batch loss: 1.6318762302398682 - avg loss: 2.0689160694986   (start: 45560, end: 45570)\n",
      "Batch 4557 - batch loss: 2.3536734580993652 - avg loss: 2.068978543695309   (start: 45570, end: 45580)\n",
      "Batch 4558 - batch loss: 1.7573068141937256 - avg loss: 2.0689101796397047   (start: 45580, end: 45590)\n",
      "Batch 4559 - batch loss: 2.6134867668151855 - avg loss: 2.0690296043298746   (start: 45590, end: 45600)\n",
      "Batch 4560 - batch loss: 1.5731605291366577 - avg loss: 2.068920884953599   (start: 45600, end: 45610)\n",
      "Batch 4561 - batch loss: 1.7220001220703125 - avg loss: 2.0688448391923355   (start: 45610, end: 45620)\n",
      "Batch 4562 - batch loss: 2.2715134620666504 - avg loss: 2.068889254844949   (start: 45620, end: 45630)\n",
      "Batch 4563 - batch loss: 1.9341274499893188 - avg loss: 2.0688597277185563   (start: 45630, end: 45640)\n",
      "Batch 4564 - batch loss: 2.2125229835510254 - avg loss: 2.0688911983112908   (start: 45640, end: 45650)\n",
      "Batch 4565 - batch loss: 1.8051984310150146 - avg loss: 2.0688334469386898   (start: 45650, end: 45660)\n",
      "Batch 4566 - batch loss: 2.390079975128174 - avg loss: 2.0689037877594014   (start: 45660, end: 45670)\n",
      "Batch 4567 - batch loss: 1.5387728214263916 - avg loss: 2.0687877345706243   (start: 45670, end: 45680)\n",
      "Batch 4568 - batch loss: 1.634690523147583 - avg loss: 2.0686927253319674   (start: 45680, end: 45690)\n",
      "Batch 4569 - batch loss: 2.182960271835327 - avg loss: 2.068717729171465   (start: 45690, end: 45700)\n",
      "Batch 4570 - batch loss: 2.234863519668579 - avg loss: 2.0687540769707424   (start: 45700, end: 45710)\n",
      "Batch 4571 - batch loss: 2.520015239715576 - avg loss: 2.0688527780124626   (start: 45710, end: 45720)\n",
      "Batch 4572 - batch loss: 1.001673936843872 - avg loss: 2.068619412860228   (start: 45720, end: 45730)\n",
      "Batch 4573 - batch loss: 2.4434571266174316 - avg loss: 2.0687013625134325   (start: 45730, end: 45740)\n",
      "Batch 4574 - batch loss: 2.851274013519287 - avg loss: 2.0688724166448   (start: 45740, end: 45750)\n",
      "Batch 4575 - batch loss: 2.3186538219451904 - avg loss: 2.068927001742112   (start: 45750, end: 45760)\n",
      "Batch 4576 - batch loss: 1.7868671417236328 - avg loss: 2.0688653762537967   (start: 45760, end: 45770)\n",
      "Batch 4577 - batch loss: 2.3501460552215576 - avg loss: 2.0689268180796963   (start: 45770, end: 45780)\n",
      "Batch 4578 - batch loss: 2.645878314971924 - avg loss: 2.0690528175330467   (start: 45780, end: 45790)\n",
      "Batch 4579 - batch loss: 2.571906328201294 - avg loss: 2.0691626108759875   (start: 45790, end: 45800)\n",
      "Batch 4580 - batch loss: 1.9196125268936157 - avg loss: 2.0691299651471113   (start: 45800, end: 45810)\n",
      "Batch 4581 - batch loss: 2.4735569953918457 - avg loss: 2.0692182294487798   (start: 45810, end: 45820)\n",
      "Batch 4582 - batch loss: 2.4446494579315186 - avg loss: 2.0693001476745017   (start: 45820, end: 45830)\n",
      "Batch 4583 - batch loss: 2.5296404361724854 - avg loss: 2.0694005709486065   (start: 45830, end: 45840)\n",
      "Batch 4584 - batch loss: 1.6481454372406006 - avg loss: 2.0693086941473617   (start: 45840, end: 45850)\n",
      "Batch 4585 - batch loss: 2.19463849067688 - avg loss: 2.0693360229298583   (start: 45850, end: 45860)\n",
      "Batch 4586 - batch loss: 2.0500431060791016 - avg loss: 2.0693318169309807   (start: 45860, end: 45870)\n",
      "Batch 4587 - batch loss: 2.2765774726867676 - avg loss: 2.069376988172427   (start: 45870, end: 45880)\n",
      "Batch 4588 - batch loss: 1.579962134361267 - avg loss: 2.0692703386074216   (start: 45880, end: 45890)\n",
      "Batch 4589 - batch loss: 1.7425315380096436 - avg loss: 2.069199153683544   (start: 45890, end: 45900)\n",
      "Batch 4590 - batch loss: 1.5567715167999268 - avg loss: 2.0690875379926523   (start: 45900, end: 45910)\n",
      "Batch 4591 - batch loss: 2.102069616317749 - avg loss: 2.069094720500998   (start: 45910, end: 45920)\n",
      "Batch 4592 - batch loss: 2.3269009590148926 - avg loss: 2.069150850751056   (start: 45920, end: 45930)\n",
      "Batch 4593 - batch loss: 1.9854984283447266 - avg loss: 2.069132641690889   (start: 45930, end: 45940)\n",
      "Batch 4594 - batch loss: 3.4407620429992676 - avg loss: 2.069431146457224   (start: 45940, end: 45950)\n",
      "Batch 4595 - batch loss: 1.699486494064331 - avg loss: 2.069350653713013   (start: 45950, end: 45960)\n",
      "Batch 4596 - batch loss: 2.66396164894104 - avg loss: 2.0694800013299868   (start: 45960, end: 45970)\n",
      "Batch 4597 - batch loss: 1.4163556098937988 - avg loss: 2.069337956007795   (start: 45970, end: 45980)\n",
      "Batch 4598 - batch loss: 1.9016144275665283 - avg loss: 2.0693014864430115   (start: 45980, end: 45990)\n",
      "Batch 4599 - batch loss: 1.5235036611557007 - avg loss: 2.069182834741862   (start: 45990, end: 46000)\n",
      "Batch 4600 - batch loss: 1.5581177473068237 - avg loss: 2.0690717577830626   (start: 46000, end: 46010)\n",
      "Batch 4601 - batch loss: 2.1328980922698975 - avg loss: 2.0690856270430555   (start: 46010, end: 46020)\n",
      "Batch 4602 - batch loss: 1.2384400367736816 - avg loss: 2.06890516960437   (start: 46020, end: 46030)\n",
      "Batch 4603 - batch loss: 2.1749682426452637 - avg loss: 2.068928206761851   (start: 46030, end: 46040)\n",
      "Batch 4604 - batch loss: 1.8738006353378296 - avg loss: 2.068885833782171   (start: 46040, end: 46050)\n",
      "Batch 4605 - batch loss: 1.1543304920196533 - avg loss: 2.068687276391428   (start: 46050, end: 46060)\n",
      "Batch 4606 - batch loss: 1.9185333251953125 - avg loss: 2.0686546838255078   (start: 46060, end: 46070)\n",
      "Batch 4607 - batch loss: 2.3903985023498535 - avg loss: 2.0687245067027913   (start: 46070, end: 46080)\n",
      "Batch 4608 - batch loss: 1.6934492588043213 - avg loss: 2.068643084431605   (start: 46080, end: 46090)\n",
      "Batch 4609 - batch loss: 1.9896581172943115 - avg loss: 2.0686259510330935   (start: 46090, end: 46100)\n",
      "Batch 4610 - batch loss: 1.272782564163208 - avg loss: 2.0684533543324064   (start: 46100, end: 46110)\n",
      "Batch 4611 - batch loss: 1.8152844905853271 - avg loss: 2.0683984608233543   (start: 46110, end: 46120)\n",
      "Batch 4612 - batch loss: 2.2420759201049805 - avg loss: 2.068436110391809   (start: 46120, end: 46130)\n",
      "Batch 4613 - batch loss: 2.4840891361236572 - avg loss: 2.0685261955729386   (start: 46130, end: 46140)\n",
      "Batch 4614 - batch loss: 1.5269954204559326 - avg loss: 2.0684088541265426   (start: 46140, end: 46150)\n",
      "Batch 4615 - batch loss: 1.6804943084716797 - avg loss: 2.0683248171799105   (start: 46150, end: 46160)\n",
      "Batch 4616 - batch loss: 2.2586095333099365 - avg loss: 2.0683660311101963   (start: 46160, end: 46170)\n",
      "Batch 4617 - batch loss: 2.3225228786468506 - avg loss: 2.0684210672400223   (start: 46170, end: 46180)\n",
      "Batch 4618 - batch loss: 2.35736346244812 - avg loss: 2.06848362242409   (start: 46180, end: 46190)\n",
      "Batch 4619 - batch loss: 1.9319576025009155 - avg loss: 2.0684540713375266   (start: 46190, end: 46200)\n",
      "Batch 4620 - batch loss: 1.4049252271652222 - avg loss: 2.0683104814556454   (start: 46200, end: 46210)\n",
      "Batch 4621 - batch loss: 1.9677387475967407 - avg loss: 2.0682887221017165   (start: 46210, end: 46220)\n",
      "Batch 4622 - batch loss: 2.2059497833251953 - avg loss: 2.0683184995322215   (start: 46220, end: 46230)\n",
      "Batch 4623 - batch loss: 1.6674811840057373 - avg loss: 2.0682318132615625   (start: 46230, end: 46240)\n",
      "Batch 4624 - batch loss: 1.707413673400879 - avg loss: 2.06815379852862   (start: 46240, end: 46250)\n",
      "Batch 4625 - batch loss: 1.3678057193756104 - avg loss: 2.0680024046507226   (start: 46250, end: 46260)\n",
      "Batch 4626 - batch loss: 1.7977750301361084 - avg loss: 2.067944002365329   (start: 46260, end: 46270)\n",
      "Batch 4627 - batch loss: 2.169907808303833 - avg loss: 2.067966034302654   (start: 46270, end: 46280)\n",
      "Batch 4628 - batch loss: 1.8390800952911377 - avg loss: 2.0679165882151596   (start: 46280, end: 46290)\n",
      "Batch 4629 - batch loss: 2.283806562423706 - avg loss: 2.067963216719308   (start: 46290, end: 46300)\n",
      "Batch 4630 - batch loss: 1.8671001195907593 - avg loss: 2.067919843128911   (start: 46300, end: 46310)\n",
      "Batch 4631 - batch loss: 2.397784948348999 - avg loss: 2.0679910575298655   (start: 46310, end: 46320)\n",
      "Batch 4632 - batch loss: 1.4594790935516357 - avg loss: 2.0678597145633257   (start: 46320, end: 46330)\n",
      "Batch 4633 - batch loss: 1.9182456731796265 - avg loss: 2.067827428408517   (start: 46330, end: 46340)\n",
      "Batch 4634 - batch loss: 1.7717926502227783 - avg loss: 2.0677635589849603   (start: 46340, end: 46350)\n",
      "Batch 4635 - batch loss: 1.9447160959243774 - avg loss: 2.0677370172543603   (start: 46350, end: 46360)\n",
      "Batch 4636 - batch loss: 2.6295058727264404 - avg loss: 2.067858166457611   (start: 46360, end: 46370)\n",
      "Batch 4637 - batch loss: 1.8864940404891968 - avg loss: 2.0678190625063455   (start: 46370, end: 46380)\n",
      "Batch 4638 - batch loss: 2.0418450832366943 - avg loss: 2.0678134634592946   (start: 46380, end: 46390)\n",
      "Batch 4639 - batch loss: 1.3883056640625 - avg loss: 2.0676670178128727   (start: 46390, end: 46400)\n",
      "Batch 4640 - batch loss: 2.442817449569702 - avg loss: 2.067747851777914   (start: 46400, end: 46410)\n",
      "Batch 4641 - batch loss: 1.8493770360946655 - avg loss: 2.0677008093790166   (start: 46410, end: 46420)\n",
      "Batch 4642 - batch loss: 2.758571147918701 - avg loss: 2.067849607642755   (start: 46420, end: 46430)\n",
      "Batch 4643 - batch loss: 2.731055736541748 - avg loss: 2.067992416886704   (start: 46430, end: 46440)\n",
      "Batch 4644 - batch loss: 2.669029712677002 - avg loss: 2.068121811352967   (start: 46440, end: 46450)\n",
      "Batch 4645 - batch loss: 1.8982973098754883 - avg loss: 2.0680852585114953   (start: 46450, end: 46460)\n",
      "Batch 4646 - batch loss: 1.5996650457382202 - avg loss: 2.0679844579492457   (start: 46460, end: 46470)\n",
      "Batch 4647 - batch loss: 2.6650214195251465 - avg loss: 2.068112908242184   (start: 46470, end: 46480)\n",
      "Batch 4648 - batch loss: 2.075287342071533 - avg loss: 2.068114451463055   (start: 46480, end: 46490)\n",
      "Batch 4649 - batch loss: 2.1107916831970215 - avg loss: 2.0681236293623524   (start: 46490, end: 46500)\n",
      "Batch 4650 - batch loss: 1.782983422279358 - avg loss: 2.0680623220720746   (start: 46500, end: 46510)\n",
      "Batch 4651 - batch loss: 2.8118155002593994 - avg loss: 2.068222200227317   (start: 46510, end: 46520)\n",
      "Batch 4652 - batch loss: 2.5559706687927246 - avg loss: 2.068327024742375   (start: 46520, end: 46530)\n",
      "Batch 4653 - batch loss: 1.6868057250976562 - avg loss: 2.068245047668966   (start: 46530, end: 46540)\n",
      "Batch 4654 - batch loss: 2.7392284870147705 - avg loss: 2.0683891901908447   (start: 46540, end: 46550)\n",
      "Batch 4655 - batch loss: 2.2818756103515625 - avg loss: 2.068435042085209   (start: 46550, end: 46560)\n",
      "Batch 4656 - batch loss: 1.6646053791046143 - avg loss: 2.0683483275344297   (start: 46560, end: 46570)\n",
      "Batch 4657 - batch loss: 1.47053861618042 - avg loss: 2.068219987106917   (start: 46570, end: 46580)\n",
      "Batch 4658 - batch loss: 1.2787799835205078 - avg loss: 2.0680505430194334   (start: 46580, end: 46590)\n",
      "Batch 4659 - batch loss: 2.1121559143066406 - avg loss: 2.0680600076913835   (start: 46590, end: 46600)\n",
      "Batch 4660 - batch loss: 2.023736000061035 - avg loss: 2.068050498142439   (start: 46600, end: 46610)\n",
      "Batch 4661 - batch loss: 1.649576187133789 - avg loss: 2.0679607353129645   (start: 46610, end: 46620)\n",
      "Batch 4662 - batch loss: 3.3876876831054688 - avg loss: 2.0682437563182816   (start: 46620, end: 46630)\n",
      "Batch 4663 - batch loss: 2.9301857948303223 - avg loss: 2.068428563787945   (start: 46630, end: 46640)\n",
      "Batch 4664 - batch loss: 2.9667112827301025 - avg loss: 2.0686211217126917   (start: 46640, end: 46650)\n",
      "Batch 4665 - batch loss: 1.9693893194198608 - avg loss: 2.0685998547169153   (start: 46650, end: 46660)\n",
      "Batch 4666 - batch loss: 2.6544384956359863 - avg loss: 2.0687253826022634   (start: 46660, end: 46670)\n",
      "Batch 4667 - batch loss: 2.5439984798431396 - avg loss: 2.068827197747345   (start: 46670, end: 46680)\n",
      "Batch 4668 - batch loss: 2.105611562728882 - avg loss: 2.068835076172057   (start: 46680, end: 46690)\n",
      "Batch 4669 - batch loss: 2.172874927520752 - avg loss: 2.0688573545128173   (start: 46690, end: 46700)\n",
      "Batch 4670 - batch loss: 2.4146053791046143 - avg loss: 2.0689313746422524   (start: 46700, end: 46710)\n",
      "Batch 4671 - batch loss: 2.144716739654541 - avg loss: 2.068947595824832   (start: 46710, end: 46720)\n",
      "Batch 4672 - batch loss: 2.7157795429229736 - avg loss: 2.069086014816293   (start: 46720, end: 46730)\n",
      "Batch 4673 - batch loss: 2.564537763595581 - avg loss: 2.069192016474141   (start: 46730, end: 46740)\n",
      "Batch 4674 - batch loss: 1.1938650608062744 - avg loss: 2.0690047807616985   (start: 46740, end: 46750)\n",
      "Batch 4675 - batch loss: 2.0661067962646484 - avg loss: 2.0690041610045347   (start: 46750, end: 46760)\n",
      "Batch 4676 - batch loss: 1.8113867044448853 - avg loss: 2.0689490792306287   (start: 46760, end: 46770)\n",
      "Batch 4677 - batch loss: 1.9146664142608643 - avg loss: 2.0689160987550044   (start: 46770, end: 46780)\n",
      "Batch 4678 - batch loss: 3.214402675628662 - avg loss: 2.069160913154849   (start: 46780, end: 46790)\n",
      "Batch 4679 - batch loss: 2.509568929672241 - avg loss: 2.0692550174318827   (start: 46790, end: 46800)\n",
      "Batch 4680 - batch loss: 1.6682590246200562 - avg loss: 2.0691693528318376   (start: 46800, end: 46810)\n",
      "Batch 4681 - batch loss: 1.449121117591858 - avg loss: 2.0690369204877026   (start: 46810, end: 46820)\n",
      "Batch 4682 - batch loss: 1.93869149684906 - avg loss: 2.0690090867435984   (start: 46820, end: 46830)\n",
      "Batch 4683 - batch loss: 1.6011810302734375 - avg loss: 2.068909208849391   (start: 46830, end: 46840)\n",
      "Batch 4684 - batch loss: 1.5564701557159424 - avg loss: 2.0687998301827664   (start: 46840, end: 46850)\n",
      "Batch 4685 - batch loss: 1.9282608032226562 - avg loss: 2.06876983892648   (start: 46850, end: 46860)\n",
      "Batch 4686 - batch loss: 1.6537901163101196 - avg loss: 2.068681300474887   (start: 46860, end: 46870)\n",
      "Batch 4687 - batch loss: 2.9198224544525146 - avg loss: 2.068862857888278   (start: 46870, end: 46880)\n",
      "Batch 4688 - batch loss: 2.322537899017334 - avg loss: 2.068916957918376   (start: 46880, end: 46890)\n",
      "Batch 4689 - batch loss: 1.9202038049697876 - avg loss: 2.0688852493569794   (start: 46890, end: 46900)\n",
      "Batch 4690 - batch loss: 2.2404603958129883 - avg loss: 2.0689218247452668   (start: 46900, end: 46910)\n",
      "Batch 4691 - batch loss: 1.44332754611969 - avg loss: 2.068788492631323   (start: 46910, end: 46920)\n",
      "Batch 4692 - batch loss: 1.8174428939819336 - avg loss: 2.0687349350778073   (start: 46920, end: 46930)\n",
      "Batch 4693 - batch loss: 1.2796813249588013 - avg loss: 2.068566836737347   (start: 46930, end: 46940)\n",
      "Batch 4694 - batch loss: 1.7387864589691162 - avg loss: 2.068496595975309   (start: 46940, end: 46950)\n",
      "Batch 4695 - batch loss: 2.1915154457092285 - avg loss: 2.068522792493566   (start: 46950, end: 46960)\n",
      "Batch 4696 - batch loss: 2.4058737754821777 - avg loss: 2.0685946151427013   (start: 46960, end: 46970)\n",
      "Batch 4697 - batch loss: 1.4274053573608398 - avg loss: 2.0684581338192056   (start: 46970, end: 46980)\n",
      "Batch 4698 - batch loss: 2.4407641887664795 - avg loss: 2.0685373647310907   (start: 46980, end: 46990)\n",
      "Batch 4699 - batch loss: 1.5713609457015991 - avg loss: 2.068431582514276   (start: 46990, end: 47000)\n",
      "Batch 4700 - batch loss: 2.058685064315796 - avg loss: 2.068429509228124   (start: 47000, end: 47010)\n",
      "Batch 4701 - batch loss: 1.918375015258789 - avg loss: 2.0683975963200067   (start: 47010, end: 47020)\n",
      "Batch 4702 - batch loss: 3.204634428024292 - avg loss: 2.068639194625706   (start: 47020, end: 47030)\n",
      "Batch 4703 - batch loss: 1.8407723903656006 - avg loss: 2.0685907535533716   (start: 47030, end: 47040)\n",
      "Batch 4704 - batch loss: 2.6811625957489014 - avg loss: 2.068720949481575   (start: 47040, end: 47050)\n",
      "Batch 4705 - batch loss: 1.7016963958740234 - avg loss: 2.0686429587137027   (start: 47050, end: 47060)\n",
      "Batch 4706 - batch loss: 2.002312183380127 - avg loss: 2.0686288667707804   (start: 47060, end: 47070)\n",
      "Batch 4707 - batch loss: 2.1101109981536865 - avg loss: 2.0686376777587547   (start: 47070, end: 47080)\n",
      "Batch 4708 - batch loss: 1.9256006479263306 - avg loss: 2.0686073025135157   (start: 47080, end: 47090)\n",
      "Batch 4709 - batch loss: 3.018895387649536 - avg loss: 2.0688090621918884   (start: 47090, end: 47100)\n",
      "Batch 4710 - batch loss: 1.6609910726547241 - avg loss: 2.068722495010921   (start: 47100, end: 47110)\n",
      "Batch 4711 - batch loss: 1.7067266702651978 - avg loss: 2.068645670769676   (start: 47110, end: 47120)\n",
      "Batch 4712 - batch loss: 2.7557811737060547 - avg loss: 2.0687914665479354   (start: 47120, end: 47130)\n",
      "Batch 4713 - batch loss: 1.7461599111557007 - avg loss: 2.068723025403389   (start: 47130, end: 47140)\n",
      "Batch 4714 - batch loss: 3.4575600624084473 - avg loss: 2.0690175825692436   (start: 47140, end: 47150)\n",
      "Batch 4715 - batch loss: 2.6666171550750732 - avg loss: 2.069144300035848   (start: 47150, end: 47160)\n",
      "Batch 4716 - batch loss: 2.83740234375 - avg loss: 2.069307170089635   (start: 47160, end: 47170)\n",
      "Batch 4717 - batch loss: 1.5229851007461548 - avg loss: 2.069191374822712   (start: 47170, end: 47180)\n",
      "Batch 4718 - batch loss: 2.178408145904541 - avg loss: 2.069214518872528   (start: 47180, end: 47190)\n",
      "Batch 4719 - batch loss: 1.749746561050415 - avg loss: 2.069146834983159   (start: 47190, end: 47200)\n",
      "Batch 4720 - batch loss: 2.1126105785369873 - avg loss: 2.0691560414528802   (start: 47200, end: 47210)\n",
      "Batch 4721 - batch loss: 2.042020320892334 - avg loss: 2.0691502947945657   (start: 47210, end: 47220)\n",
      "Batch 4722 - batch loss: 2.6330788135528564 - avg loss: 2.069269695285516   (start: 47220, end: 47230)\n",
      "Batch 4723 - batch loss: 2.708587169647217 - avg loss: 2.0694050292131965   (start: 47230, end: 47240)\n",
      "Batch 4724 - batch loss: 1.4517019987106323 - avg loss: 2.06927429841309   (start: 47240, end: 47250)\n",
      "Batch 4725 - batch loss: 2.8739919662475586 - avg loss: 2.069444572993673   (start: 47250, end: 47260)\n",
      "Batch 4726 - batch loss: 1.7941386699676514 - avg loss: 2.069386331846428   (start: 47260, end: 47270)\n",
      "Batch 4727 - batch loss: 2.1380743980407715 - avg loss: 2.0694008597792104   (start: 47270, end: 47280)\n",
      "Batch 4728 - batch loss: 2.563082218170166 - avg loss: 2.069505254230128   (start: 47280, end: 47290)\n",
      "Batch 4729 - batch loss: 1.7211099863052368 - avg loss: 2.0694315977252815   (start: 47290, end: 47300)\n",
      "Batch 4730 - batch loss: 2.0856833457946777 - avg loss: 2.069435032886573   (start: 47300, end: 47310)\n",
      "Batch 4731 - batch loss: 2.3079135417938232 - avg loss: 2.0694854298664773   (start: 47310, end: 47320)\n",
      "Batch 4732 - batch loss: 2.1174230575561523 - avg loss: 2.0694955582475654   (start: 47320, end: 47330)\n",
      "Batch 4733 - batch loss: 2.9375765323638916 - avg loss: 2.0696789298094824   (start: 47330, end: 47340)\n",
      "Batch 4734 - batch loss: 2.1230359077453613 - avg loss: 2.0696901984426264   (start: 47340, end: 47350)\n",
      "Batch 4735 - batch loss: 1.6701819896697998 - avg loss: 2.06960584282422   (start: 47350, end: 47360)\n",
      "Batch 4736 - batch loss: 1.8641490936279297 - avg loss: 2.0695624700673703   (start: 47360, end: 47370)\n",
      "Batch 4737 - batch loss: 2.817190170288086 - avg loss: 2.0697202640100087   (start: 47370, end: 47380)\n",
      "Batch 4738 - batch loss: 1.275791883468628 - avg loss: 2.0695527332270287   (start: 47380, end: 47390)\n",
      "Batch 4739 - batch loss: 2.5973153114318848 - avg loss: 2.069664075543106   (start: 47390, end: 47400)\n",
      "Batch 4740 - batch loss: 2.8585143089294434 - avg loss: 2.0698304645398125   (start: 47400, end: 47410)\n",
      "Batch 4741 - batch loss: 2.0116095542907715 - avg loss: 2.069818186827824   (start: 47410, end: 47420)\n",
      "Batch 4742 - batch loss: 2.33328914642334 - avg loss: 2.0698737362605875   (start: 47420, end: 47430)\n",
      "Batch 4743 - batch loss: 1.1292078495025635 - avg loss: 2.0696754508713044   (start: 47430, end: 47440)\n",
      "Batch 4744 - batch loss: 1.7060730457305908 - avg loss: 2.0695988223349207   (start: 47440, end: 47450)\n",
      "Batch 4745 - batch loss: 2.4660840034484863 - avg loss: 2.0696823632496097   (start: 47450, end: 47460)\n",
      "Batch 4746 - batch loss: 1.6732927560806274 - avg loss: 2.069598860067143   (start: 47460, end: 47470)\n",
      "Batch 4747 - batch loss: 1.9305235147476196 - avg loss: 2.069569568713874   (start: 47470, end: 47480)\n",
      "Batch 4748 - batch loss: 2.0003011226654053 - avg loss: 2.069554982812411   (start: 47480, end: 47490)\n",
      "Batch 4749 - batch loss: 1.1847338676452637 - avg loss: 2.069368704682902   (start: 47490, end: 47500)\n",
      "Batch 4750 - batch loss: 2.6792032718658447 - avg loss: 2.0694970638845827   (start: 47500, end: 47510)\n",
      "Batch 4751 - batch loss: 1.5488218069076538 - avg loss: 2.069387494175623   (start: 47510, end: 47520)\n",
      "Batch 4752 - batch loss: 2.1092464923858643 - avg loss: 2.0693958802472006   (start: 47520, end: 47530)\n",
      "Batch 4753 - batch loss: 2.303168296813965 - avg loss: 2.0694450540832476   (start: 47530, end: 47540)\n",
      "Batch 4754 - batch loss: 1.9481083154678345 - avg loss: 2.0694195363674504   (start: 47540, end: 47550)\n",
      "Batch 4755 - batch loss: 2.5077764987945557 - avg loss: 2.0695117056194325   (start: 47550, end: 47560)\n",
      "Batch 4756 - batch loss: 1.4769896268844604 - avg loss: 2.069387147688229   (start: 47560, end: 47570)\n",
      "Batch 4757 - batch loss: 1.3454169034957886 - avg loss: 2.0692349891669615   (start: 47570, end: 47580)\n",
      "Batch 4758 - batch loss: 2.7490460872650146 - avg loss: 2.069377836634517   (start: 47580, end: 47590)\n",
      "Batch 4759 - batch loss: 1.2570970058441162 - avg loss: 2.069207189401158   (start: 47590, end: 47600)\n",
      "Batch 4760 - batch loss: 1.7960317134857178 - avg loss: 2.0691498116494427   (start: 47600, end: 47610)\n",
      "Batch 4761 - batch loss: 2.3067119121551514 - avg loss: 2.069199698692808   (start: 47610, end: 47620)\n",
      "Batch 4762 - batch loss: 1.3361926078796387 - avg loss: 2.0690458025998386   (start: 47620, end: 47630)\n",
      "Batch 4763 - batch loss: 1.976923942565918 - avg loss: 2.0690264655175477   (start: 47630, end: 47640)\n",
      "Batch 4764 - batch loss: 1.8910243511199951 - avg loss: 2.0689891093550297   (start: 47640, end: 47650)\n",
      "Batch 4765 - batch loss: 2.219433069229126 - avg loss: 2.0690206754397704   (start: 47650, end: 47660)\n",
      "Batch 4766 - batch loss: 1.45334792137146 - avg loss: 2.0688915223552167   (start: 47660, end: 47670)\n",
      "Batch 4767 - batch loss: 1.9327099323272705 - avg loss: 2.0688629607801268   (start: 47670, end: 47680)\n",
      "Batch 4768 - batch loss: 2.271851062774658 - avg loss: 2.0689055248610653   (start: 47680, end: 47690)\n",
      "Batch 4769 - batch loss: 2.33866810798645 - avg loss: 2.0689620788617202   (start: 47690, end: 47700)\n",
      "Batch 4770 - batch loss: 1.8569707870483398 - avg loss: 2.0689176455580496   (start: 47700, end: 47710)\n",
      "Batch 4771 - batch loss: 1.917784333229065 - avg loss: 2.0688859747046697   (start: 47710, end: 47720)\n",
      "Batch 4772 - batch loss: 2.2019758224487305 - avg loss: 2.0689138586032123   (start: 47720, end: 47730)\n",
      "Batch 4773 - batch loss: 2.8379292488098145 - avg loss: 2.0690749426815964   (start: 47730, end: 47740)\n",
      "Batch 4774 - batch loss: 2.612366199493408 - avg loss: 2.0691887209552746   (start: 47740, end: 47750)\n",
      "Batch 4775 - batch loss: 2.192621946334839 - avg loss: 2.0692145654329503   (start: 47750, end: 47760)\n",
      "Batch 4776 - batch loss: 2.0542984008789062 - avg loss: 2.069211442936707   (start: 47760, end: 47770)\n",
      "Batch 4777 - batch loss: 1.5319445133209229 - avg loss: 2.0690989969489264   (start: 47770, end: 47780)\n",
      "Batch 4778 - batch loss: 2.2718708515167236 - avg loss: 2.0691414267155235   (start: 47780, end: 47790)\n",
      "Batch 4779 - batch loss: 2.3595027923583984 - avg loss: 2.0692021717710976   (start: 47790, end: 47800)\n",
      "Batch 4780 - batch loss: 1.6458442211151123 - avg loss: 2.0691136216872956   (start: 47800, end: 47810)\n",
      "Batch 4781 - batch loss: 2.2367186546325684 - avg loss: 2.069148670836803   (start: 47810, end: 47820)\n",
      "Batch 4782 - batch loss: 2.3290798664093018 - avg loss: 2.0692030156403938   (start: 47820, end: 47830)\n",
      "Batch 4783 - batch loss: 2.54141902923584 - avg loss: 2.069301723001095   (start: 47830, end: 47840)\n",
      "Batch 4784 - batch loss: 2.3981854915618896 - avg loss: 2.0693704552411285   (start: 47840, end: 47850)\n",
      "Batch 4785 - batch loss: 3.544536590576172 - avg loss: 2.06967868050969   (start: 47850, end: 47860)\n",
      "Batch 4786 - batch loss: 2.7414793968200684 - avg loss: 2.0698190190758714   (start: 47860, end: 47870)\n",
      "Batch 4787 - batch loss: 1.6884174346923828 - avg loss: 2.0697393612679384   (start: 47870, end: 47880)\n",
      "Batch 4788 - batch loss: 1.5877710580825806 - avg loss: 2.0696387205698414   (start: 47880, end: 47890)\n",
      "Batch 4789 - batch loss: 1.7356479167938232 - avg loss: 2.0695689938884687   (start: 47890, end: 47900)\n",
      "Batch 4790 - batch loss: 2.5232768058776855 - avg loss: 2.069663693911844   (start: 47900, end: 47910)\n",
      "Batch 4791 - batch loss: 1.9525953531265259 - avg loss: 2.0696392639575896   (start: 47910, end: 47920)\n",
      "Batch 4792 - batch loss: 1.5337464809417725 - avg loss: 2.0695274565753623   (start: 47920, end: 47930)\n",
      "Batch 4793 - batch loss: 2.0981836318969727 - avg loss: 2.069533434083773   (start: 47930, end: 47940)\n",
      "Batch 4794 - batch loss: 1.851163625717163 - avg loss: 2.0694878929350002   (start: 47940, end: 47950)\n",
      "Batch 4795 - batch loss: 1.9809751510620117 - avg loss: 2.0694694374008313   (start: 47950, end: 47960)\n",
      "Batch 4796 - batch loss: 2.319526195526123 - avg loss: 2.06952156513861   (start: 47960, end: 47970)\n",
      "Batch 4797 - batch loss: 2.439971446990967 - avg loss: 2.069598774367842   (start: 47970, end: 47980)\n",
      "Batch 4798 - batch loss: 2.723698854446411 - avg loss: 2.069735073613534   (start: 47980, end: 47990)\n",
      "Batch 4799 - batch loss: 3.002444267272949 - avg loss: 2.06992938802888   (start: 47990, end: 48000)\n",
      "Batch 4800 - batch loss: 2.544297695159912 - avg loss: 2.0700281941749186   (start: 48000, end: 48010)\n",
      "Batch 4801 - batch loss: 1.807037353515625 - avg loss: 2.0699734272360057   (start: 48010, end: 48020)\n",
      "Batch 4802 - batch loss: 2.718628406524658 - avg loss: 2.0701084792824953   (start: 48020, end: 48030)\n",
      "Batch 4803 - batch loss: 2.1985039710998535 - avg loss: 2.0701352060709666   (start: 48030, end: 48040)\n",
      "Batch 4804 - batch loss: 1.999002456665039 - avg loss: 2.070120402168905   (start: 48040, end: 48050)\n",
      "Batch 4805 - batch loss: 2.5500614643096924 - avg loss: 2.0702202650615686   (start: 48050, end: 48060)\n",
      "Batch 4806 - batch loss: 2.5354294776916504 - avg loss: 2.0703170425137487   (start: 48060, end: 48070)\n",
      "Batch 4807 - batch loss: 2.573498010635376 - avg loss: 2.070421697457202   (start: 48070, end: 48080)\n",
      "Batch 4808 - batch loss: 1.4833712577819824 - avg loss: 2.0702996241696834   (start: 48080, end: 48090)\n",
      "Batch 4809 - batch loss: 1.8721981048583984 - avg loss: 2.0702584388226333   (start: 48090, end: 48100)\n",
      "Batch 4810 - batch loss: 1.8084161281585693 - avg loss: 2.070204013066935   (start: 48100, end: 48110)\n",
      "Batch 4811 - batch loss: 2.001959800720215 - avg loss: 2.0701898309779185   (start: 48110, end: 48120)\n",
      "Batch 4812 - batch loss: 1.7155876159667969 - avg loss: 2.070116155055415   (start: 48120, end: 48130)\n",
      "Batch 4813 - batch loss: 1.7512128353118896 - avg loss: 2.070049910078318   (start: 48130, end: 48140)\n",
      "Batch 4814 - batch loss: 2.122060537338257 - avg loss: 2.070060711870065   (start: 48140, end: 48150)\n",
      "Batch 4815 - batch loss: 2.7166831493377686 - avg loss: 2.0701949773263495   (start: 48150, end: 48160)\n",
      "Batch 4816 - batch loss: 1.858283281326294 - avg loss: 2.0701509848629907   (start: 48160, end: 48170)\n",
      "Batch 4817 - batch loss: 2.0129637718200684 - avg loss: 2.0701391153708686   (start: 48170, end: 48180)\n",
      "Batch 4818 - batch loss: 2.1700568199157715 - avg loss: 2.0701598494867737   (start: 48180, end: 48190)\n",
      "Batch 4819 - batch loss: 2.3352160453796387 - avg loss: 2.0702148403987843   (start: 48190, end: 48200)\n",
      "Batch 4820 - batch loss: 2.848115921020508 - avg loss: 2.0703761971879615   (start: 48200, end: 48210)\n",
      "Batch 4821 - batch loss: 2.248929500579834 - avg loss: 2.070413226077093   (start: 48210, end: 48220)\n",
      "Batch 4822 - batch loss: 1.7391446828842163 - avg loss: 2.070344540913669   (start: 48220, end: 48230)\n",
      "Batch 4823 - batch loss: 2.5874314308166504 - avg loss: 2.0704517313966506   (start: 48230, end: 48240)\n",
      "Batch 4824 - batch loss: 2.016854763031006 - avg loss: 2.0704406232166783   (start: 48240, end: 48250)\n",
      "Batch 4825 - batch loss: 2.061889171600342 - avg loss: 2.0704388512623444   (start: 48250, end: 48260)\n",
      "Batch 4826 - batch loss: 2.21305251121521 - avg loss: 2.0704683962509405   (start: 48260, end: 48270)\n",
      "Batch 4827 - batch loss: 1.4994995594024658 - avg loss: 2.070350134271477   (start: 48270, end: 48280)\n",
      "Batch 4828 - batch loss: 1.9079058170318604 - avg loss: 2.070316494942995   (start: 48280, end: 48290)\n",
      "Batch 4829 - batch loss: 1.8037607669830322 - avg loss: 2.0702613074216782   (start: 48290, end: 48300)\n",
      "Batch 4830 - batch loss: 1.9412643909454346 - avg loss: 2.0702346055139   (start: 48300, end: 48310)\n",
      "Batch 4831 - batch loss: 1.6963536739349365 - avg loss: 2.070157229493292   (start: 48310, end: 48320)\n",
      "Batch 4832 - batch loss: 1.9855140447616577 - avg loss: 2.07013971590241   (start: 48320, end: 48330)\n",
      "Batch 4833 - batch loss: 1.2937350273132324 - avg loss: 2.069979102603157   (start: 48330, end: 48340)\n",
      "Batch 4834 - batch loss: 1.6266769170761108 - avg loss: 2.069887416525489   (start: 48340, end: 48350)\n",
      "Batch 4835 - batch loss: 2.466378688812256 - avg loss: 2.0699694039680625   (start: 48350, end: 48360)\n",
      "Batch 4836 - batch loss: 1.867272138595581 - avg loss: 2.069927498393249   (start: 48360, end: 48370)\n",
      "Batch 4837 - batch loss: 2.0911426544189453 - avg loss: 2.069931883501977   (start: 48370, end: 48380)\n",
      "Batch 4838 - batch loss: 1.5792911052703857 - avg loss: 2.06983049049139   (start: 48380, end: 48390)\n",
      "Batch 4839 - batch loss: 1.4548184871673584 - avg loss: 2.0697034218956616   (start: 48390, end: 48400)\n",
      "Batch 4840 - batch loss: 1.1413850784301758 - avg loss: 2.069511660205212   (start: 48400, end: 48410)\n",
      "Batch 4841 - batch loss: 2.3522801399230957 - avg loss: 2.0695700593129605   (start: 48410, end: 48420)\n",
      "Batch 4842 - batch loss: 2.1180107593536377 - avg loss: 2.0695800615223434   (start: 48420, end: 48430)\n",
      "Batch 4843 - batch loss: 1.597487449645996 - avg loss: 2.0694826022713366   (start: 48430, end: 48440)\n",
      "Batch 4844 - batch loss: 1.942516565322876 - avg loss: 2.0694563966909554   (start: 48440, end: 48450)\n",
      "Batch 4845 - batch loss: 1.7317699193954468 - avg loss: 2.0693867131421944   (start: 48450, end: 48460)\n",
      "Batch 4846 - batch loss: 1.9378124475479126 - avg loss: 2.0693595676366043   (start: 48460, end: 48470)\n",
      "Batch 4847 - batch loss: 2.363231658935547 - avg loss: 2.0694201848171527   (start: 48470, end: 48480)\n",
      "Batch 4848 - batch loss: 1.5861759185791016 - avg loss: 2.0693205262759613   (start: 48480, end: 48490)\n",
      "Batch 4849 - batch loss: 2.8824639320373535 - avg loss: 2.069488184710139   (start: 48490, end: 48500)\n",
      "Batch 4850 - batch loss: 1.5034172534942627 - avg loss: 2.0693714931143408   (start: 48500, end: 48510)\n",
      "Batch 4851 - batch loss: 1.8604724407196045 - avg loss: 2.0693284388990905   (start: 48510, end: 48520)\n",
      "Batch 4852 - batch loss: 2.7449939250946045 - avg loss: 2.069467665251078   (start: 48520, end: 48530)\n",
      "Batch 4853 - batch loss: 1.655809998512268 - avg loss: 2.0693824452950134   (start: 48530, end: 48540)\n",
      "Batch 4854 - batch loss: 2.15464448928833 - avg loss: 2.069400006993055   (start: 48540, end: 48550)\n",
      "Batch 4855 - batch loss: 2.8739044666290283 - avg loss: 2.069565679245863   (start: 48550, end: 48560)\n",
      "Batch 4856 - batch loss: 1.4669374227523804 - avg loss: 2.069441605073227   (start: 48560, end: 48570)\n",
      "Batch 4857 - batch loss: 1.7728532552719116 - avg loss: 2.069380553539715   (start: 48570, end: 48580)\n",
      "Batch 4858 - batch loss: 1.4056161642074585 - avg loss: 2.069243948396819   (start: 48580, end: 48590)\n",
      "Batch 4859 - batch loss: 2.323147773742676 - avg loss: 2.069296191982281   (start: 48590, end: 48600)\n",
      "Batch 4860 - batch loss: 2.2624380588531494 - avg loss: 2.0693359249316474   (start: 48600, end: 48610)\n",
      "Batch 4861 - batch loss: 2.1415469646453857 - avg loss: 2.0693507770582857   (start: 48610, end: 48620)\n",
      "Batch 4862 - batch loss: 1.5764974355697632 - avg loss: 2.069249429465958   (start: 48620, end: 48630)\n",
      "Batch 4863 - batch loss: 2.934784173965454 - avg loss: 2.0694273765762583   (start: 48630, end: 48640)\n",
      "Batch 4864 - batch loss: 1.9365432262420654 - avg loss: 2.0694000622596427   (start: 48640, end: 48650)\n",
      "Batch 4865 - batch loss: 1.8280102014541626 - avg loss: 2.0693504548077715   (start: 48650, end: 48660)\n",
      "Batch 4866 - batch loss: 1.1129214763641357 - avg loss: 2.069153941765149   (start: 48660, end: 48670)\n",
      "Batch 4867 - batch loss: 1.5893274545669556 - avg loss: 2.069055374286267   (start: 48670, end: 48680)\n",
      "Batch 4868 - batch loss: 1.2605373859405518 - avg loss: 2.0688893200680814   (start: 48680, end: 48690)\n",
      "Batch 4869 - batch loss: 1.930789589881897 - avg loss: 2.068860962833957   (start: 48690, end: 48700)\n",
      "Batch 4870 - batch loss: 3.1455514430999756 - avg loss: 2.069082003786588   (start: 48700, end: 48710)\n",
      "Batch 4871 - batch loss: 1.9929618835449219 - avg loss: 2.0690663797881803   (start: 48710, end: 48720)\n",
      "Batch 4872 - batch loss: 1.8301684856414795 - avg loss: 2.069017354979203   (start: 48720, end: 48730)\n",
      "Batch 4873 - batch loss: 2.857483148574829 - avg loss: 2.069179124735788   (start: 48730, end: 48740)\n",
      "Batch 4874 - batch loss: 1.997312307357788 - avg loss: 2.069164382824531   (start: 48740, end: 48750)\n",
      "Batch 4875 - batch loss: 0.9877889752388 - avg loss: 2.068942607720432   (start: 48750, end: 48760)\n",
      "Batch 4876 - batch loss: 2.235790967941284 - avg loss: 2.0689768189897   (start: 48760, end: 48770)\n",
      "Batch 4877 - batch loss: 1.4426023960113525 - avg loss: 2.0688484109489096   (start: 48770, end: 48780)\n",
      "Batch 4878 - batch loss: 2.7946763038635254 - avg loss: 2.0689971766576436   (start: 48780, end: 48790)\n",
      "Batch 4879 - batch loss: 2.0612266063690186 - avg loss: 2.0689955843276664   (start: 48790, end: 48800)\n",
      "Batch 4880 - batch loss: 1.4907687902450562 - avg loss: 2.0688771195060967   (start: 48800, end: 48810)\n",
      "Batch 4881 - batch loss: 2.0709779262542725 - avg loss: 2.0688775498229233   (start: 48810, end: 48820)\n",
      "Batch 4882 - batch loss: 1.937758445739746 - avg loss: 2.06885069766153   (start: 48820, end: 48830)\n",
      "Batch 4883 - batch loss: 2.397169589996338 - avg loss: 2.068917921021959   (start: 48830, end: 48840)\n",
      "Batch 4884 - batch loss: 2.5500705242156982 - avg loss: 2.069016416948918   (start: 48840, end: 48850)\n",
      "Batch 4885 - batch loss: 2.282956123352051 - avg loss: 2.0690602032171133   (start: 48850, end: 48860)\n",
      "Batch 4886 - batch loss: 2.9130454063415527 - avg loss: 2.06923290327914   (start: 48860, end: 48870)\n",
      "Batch 4887 - batch loss: 2.1612870693206787 - avg loss: 2.0692517359645004   (start: 48870, end: 48880)\n",
      "Batch 4888 - batch loss: 2.764183521270752 - avg loss: 2.069393877871906   (start: 48880, end: 48890)\n",
      "Batch 4889 - batch loss: 2.3695802688598633 - avg loss: 2.069455265681924   (start: 48890, end: 48900)\n",
      "Batch 4890 - batch loss: 1.7179572582244873 - avg loss: 2.069383399395386   (start: 48900, end: 48910)\n",
      "Batch 4891 - batch loss: 3.037156820297241 - avg loss: 2.0695812271592664   (start: 48910, end: 48920)\n",
      "Batch 4892 - batch loss: 1.467671513557434 - avg loss: 2.069458212707273   (start: 48920, end: 48930)\n",
      "Batch 4893 - batch loss: 1.8274122476577759 - avg loss: 2.0694087550111044   (start: 48930, end: 48940)\n",
      "Batch 4894 - batch loss: 1.3420512676239014 - avg loss: 2.0692601630831398   (start: 48940, end: 48950)\n",
      "Batch 4895 - batch loss: 1.438713550567627 - avg loss: 2.0691313749678386   (start: 48950, end: 48960)\n",
      "Batch 4896 - batch loss: 2.529590606689453 - avg loss: 2.069225403808296   (start: 48960, end: 48970)\n",
      "Batch 4897 - batch loss: 1.9248406887054443 - avg loss: 2.0691959255079486   (start: 48970, end: 48980)\n",
      "Batch 4898 - batch loss: 1.111727237701416 - avg loss: 2.069000483848874   (start: 48980, end: 48990)\n",
      "Batch 4899 - batch loss: 2.2572455406188965 - avg loss: 2.0690389012073984   (start: 48990, end: 49000)\n",
      "Batch 4900 - batch loss: 1.7877362966537476 - avg loss: 2.068981504226261   (start: 49000, end: 49010)\n",
      "Batch 4901 - batch loss: 1.7222769260406494 - avg loss: 2.0689107770581288   (start: 49010, end: 49020)\n",
      "Batch 4902 - batch loss: 2.4137158393859863 - avg loss: 2.068981102381875   (start: 49020, end: 49030)\n",
      "Batch 4903 - batch loss: 1.9668028354644775 - avg loss: 2.0689602666830744   (start: 49030, end: 49040)\n",
      "Batch 4904 - batch loss: 2.55234432220459 - avg loss: 2.0690588159298677   (start: 49040, end: 49050)\n",
      "Batch 4905 - batch loss: 1.8220279216766357 - avg loss: 2.069008463118157   (start: 49050, end: 49060)\n",
      "Batch 4906 - batch loss: 2.763734817504883 - avg loss: 2.069150041751617   (start: 49060, end: 49070)\n",
      "Batch 4907 - batch loss: 1.4511626958847046 - avg loss: 2.0690241274594676   (start: 49070, end: 49080)\n",
      "Batch 4908 - batch loss: 1.7877155542373657 - avg loss: 2.0689668228000215   (start: 49080, end: 49090)\n",
      "Batch 4909 - batch loss: 2.317262887954712 - avg loss: 2.069017392263393   (start: 49090, end: 49100)\n",
      "Batch 4910 - batch loss: 2.4338839054107666 - avg loss: 2.06909168803068   (start: 49100, end: 49110)\n",
      "Batch 4911 - batch loss: 2.2631828784942627 - avg loss: 2.0691312017095207   (start: 49110, end: 49120)\n",
      "Batch 4912 - batch loss: 2.8790931701660156 - avg loss: 2.0692960626841708   (start: 49120, end: 49130)\n",
      "Batch 4913 - batch loss: 1.7831933498382568 - avg loss: 2.0692378407238845   (start: 49130, end: 49140)\n",
      "Batch 4914 - batch loss: 1.7079343795776367 - avg loss: 2.069164330355391   (start: 49140, end: 49150)\n",
      "Batch 4915 - batch loss: 1.50017511844635 - avg loss: 2.0690485880421465   (start: 49150, end: 49160)\n",
      "Batch 4916 - batch loss: 2.5485520362854004 - avg loss: 2.0691461075557207   (start: 49160, end: 49170)\n",
      "Batch 4917 - batch loss: 2.0157461166381836 - avg loss: 2.0691352494851802   (start: 49170, end: 49180)\n",
      "Batch 4918 - batch loss: 1.580749750137329 - avg loss: 2.0690359639597995   (start: 49180, end: 49190)\n",
      "Batch 4919 - batch loss: 2.5241312980651855 - avg loss: 2.069128463011447   (start: 49190, end: 49200)\n",
      "Batch 4920 - batch loss: 1.6198170185089111 - avg loss: 2.069037158105025   (start: 49200, end: 49210)\n",
      "Batch 4921 - batch loss: 2.3190784454345703 - avg loss: 2.0690879588541775   (start: 49210, end: 49220)\n",
      "Batch 4922 - batch loss: 1.5431263446807861 - avg loss: 2.068981121231961   (start: 49220, end: 49230)\n",
      "Batch 4923 - batch loss: 2.2116293907165527 - avg loss: 2.0690100912298255   (start: 49230, end: 49240)\n",
      "Batch 4924 - batch loss: 1.380736231803894 - avg loss: 2.0688703401923783   (start: 49240, end: 49250)\n",
      "Batch 4925 - batch loss: 1.8123811483383179 - avg loss: 2.068818271740926   (start: 49250, end: 49260)\n",
      "Batch 4926 - batch loss: 2.9382307529449463 - avg loss: 2.068994730535569   (start: 49260, end: 49270)\n",
      "Batch 4927 - batch loss: 2.2996320724487305 - avg loss: 2.069041531944236   (start: 49270, end: 49280)\n",
      "Batch 4928 - batch loss: 1.834659218788147 - avg loss: 2.0689939802475115   (start: 49280, end: 49290)\n",
      "Batch 4929 - batch loss: 2.096449375152588 - avg loss: 2.0689995492931312   (start: 49290, end: 49300)\n",
      "Batch 4930 - batch loss: 1.7951492071151733 - avg loss: 2.0689440128213854   (start: 49300, end: 49310)\n",
      "Batch 4931 - batch loss: 1.3168525695800781 - avg loss: 2.0687915206390577   (start: 49310, end: 49320)\n",
      "Batch 4932 - batch loss: 1.9808295965194702 - avg loss: 2.0687736893144844   (start: 49320, end: 49330)\n",
      "Batch 4933 - batch loss: 1.5171492099761963 - avg loss: 2.0686618886498436   (start: 49330, end: 49340)\n",
      "Batch 4934 - batch loss: 2.3115293979644775 - avg loss: 2.068711101924274   (start: 49340, end: 49350)\n",
      "Batch 4935 - batch loss: 2.2127881050109863 - avg loss: 2.068740290944348   (start: 49350, end: 49360)\n",
      "Batch 4936 - batch loss: 1.3334656953811646 - avg loss: 2.068591359488897   (start: 49360, end: 49370)\n",
      "Batch 4937 - batch loss: 3.1317977905273438 - avg loss: 2.068806670633295   (start: 49370, end: 49380)\n",
      "Batch 4938 - batch loss: 2.397125244140625 - avg loss: 2.068873145339411   (start: 49380, end: 49390)\n",
      "Batch 4939 - batch loss: 2.987428665161133 - avg loss: 2.0690590877523305   (start: 49390, end: 49400)\n",
      "Batch 4940 - batch loss: 1.6268584728240967 - avg loss: 2.068969591574446   (start: 49400, end: 49410)\n",
      "Batch 4941 - batch loss: 1.3810778856277466 - avg loss: 2.0688303985946916   (start: 49410, end: 49420)\n",
      "Batch 4942 - batch loss: 1.7650521993637085 - avg loss: 2.068768942353698   (start: 49420, end: 49430)\n",
      "Batch 4943 - batch loss: 2.7157235145568848 - avg loss: 2.0688997988610205   (start: 49430, end: 49440)\n",
      "Batch 4944 - batch loss: 1.9418799877166748 - avg loss: 2.068874112347139   (start: 49440, end: 49450)\n",
      "Batch 4945 - batch loss: 2.3278167247772217 - avg loss: 2.068926466292232   (start: 49450, end: 49460)\n",
      "Batch 4946 - batch loss: 1.8835557699203491 - avg loss: 2.068888994956802   (start: 49460, end: 49470)\n",
      "Batch 4947 - batch loss: 2.5489726066589355 - avg loss: 2.0689860207473645   (start: 49470, end: 49480)\n",
      "Batch 4948 - batch loss: 1.6769020557403564 - avg loss: 2.068906795860517   (start: 49480, end: 49490)\n",
      "Batch 4949 - batch loss: 1.0827950239181519 - avg loss: 2.0687075813611346   (start: 49490, end: 49500)\n",
      "Batch 4950 - batch loss: 1.7606918811798096 - avg loss: 2.0686453685354063   (start: 49500, end: 49510)\n",
      "Batch 4951 - batch loss: 2.21285080909729 - avg loss: 2.0686744891817233   (start: 49510, end: 49520)\n",
      "Batch 4952 - batch loss: 2.349576473236084 - avg loss: 2.0687312026854694   (start: 49520, end: 49530)\n",
      "Batch 4953 - batch loss: 2.1482796669006348 - avg loss: 2.068747260106587   (start: 49530, end: 49540)\n",
      "Batch 4954 - batch loss: 1.8167644739151 - avg loss: 2.0686964058611395   (start: 49540, end: 49550)\n",
      "Batch 4955 - batch loss: 2.2874507904052734 - avg loss: 2.0687405451639127   (start: 49550, end: 49560)\n",
      "Batch 4956 - batch loss: 1.9584500789642334 - avg loss: 2.0687182957255024   (start: 49560, end: 49570)\n",
      "Batch 4957 - batch loss: 1.701655626296997 - avg loss: 2.068644261302463   (start: 49570, end: 49580)\n",
      "Batch 4958 - batch loss: 2.378087282180786 - avg loss: 2.0687066615889886   (start: 49580, end: 49590)\n",
      "Batch 4959 - batch loss: 2.059015989303589 - avg loss: 2.068704707824415   (start: 49590, end: 49600)\n",
      "Batch 4960 - batch loss: 2.213129997253418 - avg loss: 2.0687338199569343   (start: 49600, end: 49610)\n",
      "Batch 4961 - batch loss: 1.1968281269073486 - avg loss: 2.068558103372281   (start: 49610, end: 49620)\n",
      "Batch 4962 - batch loss: 2.020688533782959 - avg loss: 2.068548458083224   (start: 49620, end: 49630)\n",
      "Batch 4963 - batch loss: 1.7690426111221313 - avg loss: 2.0684881224976155   (start: 49630, end: 49640)\n",
      "Batch 4964 - batch loss: 1.079017162322998 - avg loss: 2.0682888332810645   (start: 49640, end: 49650)\n",
      "Batch 4965 - batch loss: 2.532395601272583 - avg loss: 2.0683822901413125   (start: 49650, end: 49660)\n",
      "Batch 4966 - batch loss: 1.8975436687469482 - avg loss: 2.068347895411819   (start: 49660, end: 49670)\n",
      "Batch 4967 - batch loss: 2.94785737991333 - avg loss: 2.06852493033221   (start: 49670, end: 49680)\n",
      "Batch 4968 - batch loss: 1.9167616367340088 - avg loss: 2.068494388312971   (start: 49680, end: 49690)\n",
      "Batch 4969 - batch loss: 1.809430480003357 - avg loss: 2.0684422627781   (start: 49690, end: 49700)\n",
      "Batch 4970 - batch loss: 2.1761536598205566 - avg loss: 2.068463930731639   (start: 49700, end: 49710)\n",
      "Batch 4971 - batch loss: 2.058727741241455 - avg loss: 2.0684619725277993   (start: 49710, end: 49720)\n",
      "Batch 4972 - batch loss: 1.9949928522109985 - avg loss: 2.068447198926288   (start: 49720, end: 49730)\n",
      "Batch 4973 - batch loss: 2.7665274143218994 - avg loss: 2.0685875447677424   (start: 49730, end: 49740)\n",
      "Batch 4974 - batch loss: 2.3685262203216553 - avg loss: 2.0686478339487584   (start: 49740, end: 49750)\n",
      "Batch 4975 - batch loss: 1.4533370733261108 - avg loss: 2.0685241782492763   (start: 49750, end: 49760)\n",
      "Batch 4976 - batch loss: 2.679396152496338 - avg loss: 2.068646917243499   (start: 49760, end: 49770)\n",
      "Batch 4977 - batch loss: 1.2885698080062866 - avg loss: 2.0684902123199884   (start: 49770, end: 49780)\n",
      "Batch 4978 - batch loss: 1.966443657875061 - avg loss: 2.0684697169284547   (start: 49780, end: 49790)\n",
      "Batch 4979 - batch loss: 1.6159948110580444 - avg loss: 2.0683788585136216   (start: 49790, end: 49800)\n",
      "Batch 4980 - batch loss: 2.0926096439361572 - avg loss: 2.0683837231563484   (start: 49800, end: 49810)\n",
      "Batch 4981 - batch loss: 1.7383449077606201 - avg loss: 2.0683174769067705   (start: 49810, end: 49820)\n",
      "Batch 4982 - batch loss: 1.915895700454712 - avg loss: 2.068286888551071   (start: 49820, end: 49830)\n",
      "Batch 4983 - batch loss: 1.7554845809936523 - avg loss: 2.0682241272534068   (start: 49830, end: 49840)\n",
      "Batch 4984 - batch loss: 1.9218215942382812 - avg loss: 2.0681947586409666   (start: 49840, end: 49850)\n",
      "Batch 4985 - batch loss: 3.1121132373809814 - avg loss: 2.068404128572523   (start: 49850, end: 49860)\n",
      "Batch 4986 - batch loss: 1.995733618736267 - avg loss: 2.068389556583384   (start: 49860, end: 49870)\n",
      "Batch 4987 - batch loss: 2.4322993755340576 - avg loss: 2.06846251364412   (start: 49870, end: 49880)\n",
      "Batch 4988 - batch loss: 1.9070056676864624 - avg loss: 2.068430151077281   (start: 49880, end: 49890)\n",
      "Batch 4989 - batch loss: 3.397764205932617 - avg loss: 2.0686965506874726   (start: 49890, end: 49900)\n",
      "Batch 4990 - batch loss: 1.6287676095962524 - avg loss: 2.068608406239248   (start: 49900, end: 49910)\n",
      "Batch 4991 - batch loss: 1.5710861682891846 - avg loss: 2.068508742329402   (start: 49910, end: 49920)\n",
      "Batch 4992 - batch loss: 2.1932313442230225 - avg loss: 2.068533721821069   (start: 49920, end: 49930)\n",
      "Batch 4993 - batch loss: 1.7497678995132446 - avg loss: 2.0684698920608953   (start: 49930, end: 49940)\n",
      "Batch 4994 - batch loss: 2.328024387359619 - avg loss: 2.0685218549228166   (start: 49940, end: 49950)\n",
      "Batch 4995 - batch loss: 1.9890177249908447 - avg loss: 2.0685059413659848   (start: 49950, end: 49960)\n",
      "Batch 4996 - batch loss: 2.4906296730041504 - avg loss: 2.0685904167975715   (start: 49960, end: 49970)\n",
      "Batch 4997 - batch loss: 2.4205899238586426 - avg loss: 2.068660844870213   (start: 49970, end: 49980)\n",
      "Batch 4998 - batch loss: 1.7964379787445068 - avg loss: 2.0686063894058946   (start: 49980, end: 49990)\n",
      "Batch 4999 - batch loss: 2.6642649173736572 - avg loss: 2.0687255211114883   (start: 49990, end: 50000)\n",
      "Batch 5000 - batch loss: 2.104387044906616 - avg loss: 2.0687326519900715   (start: 50000, end: 50010)\n",
      "Batch 5001 - batch loss: 2.234398603439331 - avg loss: 2.0687657719323846   (start: 50010, end: 50020)\n",
      "Batch 5002 - batch loss: 2.957043409347534 - avg loss: 2.068943320930469   (start: 50020, end: 50030)\n",
      "Batch 5003 - batch loss: 1.7643048763275146 - avg loss: 2.0688824419447367   (start: 50030, end: 50040)\n",
      "Batch 5004 - batch loss: 2.484497547149658 - avg loss: 2.0689654819257965   (start: 50040, end: 50050)\n",
      "Batch 5005 - batch loss: 1.8453083038330078 - avg loss: 2.068920804103565   (start: 50050, end: 50060)\n",
      "Batch 5006 - batch loss: 1.6893084049224854 - avg loss: 2.0688449877666004   (start: 50060, end: 50070)\n",
      "Batch 5007 - batch loss: 2.2450203895568848 - avg loss: 2.0688801665608874   (start: 50070, end: 50080)\n",
      "Batch 5008 - batch loss: 1.6859391927719116 - avg loss: 2.0688037159771806   (start: 50080, end: 50090)\n",
      "Batch 5009 - batch loss: 3.0912325382232666 - avg loss: 2.0690077935864113   (start: 50090, end: 50100)\n",
      "Batch 5010 - batch loss: 1.1910059452056885 - avg loss: 2.0688325786895083   (start: 50100, end: 50110)\n",
      "Batch 5011 - batch loss: 2.4730610847473145 - avg loss: 2.068913230825593   (start: 50110, end: 50120)\n",
      "Batch 5012 - batch loss: 2.196383476257324 - avg loss: 2.0689386587620446   (start: 50120, end: 50130)\n",
      "Batch 5013 - batch loss: 2.0600764751434326 - avg loss: 2.0689368912742867   (start: 50130, end: 50140)\n",
      "Batch 5014 - batch loss: 1.846006155014038 - avg loss: 2.0688924384854013   (start: 50140, end: 50150)\n",
      "Batch 5015 - batch loss: 2.9500739574432373 - avg loss: 2.069068112631924   (start: 50150, end: 50160)\n",
      "Batch 5016 - batch loss: 1.5408401489257812 - avg loss: 2.0689628250170733   (start: 50160, end: 50170)\n",
      "Batch 5017 - batch loss: 2.0671639442443848 - avg loss: 2.0689624665314668   (start: 50170, end: 50180)\n",
      "Batch 5018 - batch loss: 1.531243920326233 - avg loss: 2.0688553299412686   (start: 50180, end: 50190)\n",
      "Batch 5019 - batch loss: 1.776894211769104 - avg loss: 2.0687971703559755   (start: 50190, end: 50200)\n",
      "Batch 5020 - batch loss: 1.8652034997940063 - avg loss: 2.0687566219252718   (start: 50200, end: 50210)\n",
      "Batch 5021 - batch loss: 1.591604232788086 - avg loss: 2.0686616095021066   (start: 50210, end: 50220)\n",
      "Batch 5022 - batch loss: 1.7046096324920654 - avg loss: 2.06858913250091   (start: 50220, end: 50230)\n",
      "Batch 5023 - batch loss: 2.1068739891052246 - avg loss: 2.0685967528943423   (start: 50230, end: 50240)\n",
      "Batch 5024 - batch loss: 1.3444108963012695 - avg loss: 2.0684526363059654   (start: 50240, end: 50250)\n",
      "Batch 5025 - batch loss: 2.0016045570373535 - avg loss: 2.06843933585247   (start: 50250, end: 50260)\n",
      "Batch 5026 - batch loss: 1.6790649890899658 - avg loss: 2.0683618792487777   (start: 50260, end: 50270)\n",
      "Batch 5027 - batch loss: 2.77453875541687 - avg loss: 2.068502328110386   (start: 50270, end: 50280)\n",
      "Batch 5028 - batch loss: 2.581886053085327 - avg loss: 2.0686044127643877   (start: 50280, end: 50290)\n",
      "Batch 5029 - batch loss: 1.5929661989212036 - avg loss: 2.068509852483306   (start: 50290, end: 50300)\n",
      "Batch 5030 - batch loss: 1.3300578594207764 - avg loss: 2.0683630721229274   (start: 50300, end: 50310)\n",
      "Batch 5031 - batch loss: 1.7903883457183838 - avg loss: 2.068307830722609   (start: 50310, end: 50320)\n",
      "Batch 5032 - batch loss: 2.1129822731018066 - avg loss: 2.0683167070274724   (start: 50320, end: 50330)\n",
      "Batch 5033 - batch loss: 1.9524770975112915 - avg loss: 2.068293695583389   (start: 50330, end: 50340)\n",
      "Batch 5034 - batch loss: 2.221261739730835 - avg loss: 2.068324076525623   (start: 50340, end: 50350)\n",
      "Batch 5035 - batch loss: 3.1151833534240723 - avg loss: 2.068531951679892   (start: 50350, end: 50360)\n",
      "Batch 5036 - batch loss: 2.1709237098693848 - avg loss: 2.0685522796048845   (start: 50360, end: 50370)\n",
      "Batch 5037 - batch loss: 1.9879920482635498 - avg loss: 2.068536289086556   (start: 50370, end: 50380)\n",
      "Batch 5038 - batch loss: 1.8509479761123657 - avg loss: 2.0684931082346063   (start: 50380, end: 50390)\n",
      "Batch 5039 - batch loss: 2.580418109893799 - avg loss: 2.06859468065557   (start: 50390, end: 50400)\n",
      "Batch 5040 - batch loss: 2.1493325233459473 - avg loss: 2.068610696890978   (start: 50400, end: 50410)\n",
      "Batch 5041 - batch loss: 3.539109468460083 - avg loss: 2.0689023467861722   (start: 50410, end: 50420)\n",
      "Batch 5042 - batch loss: 2.2288432121276855 - avg loss: 2.068934062206625   (start: 50420, end: 50430)\n",
      "Batch 5043 - batch loss: 2.901918888092041 - avg loss: 2.069099205907236   (start: 50430, end: 50440)\n",
      "Batch 5044 - batch loss: 2.042201519012451 - avg loss: 2.069093874353838   (start: 50440, end: 50450)\n",
      "Batch 5045 - batch loss: 1.7807109355926514 - avg loss: 2.0690367235534493   (start: 50450, end: 50460)\n",
      "Batch 5046 - batch loss: 1.7541208267211914 - avg loss: 2.068974326902601   (start: 50460, end: 50470)\n",
      "Batch 5047 - batch loss: 3.26965594291687 - avg loss: 2.0692121798376273   (start: 50470, end: 50480)\n",
      "Batch 5048 - batch loss: 1.610975980758667 - avg loss: 2.069121422024381   (start: 50480, end: 50490)\n",
      "Batch 5049 - batch loss: 1.5777842998504639 - avg loss: 2.069024127544743   (start: 50490, end: 50500)\n",
      "Batch 5050 - batch loss: 2.053077220916748 - avg loss: 2.0690209703666342   (start: 50500, end: 50510)\n",
      "Batch 5051 - batch loss: 2.0885720252990723 - avg loss: 2.069024840330002   (start: 50510, end: 50520)\n",
      "Batch 5052 - batch loss: 2.1198031902313232 - avg loss: 2.0690348894790023   (start: 50520, end: 50530)\n",
      "Batch 5053 - batch loss: 2.512777090072632 - avg loss: 2.0691226896769828   (start: 50530, end: 50540)\n",
      "Batch 5054 - batch loss: 2.3124747276306152 - avg loss: 2.069170830535134   (start: 50540, end: 50550)\n",
      "Batch 5055 - batch loss: 1.667415976524353 - avg loss: 2.0690913695276163   (start: 50550, end: 50560)\n",
      "Batch 5056 - batch loss: 2.687567710876465 - avg loss: 2.0692136705640705   (start: 50560, end: 50570)\n",
      "Batch 5057 - batch loss: 2.570873260498047 - avg loss: 2.0693128519776596   (start: 50570, end: 50580)\n",
      "Batch 5058 - batch loss: 2.323416233062744 - avg loss: 2.069363079963642   (start: 50580, end: 50590)\n",
      "Batch 5059 - batch loss: 1.3163964748382568 - avg loss: 2.0692142723341704   (start: 50590, end: 50600)\n",
      "Batch 5060 - batch loss: 2.311638355255127 - avg loss: 2.0692621727654923   (start: 50600, end: 50610)\n",
      "Batch 5061 - batch loss: 1.7621171474456787 - avg loss: 2.069201496150455   (start: 50610, end: 50620)\n",
      "Batch 5062 - batch loss: 2.162691116333008 - avg loss: 2.0692199614121938   (start: 50620, end: 50630)\n",
      "Batch 5063 - batch loss: 2.420738458633423 - avg loss: 2.069289376597269   (start: 50630, end: 50640)\n",
      "Batch 5064 - batch loss: 2.4582362174987793 - avg loss: 2.069366167681356   (start: 50640, end: 50650)\n",
      "Batch 5065 - batch loss: 1.6059335470199585 - avg loss: 2.069274688680041   (start: 50650, end: 50660)\n",
      "Batch 5066 - batch loss: 1.9738304615020752 - avg loss: 2.069255852242864   (start: 50660, end: 50670)\n",
      "Batch 5067 - batch loss: 2.8039298057556152 - avg loss: 2.069400815532823   (start: 50670, end: 50680)\n",
      "Batch 5068 - batch loss: 1.5857982635498047 - avg loss: 2.069305411596744   (start: 50680, end: 50690)\n",
      "Batch 5069 - batch loss: 1.7570186853408813 - avg loss: 2.0692438165817033   (start: 50690, end: 50700)\n",
      "Batch 5070 - batch loss: 1.925551414489746 - avg loss: 2.069215480474014   (start: 50700, end: 50710)\n",
      "Batch 5071 - batch loss: 1.8673040866851807 - avg loss: 2.0691756714452705   (start: 50710, end: 50720)\n",
      "Batch 5072 - batch loss: 2.451345443725586 - avg loss: 2.069251005522203   (start: 50720, end: 50730)\n",
      "Batch 5073 - batch loss: 2.1793723106384277 - avg loss: 2.069272708578001   (start: 50730, end: 50740)\n",
      "Batch 5074 - batch loss: 1.6201446056365967 - avg loss: 2.069184210429638   (start: 50740, end: 50750)\n",
      "Batch 5075 - batch loss: 1.5579713582992554 - avg loss: 2.069083498677839   (start: 50750, end: 50760)\n",
      "Batch 5076 - batch loss: 1.4082434177398682 - avg loss: 2.0689533351795255   (start: 50760, end: 50770)\n",
      "Batch 5077 - batch loss: 1.5890207290649414 - avg loss: 2.0688588230475613   (start: 50770, end: 50780)\n",
      "Batch 5078 - batch loss: 1.2376936674118042 - avg loss: 2.0686951756453884   (start: 50780, end: 50790)\n",
      "Batch 5079 - batch loss: 1.6725270748138428 - avg loss: 2.068617189798768   (start: 50790, end: 50800)\n",
      "Batch 5080 - batch loss: 1.4701697826385498 - avg loss: 2.0684994083763786   (start: 50800, end: 50810)\n",
      "Batch 5081 - batch loss: 1.8845317363739014 - avg loss: 2.068463208519629   (start: 50810, end: 50820)\n",
      "Batch 5082 - batch loss: 1.7067590951919556 - avg loss: 2.0683920489458876   (start: 50820, end: 50830)\n",
      "Batch 5083 - batch loss: 1.2508299350738525 - avg loss: 2.068231238144575   (start: 50830, end: 50840)\n",
      "Batch 5084 - batch loss: 1.2241566181182861 - avg loss: 2.068065245102289   (start: 50840, end: 50850)\n",
      "Batch 5085 - batch loss: 2.3441948890686035 - avg loss: 2.0681195372068832   (start: 50850, end: 50860)\n",
      "Batch 5086 - batch loss: 2.5119197368621826 - avg loss: 2.068206779235516   (start: 50860, end: 50870)\n",
      "Batch 5087 - batch loss: 1.9930105209350586 - avg loss: 2.068192000096699   (start: 50870, end: 50880)\n",
      "Batch 5088 - batch loss: 1.9971277713775635 - avg loss: 2.0681780358151665   (start: 50880, end: 50890)\n",
      "Batch 5089 - batch loss: 1.4453948736190796 - avg loss: 2.0680556815593323   (start: 50890, end: 50900)\n",
      "Batch 5090 - batch loss: 2.685549259185791 - avg loss: 2.068176972774737   (start: 50900, end: 50910)\n",
      "Batch 5091 - batch loss: 2.061305522918701 - avg loss: 2.0681756233148283   (start: 50910, end: 50920)\n",
      "Batch 5092 - batch loss: 1.820203185081482 - avg loss: 2.0681269344402486   (start: 50920, end: 50930)\n",
      "Batch 5093 - batch loss: 2.4463324546813965 - avg loss: 2.0682011797327973   (start: 50930, end: 50940)\n",
      "Batch 5094 - batch loss: 1.3546944856643677 - avg loss: 2.068061139164776   (start: 50940, end: 50950)\n",
      "Batch 5095 - batch loss: 1.8422925472259521 - avg loss: 2.068016836065887   (start: 50950, end: 50960)\n",
      "Batch 5096 - batch loss: 1.4271382093429565 - avg loss: 2.067891099627448   (start: 50960, end: 50970)\n",
      "Batch 5097 - batch loss: 2.2668306827545166 - avg loss: 2.0679301226920077   (start: 50970, end: 50980)\n",
      "Batch 5098 - batch loss: 2.229064464569092 - avg loss: 2.0679617238573105   (start: 50980, end: 50990)\n",
      "Batch 5099 - batch loss: 1.5842201709747314 - avg loss: 2.067866872572431   (start: 50990, end: 51000)\n",
      "Batch 5100 - batch loss: 1.3806736469268799 - avg loss: 2.0677321552178647   (start: 51000, end: 51010)\n",
      "Batch 5101 - batch loss: 2.2187106609344482 - avg loss: 2.0677617472417213   (start: 51010, end: 51020)\n",
      "Batch 5102 - batch loss: 2.815398693084717 - avg loss: 2.0679082565393583   (start: 51020, end: 51030)\n",
      "Batch 5103 - batch loss: 2.0648138523101807 - avg loss: 2.0679076502689373   (start: 51030, end: 51040)\n",
      "Batch 5104 - batch loss: 1.4677131175994873 - avg loss: 2.0677900803310982   (start: 51040, end: 51050)\n",
      "Batch 5105 - batch loss: 1.8438284397125244 - avg loss: 2.0677462178867936   (start: 51050, end: 51060)\n",
      "Batch 5106 - batch loss: 1.4590785503387451 - avg loss: 2.067627034869847   (start: 51060, end: 51070)\n",
      "Batch 5107 - batch loss: 1.3715121746063232 - avg loss: 2.067490755531502   (start: 51070, end: 51080)\n",
      "Batch 5108 - batch loss: 2.030728816986084 - avg loss: 2.067483560006244   (start: 51080, end: 51090)\n",
      "Batch 5109 - batch loss: 2.2657909393310547 - avg loss: 2.0675223677125696   (start: 51090, end: 51100)\n",
      "Batch 5110 - batch loss: 2.712937593460083 - avg loss: 2.067648647349773   (start: 51100, end: 51110)\n",
      "Batch 5111 - batch loss: 1.9164823293685913 - avg loss: 2.067619076473799   (start: 51110, end: 51120)\n",
      "Batch 5112 - batch loss: 1.724571943283081 - avg loss: 2.0675519833517195   (start: 51120, end: 51130)\n",
      "Batch 5113 - batch loss: 1.0985828638076782 - avg loss: 2.0673625095309247   (start: 51130, end: 51140)\n",
      "Batch 5114 - batch loss: 2.400665521621704 - avg loss: 2.0674276714101216   (start: 51140, end: 51150)\n",
      "Batch 5115 - batch loss: 2.820322036743164 - avg loss: 2.067574836063236   (start: 51150, end: 51160)\n",
      "Batch 5116 - batch loss: 1.957684874534607 - avg loss: 2.0675533605968437   (start: 51160, end: 51170)\n",
      "Batch 5117 - batch loss: 1.8617603778839111 - avg loss: 2.0675131509480136   (start: 51170, end: 51180)\n",
      "Batch 5118 - batch loss: 2.682004451751709 - avg loss: 2.067633192225764   (start: 51180, end: 51190)\n",
      "Batch 5119 - batch loss: 1.8742311000823975 - avg loss: 2.067595418379642   (start: 51190, end: 51200)\n",
      "Batch 5120 - batch loss: 1.8965909481048584 - avg loss: 2.0675620255910707   (start: 51200, end: 51210)\n",
      "Batch 5121 - batch loss: 1.5144293308258057 - avg loss: 2.0674540340458214   (start: 51210, end: 51220)\n",
      "Batch 5122 - batch loss: 3.251368284225464 - avg loss: 2.067685131888917   (start: 51220, end: 51230)\n",
      "Batch 5123 - batch loss: 2.2307498455047607 - avg loss: 2.0677169556035184   (start: 51230, end: 51240)\n",
      "Batch 5124 - batch loss: 1.9915354251861572 - avg loss: 2.0677020909146564   (start: 51240, end: 51250)\n",
      "Batch 5125 - batch loss: 2.738165855407715 - avg loss: 2.0678328875913037   (start: 51250, end: 51260)\n",
      "Batch 5126 - batch loss: 2.4569756984710693 - avg loss: 2.0679087882760863   (start: 51260, end: 51270)\n",
      "Batch 5127 - batch loss: 1.9362026453018188 - avg loss: 2.067883104550857   (start: 51270, end: 51280)\n",
      "Batch 5128 - batch loss: 1.597210168838501 - avg loss: 2.067791337552278   (start: 51280, end: 51290)\n",
      "Batch 5129 - batch loss: 2.0362937450408936 - avg loss: 2.067785197670697   (start: 51290, end: 51300)\n",
      "Batch 5130 - batch loss: 2.3607399463653564 - avg loss: 2.067842292729885   (start: 51300, end: 51310)\n",
      "Batch 5131 - batch loss: 2.612541675567627 - avg loss: 2.0679484305675384   (start: 51310, end: 51320)\n",
      "Batch 5132 - batch loss: 1.7766540050506592 - avg loss: 2.0678916812152073   (start: 51320, end: 51330)\n",
      "Batch 5133 - batch loss: 1.5408809185028076 - avg loss: 2.0677890301122246   (start: 51330, end: 51340)\n",
      "Batch 5134 - batch loss: 1.2376574277877808 - avg loss: 2.067627368651207   (start: 51340, end: 51350)\n",
      "Batch 5135 - batch loss: 1.5764271020889282 - avg loss: 2.067531729970023   (start: 51350, end: 51360)\n",
      "Batch 5136 - batch loss: 1.6549179553985596 - avg loss: 2.067451408036098   (start: 51360, end: 51370)\n",
      "Batch 5137 - batch loss: 3.261733293533325 - avg loss: 2.0676838490414498   (start: 51370, end: 51380)\n",
      "Batch 5138 - batch loss: 1.8546192646026611 - avg loss: 2.067642388721458   (start: 51380, end: 51390)\n",
      "Batch 5139 - batch loss: 2.9517505168914795 - avg loss: 2.0678143941938645   (start: 51390, end: 51400)\n",
      "Batch 5140 - batch loss: 2.2567880153656006 - avg loss: 2.0678511523384224   (start: 51400, end: 51410)\n",
      "Batch 5141 - batch loss: 2.3339169025421143 - avg loss: 2.067902895969345   (start: 51410, end: 51420)\n",
      "Batch 5142 - batch loss: 2.2660505771636963 - avg loss: 2.0679414236149203   (start: 51420, end: 51430)\n",
      "Batch 5143 - batch loss: 1.6721864938735962 - avg loss: 2.067864488364193   (start: 51430, end: 51440)\n",
      "Batch 5144 - batch loss: 2.485717535018921 - avg loss: 2.067945703727974   (start: 51440, end: 51450)\n",
      "Batch 5145 - batch loss: 1.483030080795288 - avg loss: 2.0678320395960403   (start: 51450, end: 51460)\n",
      "Batch 5146 - batch loss: 1.907782793045044 - avg loss: 2.0678009439584746   (start: 51460, end: 51470)\n",
      "Batch 5147 - batch loss: 2.2095067501068115 - avg loss: 2.067828470338845   (start: 51470, end: 51480)\n",
      "Batch 5148 - batch loss: 3.1003940105438232 - avg loss: 2.068029007441235   (start: 51480, end: 51490)\n",
      "Batch 5149 - batch loss: 2.431802272796631 - avg loss: 2.068099643026741   (start: 51490, end: 51500)\n",
      "Batch 5150 - batch loss: 1.7936416864395142 - avg loss: 2.0680463605657455   (start: 51500, end: 51510)\n",
      "Batch 5151 - batch loss: 2.397996425628662 - avg loss: 2.0681104036684363   (start: 51510, end: 51520)\n",
      "Batch 5152 - batch loss: 1.3702521324157715 - avg loss: 2.0679749760978456   (start: 51520, end: 51530)\n",
      "Batch 5153 - batch loss: 1.941523790359497 - avg loss: 2.0679504415255257   (start: 51530, end: 51540)\n",
      "Batch 5154 - batch loss: 1.628509283065796 - avg loss: 2.0678651959079777   (start: 51540, end: 51550)\n",
      "Batch 5155 - batch loss: 1.7802776098251343 - avg loss: 2.0678094186414757   (start: 51550, end: 51560)\n",
      "Batch 5156 - batch loss: 2.7961208820343018 - avg loss: 2.0679506463830686   (start: 51560, end: 51570)\n",
      "Batch 5157 - batch loss: 2.1542677879333496 - avg loss: 2.0679673809975605   (start: 51570, end: 51580)\n",
      "Batch 5158 - batch loss: 2.518570899963379 - avg loss: 2.068054724187901   (start: 51580, end: 51590)\n",
      "Batch 5159 - batch loss: 2.7596004009246826 - avg loss: 2.068188744667889   (start: 51590, end: 51600)\n",
      "Batch 5160 - batch loss: 1.4234364032745361 - avg loss: 2.068063816874555   (start: 51600, end: 51610)\n",
      "Batch 5161 - batch loss: 1.8867990970611572 - avg loss: 2.068028701663433   (start: 51610, end: 51620)\n",
      "Batch 5162 - batch loss: 1.6304744482040405 - avg loss: 2.067943953599621   (start: 51620, end: 51630)\n",
      "Batch 5163 - batch loss: 1.2000097036361694 - avg loss: 2.0677758795775527   (start: 51630, end: 51640)\n",
      "Batch 5164 - batch loss: 2.2519383430480957 - avg loss: 2.067811535427208   (start: 51640, end: 51650)\n",
      "Batch 5165 - batch loss: 2.5456881523132324 - avg loss: 2.0679040396116615   (start: 51650, end: 51660)\n",
      "Batch 5166 - batch loss: 1.7704013586044312 - avg loss: 2.0678464621622696   (start: 51660, end: 51670)\n",
      "Batch 5167 - batch loss: 2.8459296226501465 - avg loss: 2.067997020049361   (start: 51670, end: 51680)\n",
      "Batch 5168 - batch loss: 1.9526054859161377 - avg loss: 2.0679746962857446   (start: 51680, end: 51690)\n",
      "Batch 5169 - batch loss: 2.3364930152893066 - avg loss: 2.068026634065049   (start: 51690, end: 51700)\n",
      "Batch 5170 - batch loss: 1.6637672185897827 - avg loss: 2.0679484558760186   (start: 51700, end: 51710)\n",
      "Batch 5171 - batch loss: 1.1007689237594604 - avg loss: 2.0677614528729027   (start: 51710, end: 51720)\n",
      "Batch 5172 - batch loss: 1.2848354578018188 - avg loss: 2.067610104333357   (start: 51720, end: 51730)\n",
      "Batch 5173 - batch loss: 2.3473641872406006 - avg loss: 2.0676641735414947   (start: 51730, end: 51740)\n",
      "Batch 5174 - batch loss: 1.695259690284729 - avg loss: 2.067592211322508   (start: 51740, end: 51750)\n",
      "Batch 5175 - batch loss: 2.6084885597229004 - avg loss: 2.067696712162616   (start: 51750, end: 51760)\n",
      "Batch 5176 - batch loss: 2.324126958847046 - avg loss: 2.067746244758074   (start: 51760, end: 51770)\n",
      "Batch 5177 - batch loss: 1.94634211063385 - avg loss: 2.067722798613979   (start: 51770, end: 51780)\n",
      "Batch 5178 - batch loss: 1.2675775289535522 - avg loss: 2.067568300589329   (start: 51780, end: 51790)\n",
      "Batch 5179 - batch loss: 1.520355463027954 - avg loss: 2.0674626610453983   (start: 51790, end: 51800)\n",
      "Batch 5180 - batch loss: 1.3037104606628418 - avg loss: 2.0673152469939833   (start: 51800, end: 51810)\n",
      "Batch 5181 - batch loss: 1.7249650955200195 - avg loss: 2.067249181738971   (start: 51810, end: 51820)\n",
      "Batch 5182 - batch loss: 3.1446993350982666 - avg loss: 2.06745706330435   (start: 51820, end: 51830)\n",
      "Batch 5183 - batch loss: 1.3685437440872192 - avg loss: 2.067322242062217   (start: 51830, end: 51840)\n",
      "Batch 5184 - batch loss: 1.6758034229278564 - avg loss: 2.0672467321646018   (start: 51840, end: 51850)\n",
      "Batch 5185 - batch loss: 1.8627545833587646 - avg loss: 2.0672073005894367   (start: 51850, end: 51860)\n",
      "Batch 5186 - batch loss: 2.4813287258148193 - avg loss: 2.067287138920886   (start: 51860, end: 51870)\n",
      "Batch 5187 - batch loss: 0.9521093368530273 - avg loss: 2.067072185605144   (start: 51870, end: 51880)\n",
      "Batch 5188 - batch loss: 1.8831018209457397 - avg loss: 2.067036731690197   (start: 51880, end: 51890)\n",
      "Batch 5189 - batch loss: 2.17999005317688 - avg loss: 2.0670584953359556   (start: 51890, end: 51900)\n",
      "Batch 5190 - batch loss: 1.6970897912979126 - avg loss: 2.0669872241542877   (start: 51900, end: 51910)\n",
      "Batch 5191 - batch loss: 1.5878350734710693 - avg loss: 2.0668949375305044   (start: 51910, end: 51920)\n",
      "Batch 5192 - batch loss: 2.8238162994384766 - avg loss: 2.0670406955435814   (start: 51920, end: 51930)\n",
      "Batch 5193 - batch loss: 1.4831568002700806 - avg loss: 2.0669282804694045   (start: 51930, end: 51940)\n",
      "Batch 5194 - batch loss: 2.6597542762756348 - avg loss: 2.067042395194295   (start: 51940, end: 51950)\n",
      "Batch 5195 - batch loss: 2.038084030151367 - avg loss: 2.067036821990861   (start: 51950, end: 51960)\n",
      "Batch 5196 - batch loss: 1.7662334442138672 - avg loss: 2.0669789417950217   (start: 51960, end: 51970)\n",
      "Batch 5197 - batch loss: 1.9753122329711914 - avg loss: 2.0669613067990955   (start: 51970, end: 51980)\n",
      "Batch 5198 - batch loss: 2.3678910732269287 - avg loss: 2.067019189039224   (start: 51980, end: 51990)\n",
      "Batch 5199 - batch loss: 1.4361588954925537 - avg loss: 2.0668978697520037   (start: 51990, end: 52000)\n",
      "Batch 5200 - batch loss: 1.920192003250122 - avg loss: 2.066869662509838   (start: 52000, end: 52010)\n",
      "Batch 5201 - batch loss: 1.3102200031280518 - avg loss: 2.066724208903652   (start: 52010, end: 52020)\n",
      "Batch 5202 - batch loss: 1.7169586420059204 - avg loss: 2.0666569850776098   (start: 52020, end: 52030)\n",
      "Batch 5203 - batch loss: 2.609898090362549 - avg loss: 2.066761374221592   (start: 52030, end: 52040)\n",
      "Batch 5204 - batch loss: 3.25091552734375 - avg loss: 2.066988877421039   (start: 52040, end: 52050)\n",
      "Batch 5205 - batch loss: 1.609235405921936 - avg loss: 2.0669009493627413   (start: 52050, end: 52060)\n",
      "Batch 5206 - batch loss: 2.1759352684020996 - avg loss: 2.0669218893126238   (start: 52060, end: 52070)\n",
      "Batch 5207 - batch loss: 1.845985770225525 - avg loss: 2.0668794668627224   (start: 52070, end: 52080)\n",
      "Batch 5208 - batch loss: 1.5739500522613525 - avg loss: 2.066784836527802   (start: 52080, end: 52090)\n",
      "Batch 5209 - batch loss: 1.6913087368011475 - avg loss: 2.066712768178526   (start: 52090, end: 52100)\n",
      "Batch 5210 - batch loss: 2.6074070930480957 - avg loss: 2.0668165283636863   (start: 52100, end: 52110)\n",
      "Batch 5211 - batch loss: 1.690338134765625 - avg loss: 2.0667442953641473   (start: 52110, end: 52120)\n",
      "Batch 5212 - batch loss: 2.7451369762420654 - avg loss: 2.0668744301581   (start: 52120, end: 52130)\n",
      "Batch 5213 - batch loss: 2.114872455596924 - avg loss: 2.0668836357632863   (start: 52130, end: 52140)\n",
      "Batch 5214 - batch loss: 3.1654515266418457 - avg loss: 2.0670942911594277   (start: 52140, end: 52150)\n",
      "Batch 5215 - batch loss: 2.5062460899353027 - avg loss: 2.0671784843723833   (start: 52150, end: 52160)\n",
      "Batch 5216 - batch loss: 1.5230592489242554 - avg loss: 2.067074187029955   (start: 52160, end: 52170)\n",
      "Batch 5217 - batch loss: 2.0509629249572754 - avg loss: 2.0670710993982815   (start: 52170, end: 52180)\n",
      "Batch 5218 - batch loss: 2.0782415866851807 - avg loss: 2.0670732397484035   (start: 52180, end: 52190)\n",
      "Batch 5219 - batch loss: 2.1063826084136963 - avg loss: 2.067080770278799   (start: 52190, end: 52200)\n",
      "Batch 5220 - batch loss: 1.2393310070037842 - avg loss: 2.066922227899317   (start: 52200, end: 52210)\n",
      "Batch 5221 - batch loss: 2.608882427215576 - avg loss: 2.0670260119282937   (start: 52210, end: 52220)\n",
      "Batch 5222 - batch loss: 2.045358657836914 - avg loss: 2.0670218634783435   (start: 52220, end: 52230)\n",
      "Batch 5223 - batch loss: 1.2636382579803467 - avg loss: 2.0668680764175664   (start: 52230, end: 52240)\n",
      "Batch 5224 - batch loss: 1.9575843811035156 - avg loss: 2.0668471608777934   (start: 52240, end: 52250)\n",
      "Batch 5225 - batch loss: 1.622083306312561 - avg loss: 2.0667620548972034   (start: 52250, end: 52260)\n",
      "Batch 5226 - batch loss: 1.636400580406189 - avg loss: 2.066679720580293   (start: 52260, end: 52270)\n",
      "Batch 5227 - batch loss: 1.344555377960205 - avg loss: 2.066541594271452   (start: 52270, end: 52280)\n",
      "Batch 5228 - batch loss: 1.5868028402328491 - avg loss: 2.0664498484779847   (start: 52280, end: 52290)\n",
      "Batch 5229 - batch loss: 2.519287347793579 - avg loss: 2.0665364330858846   (start: 52290, end: 52300)\n",
      "Batch 5230 - batch loss: 2.1293187141418457 - avg loss: 2.066548435051294   (start: 52300, end: 52310)\n",
      "Batch 5231 - batch loss: 2.9033424854278564 - avg loss: 2.066708372752054   (start: 52310, end: 52320)\n",
      "Batch 5232 - batch loss: 1.8427810668945312 - avg loss: 2.066665581369318   (start: 52320, end: 52330)\n",
      "Batch 5233 - batch loss: 1.6469414234161377 - avg loss: 2.066585389516442   (start: 52330, end: 52340)\n",
      "Batch 5234 - batch loss: 1.7543351650238037 - avg loss: 2.066525742864199   (start: 52340, end: 52350)\n",
      "Batch 5235 - batch loss: 1.3357360363006592 - avg loss: 2.0663861726375825   (start: 52350, end: 52360)\n",
      "Batch 5236 - batch loss: 2.5192461013793945 - avg loss: 2.066472645795639   (start: 52360, end: 52370)\n",
      "Batch 5237 - batch loss: 1.6497957706451416 - avg loss: 2.0663930969458586   (start: 52370, end: 52380)\n",
      "Batch 5238 - batch loss: 3.032350778579712 - avg loss: 2.0665774752015627   (start: 52380, end: 52390)\n",
      "Batch 5239 - batch loss: 2.500990629196167 - avg loss: 2.0666603784752255   (start: 52390, end: 52400)\n",
      "Batch 5240 - batch loss: 2.55525541305542 - avg loss: 2.0667536040113026   (start: 52400, end: 52410)\n",
      "Batch 5241 - batch loss: 1.6348381042480469 - avg loss: 2.06667120883775   (start: 52410, end: 52420)\n",
      "Batch 5242 - batch loss: 0.9675227999687195 - avg loss: 2.0664615677145632   (start: 52420, end: 52430)\n",
      "Batch 5243 - batch loss: 1.754800796508789 - avg loss: 2.066402135835996   (start: 52430, end: 52440)\n",
      "Batch 5244 - batch loss: 1.3222486972808838 - avg loss: 2.0662602572013813   (start: 52440, end: 52450)\n",
      "Batch 5245 - batch loss: 2.4190354347229004 - avg loss: 2.0663275037087243   (start: 52450, end: 52460)\n",
      "Batch 5246 - batch loss: 1.3281689882278442 - avg loss: 2.066186821697007   (start: 52460, end: 52470)\n",
      "Batch 5247 - batch loss: 2.2389049530029297 - avg loss: 2.0662197329262955   (start: 52470, end: 52480)\n",
      "Batch 5248 - batch loss: 1.599374771118164 - avg loss: 2.0661307931355144   (start: 52480, end: 52490)\n",
      "Batch 5249 - batch loss: 1.7987987995147705 - avg loss: 2.066079872755777   (start: 52490, end: 52500)\n",
      "Batch 5250 - batch loss: 2.2066330909729004 - avg loss: 2.066106639698877   (start: 52500, end: 52510)\n",
      "Batch 5251 - batch loss: 2.145622968673706 - avg loss: 2.0661217798986056   (start: 52510, end: 52520)\n",
      "Batch 5252 - batch loss: 1.8412669897079468 - avg loss: 2.0660789748747734   (start: 52520, end: 52530)\n",
      "Batch 5253 - batch loss: 1.9137893915176392 - avg loss: 2.066049989419243   (start: 52530, end: 52540)\n",
      "Batch 5254 - batch loss: 2.104390859603882 - avg loss: 2.066057285493493   (start: 52540, end: 52550)\n",
      "Batch 5255 - batch loss: 1.2072261571884155 - avg loss: 2.0658938853549267   (start: 52550, end: 52560)\n",
      "Batch 5256 - batch loss: 2.279801845550537 - avg loss: 2.065934575474804   (start: 52560, end: 52570)\n",
      "Batch 5257 - batch loss: 1.9215619564056396 - avg loss: 2.065907117768629   (start: 52570, end: 52580)\n",
      "Batch 5258 - batch loss: 2.7560274600982666 - avg loss: 2.0660383443026333   (start: 52580, end: 52590)\n",
      "Batch 5259 - batch loss: 2.330796241760254 - avg loss: 2.066088678503671   (start: 52590, end: 52600)\n",
      "Batch 5260 - batch loss: 3.1910922527313232 - avg loss: 2.066302516856499   (start: 52600, end: 52610)\n",
      "Batch 5261 - batch loss: 1.5765290260314941 - avg loss: 2.0662094394162054   (start: 52610, end: 52620)\n",
      "Batch 5262 - batch loss: 2.603506088256836 - avg loss: 2.066311528842168   (start: 52620, end: 52630)\n",
      "Batch 5263 - batch loss: 2.19769024848938 - avg loss: 2.0663364868056266   (start: 52630, end: 52640)\n",
      "Batch 5264 - batch loss: 0.6975225210189819 - avg loss: 2.066076503146408   (start: 52640, end: 52650)\n",
      "Batch 5265 - batch loss: 1.9542958736419678 - avg loss: 2.0660552762893047   (start: 52650, end: 52660)\n",
      "Batch 5266 - batch loss: 2.53519868850708 - avg loss: 2.066144348514902   (start: 52660, end: 52670)\n",
      "Batch 5267 - batch loss: 2.4664313793182373 - avg loss: 2.0662203331448947   (start: 52670, end: 52680)\n",
      "Batch 5268 - batch loss: 1.5987956523895264 - avg loss: 2.066131620926114   (start: 52680, end: 52690)\n",
      "Batch 5269 - batch loss: 1.7681699991226196 - avg loss: 2.0660750817189406   (start: 52690, end: 52700)\n",
      "Batch 5270 - batch loss: 2.5447134971618652 - avg loss: 2.066165887716938   (start: 52700, end: 52710)\n",
      "Batch 5271 - batch loss: 1.8343359231948853 - avg loss: 2.066121913899692   (start: 52710, end: 52720)\n",
      "Batch 5272 - batch loss: 3.1605982780456543 - avg loss: 2.066329476267252   (start: 52720, end: 52730)\n",
      "Batch 5273 - batch loss: 1.6139965057373047 - avg loss: 2.0662437096820168   (start: 52730, end: 52740)\n",
      "Batch 5274 - batch loss: 1.2228039503097534 - avg loss: 2.066083815888771   (start: 52740, end: 52750)\n",
      "Batch 5275 - batch loss: 2.6833672523498535 - avg loss: 2.066200814265659   (start: 52750, end: 52760)\n",
      "Batch 5276 - batch loss: 2.383094310760498 - avg loss: 2.0662608660936854   (start: 52760, end: 52770)\n",
      "Batch 5277 - batch loss: 2.6392223834991455 - avg loss: 2.0663694226524965   (start: 52770, end: 52780)\n",
      "Batch 5278 - batch loss: 1.549068808555603 - avg loss: 2.0662714304922205   (start: 52780, end: 52790)\n",
      "Batch 5279 - batch loss: 2.6848368644714355 - avg loss: 2.0663885830365345   (start: 52790, end: 52800)\n",
      "Batch 5280 - batch loss: 1.7280728816986084 - avg loss: 2.0663245202262077   (start: 52800, end: 52810)\n",
      "Batch 5281 - batch loss: 2.23736310005188 - avg loss: 2.0663569016309453   (start: 52810, end: 52820)\n",
      "Batch 5282 - batch loss: 1.9285024404525757 - avg loss: 2.066330807657601   (start: 52820, end: 52830)\n",
      "Batch 5283 - batch loss: 1.587998628616333 - avg loss: 2.0662402830211435   (start: 52830, end: 52840)\n",
      "Batch 5284 - batch loss: 2.7193500995635986 - avg loss: 2.0663638610375186   (start: 52840, end: 52850)\n",
      "Batch 5285 - batch loss: 2.0217864513397217 - avg loss: 2.0663554279293654   (start: 52850, end: 52860)\n",
      "Batch 5286 - batch loss: 1.9576231241226196 - avg loss: 2.0663348619555038   (start: 52860, end: 52870)\n",
      "Batch 5287 - batch loss: 2.0926105976104736 - avg loss: 2.0663398308918985   (start: 52870, end: 52880)\n",
      "Batch 5288 - batch loss: 1.2769649028778076 - avg loss: 2.06619058246535   (start: 52880, end: 52890)\n",
      "Batch 5289 - batch loss: 2.852839946746826 - avg loss: 2.0663392874491464   (start: 52890, end: 52900)\n",
      "Batch 5290 - batch loss: 3.1431784629821777 - avg loss: 2.066542810256845   (start: 52900, end: 52910)\n",
      "Batch 5291 - batch loss: 2.923776626586914 - avg loss: 2.0667047969946246   (start: 52910, end: 52920)\n",
      "Batch 5292 - batch loss: 1.64682936668396 - avg loss: 2.0666254704444054   (start: 52920, end: 52930)\n",
      "Batch 5293 - batch loss: 1.5745524168014526 - avg loss: 2.0665325212465127   (start: 52930, end: 52940)\n",
      "Batch 5294 - batch loss: 2.3834688663482666 - avg loss: 2.0665923770246244   (start: 52940, end: 52950)\n",
      "Batch 5295 - batch loss: 1.8944774866104126 - avg loss: 2.0665598779894254   (start: 52950, end: 52960)\n",
      "Batch 5296 - batch loss: 2.377243995666504 - avg loss: 2.0666185308339933   (start: 52960, end: 52970)\n",
      "Batch 5297 - batch loss: 2.63720965385437 - avg loss: 2.066726230177712   (start: 52970, end: 52980)\n",
      "Batch 5298 - batch loss: 2.6088199615478516 - avg loss: 2.066828531315921   (start: 52980, end: 52990)\n",
      "Batch 5299 - batch loss: 2.1777842044830322 - avg loss: 2.066849466348594   (start: 52990, end: 53000)\n",
      "Batch 5300 - batch loss: 2.083667755126953 - avg loss: 2.066852639012012   (start: 53000, end: 53010)\n",
      "Batch 5301 - batch loss: 1.549168586730957 - avg loss: 2.0667549996207857   (start: 53010, end: 53020)\n",
      "Batch 5302 - batch loss: 1.079355239868164 - avg loss: 2.0665688031735385   (start: 53020, end: 53030)\n",
      "Batch 5303 - batch loss: 1.9498910903930664 - avg loss: 2.0665468051130595   (start: 53030, end: 53040)\n",
      "Batch 5304 - batch loss: 2.461017608642578 - avg loss: 2.0666211634172122   (start: 53040, end: 53050)\n",
      "Batch 5305 - batch loss: 2.025026321411133 - avg loss: 2.0666133242083906   (start: 53050, end: 53060)\n",
      "Batch 5306 - batch loss: 1.9800987243652344 - avg loss: 2.0665970222299013   (start: 53060, end: 53070)\n",
      "Batch 5307 - batch loss: 1.8529329299926758 - avg loss: 2.0665567690098117   (start: 53070, end: 53080)\n",
      "Batch 5308 - batch loss: 1.7521740198135376 - avg loss: 2.0664975520670357   (start: 53080, end: 53090)\n",
      "Batch 5309 - batch loss: 1.7411082983016968 - avg loss: 2.066436273488172   (start: 53090, end: 53100)\n",
      "Batch 5310 - batch loss: 1.5936378240585327 - avg loss: 2.0663472509972234   (start: 53100, end: 53110)\n",
      "Batch 5311 - batch loss: 3.088146686553955 - avg loss: 2.066539607818676   (start: 53110, end: 53120)\n",
      "Batch 5312 - batch loss: 1.5130929946899414 - avg loss: 2.0664354394367583   (start: 53120, end: 53130)\n",
      "Batch 5313 - batch loss: 2.1088290214538574 - avg loss: 2.0664434171526067   (start: 53130, end: 53140)\n",
      "Batch 5314 - batch loss: 1.5958836078643799 - avg loss: 2.0663548828517055   (start: 53140, end: 53150)\n",
      "Batch 5315 - batch loss: 1.5276226997375488 - avg loss: 2.0662535412070264   (start: 53150, end: 53160)\n",
      "Batch 5316 - batch loss: 2.3837172985076904 - avg loss: 2.0663132485151516   (start: 53160, end: 53170)\n",
      "Batch 5317 - batch loss: 1.1756484508514404 - avg loss: 2.0661457673572605   (start: 53170, end: 53180)\n",
      "Batch 5318 - batch loss: 2.4038031101226807 - avg loss: 2.0662092487151784   (start: 53180, end: 53190)\n",
      "Batch 5319 - batch loss: 1.5937879085540771 - avg loss: 2.066120447711389   (start: 53190, end: 53200)\n",
      "Batch 5320 - batch loss: 1.333719253540039 - avg loss: 2.0659828041868313   (start: 53200, end: 53210)\n",
      "Batch 5321 - batch loss: 1.6422698497772217 - avg loss: 2.065903188825236   (start: 53210, end: 53220)\n",
      "Batch 5322 - batch loss: 3.14772629737854 - avg loss: 2.066106424427068   (start: 53220, end: 53230)\n",
      "Batch 5323 - batch loss: 1.9550058841705322 - avg loss: 2.0660855565569975   (start: 53230, end: 53240)\n",
      "Batch 5324 - batch loss: 1.8664264678955078 - avg loss: 2.0660480618924604   (start: 53240, end: 53250)\n",
      "Batch 5325 - batch loss: 2.548103094100952 - avg loss: 2.0661385716619325   (start: 53250, end: 53260)\n",
      "Batch 5326 - batch loss: 2.4977452754974365 - avg loss: 2.066219594133086   (start: 53260, end: 53270)\n",
      "Batch 5327 - batch loss: 1.6598848104476929 - avg loss: 2.066143330097109   (start: 53270, end: 53280)\n",
      "Batch 5328 - batch loss: 2.271573543548584 - avg loss: 2.066181879583589   (start: 53280, end: 53290)\n",
      "Batch 5329 - batch loss: 2.348757743835449 - avg loss: 2.0662348956932046   (start: 53290, end: 53300)\n",
      "Batch 5330 - batch loss: 1.558511734008789 - avg loss: 2.066139655932994   (start: 53300, end: 53310)\n",
      "Batch 5331 - batch loss: 1.128477931022644 - avg loss: 2.065963800395689   (start: 53310, end: 53320)\n",
      "Batch 5332 - batch loss: 2.0626633167266846 - avg loss: 2.0659631815163206   (start: 53320, end: 53330)\n",
      "Batch 5333 - batch loss: 3.2244982719421387 - avg loss: 2.066180379696003   (start: 53330, end: 53340)\n",
      "Batch 5334 - batch loss: 2.5298590660095215 - avg loss: 2.0662672922895013   (start: 53340, end: 53350)\n",
      "Batch 5335 - batch loss: 1.1022746562957764 - avg loss: 2.0660866339993977   (start: 53350, end: 53360)\n",
      "Batch 5336 - batch loss: 2.0054943561553955 - avg loss: 2.066075280752659   (start: 53360, end: 53370)\n",
      "Batch 5337 - batch loss: 2.269115924835205 - avg loss: 2.0661133175911908   (start: 53370, end: 53380)\n",
      "Batch 5338 - batch loss: 1.626989722251892 - avg loss: 2.066031069305868   (start: 53380, end: 53390)\n",
      "Batch 5339 - batch loss: 1.1194263696670532 - avg loss: 2.0658538025081827   (start: 53390, end: 53400)\n",
      "Batch 5340 - batch loss: 1.1650036573410034 - avg loss: 2.0656851355646952   (start: 53400, end: 53410)\n",
      "Batch 5341 - batch loss: 3.0046119689941406 - avg loss: 2.065860898730818   (start: 53410, end: 53420)\n",
      "Batch 5342 - batch loss: 2.4406940937042236 - avg loss: 2.0659310528006243   (start: 53420, end: 53430)\n",
      "Batch 5343 - batch loss: 1.6174681186676025 - avg loss: 2.065847133838399   (start: 53430, end: 53440)\n",
      "Batch 5344 - batch loss: 1.9753777980804443 - avg loss: 2.065830207863514   (start: 53440, end: 53450)\n",
      "Batch 5345 - batch loss: 2.415088176727295 - avg loss: 2.065895538572243   (start: 53450, end: 53460)\n",
      "Batch 5346 - batch loss: 2.4662365913391113 - avg loss: 2.065970410659912   (start: 53460, end: 53470)\n",
      "Batch 5347 - batch loss: 2.2418031692504883 - avg loss: 2.066003288887023   (start: 53470, end: 53480)\n",
      "Batch 5348 - batch loss: 2.865962266921997 - avg loss: 2.066152841883478   (start: 53480, end: 53490)\n",
      "Batch 5349 - batch loss: 1.7604153156280518 - avg loss: 2.0660956946823084   (start: 53490, end: 53500)\n",
      "Batch 5350 - batch loss: 2.607818603515625 - avg loss: 2.0661969323778484   (start: 53500, end: 53510)\n",
      "Batch 5351 - batch loss: 1.8276395797729492 - avg loss: 2.0661523588814723   (start: 53510, end: 53520)\n",
      "Batch 5352 - batch loss: 2.157327890396118 - avg loss: 2.0661693914859023   (start: 53520, end: 53530)\n",
      "Batch 5353 - batch loss: 1.8961708545684814 - avg loss: 2.0661376397980207   (start: 53530, end: 53540)\n",
      "Batch 5354 - batch loss: 1.947082281112671 - avg loss: 2.0661154072380423   (start: 53540, end: 53550)\n",
      "Batch 5355 - batch loss: 2.5093443393707275 - avg loss: 2.066198160959501   (start: 53550, end: 53560)\n",
      "Batch 5356 - batch loss: 2.350205659866333 - avg loss: 2.0662511771063943   (start: 53560, end: 53570)\n",
      "Batch 5357 - batch loss: 1.7780002355575562 - avg loss: 2.066197378871689   (start: 53570, end: 53580)\n",
      "Batch 5358 - batch loss: 1.679722547531128 - avg loss: 2.0661252619037214   (start: 53580, end: 53590)\n",
      "Batch 5359 - batch loss: 2.221464157104492 - avg loss: 2.0661542430408857   (start: 53590, end: 53600)\n",
      "Batch 5360 - batch loss: 1.7904794216156006 - avg loss: 2.066102820764925   (start: 53600, end: 53610)\n",
      "Batch 5361 - batch loss: 2.380359649658203 - avg loss: 2.0661614289016077   (start: 53610, end: 53620)\n",
      "Batch 5362 - batch loss: 2.245272159576416 - avg loss: 2.0661948263900793   (start: 53620, end: 53630)\n",
      "Batch 5363 - batch loss: 1.5712687969207764 - avg loss: 2.0661025583010657   (start: 53630, end: 53640)\n",
      "Batch 5364 - batch loss: 1.6949126720428467 - avg loss: 2.0660333709970105   (start: 53640, end: 53650)\n",
      "Batch 5365 - batch loss: 1.3603708744049072 - avg loss: 2.065901864754634   (start: 53650, end: 53660)\n",
      "Batch 5366 - batch loss: 1.856421709060669 - avg loss: 2.0658628336095446   (start: 53660, end: 53670)\n",
      "Batch 5367 - batch loss: 2.1019175052642822 - avg loss: 2.065869550202625   (start: 53670, end: 53680)\n",
      "Batch 5368 - batch loss: 2.243447780609131 - avg loss: 2.065902624933563   (start: 53680, end: 53690)\n",
      "Batch 5369 - batch loss: 2.1937506198883057 - avg loss: 2.0659264327538525   (start: 53690, end: 53700)\n",
      "Batch 5370 - batch loss: 2.5183889865875244 - avg loss: 2.0660106745251863   (start: 53700, end: 53710)\n",
      "Batch 5371 - batch loss: 1.853168249130249 - avg loss: 2.065971053820533   (start: 53710, end: 53720)\n",
      "Batch 5372 - batch loss: 1.8323745727539062 - avg loss: 2.0659275778329906   (start: 53720, end: 53730)\n",
      "Batch 5373 - batch loss: 1.3547098636627197 - avg loss: 2.065795233636085   (start: 53730, end: 53740)\n",
      "Batch 5374 - batch loss: 0.9606939554214478 - avg loss: 2.065589633398278   (start: 53740, end: 53750)\n",
      "Batch 5375 - batch loss: 1.8290189504623413 - avg loss: 2.065545628434934   (start: 53750, end: 53760)\n",
      "Batch 5376 - batch loss: 1.6831800937652588 - avg loss: 2.065474517121066   (start: 53760, end: 53770)\n",
      "Batch 5377 - batch loss: 1.9043304920196533 - avg loss: 2.065444553561173   (start: 53770, end: 53780)\n",
      "Batch 5378 - batch loss: 2.520673990249634 - avg loss: 2.065529184428749   (start: 53780, end: 53790)\n",
      "Batch 5379 - batch loss: 1.7481849193572998 - avg loss: 2.065470198505873   (start: 53790, end: 53800)\n",
      "Batch 5380 - batch loss: 1.8212201595306396 - avg loss: 2.0654248073074015   (start: 53800, end: 53810)\n",
      "Batch 5381 - batch loss: 1.805458426475525 - avg loss: 2.0653765043752514   (start: 53810, end: 53820)\n",
      "Batch 5382 - batch loss: 2.5130412578582764 - avg loss: 2.065459667063991   (start: 53820, end: 53830)\n",
      "Batch 5383 - batch loss: 1.4168615341186523 - avg loss: 2.065339199357277   (start: 53830, end: 53840)\n",
      "Batch 5384 - batch loss: 1.9327138662338257 - avg loss: 2.0653145706974585   (start: 53840, end: 53850)\n",
      "Batch 5385 - batch loss: 1.6314830780029297 - avg loss: 2.0652340227040136   (start: 53850, end: 53860)\n",
      "Batch 5386 - batch loss: 2.0256009101867676 - avg loss: 2.0652266655270104   (start: 53860, end: 53870)\n",
      "Batch 5387 - batch loss: 1.959247350692749 - avg loss: 2.0652069960179467   (start: 53870, end: 53880)\n",
      "Batch 5388 - batch loss: 1.387629747390747 - avg loss: 2.0650812626261064   (start: 53880, end: 53890)\n",
      "Batch 5389 - batch loss: 1.8343099355697632 - avg loss: 2.0650384479086563   (start: 53890, end: 53900)\n",
      "Batch 5390 - batch loss: 2.1136536598205566 - avg loss: 2.0650474657554216   (start: 53900, end: 53910)\n",
      "Batch 5391 - batch loss: 2.2669730186462402 - avg loss: 2.065084914856477   (start: 53910, end: 53920)\n",
      "Batch 5392 - batch loss: 2.3627350330352783 - avg loss: 2.0651401067938364   (start: 53920, end: 53930)\n",
      "Batch 5393 - batch loss: 1.662689447402954 - avg loss: 2.065065495993059   (start: 53930, end: 53940)\n",
      "Batch 5394 - batch loss: 1.8113552331924438 - avg loss: 2.06501846906761   (start: 53940, end: 53950)\n",
      "Batch 5395 - batch loss: 1.815804123878479 - avg loss: 2.0649722840518225   (start: 53950, end: 53960)\n",
      "Batch 5396 - batch loss: 2.481710195541382 - avg loss: 2.065049500637238   (start: 53960, end: 53970)\n",
      "Batch 5397 - batch loss: 2.6685636043548584 - avg loss: 2.065161303916919   (start: 53970, end: 53980)\n",
      "Batch 5398 - batch loss: 1.991676688194275 - avg loss: 2.065147693134233   (start: 53980, end: 53990)\n",
      "Batch 5399 - batch loss: 2.066049814224243 - avg loss: 2.065147860193694   (start: 53990, end: 54000)\n",
      "Batch 5400 - batch loss: 1.8770643472671509 - avg loss: 2.06511303636238   (start: 54000, end: 54010)\n",
      "Batch 5401 - batch loss: 1.9976800680160522 - avg loss: 2.065100553398969   (start: 54010, end: 54020)\n",
      "Batch 5402 - batch loss: 1.975990653038025 - avg loss: 2.065084060728164   (start: 54020, end: 54030)\n",
      "Batch 5403 - batch loss: 1.594239592552185 - avg loss: 2.0649969318480426   (start: 54030, end: 54040)\n",
      "Batch 5404 - batch loss: 2.162323236465454 - avg loss: 2.0650149385649006   (start: 54040, end: 54050)\n",
      "Batch 5405 - batch loss: 1.8327243328094482 - avg loss: 2.0649719695294295   (start: 54050, end: 54060)\n",
      "Batch 5406 - batch loss: 1.167389988899231 - avg loss: 2.064805965834103   (start: 54060, end: 54070)\n",
      "Batch 5407 - batch loss: 2.6938135623931885 - avg loss: 2.06492227641039   (start: 54070, end: 54080)\n",
      "Batch 5408 - batch loss: 1.6593866348266602 - avg loss: 2.064847302174564   (start: 54080, end: 54090)\n",
      "Batch 5409 - batch loss: 2.398134708404541 - avg loss: 2.064908907979782   (start: 54090, end: 54100)\n",
      "Batch 5410 - batch loss: 1.605592966079712 - avg loss: 2.0648240223871186   (start: 54100, end: 54110)\n",
      "Batch 5411 - batch loss: 1.7396981716156006 - avg loss: 2.064763947396215   (start: 54110, end: 54120)\n",
      "Batch 5412 - batch loss: 2.151782512664795 - avg loss: 2.0647800232442233   (start: 54120, end: 54130)\n",
      "Batch 5413 - batch loss: 2.1978278160095215 - avg loss: 2.064804598012004   (start: 54130, end: 54140)\n",
      "Batch 5414 - batch loss: 2.9725582599639893 - avg loss: 2.064972234884017   (start: 54140, end: 54150)\n",
      "Batch 5415 - batch loss: 1.661590814590454 - avg loss: 2.0648977553012453   (start: 54150, end: 54160)\n",
      "Batch 5416 - batch loss: 1.8262360095977783 - avg loss: 2.0648536973825258   (start: 54160, end: 54170)\n",
      "Batch 5417 - batch loss: 2.2715725898742676 - avg loss: 2.0648918514785928   (start: 54170, end: 54180)\n",
      "Batch 5418 - batch loss: 1.9421517848968506 - avg loss: 2.0648692015308936   (start: 54180, end: 54190)\n",
      "Batch 5419 - batch loss: 1.8199083805084229 - avg loss: 2.064824005807458   (start: 54190, end: 54200)\n",
      "Batch 5420 - batch loss: 2.2790534496307373 - avg loss: 2.064863524243876   (start: 54200, end: 54210)\n",
      "Batch 5421 - batch loss: 2.581869602203369 - avg loss: 2.064958877633393   (start: 54210, end: 54220)\n",
      "Batch 5422 - batch loss: 2.371014356613159 - avg loss: 2.0650153141959926   (start: 54220, end: 54230)\n",
      "Batch 5423 - batch loss: 2.509073495864868 - avg loss: 2.0650971833297813   (start: 54230, end: 54240)\n",
      "Batch 5424 - batch loss: 1.8251514434814453 - avg loss: 2.065052953700316   (start: 54240, end: 54250)\n",
      "Batch 5425 - batch loss: 1.8442884683609009 - avg loss: 2.0650122672857676   (start: 54250, end: 54260)\n",
      "Batch 5426 - batch loss: 2.599092960357666 - avg loss: 2.0651106790589524   (start: 54260, end: 54270)\n",
      "Batch 5427 - batch loss: 1.7197027206420898 - avg loss: 2.065047044578772   (start: 54270, end: 54280)\n",
      "Batch 5428 - batch loss: 1.0449512004852295 - avg loss: 2.064859147020457   (start: 54280, end: 54290)\n",
      "Batch 5429 - batch loss: 2.6782073974609375 - avg loss: 2.0649721024993593   (start: 54290, end: 54300)\n",
      "Batch 5430 - batch loss: 1.5313743352890015 - avg loss: 2.0648738521279344   (start: 54300, end: 54310)\n",
      "Batch 5431 - batch loss: 2.234157085418701 - avg loss: 2.064905016198864   (start: 54310, end: 54320)\n",
      "Batch 5432 - batch loss: 2.3433585166931152 - avg loss: 2.0649562684536944   (start: 54320, end: 54330)\n",
      "Batch 5433 - batch loss: 3.1218478679656982 - avg loss: 2.0651507645154377   (start: 54330, end: 54340)\n",
      "Batch 5434 - batch loss: 2.137629508972168 - avg loss: 2.0651641000709953   (start: 54340, end: 54350)\n",
      "Batch 5435 - batch loss: 1.3284040689468384 - avg loss: 2.0650285665847696   (start: 54350, end: 54360)\n",
      "Batch 5436 - batch loss: 2.6221776008605957 - avg loss: 2.065131040197842   (start: 54360, end: 54370)\n",
      "Batch 5437 - batch loss: 1.5446161031723022 - avg loss: 2.065035322114535   (start: 54370, end: 54380)\n",
      "Batch 5438 - batch loss: 1.9566261768341064 - avg loss: 2.065015390298892   (start: 54380, end: 54390)\n",
      "Batch 5439 - batch loss: 1.6840540170669556 - avg loss: 2.0649453606346952   (start: 54390, end: 54400)\n",
      "Batch 5440 - batch loss: 2.257762908935547 - avg loss: 2.0649807985226385   (start: 54400, end: 54410)\n",
      "Batch 5441 - batch loss: 1.8315786123275757 - avg loss: 2.0649379094770315   (start: 54410, end: 54420)\n",
      "Batch 5442 - batch loss: 2.03259015083313 - avg loss: 2.0649319664752595   (start: 54420, end: 54430)\n",
      "Batch 5443 - batch loss: 2.5073113441467285 - avg loss: 2.0650132264638104   (start: 54430, end: 54440)\n",
      "Batch 5444 - batch loss: 1.6083539724349976 - avg loss: 2.0649293588322166   (start: 54440, end: 54450)\n",
      "Batch 5445 - batch loss: 1.8043019771575928 - avg loss: 2.064881502170139   (start: 54450, end: 54460)\n",
      "Batch 5446 - batch loss: 1.9611037969589233 - avg loss: 2.064862449901879   (start: 54460, end: 54470)\n",
      "Batch 5447 - batch loss: 1.270286202430725 - avg loss: 2.064716602573048   (start: 54470, end: 54480)\n",
      "Batch 5448 - batch loss: 2.3062446117401123 - avg loss: 2.0647609277720145   (start: 54480, end: 54490)\n",
      "Batch 5449 - batch loss: 1.293534278869629 - avg loss: 2.0646194182951514   (start: 54490, end: 54500)\n",
      "Batch 5450 - batch loss: 1.9752817153930664 - avg loss: 2.0646030290632855   (start: 54500, end: 54510)\n",
      "Batch 5451 - batch loss: 1.8663685321807861 - avg loss: 2.0645666691042095   (start: 54510, end: 54520)\n",
      "Batch 5452 - batch loss: 1.9457651376724243 - avg loss: 2.0645448826506185   (start: 54520, end: 54530)\n",
      "Batch 5453 - batch loss: 1.442763090133667 - avg loss: 2.0644308779215175   (start: 54530, end: 54540)\n",
      "Batch 5454 - batch loss: 2.149414539337158 - avg loss: 2.0644464569611904   (start: 54540, end: 54550)\n",
      "Batch 5455 - batch loss: 2.5046801567077637 - avg loss: 2.064527144956012   (start: 54550, end: 54560)\n",
      "Batch 5456 - batch loss: 1.3866279125213623 - avg loss: 2.0644029193315965   (start: 54560, end: 54570)\n",
      "Batch 5457 - batch loss: 1.9809420108795166 - avg loss: 2.0643876278496522   (start: 54570, end: 54580)\n",
      "Batch 5458 - batch loss: 2.2903075218200684 - avg loss: 2.064429012699253   (start: 54580, end: 54590)\n",
      "Batch 5459 - batch loss: 1.5841665267944336 - avg loss: 2.0643410525369994   (start: 54590, end: 54600)\n",
      "Batch 5460 - batch loss: 2.1941466331481934 - avg loss: 2.0643648220994626   (start: 54600, end: 54610)\n",
      "Batch 5461 - batch loss: 1.9366533756256104 - avg loss: 2.064341440289416   (start: 54610, end: 54620)\n",
      "Batch 5462 - batch loss: 2.1675610542297363 - avg loss: 2.064360334599125   (start: 54620, end: 54630)\n",
      "Batch 5463 - batch loss: 1.9422948360443115 - avg loss: 2.0643379946469738   (start: 54630, end: 54640)\n",
      "Batch 5464 - batch loss: 2.1807801723480225 - avg loss: 2.064359301541338   (start: 54640, end: 54650)\n",
      "Batch 5465 - batch loss: 2.1435770988464355 - avg loss: 2.0643737943692386   (start: 54650, end: 54660)\n",
      "Batch 5466 - batch loss: 1.5628525018692017 - avg loss: 2.064282058263056   (start: 54660, end: 54670)\n",
      "Batch 5467 - batch loss: 0.9653148651123047 - avg loss: 2.064081076698837   (start: 54670, end: 54680)\n",
      "Batch 5468 - batch loss: 2.0526533126831055 - avg loss: 2.064078987146082   (start: 54680, end: 54690)\n",
      "Batch 5469 - batch loss: 1.808947205543518 - avg loss: 2.0640323451384766   (start: 54690, end: 54700)\n",
      "Batch 5470 - batch loss: 2.397429943084717 - avg loss: 2.064093284198602   (start: 54700, end: 54710)\n",
      "Batch 5471 - batch loss: 1.7804431915283203 - avg loss: 2.0640414475588598   (start: 54710, end: 54720)\n",
      "Batch 5472 - batch loss: 1.5703362226486206 - avg loss: 2.063951240136073   (start: 54720, end: 54730)\n",
      "Batch 5473 - batch loss: 2.400240421295166 - avg loss: 2.0640126740383673   (start: 54730, end: 54740)\n",
      "Batch 5474 - batch loss: 1.639565110206604 - avg loss: 2.0639351493691747   (start: 54740, end: 54750)\n",
      "Batch 5475 - batch loss: 1.5225883722305298 - avg loss: 2.0638362913017643   (start: 54750, end: 54760)\n",
      "Batch 5476 - batch loss: 2.211630344390869 - avg loss: 2.0638632757920123   (start: 54760, end: 54770)\n",
      "Batch 5477 - batch loss: 1.896679162979126 - avg loss: 2.0638327566038392   (start: 54770, end: 54780)\n",
      "Batch 5478 - batch loss: 2.024881601333618 - avg loss: 2.0638256474314955   (start: 54780, end: 54790)\n",
      "Batch 5479 - batch loss: 1.5600473880767822 - avg loss: 2.063733717092197   (start: 54790, end: 54800)\n",
      "Batch 5480 - batch loss: 2.8466506004333496 - avg loss: 2.063876559070548   (start: 54800, end: 54810)\n",
      "Batch 5481 - batch loss: 2.4426140785217285 - avg loss: 2.063945646542174   (start: 54810, end: 54820)\n",
      "Batch 5482 - batch loss: 2.473550319671631 - avg loss: 2.064020351023868   (start: 54820, end: 54830)\n",
      "Batch 5483 - batch loss: 3.078848123550415 - avg loss: 2.0642054034988   (start: 54830, end: 54840)\n",
      "Batch 5484 - batch loss: 1.9178082942962646 - avg loss: 2.0641787130504494   (start: 54840, end: 54850)\n",
      "Batch 5485 - batch loss: 2.191298007965088 - avg loss: 2.0642018846317316   (start: 54850, end: 54860)\n",
      "Batch 5486 - batch loss: 2.107316732406616 - avg loss: 2.064209742267557   (start: 54860, end: 54870)\n",
      "Batch 5487 - batch loss: 1.8604825735092163 - avg loss: 2.064172619970043   (start: 54870, end: 54880)\n",
      "Batch 5488 - batch loss: 1.1302192211151123 - avg loss: 2.0640024699611423   (start: 54880, end: 54890)\n",
      "Batch 5489 - batch loss: 2.2831943035125732 - avg loss: 2.064042395613884   (start: 54890, end: 54900)\n",
      "Batch 5490 - batch loss: 1.6722265481948853 - avg loss: 2.0639710396045197   (start: 54900, end: 54910)\n",
      "Batch 5491 - batch loss: 2.5651113986968994 - avg loss: 2.064062288759489   (start: 54910, end: 54920)\n",
      "Batch 5492 - batch loss: 1.5031464099884033 - avg loss: 2.0639601740901337   (start: 54920, end: 54930)\n",
      "Batch 5493 - batch loss: 2.770479679107666 - avg loss: 2.0640887724710977   (start: 54930, end: 54940)\n",
      "Batch 5494 - batch loss: 1.7771337032318115 - avg loss: 2.0640365513483974   (start: 54940, end: 54950)\n",
      "Batch 5495 - batch loss: 2.139545440673828 - avg loss: 2.0640502902292788   (start: 54950, end: 54960)\n",
      "Batch 5496 - batch loss: 1.9415746927261353 - avg loss: 2.0640280097858548   (start: 54960, end: 54970)\n",
      "Batch 5497 - batch loss: 2.2389838695526123 - avg loss: 2.0640598315137133   (start: 54970, end: 54980)\n",
      "Batch 5498 - batch loss: 2.1973938941955566 - avg loss: 2.0640840784791035   (start: 54980, end: 54990)\n",
      "Batch 5499 - batch loss: 1.6285560131072998 - avg loss: 2.064004891558127   (start: 54990, end: 55000)\n",
      "Batch 5500 - batch loss: 2.6398346424102783 - avg loss: 2.0641095688442297   (start: 55000, end: 55010)\n",
      "Batch 5501 - batch loss: 2.209974765777588 - avg loss: 2.0641360801486526   (start: 55010, end: 55020)\n",
      "Batch 5502 - batch loss: 2.068535804748535 - avg loss: 2.0641368796624815   (start: 55020, end: 55030)\n",
      "Batch 5503 - batch loss: 2.7246627807617188 - avg loss: 2.0642568880020704   (start: 55030, end: 55040)\n",
      "Batch 5504 - batch loss: 1.781986951828003 - avg loss: 2.064205612809305   (start: 55040, end: 55050)\n",
      "Batch 5505 - batch loss: 2.375588893890381 - avg loss: 2.06426216625665   (start: 55050, end: 55060)\n",
      "Batch 5506 - batch loss: 2.1211726665496826 - avg loss: 2.064272500467707   (start: 55060, end: 55070)\n",
      "Batch 5507 - batch loss: 1.763353705406189 - avg loss: 2.064217867425757   (start: 55070, end: 55080)\n",
      "Batch 5508 - batch loss: 2.2662010192871094 - avg loss: 2.0642545316392007   (start: 55080, end: 55090)\n",
      "Batch 5509 - batch loss: 1.1725270748138428 - avg loss: 2.064092693625258   (start: 55090, end: 55100)\n",
      "Batch 5510 - batch loss: 2.112828493118286 - avg loss: 2.064101536992976   (start: 55100, end: 55110)\n",
      "Batch 5511 - batch loss: 1.4353342056274414 - avg loss: 2.0639874645453404   (start: 55110, end: 55120)\n",
      "Batch 5512 - batch loss: 1.3559863567352295 - avg loss: 2.0638590406186563   (start: 55120, end: 55130)\n",
      "Batch 5513 - batch loss: 1.6652559041976929 - avg loss: 2.0637867513302233   (start: 55130, end: 55140)\n",
      "Batch 5514 - batch loss: 2.439279794692993 - avg loss: 2.0638548371041785   (start: 55140, end: 55150)\n",
      "Batch 5515 - batch loss: 1.650490403175354 - avg loss: 2.0637798979392166   (start: 55150, end: 55160)\n",
      "Batch 5516 - batch loss: 2.3248863220214844 - avg loss: 2.0638272255491645   (start: 55160, end: 55170)\n",
      "Batch 5517 - batch loss: 2.1274962425231934 - avg loss: 2.0638387639719578   (start: 55170, end: 55180)\n",
      "Batch 5518 - batch loss: 1.7440955638885498 - avg loss: 2.0637808289837203   (start: 55180, end: 55190)\n",
      "Batch 5519 - batch loss: 2.0291085243225098 - avg loss: 2.063774547769108   (start: 55190, end: 55200)\n",
      "Batch 5520 - batch loss: 1.5336664915084839 - avg loss: 2.0636785310952694   (start: 55200, end: 55210)\n",
      "Batch 5521 - batch loss: 2.1208040714263916 - avg loss: 2.0636888761768217   (start: 55210, end: 55220)\n",
      "Batch 5522 - batch loss: 2.5351052284240723 - avg loss: 2.0637742313012555   (start: 55220, end: 55230)\n",
      "Batch 5523 - batch loss: 1.792646050453186 - avg loss: 2.0637251494437523   (start: 55230, end: 55240)\n",
      "Batch 5524 - batch loss: 2.164494037628174 - avg loss: 2.0637433881565457   (start: 55240, end: 55250)\n",
      "Batch 5525 - batch loss: 2.363616943359375 - avg loss: 2.0637976540912546   (start: 55250, end: 55260)\n",
      "Batch 5526 - batch loss: 2.0535974502563477 - avg loss: 2.063795808568578   (start: 55260, end: 55270)\n",
      "Batch 5527 - batch loss: 1.862273931503296 - avg loss: 2.0637593538151293   (start: 55270, end: 55280)\n",
      "Batch 5528 - batch loss: 1.9525249004364014 - avg loss: 2.063739235447725   (start: 55280, end: 55290)\n",
      "Batch 5529 - batch loss: 2.0605180263519287 - avg loss: 2.0637386529506005   (start: 55290, end: 55300)\n",
      "Batch 5530 - batch loss: 2.272181510925293 - avg loss: 2.0637763392384283   (start: 55300, end: 55310)\n",
      "Batch 5531 - batch loss: 1.958237886428833 - avg loss: 2.063757261427002   (start: 55310, end: 55320)\n",
      "Batch 5532 - batch loss: 2.837575674057007 - avg loss: 2.0638971165530875   (start: 55320, end: 55330)\n",
      "Batch 5533 - batch loss: 1.7652734518051147 - avg loss: 2.0638431549223055   (start: 55330, end: 55340)\n",
      "Batch 5534 - batch loss: 2.383193016052246 - avg loss: 2.063900851374181   (start: 55340, end: 55350)\n",
      "Batch 5535 - batch loss: 1.7834516763687134 - avg loss: 2.0638501922023953   (start: 55350, end: 55360)\n",
      "Batch 5536 - batch loss: 2.4521405696868896 - avg loss: 2.06392031869282   (start: 55360, end: 55370)\n",
      "Batch 5537 - batch loss: 1.648751974105835 - avg loss: 2.063845351494448   (start: 55370, end: 55380)\n",
      "Batch 5538 - batch loss: 2.0908312797546387 - avg loss: 2.0638502234800518   (start: 55380, end: 55390)\n",
      "Batch 5539 - batch loss: 2.0385875701904297 - avg loss: 2.0638456634343316   (start: 55390, end: 55400)\n",
      "Batch 5540 - batch loss: 1.3644859790802002 - avg loss: 2.0637194480067276   (start: 55400, end: 55410)\n",
      "Batch 5541 - batch loss: 1.9709408283233643 - avg loss: 2.0637027070071454   (start: 55410, end: 55420)\n",
      "Batch 5542 - batch loss: 1.1931984424591064 - avg loss: 2.0635456613162657   (start: 55420, end: 55430)\n",
      "Batch 5543 - batch loss: 1.9327428340911865 - avg loss: 2.0635220677327113   (start: 55430, end: 55440)\n",
      "Batch 5544 - batch loss: 1.9991247653961182 - avg loss: 2.063510454152488   (start: 55440, end: 55450)\n",
      "Batch 5545 - batch loss: 1.9025437831878662 - avg loss: 2.063481430230569   (start: 55450, end: 55460)\n",
      "Batch 5546 - batch loss: 2.115255355834961 - avg loss: 2.063490763911046   (start: 55460, end: 55470)\n",
      "Batch 5547 - batch loss: 2.0193591117858887 - avg loss: 2.0634828093955218   (start: 55470, end: 55480)\n",
      "Batch 5548 - batch loss: 1.7245250940322876 - avg loss: 2.0634217249270836   (start: 55480, end: 55490)\n",
      "Batch 5549 - batch loss: 1.9438352584838867 - avg loss: 2.063400177816013   (start: 55490, end: 55500)\n",
      "Batch 5550 - batch loss: 2.823374032974243 - avg loss: 2.063537085374139   (start: 55500, end: 55510)\n",
      "Batch 5551 - batch loss: 1.794083595275879 - avg loss: 2.0634885526850004   (start: 55510, end: 55520)\n",
      "Batch 5552 - batch loss: 1.4324915409088135 - avg loss: 2.063374920952284   (start: 55520, end: 55530)\n",
      "Batch 5553 - batch loss: 2.2412121295928955 - avg loss: 2.0634069406153444   (start: 55530, end: 55540)\n",
      "Batch 5554 - batch loss: 1.8432178497314453 - avg loss: 2.0633673026151853   (start: 55540, end: 55550)\n",
      "Batch 5555 - batch loss: 1.6647945642471313 - avg loss: 2.0632955652612677   (start: 55550, end: 55560)\n",
      "Batch 5556 - batch loss: 2.3543214797973633 - avg loss: 2.0633479363094116   (start: 55560, end: 55570)\n",
      "Batch 5557 - batch loss: 1.950505018234253 - avg loss: 2.0633276335173862   (start: 55570, end: 55580)\n",
      "Batch 5558 - batch loss: 1.9360682964324951 - avg loss: 2.0633047410300533   (start: 55580, end: 55590)\n",
      "Batch 5559 - batch loss: 1.5137101411819458 - avg loss: 2.0632058930804402   (start: 55590, end: 55600)\n",
      "Batch 5560 - batch loss: 2.0619542598724365 - avg loss: 2.063205668007035   (start: 55600, end: 55610)\n",
      "Batch 5561 - batch loss: 3.2866415977478027 - avg loss: 2.0634256313169486   (start: 55610, end: 55620)\n",
      "Batch 5562 - batch loss: 1.4737101793289185 - avg loss: 2.0633196245846124   (start: 55620, end: 55630)\n",
      "Batch 5563 - batch loss: 1.6327660083770752 - avg loss: 2.063242242554381   (start: 55630, end: 55640)\n",
      "Batch 5564 - batch loss: 2.4726037979125977 - avg loss: 2.0633158025822977   (start: 55640, end: 55650)\n",
      "Batch 5565 - batch loss: 2.2757933139801025 - avg loss: 2.0633539767668823   (start: 55650, end: 55660)\n",
      "Batch 5566 - batch loss: 2.11710524559021 - avg loss: 2.0633636321052733   (start: 55660, end: 55670)\n",
      "Batch 5567 - batch loss: 1.6159073114395142 - avg loss: 2.0632832699787174   (start: 55670, end: 55680)\n",
      "Batch 5568 - batch loss: 2.25136399269104 - avg loss: 2.0633170427786296   (start: 55680, end: 55690)\n",
      "Batch 5569 - batch loss: 2.69920015335083 - avg loss: 2.0634312049169727   (start: 55690, end: 55700)\n",
      "Batch 5570 - batch loss: 1.895583152770996 - avg loss: 2.0634010760259036   (start: 55700, end: 55710)\n",
      "Batch 5571 - batch loss: 2.8287501335144043 - avg loss: 2.0635384322817343   (start: 55710, end: 55720)\n",
      "Batch 5572 - batch loss: 3.1500344276428223 - avg loss: 2.063733389395562   (start: 55720, end: 55730)\n",
      "Batch 5573 - batch loss: 1.480546236038208 - avg loss: 2.063628763067367   (start: 55730, end: 55740)\n",
      "Batch 5574 - batch loss: 2.195953130722046 - avg loss: 2.063652498379951   (start: 55740, end: 55750)\n",
      "Batch 5575 - batch loss: 3.2298583984375 - avg loss: 2.063861645779531   (start: 55750, end: 55760)\n",
      "Batch 5576 - batch loss: 2.063011646270752 - avg loss: 2.063861493367928   (start: 55760, end: 55770)\n",
      "Batch 5577 - batch loss: 2.3042633533477783 - avg loss: 2.0639045915859238   (start: 55770, end: 55780)\n",
      "Batch 5578 - batch loss: 2.418733835220337 - avg loss: 2.0639681924541144   (start: 55780, end: 55790)\n",
      "Batch 5579 - batch loss: 1.6066513061523438 - avg loss: 2.0638862360228774   (start: 55790, end: 55800)\n",
      "Batch 5580 - batch loss: 2.736679792404175 - avg loss: 2.0640067867407383   (start: 55800, end: 55810)\n",
      "Batch 5581 - batch loss: 1.3233182430267334 - avg loss: 2.0638740944183245   (start: 55810, end: 55820)\n",
      "Batch 5582 - batch loss: 2.4797885417938232 - avg loss: 2.0639485910057105   (start: 55820, end: 55830)\n",
      "Batch 5583 - batch loss: 1.6920312643051147 - avg loss: 2.0638819868999256   (start: 55830, end: 55840)\n",
      "Batch 5584 - batch loss: 2.194669246673584 - avg loss: 2.0639054044934393   (start: 55840, end: 55850)\n",
      "Batch 5585 - batch loss: 2.2216196060180664 - avg loss: 2.0639336383282987   (start: 55850, end: 55860)\n",
      "Batch 5586 - batch loss: 1.6076421737670898 - avg loss: 2.0638519681180676   (start: 55860, end: 55870)\n",
      "Batch 5587 - batch loss: 2.7972257137298584 - avg loss: 2.0639832089458436   (start: 55870, end: 55880)\n",
      "Batch 5588 - batch loss: 1.4031509160995483 - avg loss: 2.063864970926011   (start: 55880, end: 55890)\n",
      "Batch 5589 - batch loss: 2.4728660583496094 - avg loss: 2.063938137489056   (start: 55890, end: 55900)\n",
      "Batch 5590 - batch loss: 2.065983295440674 - avg loss: 2.0639385032837176   (start: 55900, end: 55910)\n",
      "Batch 5591 - batch loss: 1.7937171459197998 - avg loss: 2.063890180437265   (start: 55910, end: 55920)\n",
      "Batch 5592 - batch loss: 2.163270950317383 - avg loss: 2.0639079492142858   (start: 55920, end: 55930)\n",
      "Batch 5593 - batch loss: 2.1473357677459717 - avg loss: 2.0639228630180995   (start: 55930, end: 55940)\n",
      "Batch 5594 - batch loss: 1.6030851602554321 - avg loss: 2.0638404970301165   (start: 55940, end: 55950)\n",
      "Batch 5595 - batch loss: 2.998727321624756 - avg loss: 2.064007560436942   (start: 55950, end: 55960)\n",
      "Batch 5596 - batch loss: 1.929165244102478 - avg loss: 2.063983468545512   (start: 55960, end: 55970)\n",
      "Batch 5597 - batch loss: 1.0994226932525635 - avg loss: 2.063811164012591   (start: 55970, end: 55980)\n",
      "Batch 5598 - batch loss: 1.7666785717010498 - avg loss: 2.063758095144523   (start: 55980, end: 55990)\n",
      "Batch 5599 - batch loss: 2.6845409870147705 - avg loss: 2.063868949232357   (start: 55990, end: 56000)\n",
      "Batch 5600 - batch loss: 2.549144744873047 - avg loss: 2.0639555901528426   (start: 56000, end: 56010)\n",
      "Batch 5601 - batch loss: 1.9688758850097656 - avg loss: 2.063938617695659   (start: 56010, end: 56020)\n",
      "Batch 5602 - batch loss: 1.8461040258407593 - avg loss: 2.0638997394890097   (start: 56020, end: 56030)\n",
      "Batch 5603 - batch loss: 2.179497003555298 - avg loss: 2.0639203671235684   (start: 56030, end: 56040)\n",
      "Batch 5604 - batch loss: 1.745560646057129 - avg loss: 2.063863567886982   (start: 56040, end: 56050)\n",
      "Batch 5605 - batch loss: 1.4767190217971802 - avg loss: 2.0637588328627063   (start: 56050, end: 56060)\n",
      "Batch 5606 - batch loss: 2.533310890197754 - avg loss: 2.063842576764496   (start: 56060, end: 56070)\n",
      "Batch 5607 - batch loss: 2.3037028312683105 - avg loss: 2.0638853478512478   (start: 56070, end: 56080)\n",
      "Batch 5608 - batch loss: 1.3949321508407593 - avg loss: 2.063766083597903   (start: 56080, end: 56090)\n",
      "Batch 5609 - batch loss: 2.3936855792999268 - avg loss: 2.0638248927771725   (start: 56090, end: 56100)\n",
      "Batch 5610 - batch loss: 2.2133076190948486 - avg loss: 2.063851533790596   (start: 56100, end: 56110)\n",
      "Batch 5611 - batch loss: 2.4608054161071777 - avg loss: 2.0639222668416144   (start: 56110, end: 56120)\n",
      "Batch 5612 - batch loss: 2.00639009475708 - avg loss: 2.063912017033654   (start: 56120, end: 56130)\n",
      "Batch 5613 - batch loss: 2.1453070640563965 - avg loss: 2.0639265156170206   (start: 56130, end: 56140)\n",
      "Batch 5614 - batch loss: 1.603646993637085 - avg loss: 2.063844542416312   (start: 56140, end: 56150)\n",
      "Batch 5615 - batch loss: 0.9748504757881165 - avg loss: 2.0636506332164135   (start: 56150, end: 56160)\n",
      "Batch 5616 - batch loss: 2.163499116897583 - avg loss: 2.0636684093395545   (start: 56160, end: 56170)\n",
      "Batch 5617 - batch loss: 2.163041353225708 - avg loss: 2.0636860976528126   (start: 56170, end: 56180)\n",
      "Batch 5618 - batch loss: 2.0164713859558105 - avg loss: 2.0636776949634204   (start: 56180, end: 56190)\n",
      "Batch 5619 - batch loss: 1.5557315349578857 - avg loss: 2.063587313084416   (start: 56190, end: 56200)\n",
      "Batch 5620 - batch loss: 2.521740198135376 - avg loss: 2.0636688204469937   (start: 56200, end: 56210)\n",
      "Batch 5621 - batch loss: 1.906009316444397 - avg loss: 2.0636407771342933   (start: 56210, end: 56220)\n",
      "Batch 5622 - batch loss: 2.4105138778686523 - avg loss: 2.063702465396917   (start: 56220, end: 56230)\n",
      "Batch 5623 - batch loss: 2.9808802604675293 - avg loss: 2.0638655482196535   (start: 56230, end: 56240)\n",
      "Batch 5624 - batch loss: 2.069121837615967 - avg loss: 2.0638664826711017   (start: 56240, end: 56250)\n",
      "Batch 5625 - batch loss: 1.929600477218628 - avg loss: 2.063842617401736   (start: 56250, end: 56260)\n",
      "Batch 5626 - batch loss: 2.097977638244629 - avg loss: 2.0638486836929824   (start: 56260, end: 56270)\n",
      "Batch 5627 - batch loss: 1.446009874343872 - avg loss: 2.0637389042314775   (start: 56270, end: 56280)\n",
      "Batch 5628 - batch loss: 1.6169960498809814 - avg loss: 2.063659539716581   (start: 56280, end: 56290)\n",
      "Batch 5629 - batch loss: 2.0682053565979004 - avg loss: 2.0636603471440913   (start: 56290, end: 56300)\n",
      "Batch 5630 - batch loss: 1.5519864559173584 - avg loss: 2.063569479821906   (start: 56300, end: 56310)\n",
      "Batch 5631 - batch loss: 1.3826302289962769 - avg loss: 2.0634485744151543   (start: 56310, end: 56320)\n",
      "Batch 5632 - batch loss: 2.859313726425171 - avg loss: 2.063589860612919   (start: 56320, end: 56330)\n",
      "Batch 5633 - batch loss: 2.0098671913146973 - avg loss: 2.063580325172859   (start: 56330, end: 56340)\n",
      "Batch 5634 - batch loss: 1.5327175855636597 - avg loss: 2.0634861170558034   (start: 56340, end: 56350)\n",
      "Batch 5635 - batch loss: 2.1246495246887207 - avg loss: 2.0634969693282716   (start: 56350, end: 56360)\n",
      "Batch 5636 - batch loss: 2.326371431350708 - avg loss: 2.0635436030806265   (start: 56360, end: 56370)\n",
      "Batch 5637 - batch loss: 1.7279818058013916 - avg loss: 2.0634840852024285   (start: 56370, end: 56380)\n",
      "Batch 5638 - batch loss: 2.3211097717285156 - avg loss: 2.0635297716160705   (start: 56380, end: 56390)\n",
      "Batch 5639 - batch loss: 1.9374908208847046 - avg loss: 2.0635074242843805   (start: 56390, end: 56400)\n",
      "Batch 5640 - batch loss: 2.3567323684692383 - avg loss: 2.0635594053062176   (start: 56400, end: 56410)\n",
      "Batch 5641 - batch loss: 2.078946828842163 - avg loss: 2.0635621326056746   (start: 56410, end: 56420)\n",
      "Batch 5642 - batch loss: 2.8147153854370117 - avg loss: 2.0636952450020654   (start: 56420, end: 56430)\n",
      "Batch 5643 - batch loss: 1.1897026300430298 - avg loss: 2.063540391597572   (start: 56430, end: 56440)\n",
      "Batch 5644 - batch loss: 0.9920305013656616 - avg loss: 2.0633505758508526   (start: 56440, end: 56450)\n",
      "Batch 5645 - batch loss: 2.675051212310791 - avg loss: 2.0634589181527407   (start: 56450, end: 56460)\n",
      "Batch 5646 - batch loss: 2.361717462539673 - avg loss: 2.0635117353201546   (start: 56460, end: 56470)\n",
      "Batch 5647 - batch loss: 2.2447237968444824 - avg loss: 2.063543819608668   (start: 56470, end: 56480)\n",
      "Batch 5648 - batch loss: 1.8216400146484375 - avg loss: 2.0635009971967437   (start: 56480, end: 56490)\n",
      "Batch 5649 - batch loss: 2.915309429168701 - avg loss: 2.063651759751075   (start: 56490, end: 56500)\n",
      "Batch 5650 - batch loss: 2.3252458572387695 - avg loss: 2.0636980513981267   (start: 56500, end: 56510)\n",
      "Batch 5651 - batch loss: 2.3401920795440674 - avg loss: 2.063746971077558   (start: 56510, end: 56520)\n",
      "Batch 5652 - batch loss: 3.209512233734131 - avg loss: 2.0639496537704036   (start: 56520, end: 56530)\n",
      "Batch 5653 - batch loss: 2.289637804031372 - avg loss: 2.063989570316258   (start: 56530, end: 56540)\n",
      "Batch 5654 - batch loss: 2.865736961364746 - avg loss: 2.064131347043234   (start: 56540, end: 56550)\n",
      "Batch 5655 - batch loss: 2.5309042930603027 - avg loss: 2.0642138740846088   (start: 56550, end: 56560)\n",
      "Batch 5656 - batch loss: 1.5246301889419556 - avg loss: 2.064118490721494   (start: 56560, end: 56570)\n",
      "Batch 5657 - batch loss: 2.253096103668213 - avg loss: 2.0641518907944785   (start: 56570, end: 56580)\n",
      "Batch 5658 - batch loss: 1.1218719482421875 - avg loss: 2.0639853808205335   (start: 56580, end: 56590)\n",
      "Batch 5659 - batch loss: 1.3720839023590088 - avg loss: 2.063863136743067   (start: 56590, end: 56600)\n",
      "Batch 5660 - batch loss: 1.5380733013153076 - avg loss: 2.0637702574222   (start: 56600, end: 56610)\n",
      "Batch 5661 - batch loss: 1.191298246383667 - avg loss: 2.0636161648734475   (start: 56610, end: 56620)\n",
      "Batch 5662 - batch loss: 2.3279576301574707 - avg loss: 2.0636628435711843   (start: 56620, end: 56630)\n",
      "Batch 5663 - batch loss: 1.8794920444488525 - avg loss: 2.0636303275402654   (start: 56630, end: 56640)\n",
      "Batch 5664 - batch loss: 2.255816698074341 - avg loss: 2.0636642527601303   (start: 56640, end: 56650)\n",
      "Batch 5665 - batch loss: 1.6680341958999634 - avg loss: 2.0635944274765334   (start: 56650, end: 56660)\n",
      "Batch 5666 - batch loss: 1.7215684652328491 - avg loss: 2.063534073504018   (start: 56660, end: 56670)\n",
      "Batch 5667 - batch loss: 1.2192455530166626 - avg loss: 2.0633851164608834   (start: 56670, end: 56680)\n",
      "Batch 5668 - batch loss: 1.6199754476547241 - avg loss: 2.0633068999026185   (start: 56680, end: 56690)\n",
      "Batch 5669 - batch loss: 1.608424425125122 - avg loss: 2.0632266737165903   (start: 56690, end: 56700)\n",
      "Batch 5670 - batch loss: 1.8097044229507446 - avg loss: 2.06318196868207   (start: 56700, end: 56710)\n",
      "Batch 5671 - batch loss: 3.0997352600097656 - avg loss: 2.063364717851909   (start: 56710, end: 56720)\n",
      "Batch 5672 - batch loss: 1.9833924770355225 - avg loss: 2.0633506208589925   (start: 56720, end: 56730)\n",
      "Batch 5673 - batch loss: 2.0236902236938477 - avg loss: 2.0633436310110604   (start: 56730, end: 56740)\n",
      "Batch 5674 - batch loss: 2.1077191829681396 - avg loss: 2.0633514504915818   (start: 56740, end: 56750)\n",
      "Batch 5675 - batch loss: 2.057731866836548 - avg loss: 2.0633504604310366   (start: 56750, end: 56760)\n",
      "Batch 5676 - batch loss: 2.0661630630493164 - avg loss: 2.063350955869229   (start: 56760, end: 56770)\n",
      "Batch 5677 - batch loss: 1.3989852666854858 - avg loss: 2.063233948879235   (start: 56770, end: 56780)\n",
      "Batch 5678 - batch loss: 2.466054916381836 - avg loss: 2.063304880551625   (start: 56780, end: 56790)\n",
      "Batch 5679 - batch loss: 1.4666645526885986 - avg loss: 2.0631998382403816   (start: 56790, end: 56800)\n",
      "Batch 5680 - batch loss: 2.241788625717163 - avg loss: 2.0632312743937837   (start: 56800, end: 56810)\n",
      "Batch 5681 - batch loss: 2.4394547939300537 - avg loss: 2.063297487614399   (start: 56810, end: 56820)\n",
      "Batch 5682 - batch loss: 2.1079659461975098 - avg loss: 2.063305347628227   (start: 56820, end: 56830)\n",
      "Batch 5683 - batch loss: 1.091334342956543 - avg loss: 2.0631343463958776   (start: 56830, end: 56840)\n",
      "Batch 5684 - batch loss: 2.37797212600708 - avg loss: 2.063189726832045   (start: 56840, end: 56850)\n",
      "Batch 5685 - batch loss: 1.9186198711395264 - avg loss: 2.063164301250671   (start: 56850, end: 56860)\n",
      "Batch 5686 - batch loss: 2.56520414352417 - avg loss: 2.063252579752917   (start: 56860, end: 56870)\n",
      "Batch 5687 - batch loss: 1.4710592031478882 - avg loss: 2.0631484669933173   (start: 56870, end: 56880)\n",
      "Batch 5688 - batch loss: 1.9658215045928955 - avg loss: 2.063131359072347   (start: 56880, end: 56890)\n",
      "Batch 5689 - batch loss: 2.02780818939209 - avg loss: 2.0631251511339146   (start: 56890, end: 56900)\n",
      "Batch 5690 - batch loss: 2.0591607093811035 - avg loss: 2.0631244545178973   (start: 56900, end: 56910)\n",
      "Batch 5691 - batch loss: 1.8580725193023682 - avg loss: 2.063088429933355   (start: 56910, end: 56920)\n",
      "Batch 5692 - batch loss: 1.6357030868530273 - avg loss: 2.0630133578548233   (start: 56920, end: 56930)\n",
      "Batch 5693 - batch loss: 2.543595552444458 - avg loss: 2.063097759364235   (start: 56930, end: 56940)\n",
      "Batch 5694 - batch loss: 2.149834394454956 - avg loss: 2.0631129896776836   (start: 56940, end: 56950)\n",
      "Batch 5695 - batch loss: 2.2722418308258057 - avg loss: 2.063149704712998   (start: 56950, end: 56960)\n",
      "Batch 5696 - batch loss: 1.9198768138885498 - avg loss: 2.063124555881889   (start: 56960, end: 56970)\n",
      "Batch 5697 - batch loss: 2.401627540588379 - avg loss: 2.0631839632151125   (start: 56970, end: 56980)\n",
      "Batch 5698 - batch loss: 1.9547771215438843 - avg loss: 2.0631649411337527   (start: 56980, end: 56990)\n",
      "Batch 5699 - batch loss: 2.068145275115967 - avg loss: 2.0631658148765566   (start: 56990, end: 57000)\n",
      "Batch 5700 - batch loss: 1.670861005783081 - avg loss: 2.0630970015439667   (start: 57000, end: 57010)\n",
      "Batch 5701 - batch loss: 2.3692684173583984 - avg loss: 2.063150696986937   (start: 57010, end: 57020)\n",
      "Batch 5702 - batch loss: 1.1346758604049683 - avg loss: 2.0629878923513796   (start: 57020, end: 57030)\n",
      "Batch 5703 - batch loss: 2.3356072902679443 - avg loss: 2.0630356867759794   (start: 57030, end: 57040)\n",
      "Batch 5704 - batch loss: 1.9286489486694336 - avg loss: 2.0630121308183798   (start: 57040, end: 57050)\n",
      "Batch 5705 - batch loss: 1.9333709478378296 - avg loss: 2.0629894106671385   (start: 57050, end: 57060)\n",
      "Batch 5706 - batch loss: 1.951472520828247 - avg loss: 2.0629698702974455   (start: 57060, end: 57070)\n",
      "Batch 5707 - batch loss: 2.573021411895752 - avg loss: 2.0630592276102693   (start: 57070, end: 57080)\n",
      "Batch 5708 - batch loss: 1.9082963466644287 - avg loss: 2.0630321190306677   (start: 57080, end: 57090)\n",
      "Batch 5709 - batch loss: 1.7169864177703857 - avg loss: 2.0629715155803594   (start: 57090, end: 57100)\n",
      "Batch 5710 - batch loss: 1.5080626010894775 - avg loss: 2.062874350650489   (start: 57100, end: 57110)\n",
      "Batch 5711 - batch loss: 2.4440488815307617 - avg loss: 2.062941082886287   (start: 57110, end: 57120)\n",
      "Batch 5712 - batch loss: 1.5589807033538818 - avg loss: 2.0628528699719633   (start: 57120, end: 57130)\n",
      "Batch 5713 - batch loss: 2.571056604385376 - avg loss: 2.062941810072491   (start: 57130, end: 57140)\n",
      "Batch 5714 - batch loss: 2.1884915828704834 - avg loss: 2.0629637785366723   (start: 57140, end: 57150)\n",
      "Batch 5715 - batch loss: 1.8888880014419556 - avg loss: 2.062933324411918   (start: 57150, end: 57160)\n",
      "Batch 5716 - batch loss: 2.2340943813323975 - avg loss: 2.062963263375871   (start: 57160, end: 57170)\n",
      "Batch 5717 - batch loss: 1.6137421131134033 - avg loss: 2.0628847007402884   (start: 57170, end: 57180)\n",
      "Batch 5718 - batch loss: 2.5206854343414307 - avg loss: 2.062964749828171   (start: 57180, end: 57190)\n",
      "Batch 5719 - batch loss: 1.8933336734771729 - avg loss: 2.062935094045592   (start: 57190, end: 57200)\n",
      "Batch 5720 - batch loss: 2.750270366668701 - avg loss: 2.063055236550858   (start: 57200, end: 57210)\n",
      "Batch 5721 - batch loss: 2.7113139629364014 - avg loss: 2.063168528883326   (start: 57210, end: 57220)\n",
      "Batch 5722 - batch loss: 1.0993643999099731 - avg loss: 2.063000119984327   (start: 57220, end: 57230)\n",
      "Batch 5723 - batch loss: 1.429471731185913 - avg loss: 2.06288944067112   (start: 57230, end: 57240)\n",
      "Batch 5724 - batch loss: 2.3638405799865723 - avg loss: 2.062942008555716   (start: 57240, end: 57250)\n",
      "Batch 5725 - batch loss: 1.4795095920562744 - avg loss: 2.062840116761008   (start: 57250, end: 57260)\n",
      "Batch 5726 - batch loss: 2.014890670776367 - avg loss: 2.062831744236827   (start: 57260, end: 57270)\n",
      "Batch 5727 - batch loss: 2.70278263092041 - avg loss: 2.0629434675061504   (start: 57270, end: 57280)\n",
      "Batch 5728 - batch loss: 2.5416955947875977 - avg loss: 2.063027033944845   (start: 57280, end: 57290)\n",
      "Batch 5729 - batch loss: 3.3362972736358643 - avg loss: 2.0632492451559603   (start: 57290, end: 57300)\n",
      "Batch 5730 - batch loss: 1.7943761348724365 - avg loss: 2.063202329589692   (start: 57300, end: 57310)\n",
      "Batch 5731 - batch loss: 2.180281400680542 - avg loss: 2.063222755108026   (start: 57310, end: 57320)\n",
      "Batch 5732 - batch loss: 2.9668233394622803 - avg loss: 2.063380369024711   (start: 57320, end: 57330)\n",
      "Batch 5733 - batch loss: 2.0541679859161377 - avg loss: 2.06337876240052   (start: 57330, end: 57340)\n",
      "Batch 5734 - batch loss: 1.9753587245941162 - avg loss: 2.063363414529935   (start: 57340, end: 57350)\n",
      "Batch 5735 - batch loss: 2.380847454071045 - avg loss: 2.0634187639092136   (start: 57350, end: 57360)\n",
      "Batch 5736 - batch loss: 1.9874967336654663 - avg loss: 2.063405530158082   (start: 57360, end: 57370)\n",
      "Batch 5737 - batch loss: 1.9649841785430908 - avg loss: 2.063388377604646   (start: 57370, end: 57380)\n",
      "Batch 5738 - batch loss: 1.4262421131134033 - avg loss: 2.063277357171732   (start: 57380, end: 57390)\n",
      "Batch 5739 - batch loss: 2.4546852111816406 - avg loss: 2.063345546693337   (start: 57390, end: 57400)\n",
      "Batch 5740 - batch loss: 2.1881675720214844 - avg loss: 2.0633672889029393   (start: 57400, end: 57410)\n",
      "Batch 5741 - batch loss: 1.3629003763198853 - avg loss: 2.0632452988450183   (start: 57410, end: 57420)\n",
      "Batch 5742 - batch loss: 1.733926773071289 - avg loss: 2.06318795624955   (start: 57420, end: 57430)\n",
      "Batch 5743 - batch loss: 1.8485416173934937 - avg loss: 2.0631505874579665   (start: 57430, end: 57440)\n",
      "Batch 5744 - batch loss: 1.12864089012146 - avg loss: 2.0629879225846266   (start: 57440, end: 57450)\n",
      "Batch 5745 - batch loss: 2.3239846229553223 - avg loss: 2.0630333449132676   (start: 57450, end: 57460)\n",
      "Batch 5746 - batch loss: 2.444019317626953 - avg loss: 2.063099637930966   (start: 57460, end: 57470)\n",
      "Batch 5747 - batch loss: 2.261718273162842 - avg loss: 2.063134192321229   (start: 57470, end: 57480)\n",
      "Batch 5748 - batch loss: 1.3961704969406128 - avg loss: 2.0630181784587522   (start: 57480, end: 57490)\n",
      "Batch 5749 - batch loss: 2.1607069969177246 - avg loss: 2.063035167818484   (start: 57490, end: 57500)\n",
      "Batch 5750 - batch loss: 2.551335573196411 - avg loss: 2.063120074861673   (start: 57500, end: 57510)\n",
      "Batch 5751 - batch loss: 1.771148681640625 - avg loss: 2.063069314883714   (start: 57510, end: 57520)\n",
      "Batch 5752 - batch loss: 1.8764724731445312 - avg loss: 2.0630368801815164   (start: 57520, end: 57530)\n",
      "Batch 5753 - batch loss: 1.2577955722808838 - avg loss: 2.0628969355677   (start: 57530, end: 57540)\n",
      "Batch 5754 - batch loss: 1.4600651264190674 - avg loss: 2.062792186339351   (start: 57540, end: 57550)\n",
      "Batch 5755 - batch loss: 2.1300549507141113 - avg loss: 2.062803872017665   (start: 57550, end: 57560)\n",
      "Batch 5756 - batch loss: 1.6509987115859985 - avg loss: 2.062732340810364   (start: 57560, end: 57570)\n",
      "Batch 5757 - batch loss: 1.5041310787200928 - avg loss: 2.0626353277394904   (start: 57570, end: 57580)\n",
      "Batch 5758 - batch loss: 1.5855324268341064 - avg loss: 2.0625524829919812   (start: 57580, end: 57590)\n",
      "Batch 5759 - batch loss: 2.0670456886291504 - avg loss: 2.062553263062404   (start: 57590, end: 57600)\n",
      "Batch 5760 - batch loss: 1.9850246906280518 - avg loss: 2.06253980557717   (start: 57600, end: 57610)\n",
      "Batch 5761 - batch loss: 1.9794628620147705 - avg loss: 2.062525387502966   (start: 57610, end: 57620)\n",
      "Batch 5762 - batch loss: 2.2125942707061768 - avg loss: 2.0625514275659893   (start: 57620, end: 57630)\n",
      "Batch 5763 - batch loss: 2.0782904624938965 - avg loss: 2.0625541581410984   (start: 57630, end: 57640)\n",
      "Batch 5764 - batch loss: 2.0265512466430664 - avg loss: 2.06254791305671   (start: 57640, end: 57650)\n",
      "Batch 5765 - batch loss: 2.2295403480529785 - avg loss: 2.062576874630591   (start: 57650, end: 57660)\n",
      "Batch 5766 - batch loss: 2.007315158843994 - avg loss: 2.062567292227992   (start: 57660, end: 57670)\n",
      "Batch 5767 - batch loss: 1.4298824071884155 - avg loss: 2.0624576034476454   (start: 57670, end: 57680)\n",
      "Batch 5768 - batch loss: 2.0968666076660156 - avg loss: 2.062463567913622   (start: 57680, end: 57690)\n",
      "Batch 5769 - batch loss: 3.769779920578003 - avg loss: 2.0627594632953663   (start: 57690, end: 57700)\n",
      "Batch 5770 - batch loss: 2.3176512718200684 - avg loss: 2.0628036309974154   (start: 57700, end: 57710)\n",
      "Batch 5771 - batch loss: 1.6287691593170166 - avg loss: 2.0627284344500003   (start: 57710, end: 57720)\n",
      "Batch 5772 - batch loss: 2.3991944789886475 - avg loss: 2.062786717153021   (start: 57720, end: 57730)\n",
      "Batch 5773 - batch loss: 2.4146220684051514 - avg loss: 2.0628476515747827   (start: 57730, end: 57740)\n",
      "Batch 5774 - batch loss: 2.0341134071350098 - avg loss: 2.0628426759480396   (start: 57740, end: 57750)\n",
      "Batch 5775 - batch loss: 2.340367555618286 - avg loss: 2.0628907238842706   (start: 57750, end: 57760)\n",
      "Batch 5776 - batch loss: 1.6436021327972412 - avg loss: 2.062818144934801   (start: 57760, end: 57770)\n",
      "Batch 5777 - batch loss: 2.328408718109131 - avg loss: 2.0628641107660877   (start: 57770, end: 57780)\n",
      "Batch 5778 - batch loss: 2.528679609298706 - avg loss: 2.0629447156282668   (start: 57780, end: 57790)\n",
      "Batch 5779 - batch loss: 1.8487495183944702 - avg loss: 2.0629076576356655   (start: 57790, end: 57800)\n",
      "Batch 5780 - batch loss: 2.278841733932495 - avg loss: 2.0629450100100466   (start: 57800, end: 57810)\n",
      "Batch 5781 - batch loss: 1.7348451614379883 - avg loss: 2.0628882649653266   (start: 57810, end: 57820)\n",
      "Batch 5782 - batch loss: 1.8730270862579346 - avg loss: 2.0628554340508   (start: 57820, end: 57830)\n",
      "Batch 5783 - batch loss: 1.4247788190841675 - avg loss: 2.062745116517092   (start: 57830, end: 57840)\n",
      "Batch 5784 - batch loss: 2.413642644882202 - avg loss: 2.062805772961062   (start: 57840, end: 57850)\n",
      "Batch 5785 - batch loss: 2.6316871643066406 - avg loss: 2.062904093284488   (start: 57850, end: 57860)\n",
      "Batch 5786 - batch loss: 2.460524320602417 - avg loss: 2.062972802499508   (start: 57860, end: 57870)\n",
      "Batch 5787 - batch loss: 1.574747085571289 - avg loss: 2.0628884511316903   (start: 57870, end: 57880)\n",
      "Batch 5788 - batch loss: 3.024610996246338 - avg loss: 2.0630545804364258   (start: 57880, end: 57890)\n",
      "Batch 5789 - batch loss: 1.7911373376846313 - avg loss: 2.0630076171820644   (start: 57890, end: 57900)\n",
      "Batch 5790 - batch loss: 2.48899245262146 - avg loss: 2.0630811769878736   (start: 57900, end: 57910)\n",
      "Batch 5791 - batch loss: 1.4747883081436157 - avg loss: 2.0629796070864845   (start: 57910, end: 57920)\n",
      "Batch 5792 - batch loss: 1.408937692642212 - avg loss: 2.0628667049780014   (start: 57920, end: 57930)\n",
      "Batch 5793 - batch loss: 1.6410045623779297 - avg loss: 2.06279389480496   (start: 57930, end: 57940)\n",
      "Batch 5794 - batch loss: 2.2455272674560547 - avg loss: 2.0628254277424323   (start: 57940, end: 57950)\n",
      "Batch 5795 - batch loss: 2.3510117530822754 - avg loss: 2.062875149330655   (start: 57950, end: 57960)\n",
      "Batch 5796 - batch loss: 2.121042251586914 - avg loss: 2.0628851833313893   (start: 57960, end: 57970)\n",
      "Batch 5797 - batch loss: 1.8965193033218384 - avg loss: 2.062856489664606   (start: 57970, end: 57980)\n",
      "Batch 5798 - batch loss: 1.9335699081420898 - avg loss: 2.062834195030786   (start: 57980, end: 57990)\n",
      "Batch 5799 - batch loss: 1.3206664323806763 - avg loss: 2.0627062350717082   (start: 57990, end: 58000)\n",
      "Batch 5800 - batch loss: 2.0630459785461426 - avg loss: 2.062706293638072   (start: 58000, end: 58010)\n",
      "Batch 5801 - batch loss: 2.561063766479492 - avg loss: 2.0627921877216364   (start: 58010, end: 58020)\n",
      "Batch 5802 - batch loss: 1.65032958984375 - avg loss: 2.0627211102448353   (start: 58020, end: 58030)\n",
      "Batch 5803 - batch loss: 2.1417076587677 - avg loss: 2.062734719229763   (start: 58030, end: 58040)\n",
      "Batch 5804 - batch loss: 1.8118457794189453 - avg loss: 2.062691499774154   (start: 58040, end: 58050)\n",
      "Batch 5805 - batch loss: 2.1271414756774902 - avg loss: 2.062702600355605   (start: 58050, end: 58060)\n",
      "Batch 5806 - batch loss: 1.8876314163208008 - avg loss: 2.062672452054583   (start: 58060, end: 58070)\n",
      "Batch 5807 - batch loss: 2.5941450595855713 - avg loss: 2.062763959046238   (start: 58070, end: 58080)\n",
      "Batch 5808 - batch loss: 2.205811023712158 - avg loss: 2.062788584121925   (start: 58080, end: 58090)\n",
      "Batch 5809 - batch loss: 2.2312004566192627 - avg loss: 2.0628175706748504   (start: 58090, end: 58100)\n",
      "Batch 5810 - batch loss: 1.7739343643188477 - avg loss: 2.0627678575090687   (start: 58100, end: 58110)\n",
      "Batch 5811 - batch loss: 1.8460578918457031 - avg loss: 2.062730570866663   (start: 58110, end: 58120)\n",
      "Batch 5812 - batch loss: 2.931165933609009 - avg loss: 2.0628799662498976   (start: 58120, end: 58130)\n",
      "Batch 5813 - batch loss: 2.0320796966552734 - avg loss: 2.062874668645908   (start: 58130, end: 58140)\n",
      "Batch 5814 - batch loss: 1.8529846668243408 - avg loss: 2.06283857406262   (start: 58140, end: 58150)\n",
      "Batch 5815 - batch loss: 2.4714319705963135 - avg loss: 2.0629088273976497   (start: 58150, end: 58160)\n",
      "Batch 5816 - batch loss: 1.7805252075195312 - avg loss: 2.0628602828523723   (start: 58160, end: 58170)\n",
      "Batch 5817 - batch loss: 2.5236895084381104 - avg loss: 2.0629394903507543   (start: 58170, end: 58180)\n",
      "Batch 5818 - batch loss: 1.1228183507919312 - avg loss: 2.062777929749352   (start: 58180, end: 58190)\n",
      "Batch 5819 - batch loss: 1.15529203414917 - avg loss: 2.062622004337737   (start: 58190, end: 58200)\n",
      "Batch 5820 - batch loss: 1.7235273122787476 - avg loss: 2.062563750654167   (start: 58200, end: 58210)\n",
      "Batch 5821 - batch loss: 1.233949065208435 - avg loss: 2.0624214259057223   (start: 58210, end: 58220)\n",
      "Batch 5822 - batch loss: 0.9854453802108765 - avg loss: 2.0622364738113217   (start: 58220, end: 58230)\n",
      "Batch 5823 - batch loss: 1.9506210088729858 - avg loss: 2.062217309068029   (start: 58230, end: 58240)\n",
      "Batch 5824 - batch loss: 2.5599865913391113 - avg loss: 2.0623027630220667   (start: 58240, end: 58250)\n",
      "Batch 5825 - batch loss: 1.907724380493164 - avg loss: 2.062276230515625   (start: 58250, end: 58260)\n",
      "Batch 5826 - batch loss: 2.5137057304382324 - avg loss: 2.0623537025423837   (start: 58260, end: 58270)\n",
      "Batch 5827 - batch loss: 1.5354626178741455 - avg loss: 2.062263295698755   (start: 58270, end: 58280)\n",
      "Batch 5828 - batch loss: 1.6591850519180298 - avg loss: 2.0621941452023096   (start: 58280, end: 58290)\n",
      "Batch 5829 - batch loss: 1.8793566226959229 - avg loss: 2.062162783706168   (start: 58290, end: 58300)\n",
      "Batch 5830 - batch loss: 1.9527393579483032 - avg loss: 2.062144017898286   (start: 58300, end: 58310)\n",
      "Batch 5831 - batch loss: 2.132641315460205 - avg loss: 2.0621561059122713   (start: 58310, end: 58320)\n",
      "Batch 5832 - batch loss: 2.2313344478607178 - avg loss: 2.0621851095711   (start: 58320, end: 58330)\n",
      "Batch 5833 - batch loss: 2.087799310684204 - avg loss: 2.0621895000752333   (start: 58330, end: 58340)\n",
      "Batch 5834 - batch loss: 1.705141305923462 - avg loss: 2.0621283092964586   (start: 58340, end: 58350)\n",
      "Batch 5835 - batch loss: 1.6134312152862549 - avg loss: 2.0620514249417616   (start: 58350, end: 58360)\n",
      "Batch 5836 - batch loss: 1.5084547996520996 - avg loss: 2.061956582278529   (start: 58360, end: 58370)\n",
      "Batch 5837 - batch loss: 2.113430976867676 - avg loss: 2.061965399406756   (start: 58370, end: 58380)\n",
      "Batch 5838 - batch loss: 1.854163408279419 - avg loss: 2.061929810780086   (start: 58380, end: 58390)\n",
      "Batch 5839 - batch loss: 2.1100754737854004 - avg loss: 2.0619380549004633   (start: 58390, end: 58400)\n",
      "Batch 5840 - batch loss: 2.566028356552124 - avg loss: 2.0620243569551886   (start: 58400, end: 58410)\n",
      "Batch 5841 - batch loss: 1.6431481838226318 - avg loss: 2.0619526561381516   (start: 58410, end: 58420)\n",
      "Batch 5842 - batch loss: 1.220238447189331 - avg loss: 2.061808600993714   (start: 58420, end: 58430)\n",
      "Batch 5843 - batch loss: 2.385594367980957 - avg loss: 2.061864005813527   (start: 58430, end: 58440)\n",
      "Batch 5844 - batch loss: 1.6061201095581055 - avg loss: 2.061786034231618   (start: 58440, end: 58450)\n",
      "Batch 5845 - batch loss: 2.533787965774536 - avg loss: 2.061866773528837   (start: 58450, end: 58460)\n",
      "Batch 5846 - batch loss: 2.569253444671631 - avg loss: 2.061953550794297   (start: 58460, end: 58470)\n",
      "Batch 5847 - batch loss: 2.004969358444214 - avg loss: 2.061943806575359   (start: 58470, end: 58480)\n",
      "Batch 5848 - batch loss: 2.953754186630249 - avg loss: 2.0620962788578097   (start: 58480, end: 58490)\n",
      "Batch 5849 - batch loss: 1.6582485437393188 - avg loss: 2.062027245056935   (start: 58490, end: 58500)\n",
      "Batch 5850 - batch loss: 2.926302194595337 - avg loss: 2.062174959114282   (start: 58500, end: 58510)\n",
      "Batch 5851 - batch loss: 1.6132652759552002 - avg loss: 2.0620982486421084   (start: 58510, end: 58520)\n",
      "Batch 5852 - batch loss: 1.940293550491333 - avg loss: 2.0620774379983104   (start: 58520, end: 58530)\n",
      "Batch 5853 - batch loss: 2.2000393867492676 - avg loss: 2.06210100512314   (start: 58530, end: 58540)\n",
      "Batch 5854 - batch loss: 2.53884220123291 - avg loss: 2.062182429750998   (start: 58540, end: 58550)\n",
      "Batch 5855 - batch loss: 1.5238971710205078 - avg loss: 2.062090509454084   (start: 58550, end: 58560)\n",
      "Batch 5856 - batch loss: 2.3264212608337402 - avg loss: 2.062135640195313   (start: 58560, end: 58570)\n",
      "Batch 5857 - batch loss: 2.2907955646514893 - avg loss: 2.0621746739823488   (start: 58570, end: 58580)\n",
      "Batch 5858 - batch loss: 1.3961297273635864 - avg loss: 2.0620609950360067   (start: 58580, end: 58590)\n",
      "Batch 5859 - batch loss: 3.0828399658203125 - avg loss: 2.0622351893996216   (start: 58590, end: 58600)\n",
      "Batch 5860 - batch loss: 2.0473475456237793 - avg loss: 2.0622326492795438   (start: 58600, end: 58610)\n",
      "Batch 5861 - batch loss: 1.5489054918289185 - avg loss: 2.062145080675407   (start: 58610, end: 58620)\n",
      "Batch 5862 - batch loss: 2.4437365531921387 - avg loss: 2.0622101653543283   (start: 58620, end: 58630)\n",
      "Batch 5863 - batch loss: 1.8903543949127197 - avg loss: 2.0621808584357675   (start: 58630, end: 58640)\n",
      "Batch 5864 - batch loss: 1.992073655128479 - avg loss: 2.0621689049484173   (start: 58640, end: 58650)\n",
      "Batch 5865 - batch loss: 2.8228373527526855 - avg loss: 2.062298579078626   (start: 58650, end: 58660)\n",
      "Batch 5866 - batch loss: 3.0670361518859863 - avg loss: 2.062469831434653   (start: 58660, end: 58670)\n",
      "Batch 5867 - batch loss: 1.9704281091690063 - avg loss: 2.0624541460695767   (start: 58670, end: 58680)\n",
      "Batch 5868 - batch loss: 1.6355841159820557 - avg loss: 2.0623814130605314   (start: 58680, end: 58690)\n",
      "Batch 5869 - batch loss: 2.340181827545166 - avg loss: 2.062428738514447   (start: 58690, end: 58700)\n",
      "Batch 5870 - batch loss: 1.649862289428711 - avg loss: 2.0623584665932944   (start: 58700, end: 58710)\n",
      "Batch 5871 - batch loss: 2.1962971687316895 - avg loss: 2.0623812763177733   (start: 58710, end: 58720)\n",
      "Batch 5872 - batch loss: 1.891676664352417 - avg loss: 2.062352210318801   (start: 58720, end: 58730)\n",
      "Batch 5873 - batch loss: 1.936026930809021 - avg loss: 2.062330704482997   (start: 58730, end: 58740)\n",
      "Batch 5874 - batch loss: 2.4707255363464355 - avg loss: 2.0624002184969314   (start: 58740, end: 58750)\n",
      "Batch 5875 - batch loss: 3.010650873184204 - avg loss: 2.0625615953952785   (start: 58750, end: 58760)\n",
      "Batch 5876 - batch loss: 1.544535756111145 - avg loss: 2.06247345079101   (start: 58760, end: 58770)\n",
      "Batch 5877 - batch loss: 1.6702930927276611 - avg loss: 2.0624067307573144   (start: 58770, end: 58780)\n",
      "Batch 5878 - batch loss: 1.7048587799072266 - avg loss: 2.062345912939514   (start: 58780, end: 58790)\n",
      "Batch 5879 - batch loss: 1.612054467201233 - avg loss: 2.0622693327616672   (start: 58790, end: 58800)\n",
      "Batch 5880 - batch loss: 2.154132604598999 - avg loss: 2.06228495311056   (start: 58800, end: 58810)\n",
      "Batch 5881 - batch loss: 1.4164505004882812 - avg loss: 2.062175154665707   (start: 58810, end: 58820)\n",
      "Batch 5882 - batch loss: 2.1202406883239746 - avg loss: 2.062185024720723   (start: 58820, end: 58830)\n",
      "Batch 5883 - batch loss: 1.8031717538833618 - avg loss: 2.0621410047902615   (start: 58830, end: 58840)\n",
      "Batch 5884 - batch loss: 1.058157205581665 - avg loss: 2.06197040431461   (start: 58840, end: 58850)\n",
      "Batch 5885 - batch loss: 2.058779716491699 - avg loss: 2.06196986223377   (start: 58850, end: 58860)\n",
      "Batch 5886 - batch loss: 1.7486060857772827 - avg loss: 2.061916632443307   (start: 58860, end: 58870)\n",
      "Batch 5887 - batch loss: 1.9357802867889404 - avg loss: 2.0618952098302543   (start: 58870, end: 58880)\n",
      "Batch 5888 - batch loss: 1.497280478477478 - avg loss: 2.06179933366599   (start: 58880, end: 58890)\n",
      "Batch 5889 - batch loss: 1.836896538734436 - avg loss: 2.0617611498298385   (start: 58890, end: 58900)\n",
      "Batch 5890 - batch loss: 1.5665103197097778 - avg loss: 2.061677080770236   (start: 58900, end: 58910)\n",
      "Batch 5891 - batch loss: 2.4048783779144287 - avg loss: 2.0617353294628944   (start: 58910, end: 58920)\n",
      "Batch 5892 - batch loss: 2.2169594764709473 - avg loss: 2.0617616698917094   (start: 58920, end: 58930)\n",
      "Batch 5893 - batch loss: 2.552903413772583 - avg loss: 2.0618449989965417   (start: 58930, end: 58940)\n",
      "Batch 5894 - batch loss: 2.669034242630005 - avg loss: 2.061947999716412   (start: 58940, end: 58950)\n",
      "Batch 5895 - batch loss: 1.9762775897979736 - avg loss: 2.061933469456928   (start: 58950, end: 58960)\n",
      "Batch 5896 - batch loss: 1.7931993007659912 - avg loss: 2.061887898120877   (start: 58960, end: 58970)\n",
      "Batch 5897 - batch loss: 2.1485979557037354 - avg loss: 2.0619025997244007   (start: 58970, end: 58980)\n",
      "Batch 5898 - batch loss: 1.7372537851333618 - avg loss: 2.0618475651736987   (start: 58980, end: 58990)\n",
      "Batch 5899 - batch loss: 1.9034589529037476 - avg loss: 2.061820719646195   (start: 58990, end: 59000)\n",
      "Batch 5900 - batch loss: 3.176593542098999 - avg loss: 2.0620096321733015   (start: 59000, end: 59010)\n",
      "Batch 5901 - batch loss: 2.601106882095337 - avg loss: 2.062100973625338   (start: 59010, end: 59020)\n",
      "Batch 5902 - batch loss: 1.3138481378555298 - avg loss: 2.0619742155640526   (start: 59020, end: 59030)\n",
      "Batch 5903 - batch loss: 1.6676355600357056 - avg loss: 2.0619074237863546   (start: 59030, end: 59040)\n",
      "Batch 5904 - batch loss: 2.187108039855957 - avg loss: 2.061928626261557   (start: 59040, end: 59050)\n",
      "Batch 5905 - batch loss: 1.662264108657837 - avg loss: 2.0618609553307063   (start: 59050, end: 59060)\n",
      "Batch 5906 - batch loss: 2.584575891494751 - avg loss: 2.061949446093558   (start: 59060, end: 59070)\n",
      "Batch 5907 - batch loss: 2.2029125690460205 - avg loss: 2.0619733057961565   (start: 59070, end: 59080)\n",
      "Batch 5908 - batch loss: 1.4874988794326782 - avg loss: 2.0618760855513836   (start: 59080, end: 59090)\n",
      "Batch 5909 - batch loss: 2.0002596378326416 - avg loss: 2.0618656597565073   (start: 59090, end: 59100)\n",
      "Batch 5910 - batch loss: 1.8794896602630615 - avg loss: 2.06183480609393   (start: 59100, end: 59110)\n",
      "Batch 5911 - batch loss: 2.7637429237365723 - avg loss: 2.0619535320948845   (start: 59110, end: 59120)\n",
      "Batch 5912 - batch loss: 2.3131885528564453 - avg loss: 2.0619960206828707   (start: 59120, end: 59130)\n",
      "Batch 5913 - batch loss: 1.7526185512542725 - avg loss: 2.0619437079555407   (start: 59130, end: 59140)\n",
      "Batch 5914 - batch loss: 2.3864707946777344 - avg loss: 2.0619985730589594   (start: 59140, end: 59150)\n",
      "Batch 5915 - batch loss: 1.9733936786651611 - avg loss: 2.061983595896283   (start: 59150, end: 59160)\n",
      "Batch 5916 - batch loss: 3.191368579864502 - avg loss: 2.0621744671120967   (start: 59160, end: 59170)\n",
      "Batch 5917 - batch loss: 2.8217098712921143 - avg loss: 2.06230281037066   (start: 59170, end: 59180)\n",
      "Batch 5918 - batch loss: 2.5437731742858887 - avg loss: 2.0623841535644285   (start: 59180, end: 59190)\n",
      "Batch 5919 - batch loss: 1.5382752418518066 - avg loss: 2.0622956216536665   (start: 59190, end: 59200)\n",
      "Batch 5920 - batch loss: 2.635580062866211 - avg loss: 2.062392443886602   (start: 59200, end: 59210)\n",
      "Batch 5921 - batch loss: 2.4359960556030273 - avg loss: 2.062455531291485   (start: 59210, end: 59220)\n",
      "Batch 5922 - batch loss: 1.2935802936553955 - avg loss: 2.062325719500562   (start: 59220, end: 59230)\n",
      "Batch 5923 - batch loss: 2.0141327381134033 - avg loss: 2.06231758429101   (start: 59230, end: 59240)\n",
      "Batch 5924 - batch loss: 1.5513598918914795 - avg loss: 2.062231346705795   (start: 59240, end: 59250)\n",
      "Batch 5925 - batch loss: 1.9522005319595337 - avg loss: 2.0622127792378997   (start: 59250, end: 59260)\n",
      "Batch 5926 - batch loss: 1.403186321258545 - avg loss: 2.062101588676405   (start: 59260, end: 59270)\n",
      "Batch 5927 - batch loss: 1.9604486227035522 - avg loss: 2.0620844407401746   (start: 59270, end: 59280)\n",
      "Batch 5928 - batch loss: 1.4294673204421997 - avg loss: 2.061977741951121   (start: 59280, end: 59290)\n",
      "Batch 5929 - batch loss: 0.8583357930183411 - avg loss: 2.0617747669175746   (start: 59290, end: 59300)\n",
      "Batch 5930 - batch loss: 2.863722562789917 - avg loss: 2.0619099798320697   (start: 59300, end: 59310)\n",
      "Batch 5931 - batch loss: 2.324157953262329 - avg loss: 2.061954188863329   (start: 59310, end: 59320)\n",
      "Batch 5932 - batch loss: 1.976890206336975 - avg loss: 2.061939851431587   (start: 59320, end: 59330)\n",
      "Batch 5933 - batch loss: 1.808180570602417 - avg loss: 2.06189708781837   (start: 59330, end: 59340)\n",
      "Batch 5934 - batch loss: 2.8724658489227295 - avg loss: 2.0620336621673347   (start: 59340, end: 59350)\n",
      "Batch 5935 - batch loss: 2.750063419342041 - avg loss: 2.062149570145295   (start: 59350, end: 59360)\n",
      "Batch 5936 - batch loss: 1.764481544494629 - avg loss: 2.0620994323609514   (start: 59360, end: 59370)\n",
      "Batch 5937 - batch loss: 1.499884009361267 - avg loss: 2.062004751420736   (start: 59370, end: 59380)\n",
      "Batch 5938 - batch loss: 1.2588608264923096 - avg loss: 2.061869519239404   (start: 59380, end: 59390)\n",
      "Batch 5939 - batch loss: 1.6864945888519287 - avg loss: 2.061806324806679   (start: 59390, end: 59400)\n",
      "Batch 5940 - batch loss: 2.2007675170898438 - avg loss: 2.0618297150090497   (start: 59400, end: 59410)\n",
      "Batch 5941 - batch loss: 1.9256336688995361 - avg loss: 2.0618067940992364   (start: 59410, end: 59420)\n",
      "Batch 5942 - batch loss: 2.5483767986297607 - avg loss: 2.061888666891518   (start: 59420, end: 59430)\n",
      "Batch 5943 - batch loss: 1.3401025533676147 - avg loss: 2.061767235849539   (start: 59430, end: 59440)\n",
      "Batch 5944 - batch loss: 2.431840419769287 - avg loss: 2.061829485333798   (start: 59440, end: 59450)\n",
      "Batch 5945 - batch loss: 1.4444751739501953 - avg loss: 2.0617256585071275   (start: 59450, end: 59460)\n",
      "Batch 5946 - batch loss: 1.5814138650894165 - avg loss: 2.0616448931139177   (start: 59460, end: 59470)\n",
      "Batch 5947 - batch loss: 2.223616123199463 - avg loss: 2.0616721243227416   (start: 59470, end: 59480)\n",
      "Batch 5948 - batch loss: 1.3574458360671997 - avg loss: 2.061553747068034   (start: 59480, end: 59490)\n",
      "Batch 5949 - batch loss: 2.189483165740967 - avg loss: 2.0615752478106684   (start: 59490, end: 59500)\n",
      "Batch 5950 - batch loss: 2.2012808322906494 - avg loss: 2.0615987237952895   (start: 59500, end: 59510)\n",
      "Batch 5951 - batch loss: 1.0063525438308716 - avg loss: 2.0614214310903223   (start: 59510, end: 59520)\n",
      "Batch 5952 - batch loss: 2.0723934173583984 - avg loss: 2.061423274192333   (start: 59520, end: 59530)\n",
      "Batch 5953 - batch loss: 1.972486138343811 - avg loss: 2.061408336816476   (start: 59530, end: 59540)\n",
      "Batch 5954 - batch loss: 1.873197317123413 - avg loss: 2.061376731271608   (start: 59540, end: 59550)\n",
      "Batch 5955 - batch loss: 2.267357587814331 - avg loss: 2.0614113150285824   (start: 59550, end: 59560)\n",
      "Batch 5956 - batch loss: 1.6089204549789429 - avg loss: 2.0613353555086817   (start: 59560, end: 59570)\n",
      "Batch 5957 - batch loss: 1.6477367877960205 - avg loss: 2.061265936480868   (start: 59570, end: 59580)\n",
      "Batch 5958 - batch loss: 1.5459092855453491 - avg loss: 2.0611794527334384   (start: 59580, end: 59590)\n",
      "Batch 5959 - batch loss: 1.8391813039779663 - avg loss: 2.061142204721902   (start: 59590, end: 59600)\n",
      "Batch 5960 - batch loss: 1.9990341663360596 - avg loss: 2.061131785658257   (start: 59600, end: 59610)\n",
      "Batch 5961 - batch loss: 1.8645973205566406 - avg loss: 2.061098821138784   (start: 59610, end: 59620)\n",
      "Batch 5962 - batch loss: 1.6627004146575928 - avg loss: 2.061032009398639   (start: 59620, end: 59630)\n",
      "Batch 5963 - batch loss: 1.8469432592391968 - avg loss: 2.060996112559243   (start: 59630, end: 59640)\n",
      "Batch 5964 - batch loss: 2.445538282394409 - avg loss: 2.061060578974974   (start: 59640, end: 59650)\n",
      "Batch 5965 - batch loss: 2.2091588973999023 - avg loss: 2.061085402695796   (start: 59650, end: 59660)\n",
      "Batch 5966 - batch loss: 2.1472060680389404 - avg loss: 2.0610998355205563   (start: 59660, end: 59670)\n",
      "Batch 5967 - batch loss: 1.1529052257537842 - avg loss: 2.060947658139563   (start: 59670, end: 59680)\n",
      "Batch 5968 - batch loss: 2.8220973014831543 - avg loss: 2.0610751752518675   (start: 59680, end: 59690)\n",
      "Batch 5969 - batch loss: 2.316624402999878 - avg loss: 2.061117980817654   (start: 59690, end: 59700)\n",
      "Batch 5970 - batch loss: 1.9540910720825195 - avg loss: 2.0611000563646757   (start: 59700, end: 59710)\n",
      "Batch 5971 - batch loss: 3.540977954864502 - avg loss: 2.061347859093828   (start: 59710, end: 59720)\n",
      "Batch 5972 - batch loss: 1.9338210821151733 - avg loss: 2.0613265085535675   (start: 59720, end: 59730)\n",
      "Batch 5973 - batch loss: 2.0765295028686523 - avg loss: 2.0613290534136803   (start: 59730, end: 59740)\n",
      "Batch 5974 - batch loss: 1.474103331565857 - avg loss: 2.061230772958141   (start: 59740, end: 59750)\n",
      "Batch 5975 - batch loss: 2.410600185394287 - avg loss: 2.061289235041882   (start: 59750, end: 59760)\n",
      "Batch 5976 - batch loss: 3.4405770301818848 - avg loss: 2.0615200009436956   (start: 59760, end: 59770)\n",
      "Batch 5977 - batch loss: 1.5919119119644165 - avg loss: 2.061441444890002   (start: 59770, end: 59780)\n",
      "Batch 5978 - batch loss: 2.187246561050415 - avg loss: 2.061462486053434   (start: 59780, end: 59790)\n",
      "Batch 5979 - batch loss: 2.277085781097412 - avg loss: 2.0614985434606323   (start: 59790, end: 59800)\n",
      "Batch 5980 - batch loss: 2.2183451652526855 - avg loss: 2.061524767607396   (start: 59800, end: 59810)\n",
      "Batch 5981 - batch loss: 1.2705700397491455 - avg loss: 2.0613925451520534   (start: 59810, end: 59820)\n",
      "Batch 5982 - batch loss: 1.7175521850585938 - avg loss: 2.0613350755949593   (start: 59820, end: 59830)\n",
      "Batch 5983 - batch loss: 2.510103702545166 - avg loss: 2.061410070352137   (start: 59830, end: 59840)\n",
      "Batch 5984 - batch loss: 1.4496690034866333 - avg loss: 2.0613078579767206   (start: 59840, end: 59850)\n",
      "Batch 5985 - batch loss: 2.221748113632202 - avg loss: 2.0613346605586877   (start: 59850, end: 59860)\n",
      "Batch 5986 - batch loss: 0.857390284538269 - avg loss: 2.0611335674609728   (start: 59860, end: 59870)\n",
      "Batch 5987 - batch loss: 2.632239580154419 - avg loss: 2.061228942546593   (start: 59870, end: 59880)\n",
      "Batch 5988 - batch loss: 2.4974188804626465 - avg loss: 2.0613017743946336   (start: 59880, end: 59890)\n",
      "Batch 5989 - batch loss: 3.0734803676605225 - avg loss: 2.061470752456948   (start: 59890, end: 59900)\n",
      "Batch 5990 - batch loss: 1.8606147766113281 - avg loss: 2.061437226171546   (start: 59900, end: 59910)\n",
      "Batch 5991 - batch loss: 1.5728996992111206 - avg loss: 2.061355694541546   (start: 59910, end: 59920)\n",
      "Batch 5992 - batch loss: 2.429166793823242 - avg loss: 2.061417067993787   (start: 59920, end: 59930)\n",
      "Batch 5993 - batch loss: 2.6611034870147705 - avg loss: 2.061517115778075   (start: 59930, end: 59940)\n",
      "Batch 5994 - batch loss: 2.361785888671875 - avg loss: 2.061567202312336   (start: 59940, end: 59950)\n",
      "Batch 5995 - batch loss: 1.8492162227630615 - avg loss: 2.0615317868721177   (start: 59950, end: 59960)\n",
      "Batch 5996 - batch loss: 1.5281305313110352 - avg loss: 2.0614428421905164   (start: 59960, end: 59970)\n",
      "Batch 5997 - batch loss: 1.8441555500030518 - avg loss: 2.0614066155662774   (start: 59970, end: 59980)\n",
      "Batch 5998 - batch loss: 2.8649611473083496 - avg loss: 2.0615405636462474   (start: 59980, end: 59990)\n",
      "Batch 5999 - batch loss: 1.5692646503448486 - avg loss: 2.0614585176606974   (start: 59990, end: 60000)\n",
      "Batch 6000 - batch loss: 1.8149315118789673 - avg loss: 2.0614174366732314   (start: 60000, end: 60010)\n",
      "Batch 6001 - batch loss: 1.6888656616210938 - avg loss: 2.061355365401147   (start: 60010, end: 60020)\n",
      "Batch 6002 - batch loss: 1.1579793691635132 - avg loss: 2.0612048779788186   (start: 60020, end: 60030)\n",
      "Batch 6003 - batch loss: 1.487641453742981 - avg loss: 2.0611093477615907   (start: 60030, end: 60040)\n",
      "Batch 6004 - batch loss: 2.0232391357421875 - avg loss: 2.0611030413149596   (start: 60040, end: 60050)\n",
      "Batch 6005 - batch loss: 1.49370276927948 - avg loss: 2.0610085690751934   (start: 60050, end: 60060)\n",
      "Batch 6006 - batch loss: 1.3729157447814941 - avg loss: 2.060894020577725   (start: 60060, end: 60070)\n",
      "Batch 6007 - batch loss: 1.7990562915802002 - avg loss: 2.060850439064909   (start: 60070, end: 60080)\n",
      "Batch 6008 - batch loss: 2.145477771759033 - avg loss: 2.0608645224952125   (start: 60080, end: 60090)\n",
      "Batch 6009 - batch loss: 1.956993818283081 - avg loss: 2.0608472395161423   (start: 60090, end: 60100)\n",
      "Batch 6010 - batch loss: 1.5258736610412598 - avg loss: 2.0607582404180764   (start: 60100, end: 60110)\n",
      "Batch 6011 - batch loss: 1.8869822025299072 - avg loss: 2.0607293355548215   (start: 60110, end: 60120)\n",
      "Batch 6012 - batch loss: 1.5047438144683838 - avg loss: 2.06063687163979   (start: 60120, end: 60130)\n",
      "Batch 6013 - batch loss: 2.1984057426452637 - avg loss: 2.0606597796662287   (start: 60130, end: 60140)\n",
      "Batch 6014 - batch loss: 1.9386532306671143 - avg loss: 2.0606394959506846   (start: 60140, end: 60150)\n",
      "Batch 6015 - batch loss: 1.695868730545044 - avg loss: 2.060578862512286   (start: 60150, end: 60160)\n",
      "Batch 6016 - batch loss: 1.4122086763381958 - avg loss: 2.060471106124356   (start: 60160, end: 60170)\n",
      "Batch 6017 - batch loss: 1.5223703384399414 - avg loss: 2.0603816909087223   (start: 60170, end: 60180)\n",
      "Batch 6018 - batch loss: 1.6493871212005615 - avg loss: 2.060313408042846   (start: 60180, end: 60190)\n",
      "Batch 6019 - batch loss: 1.6393171548843384 - avg loss: 2.0602434751104277   (start: 60190, end: 60200)\n",
      "Batch 6020 - batch loss: 3.1289870738983154 - avg loss: 2.060420977784201   (start: 60200, end: 60210)\n",
      "Batch 6021 - batch loss: 1.7634319067001343 - avg loss: 2.0603716604359636   (start: 60210, end: 60220)\n",
      "Batch 6022 - batch loss: 2.2907450199127197 - avg loss: 2.0604099093749437   (start: 60220, end: 60230)\n",
      "Batch 6023 - batch loss: 1.3290107250213623 - avg loss: 2.060288495167714   (start: 60230, end: 60240)\n",
      "Batch 6024 - batch loss: 1.8720731735229492 - avg loss: 2.0602572561101793   (start: 60240, end: 60250)\n",
      "Batch 6025 - batch loss: 1.8690059185028076 - avg loss: 2.0602255184172478   (start: 60250, end: 60260)\n",
      "Batch 6026 - batch loss: 1.5645196437835693 - avg loss: 2.060143270885369   (start: 60260, end: 60270)\n",
      "Batch 6027 - batch loss: 2.052353620529175 - avg loss: 2.060141978640784   (start: 60270, end: 60280)\n",
      "Batch 6028 - batch loss: 2.6542115211486816 - avg loss: 2.0602405139770767   (start: 60280, end: 60290)\n",
      "Batch 6029 - batch loss: 2.2859463691711426 - avg loss: 2.060277944467159   (start: 60290, end: 60300)\n",
      "Batch 6030 - batch loss: 1.6210368871688843 - avg loss: 2.0602051139154596   (start: 60300, end: 60310)\n",
      "Batch 6031 - batch loss: 2.078413724899292 - avg loss: 2.060208132584389   (start: 60310, end: 60320)\n",
      "Batch 6032 - batch loss: 2.3326122760772705 - avg loss: 2.0602532849370316   (start: 60320, end: 60330)\n",
      "Batch 6033 - batch loss: 3.2424819469451904 - avg loss: 2.0604492127895355   (start: 60330, end: 60340)\n",
      "Batch 6034 - batch loss: 2.5311286449432373 - avg loss: 2.0605272044104392   (start: 60340, end: 60350)\n",
      "Batch 6035 - batch loss: 2.2286314964294434 - avg loss: 2.0605550546907603   (start: 60350, end: 60360)\n",
      "Batch 6036 - batch loss: 1.4744417667388916 - avg loss: 2.060457967844984   (start: 60360, end: 60370)\n",
      "Batch 6037 - batch loss: 2.157043695449829 - avg loss: 2.06047396415628   (start: 60370, end: 60380)\n",
      "Batch 6038 - batch loss: 1.8130460977554321 - avg loss: 2.060432992494349   (start: 60380, end: 60390)\n",
      "Batch 6039 - batch loss: 2.6717207431793213 - avg loss: 2.0605341990755885   (start: 60390, end: 60400)\n",
      "Batch 6040 - batch loss: 2.3197264671325684 - avg loss: 2.0605771045991865   (start: 60400, end: 60410)\n",
      "Batch 6041 - batch loss: 2.478569746017456 - avg loss: 2.060646285771219   (start: 60410, end: 60420)\n",
      "Batch 6042 - batch loss: 1.3196457624435425 - avg loss: 2.0605236644699896   (start: 60420, end: 60430)\n",
      "Batch 6043 - batch loss: 1.6628170013427734 - avg loss: 2.0604578625733767   (start: 60430, end: 60440)\n",
      "Batch 6044 - batch loss: 1.3096297979354858 - avg loss: 2.060333656111071   (start: 60440, end: 60450)\n",
      "Batch 6045 - batch loss: 2.8075544834136963 - avg loss: 2.060457245397757   (start: 60450, end: 60460)\n",
      "Batch 6046 - batch loss: 2.8431482315063477 - avg loss: 2.060586679991127   (start: 60460, end: 60470)\n",
      "Batch 6047 - batch loss: 2.249011993408203 - avg loss: 2.0606178349701976   (start: 60470, end: 60480)\n",
      "Batch 6048 - batch loss: 1.0903029441833496 - avg loss: 2.0604574258297137   (start: 60480, end: 60490)\n",
      "Batch 6049 - batch loss: 2.7669055461883545 - avg loss: 2.0605741941140705   (start: 60490, end: 60500)\n",
      "Batch 6050 - batch loss: 1.3544785976409912 - avg loss: 2.060457503385848   (start: 60500, end: 60510)\n",
      "Batch 6051 - batch loss: 1.6377404928207397 - avg loss: 2.060387655895669   (start: 60510, end: 60520)\n",
      "Batch 6052 - batch loss: 1.7892513275146484 - avg loss: 2.0603428621853794   (start: 60520, end: 60530)\n",
      "Batch 6053 - batch loss: 1.5339032411575317 - avg loss: 2.0602559048644298   (start: 60530, end: 60540)\n",
      "Batch 6054 - batch loss: 1.2682543992996216 - avg loss: 2.0601251036248653   (start: 60540, end: 60550)\n",
      "Batch 6055 - batch loss: 1.940314531326294 - avg loss: 2.060105319844763   (start: 60550, end: 60560)\n",
      "Batch 6056 - batch loss: 1.6888840198516846 - avg loss: 2.0600440318639155   (start: 60560, end: 60570)\n",
      "Batch 6057 - batch loss: 1.2028589248657227 - avg loss: 2.0599025354778147   (start: 60570, end: 60580)\n",
      "Batch 6058 - batch loss: 1.8467031717300415 - avg loss: 2.0598673482581833   (start: 60580, end: 60590)\n",
      "Batch 6059 - batch loss: 1.9931018352508545 - avg loss: 2.059856330846796   (start: 60590, end: 60600)\n",
      "Batch 6060 - batch loss: 1.6860268115997314 - avg loss: 2.059794652985181   (start: 60600, end: 60610)\n",
      "Batch 6061 - batch loss: 1.5813724994659424 - avg loss: 2.059715731481796   (start: 60610, end: 60620)\n",
      "Batch 6062 - batch loss: 1.8211195468902588 - avg loss: 2.059676378655705   (start: 60620, end: 60630)\n",
      "Batch 6063 - batch loss: 1.7375648021697998 - avg loss: 2.0596232599920365   (start: 60630, end: 60640)\n",
      "Batch 6064 - batch loss: 1.8019390106201172 - avg loss: 2.0595807728940363   (start: 60640, end: 60650)\n",
      "Batch 6065 - batch loss: 2.3748927116394043 - avg loss: 2.0596327531015444   (start: 60650, end: 60660)\n",
      "Batch 6066 - batch loss: 1.4586610794067383 - avg loss: 2.0595336972792775   (start: 60660, end: 60670)\n",
      "Batch 6067 - batch loss: 2.5093300342559814 - avg loss: 2.0596078232412047   (start: 60670, end: 60680)\n",
      "Batch 6068 - batch loss: 2.290203809738159 - avg loss: 2.0596458189549134   (start: 60680, end: 60690)\n",
      "Batch 6069 - batch loss: 2.6917219161987305 - avg loss: 2.0597499501076717   (start: 60690, end: 60700)\n",
      "Batch 6070 - batch loss: 2.1718242168426514 - avg loss: 2.059768410701764   (start: 60700, end: 60710)\n",
      "Batch 6071 - batch loss: 1.4088348150253296 - avg loss: 2.0596612081991825   (start: 60710, end: 60720)\n",
      "Batch 6072 - batch loss: 2.3015434741973877 - avg loss: 2.059701037322515   (start: 60720, end: 60730)\n",
      "Batch 6073 - batch loss: 1.8550999164581299 - avg loss: 2.0596673525808513   (start: 60730, end: 60740)\n",
      "Batch 6074 - batch loss: 2.220050573348999 - avg loss: 2.059693753111019   (start: 60740, end: 60750)\n",
      "Batch 6075 - batch loss: 1.5012791156768799 - avg loss: 2.059601848134483   (start: 60750, end: 60760)\n",
      "Batch 6076 - batch loss: 2.4249939918518066 - avg loss: 2.0596619751944987   (start: 60760, end: 60770)\n",
      "Batch 6077 - batch loss: 2.486786365509033 - avg loss: 2.0597322490329844   (start: 60770, end: 60780)\n",
      "Batch 6078 - batch loss: 1.9375098943710327 - avg loss: 2.059712143365167   (start: 60780, end: 60790)\n",
      "Batch 6079 - batch loss: 2.0067858695983887 - avg loss: 2.0597034383859287   (start: 60790, end: 60800)\n",
      "Batch 6080 - batch loss: 2.8869433403015137 - avg loss: 2.059839475205846   (start: 60800, end: 60810)\n",
      "Batch 6081 - batch loss: 1.7882391214370728 - avg loss: 2.059794818784641   (start: 60810, end: 60820)\n",
      "Batch 6082 - batch loss: 1.9161745309829712 - avg loss: 2.059771208676503   (start: 60820, end: 60830)\n",
      "Batch 6083 - batch loss: 2.001040458679199 - avg loss: 2.059761555364538   (start: 60830, end: 60840)\n",
      "Batch 6084 - batch loss: 1.8924882411956787 - avg loss: 2.0597340659127434   (start: 60840, end: 60850)\n",
      "Batch 6085 - batch loss: 2.3379101753234863 - avg loss: 2.059779773456189   (start: 60850, end: 60860)\n",
      "Batch 6086 - batch loss: 1.8587396144866943 - avg loss: 2.0597467456659855   (start: 60860, end: 60870)\n",
      "Batch 6087 - batch loss: 2.2163639068603516 - avg loss: 2.0597724712180874   (start: 60870, end: 60880)\n",
      "Batch 6088 - batch loss: 1.4817392826080322 - avg loss: 2.0596775404924164   (start: 60880, end: 60890)\n",
      "Batch 6089 - batch loss: 1.685234785079956 - avg loss: 2.0596160556393106   (start: 60890, end: 60900)\n",
      "Batch 6090 - batch loss: 1.9781720638275146 - avg loss: 2.05960268443724   (start: 60900, end: 60910)\n",
      "Batch 6091 - batch loss: 1.6192480325698853 - avg loss: 2.059530400351248   (start: 60910, end: 60920)\n",
      "Batch 6092 - batch loss: 2.1363844871520996 - avg loss: 2.0595430138563846   (start: 60920, end: 60930)\n",
      "Batch 6093 - batch loss: 2.6674270629882812 - avg loss: 2.0596427650951656   (start: 60930, end: 60940)\n",
      "Batch 6094 - batch loss: 1.9987165927886963 - avg loss: 2.0596327690045495   (start: 60940, end: 60950)\n",
      "Batch 6095 - batch loss: 2.0939784049987793 - avg loss: 2.059638403131189   (start: 60950, end: 60960)\n",
      "Batch 6096 - batch loss: 1.3634276390075684 - avg loss: 2.0595242140604784   (start: 60960, end: 60970)\n",
      "Batch 6097 - batch loss: 2.125760078430176 - avg loss: 2.059535075960178   (start: 60970, end: 60980)\n",
      "Batch 6098 - batch loss: 1.7949689626693726 - avg loss: 2.0594916973549493   (start: 60980, end: 60990)\n",
      "Batch 6099 - batch loss: 2.7746901512145996 - avg loss: 2.059608943003123   (start: 60990, end: 61000)\n",
      "Batch 6100 - batch loss: 3.2649471759796143 - avg loss: 2.05980650704721   (start: 61000, end: 61010)\n",
      "Batch 6101 - batch loss: 1.7453925609588623 - avg loss: 2.0597549806712534   (start: 61010, end: 61020)\n",
      "Batch 6102 - batch loss: 1.5848426818847656 - avg loss: 2.0596771644663074   (start: 61020, end: 61030)\n",
      "Batch 6103 - batch loss: 2.3194034099578857 - avg loss: 2.059719714637587   (start: 61030, end: 61040)\n",
      "Batch 6104 - batch loss: 2.458134412765503 - avg loss: 2.059784975030401   (start: 61040, end: 61050)\n",
      "Batch 6105 - batch loss: 2.1002252101898193 - avg loss: 2.05979159806269   (start: 61050, end: 61060)\n",
      "Batch 6106 - batch loss: 1.9004065990447998 - avg loss: 2.0597654993236993   (start: 61060, end: 61070)\n",
      "Batch 6107 - batch loss: 3.6834893226623535 - avg loss: 2.0600313349201858   (start: 61070, end: 61080)\n",
      "Batch 6108 - batch loss: 3.0150527954101562 - avg loss: 2.060187665164168   (start: 61080, end: 61090)\n",
      "Batch 6109 - batch loss: 1.9927984476089478 - avg loss: 2.060176635832326   (start: 61090, end: 61100)\n",
      "Batch 6110 - batch loss: 2.4059996604919434 - avg loss: 2.0602332260834566   (start: 61100, end: 61110)\n",
      "Batch 6111 - batch loss: 2.1431655883789062 - avg loss: 2.0602467948600105   (start: 61110, end: 61120)\n",
      "Batch 6112 - batch loss: 1.3068954944610596 - avg loss: 2.0601235572842866   (start: 61120, end: 61130)\n",
      "Batch 6113 - batch loss: 1.3163048028945923 - avg loss: 2.060001898999303   (start: 61130, end: 61140)\n",
      "Batch 6114 - batch loss: 2.2237095832824707 - avg loss: 2.0600286704930535   (start: 61140, end: 61150)\n",
      "Batch 6115 - batch loss: 1.4367820024490356 - avg loss: 2.0599267661980822   (start: 61150, end: 61160)\n",
      "Batch 6116 - batch loss: 2.2305331230163574 - avg loss: 2.0599546567255986   (start: 61160, end: 61170)\n",
      "Batch 6117 - batch loss: 1.9835792779922485 - avg loss: 2.059942173008905   (start: 61170, end: 61180)\n",
      "Batch 6118 - batch loss: 2.189913749694824 - avg loss: 2.0599634136653333   (start: 61180, end: 61190)\n",
      "Batch 6119 - batch loss: 2.702636241912842 - avg loss: 2.060068425565374   (start: 61190, end: 61200)\n",
      "Batch 6120 - batch loss: 1.9412885904312134 - avg loss: 2.060049020266381   (start: 61200, end: 61210)\n",
      "Batch 6121 - batch loss: 2.3940160274505615 - avg loss: 2.060103572211364   (start: 61210, end: 61220)\n",
      "Batch 6122 - batch loss: 2.066934108734131 - avg loss: 2.0601046877652625   (start: 61220, end: 61230)\n",
      "Batch 6123 - batch loss: 2.516646146774292 - avg loss: 2.060179237317681   (start: 61230, end: 61240)\n",
      "Batch 6124 - batch loss: 2.5825281143188477 - avg loss: 2.0602645187669872   (start: 61240, end: 61250)\n",
      "Batch 6125 - batch loss: 3.528789520263672 - avg loss: 2.060504238812938   (start: 61250, end: 61260)\n",
      "Batch 6126 - batch loss: 1.8850631713867188 - avg loss: 2.060475604723265   (start: 61260, end: 61270)\n",
      "Batch 6127 - batch loss: 2.3005211353302 - avg loss: 2.060514776644056   (start: 61270, end: 61280)\n",
      "Batch 6128 - batch loss: 2.3182692527770996 - avg loss: 2.0605568315430824   (start: 61280, end: 61290)\n",
      "Batch 6129 - batch loss: 2.534061908721924 - avg loss: 2.0606340754382177   (start: 61290, end: 61300)\n",
      "Batch 6130 - batch loss: 1.8608980178833008 - avg loss: 2.0606014973828346   (start: 61300, end: 61310)\n",
      "Batch 6131 - batch loss: 1.1712532043457031 - avg loss: 2.0604564634146287   (start: 61310, end: 61320)\n",
      "Batch 6132 - batch loss: 1.1353126764297485 - avg loss: 2.0603056165555085   (start: 61320, end: 61330)\n",
      "Batch 6133 - batch loss: 1.4152275323867798 - avg loss: 2.0602004522118227   (start: 61330, end: 61340)\n",
      "Batch 6134 - batch loss: 1.281919240951538 - avg loss: 2.0600735930086835   (start: 61340, end: 61350)\n",
      "Batch 6135 - batch loss: 1.9230693578720093 - avg loss: 2.06005126506945   (start: 61350, end: 61360)\n",
      "Batch 6136 - batch loss: 2.176786422729492 - avg loss: 2.0600702866040206   (start: 61360, end: 61370)\n",
      "> Final result ...\n",
      "loss =  2.0600702866040206\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, default_data_collator \n",
    "from datasets import load_dataset\n",
    "import torch, gc, os, sys, copy\n",
    "\n",
    "# Disable WANDB\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "# Lets clear out items and GC\n",
    "tokenizer = None\n",
    "eval_dataset = None\n",
    "model = None\n",
    "trainer = None\n",
    "\n",
    "# Clear everything!\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('codeparrot/codeparrot', use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.bos_token\n",
    "# Load 'codeparrot/codeparrot-clean-valid' Dataset\n",
    "eval_dataset = load_dataset('codeparrot/codeparrot-clean-valid')[\"train\"]\n",
    "\n",
    "# # Reduce top level dataset, to speed up debug iteration\n",
    "# eval_dataset = eval_dataset.select(range(0, 200, 1))\n",
    "\n",
    "# Tokenize content to input_ids, attention_mask, and labels\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"content\"], truncation=True, max_length=1024, \n",
    "        padding='max_length', return_attention_mask=True\n",
    "    ) \n",
    "\n",
    "    # Shift the input_ids one position to the right to create the labels.\n",
    "    tokenized[\"labels\"] = copy.deepcopy(tokenized[\"input_ids\"][1:1024])\n",
    "    tokenized[\"input_ids\"] = tokenized[\"input_ids\"][:1023]\n",
    "    tokenized[\"attention_mask\"] = tokenized[\"attention_mask\"][:1023]\n",
    "\n",
    "    # Tokenized output\n",
    "    return tokenized\n",
    "\n",
    "# Tokenize the content datasets\n",
    "eval_dataset = eval_dataset.map(tokenize_function, batched=False, remove_columns=[\"content\"])\n",
    "eval_dataset.set_format(type='torch', columns=[\"labels\",'input_ids', 'attention_mask'])\n",
    "\n",
    "# Compute metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    # No gradient\n",
    "    with torch.no_grad():\n",
    "        # Logits and labels batches\n",
    "        logits_batch, labels_batch = eval_pred\n",
    "\n",
    "        # Batch size\n",
    "        batch_size = logits_batch.shape[0]\n",
    "        # print(\"BATCH_SIZE??\", batch_size)\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = 0\n",
    "\n",
    "        # Looks like i gotten logits, and labels in batchs of X\n",
    "        # I have to compute the loss for each sample, and then average it\n",
    "        for i in range(batch_size):\n",
    "            # Get logits and labels for sample\n",
    "            logits = torch.tensor(logits_batch[i])\n",
    "            labels = torch.tensor(labels_batch[i])\n",
    "\n",
    "            # Compute loss\n",
    "            loss = torch.nn.functional.cross_entropy(logits, labels, reduction=\"mean\")\n",
    "\n",
    "            # Add to total loss\n",
    "            total_loss += loss\n",
    "\n",
    "        # Average loss\n",
    "        loss = total_loss / batch_size\n",
    "\n",
    "        # Clear everything!\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Print loss\n",
    "        return {'loss': loss}\n",
    "\n",
    "# Reduce it to 10 items (for debugging)\n",
    "eval_dataset_10 = eval_dataset.select(range(0, 10, 1))\n",
    "eval_dataset_20 = eval_dataset.select(range(10, 30, 1))\n",
    "eval_dataset_100 = eval_dataset.select(range(30, 130, 1))\n",
    "\n",
    "# Setup the model, use eval mode, and move it to GPU\n",
    "model = AutoModelForCausalLM.from_pretrained('codeparrot/codeparrot').to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "# # Define training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     per_device_train_batch_size=2,  # batch size per device during training\n",
    "#     per_device_eval_batch_size=2,   # batch size for evaluation\n",
    "#     output_dir='./results',          # output directory\n",
    "#     num_train_epochs=3,              # total number of training epochs\n",
    "#     warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "#     weight_decay=0.01,               # strength of weight decay\n",
    "#     logging_dir='./logs',            # directory for storing logs\n",
    "# )\n",
    "\n",
    "# Initializing Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    "    # THESE THINGS DO NOT WORK????\n",
    "    # args=training_args,\n",
    ")\n",
    "\n",
    "# Because somehow the eval batching is happening for the full datasample i pass in\n",
    "# we need to do some form of manual batching ???\n",
    "def evaluate_dataset(in_dataset, batch_size=10):\n",
    "    # Lets get the dataset size\n",
    "    dataset_size = len(in_dataset)\n",
    "\n",
    "    # Lets get the number of batches\n",
    "    num_batches = dataset_size // batch_size\n",
    "\n",
    "    # the total loss\n",
    "    total_loss = 0\n",
    "\n",
    "    # Lets iterate over the batches\n",
    "    for i in range(num_batches):\n",
    "        # Get the start and end index\n",
    "        start_index = i * batch_size\n",
    "        end_index = (i + 1) * batch_size\n",
    "\n",
    "        # Get the batch\n",
    "        batch = in_dataset.select(range(start_index, end_index, 1))\n",
    "\n",
    "        # Perform evaluation\n",
    "        batch_res = trainer.evaluate(batch, metric_key_prefix=\"eval\")\n",
    "        batch_loss = float(batch_res[\"eval_loss\"])\n",
    "        total_loss += batch_loss\n",
    "        avg_loss = total_loss / (i + 1)\n",
    "\n",
    "        # Print loss\n",
    "        print(f\"Batch {i} - batch loss: {batch_loss} - avg loss: {avg_loss}   (start: {start_index}, end: {end_index})\")\n",
    "\n",
    "    # Garbage collection, to keep vram managable\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Return the average loss\n",
    "    return total_loss / num_batches\n",
    "\n",
    "# # Perform validation for first 10\n",
    "# print(\"=== 10 objects ===\")\n",
    "# output_10 = trainer.evaluate(eval_dataset_10)\n",
    "# print(\"> Output object ...\")\n",
    "# print(output_10)\n",
    "\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# Perform validation for first 20\n",
    "# print(\"=== 20 objects ===\")\n",
    "# output_20 = evaluate_dataset(eval_dataset_20)\n",
    "# print(\"> Output object ...\")\n",
    "# print(output_20)\n",
    "\n",
    "# # Perform validation for first 100\n",
    "# print(\"=== 100 objects ===\")\n",
    "# output_100 = trainer.evaluate(eval_dataset_100)\n",
    "# print(\"> Output object ...\")\n",
    "# print(output_100)\n",
    "\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "# Perform validation for all objects\n",
    "print(f\"=== All objects {len(eval_dataset)} ===\")\n",
    "output = evaluate_dataset(eval_dataset)\n",
    "print(\"> Final result ...\")\n",
    "print(\"loss = \", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-infctx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
