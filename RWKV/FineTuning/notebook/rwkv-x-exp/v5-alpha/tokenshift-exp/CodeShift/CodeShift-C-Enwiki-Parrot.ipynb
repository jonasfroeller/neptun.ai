{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWKV CodeParrot + Enwiki (& instruct)\n",
    "This model is a custom model containing\n",
    "- 24 layers\n",
    "- 2048 embedding size\n",
    "\n",
    "And follows up on the memory tuned 4 model, and applies code training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘TokenShift-C-Stage2.pth’ already there; not retrieving.\n",
      "\n",
      "-rw-r--r-- 1 root root 5.7G Jul 21 16:02 ../../../../model/TokenShift-C-Stage2.pth\n"
     ]
    }
   ],
   "source": [
    "# First lets setup the various directories, and get the model we need\n",
    "!mkdir -p ../../../../model/\n",
    "!mkdir -p ../../../../datapath/\n",
    "!mkdir -p ../../../../checkpoint/\n",
    "!cd ../../../../model/ && wget -nc https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/TokenShift-C-Stage2.pth\n",
    "!ls -alh ../../../../model/TokenShift-C-Stage2.pth\n",
    "\n",
    "# The various other stages, if you want to skip stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_1\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: [4,5,6,7]\n",
      "NOTEBOOK_DIR: /root/rwkv5x-tokenshift-exp-A/notebook/experiment/tokenshift-exp/CodeShift\n",
      "INFERENCE_DIR: /root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet\n",
      "TRAINER_DIR: /root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet\n",
      "PROJECT_DIR: /root/rwkv5x-tokenshift-exp-A\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "GPU_DEVICES=\"[4,5,6,7]\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"CodeShift-C\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v4wavenet/\"))\n",
    "INFERENCE_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v4wavenet/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"INFERENCE_DIR:\", INFERENCE_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeParrot training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/root/.cache/huggingface/datasets/codeparrot___json/codeparrot--codeparrot-clean-fb728533b9673c8b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.41s/it]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/codeparrot___json/codeparrot--codeparrot-clean-fb728533b9673c8b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c29625cf93303090_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/codeparrot___json/codeparrot--codeparrot-clean-fb728533b9673c8b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2a1230f1bc0c64bf_*_of_00064.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/codeparrot___json/codeparrot--codeparrot-clean-fb728533b9673c8b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6f52cf55daacc7de.arrow and /root/.cache/huggingface/datasets/codeparrot___json/codeparrot--codeparrot-clean-fb728533b9673c8b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5b4e8c4f3197f0dc.arrow\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/CodeShift-C-Enwiki-Parrot.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 4107967262\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 4107967262\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230727_065722-ntraguqo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mCodeShift-C - Enwiki-Parrot (ctx=4096, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/ntraguqo\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             4\n",
      "   - accumulate_grad_batches: 64\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/codeparrot___json/codeparrot--codeparrot-clean-fb728533b9673c8b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  6.11it/s]\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 2] Global seed set to 4107967262\n",
      "[rank: 3] Global seed set to 4107967262\n",
      "[rank: 1] Global seed set to 4107967262\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_4096_bf16/build.ninja...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/codeparrot___json/codeparrot--codeparrot-clean-fb728533b9673c8b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c29625cf93303090_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/codeparrot___json/codeparrot--codeparrot-clean-fb728533b9673c8b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-2a1230f1bc0c64bf_*_of_00064.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/codeparrot___json/codeparrot--codeparrot-clean-fb728533b9673c8b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-6f52cf55daacc7de.arrow and /root/.cache/huggingface/datasets/codeparrot___json/codeparrot--codeparrot-clean-fb728533b9673c8b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-5b4e8c4f3197f0dc.arrow\n",
      "Saving the dataset (0/194 shards):   0%| | 14000/5334566 [00:08<55:03, 1610.44 e[rank: 1] Global seed set to 4107967262\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "[2023-07-27 06:58:13,257] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Saving the dataset (0/194 shards):   0%| | 15000/5334566 [00:08<45:52, 1932.73 e[rank: 2] Global seed set to 4107967262\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "[2023-07-27 06:58:13,480] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Saving the dataset (0/194 shards):   0%| | 16000/5334566 [00:09<51:17, 1728.44 e[rank: 3] Global seed set to 4107967262\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "[2023-07-27 06:58:14,189] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 0] Global seed set to 4107967262                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "[2023-07-27 07:08:20,107] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  4.000e-04 (0.0004)\n",
      "    - lr_final: 1.000e-04 (0.0001)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06224656105041504 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10188031196594238 seconds\n",
      "Time to load fused_adam op: 0.10212874412536621 seconds\n",
      "Time to load fused_adam op: 0.10212874412536621 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06557559967041016 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10179924964904785 seconds\n",
      "Time to load utils op: 0.10184931755065918 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10230016708374023 seconds\n",
      "Rank: 0 partition count [4, 4, 4] and sizes[(378752000, False), (12288, False), (12288, False)] \n",
      "Rank: 1 partition count [4, 4, 4] and sizes[(378752000, False), (12288, False), (12288, False)] \n",
      "Rank: 3 partition count [4, 4, 4] and sizes[(378752000, False), (12288, False), (12288, False)] \n",
      "Rank: 2 partition count [4, 4, 4] and sizes[(378752000, False), (12288, False), (12288, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002655982971191406 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002796649932861328 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.000278472900390625 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005147457122802734 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0:   1%| | 10225/1333642 [2:13:10<287:17:45,  1.28it/s, v_num=guqo, train/\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ReadTimeout), entering retry loop.\n",
      "Epoch 0:   1%| | 16000/1333642 [3:28:05<285:36:38,  1.28it/s, v_num=guqo, train//usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0:  15%|▏| 204195/1333642 [44:08:59<244:12:09,  1.28it/s, v_num=guqo, trai^C\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the foundation model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/CodeShift-C-Enwiki-Parrot.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Enwiki-Parrot (ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/CodeShift-C-Enwiki-Parrot/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 4\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 438 params 1515106304 elements\n",
      "Saving fp32 state dict to ../model/CodeShift-C-Enwiki-Parrot.pth\n",
      "-rw-r--r-- 1 root root 5.7G Jul 29 03:18 ../model/CodeShift-C-Enwiki-Parrot.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/CodeShift-C-Enwiki-Parrot/last.ckpt\" \"../model/CodeShift-C-Enwiki-Parrot.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/CodeShift-C-Enwiki-Parrot.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_1024_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -t 4 -std=c++17 -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=1024 -c /root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/cuda/wkv_cuda_bf16.cu -o wkv_cuda_bf16.cuda.o \n",
      "ptxas info    : 1 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf\n",
      "    12288 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 56 registers, 464 bytes cmem[0], 8 bytes cmem[2]\n",
      "ptxas info    : Compiling entry function '_Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, 424 bytes cmem[0]\n",
      "[2/3] c++ -MMD -MF wkv_op_bf16.o.d -DTORCH_EXTENSION_NAME=wkv_1024_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -std=c++17 -O3 -DTmax=1024 -c /root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/cuda/wkv_op_bf16.cpp -o wkv_op_bf16.o \n",
      "[3/3] c++ wkv_op_bf16.o wkv_cuda_bf16.cuda.o -shared -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_1024_bf16.so\n",
      "Loading extension module wkv_1024_bf16...\n",
      "--- DRAGON PROMPT ---\n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese. But the freedom of the universe is so much better than a native-world system, and a complex continent.\n",
      "In a few years the universe also lives in the real world, such as in the Union Games, and in the Chandra-style game of life, with a brain.\n",
      "The universe is an entire human, composed of almost-equal worlds and in the Arctic, the island of Quelah.\n",
      "\n",
      "The world is in the southern hemisphere and is in the southern hemisphere. The world's island is in a natural land, with a low observation in the ocean and a perpetual position in the world.\n",
      "\n",
      "This system also has a finite number of people, each agent in a band.\n",
      "\n",
      "If you find the points and be in a high band in the natural world, you know they are near the sea.\n",
      "\n",
      "To decide whether the Earth is in the southern hemisphere or not, you must take an object out of the Earth and enter the real world.\n"
     ]
    }
   ],
   "source": [
    "# # Lets do a quick dragon prompt validation\n",
    "!cd \"{INFERENCE_DIR}\" && python3 dragon_test.py ../model/CodeShift-C-Enwiki-Parrot.pth \"cuda fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "## Model validation for 5 tokens : 20.0% similarity, with 1 matched token, and 4 token mismatch\n",
      "## Model validation for 10 tokens : 20.0% similarity, with 2 matched token, and 8 token mismatch\n",
      "## Model validation for 15 tokens : 13.333333333333334% similarity, with 2 matched token, and 13 token mismatch\n",
      "## Model validation for 20 tokens : 10.0% similarity, with 2 matched token, and 18 token mismatch\n",
      "## Model validation for 25 tokens : 8.0% similarity, with 2 matched token, and 23 token mismatch\n",
      "## Model validation for 30 tokens : 6.666666666666667% similarity, with 2 matched token, and 28 token mismatch\n",
      "## Model validation for 35 tokens : 8.571428571428571% similarity, with 3 matched token, and 32 token mismatch\n",
      "## Model validation for 40 tokens : 2.5% similarity, with 1 matched token, and 39 token mismatch\n",
      "## Model validation for 45 tokens : 2.2222222222222223% similarity, with 1 matched token, and 44 token mismatch\n",
      "## Model validation for 50 tokens : 4.0% similarity, with 2 matched token, and 48 token mismatch\n",
      "## Model validation for 55 tokens : 3.6363636363636362% similarity, with 2 matched token, and 53 token mismatch\n",
      "## Model validation for 60 tokens : 1.6666666666666667% similarity, with 1 matched token, and 59 token mismatch\n",
      "## Model validation for 65 tokens : 1.5384615384615385% similarity, with 1 matched token, and 64 token mismatch\n",
      "## Model validation for 70 tokens : 1.4285714285714286% similarity, with 1 matched token, and 69 token mismatch\n",
      "## Model validation for 75 tokens : 1.3333333333333335% similarity, with 1 matched token, and 74 token mismatch\n",
      "## Model validation for 80 tokens : 1.25% similarity, with 1 matched token, and 79 token mismatch\n",
      "## Model validation for 85 tokens : 1.1764705882352942% similarity, with 1 matched token, and 84 token mismatch\n",
      "## Model validation for 90 tokens : 2.2222222222222223% similarity, with 2 matched token, and 88 token mismatch\n",
      "## Model validation for 95 tokens : 2.1052631578947367% similarity, with 2 matched token, and 93 token mismatch\n",
      "## Model validation for 100 tokens : 2.0% similarity, with 2 matched token, and 98 token mismatch\n",
      "## Model validation for 105 tokens : 1.9047619047619049% similarity, with 2 matched token, and 103 token mismatch\n",
      "## Model validation for 110 tokens : 1.8181818181818181% similarity, with 2 matched token, and 108 token mismatch\n",
      "## Model validation for 115 tokens : 1.7391304347826086% similarity, with 2 matched token, and 113 token mismatch\n",
      "## Model validation for 120 tokens : 1.6666666666666667% similarity, with 2 matched token, and 118 token mismatch\n",
      "## Model validation for 125 tokens : 1.6% similarity, with 2 matched token, and 123 token mismatch\n",
      "## Model validation for 130 tokens : 1.5384615384615385% similarity, with 2 matched token, and 128 token mismatch\n",
      "## Model validation for 135 tokens : 1.4814814814814816% similarity, with 2 matched token, and 133 token mismatch\n",
      "## Model validation for 140 tokens : 1.4285714285714286% similarity, with 2 matched token, and 138 token mismatch\n",
      "## Model validation for 145 tokens : 2.0689655172413794% similarity, with 3 matched token, and 142 token mismatch\n",
      "## Model validation for 150 tokens : 2.0% similarity, with 3 matched token, and 147 token mismatch\n",
      "## Model validation for 160 tokens : 1.875% similarity, with 3 matched token, and 157 token mismatch\n",
      "## Model validation for 170 tokens : 1.7647058823529411% similarity, with 3 matched token, and 167 token mismatch\n",
      "## Model validation for 180 tokens : 1.6666666666666667% similarity, with 3 matched token, and 177 token mismatch\n",
      "## Model validation for 190 tokens : 2.1052631578947367% similarity, with 4 matched token, and 186 token mismatch\n",
      "## Model validation for 200 tokens : 2.0% similarity, with 4 matched token, and 196 token mismatch\n",
      "## Model validation for 210 tokens : 1.9047619047619049% similarity, with 4 matched token, and 206 token mismatch\n",
      "## Model validation for 220 tokens : 1.8181818181818181% similarity, with 4 matched token, and 216 token mismatch\n",
      "## Model validation for 230 tokens : 1.7391304347826086% similarity, with 4 matched token, and 226 token mismatch\n",
      "## Model validation for 240 tokens : 1.6666666666666667% similarity, with 4 matched token, and 236 token mismatch\n",
      "## Model validation for 250 tokens : 1.6% similarity, with 4 matched token, and 246 token mismatch\n",
      "## Model validation for 260 tokens : 1.153846153846154% similarity, with 3 matched token, and 257 token mismatch\n",
      "## Model validation for 270 tokens : 1.4814814814814816% similarity, with 4 matched token, and 266 token mismatch\n",
      "## Model validation for 280 tokens : 1.4285714285714286% similarity, with 4 matched token, and 276 token mismatch\n",
      "## Model validation for 290 tokens : 1.7241379310344827% similarity, with 5 matched token, and 285 token mismatch\n",
      "## Model validation for 300 tokens : 1.6666666666666667% similarity, with 5 matched token, and 295 token mismatch\n",
      "## Model validation for 325 tokens : 1.5384615384615385% similarity, with 5 matched token, and 320 token mismatch\n",
      "## Model validation for 350 tokens : 1.4285714285714286% similarity, with 5 matched token, and 345 token mismatch\n",
      "## Model validation for 375 tokens : 1.6% similarity, with 6 matched token, and 369 token mismatch\n",
      "## Model validation for 400 tokens : 1.25% similarity, with 5 matched token, and 395 token mismatch\n",
      "## Model validation for 425 tokens : 1.1764705882352942% similarity, with 5 matched token, and 420 token mismatch\n",
      "## Model validation for 450 tokens : 1.3333333333333335% similarity, with 6 matched token, and 444 token mismatch\n",
      "## Model validation for 475 tokens : 1.0526315789473684% similarity, with 5 matched token, and 470 token mismatch\n",
      "## Model validation for 500 tokens : 1.2% similarity, with 6 matched token, and 494 token mismatch\n",
      "## Model validation for 525 tokens : 1.1428571428571428% similarity, with 6 matched token, and 519 token mismatch\n",
      "## Model validation for 550 tokens : 1.2727272727272727% similarity, with 7 matched token, and 543 token mismatch\n",
      "## Model validation for 575 tokens : 1.2173913043478262% similarity, with 7 matched token, and 568 token mismatch\n",
      "## Model validation for 600 tokens : 1.1666666666666667% similarity, with 7 matched token, and 593 token mismatch\n",
      "## Model validation for 625 tokens : 1.28% similarity, with 8 matched token, and 617 token mismatch\n",
      "## Model validation for 650 tokens : 1.2307692307692308% similarity, with 8 matched token, and 642 token mismatch\n",
      "## Model validation for 675 tokens : 1.1851851851851851% similarity, with 8 matched token, and 667 token mismatch\n",
      "## Model validation for 700 tokens : 1.1428571428571428% similarity, with 8 matched token, and 692 token mismatch\n",
      "## Model validation for 750 tokens : 1.2% similarity, with 9 matched token, and 741 token mismatch\n",
      "## Model validation for 800 tokens : 1.125% similarity, with 9 matched token, and 791 token mismatch\n",
      "## Model validation for 850 tokens : 1.2941176470588236% similarity, with 11 matched token, and 839 token mismatch\n",
      "## Model validation for 900 tokens : 1.2222222222222223% similarity, with 11 matched token, and 889 token mismatch\n",
      "## Model validation for 950 tokens : 1.1578947368421053% similarity, with 11 matched token, and 939 token mismatch\n",
      "## Model validation for 1000 tokens : 1.2% similarity, with 12 matched token, and 988 token mismatch\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick memory test (let see if this behaviour is removed)\n",
    "!python3 ../memory_script/eval_model_memory_guided.py \"{PROJECT_DIR}/model/CodeShift-C-Enwiki-Parrot.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
