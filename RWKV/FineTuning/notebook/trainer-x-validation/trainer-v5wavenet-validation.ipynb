{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainver v5 code validation\n",
    "\n",
    "Simple minimal training runs, to validate v5 code\n",
    "\n",
    "> Important note: These example focuses only on how to configure your dataset, and does not properly perform checkmarking - for trainer configurations refer to the training notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_1\n",
      "ENABLE_WANDB: False\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /home/picocreator/rwkv-proj/picocreator-memory-experiment/notebook/trainer-x-validation\n",
      "INFERENCE_DIR: /home/picocreator/rwkv-proj/picocreator-memory-experiment/RWKV-v5wavenet\n",
      "TRAINER_DIR: /home/picocreator/rwkv-proj/picocreator-memory-experiment/RWKV-v5wavenet\n",
      "PROJECT_DIR: /home/picocreator/rwkv-proj/picocreator-memory-experiment\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=False\n",
    "WANDB_PREFIX=\"trainer-v5wavenet-validation L6-D512\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5wavenet/\"))\n",
    "INFERENCE_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5wavenet/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"INFERENCE_DIR:\", INFERENCE_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intial setup\n",
    "\n",
    "Before we go into the dataset setup, lets perform an initial setup for all the folders we need, and a small toy model which we would use throughout the various examples within this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-07 13:17:53,853] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1'\n",
      "---- Initializing model ----\n",
      "No of layers: 6\n",
      "Embedding size: 512\n",
      "Output model path: ../model/L6-D512-v5wavenet-neox-init.pth\n",
      "Vocab size: 50277\n",
      "---- ----- ----\n",
      "Model exists, skipping init_model\n"
     ]
    }
   ],
   "source": [
    "# Setup the folders we will need\n",
    "!mkdir -p ../../model/\n",
    "!mkdir -p ../../datapath/\n",
    "!mkdir -p ../../checkpoint/\n",
    "\n",
    "# Initialized a simple L6-D512 model\n",
    "!cd \"{TRAINER_DIR}\" && python3 ./init_model.py --n_layer 6 --n_embd 512 --vocab_size neox --skip-if-exists ../model/L6-D512-v5wavenet-neox-init.pth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick train for v5\n",
    "\n",
    "Preload and train the mini-v5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-07 13:21:15,553] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Found cached dataset parquet (/home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 796.34it/s]\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-e6926d59afde2d0b_*_of_00016.arrow\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-59b52cf35738a88f_*_of_00016.arrow\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-5eb10bc9123e7e5f_*_of_00016.arrow\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py ../notebook/trainer-x-validation/mini-v5wavenet-enwiki.yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-07 13:46:13,508] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1'\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '../notebook/trainer-x-validation/mini-v5wavenet-enwiki.yaml', '--trainer.logger.init_args.name=trainer-v5wavenet-validation L6-D512 (full, train-ctx=4096, data-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto'], args=['fit', '-c', '../notebook/trainer-x-validation/mini-v5wavenet-enwiki.yaml', '--trainer.logger.init_args.name=trainer-v5wavenet-validation L6-D512 (full, train-ctx=4096, data-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto'].\n",
      "  rank_zero_warn(\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 3819846066\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 3819846066\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Found cached dataset parquet (/home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 960.45it/s]\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-e6926d59afde2d0b_*_of_00016.arrow\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-59b52cf35738a88f_*_of_00016.arrow\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-5eb10bc9123e7e5f_*_of_00016.arrow\n",
      "[rank: 0] Global seed set to 3819846066                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-08-07 13:46:18,615] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu117/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.056636810302734375 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Rank: 0 partition count [1, 1] and sizes[(71966720, False), (96, False)] \n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 25.7 M\n",
      "1 | blocks | ModuleList | 20.5 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 25.7 M\n",
      "--------------------------------------\n",
      "72.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "72.0 M    Total params\n",
      "287.867   Total estimated model params size (MB)\n",
      "Epoch 0: 100%|█| 1323/1323 [04:39<00:00,  4.74it/s, v_num=kjrx, train/loss=7.810\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▎                 | 1/14 [00:00<00:02,  4.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▋                | 2/14 [00:00<00:02,  5.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|████               | 3/14 [00:00<00:02,  5.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|█████▍             | 4/14 [00:00<00:01,  5.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▊            | 5/14 [00:00<00:01,  5.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████████▏          | 6/14 [00:01<00:01,  5.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████▌         | 7/14 [00:01<00:01,  5.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|██████████▊        | 8/14 [00:01<00:01,  5.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|████████████▏      | 9/14 [00:01<00:00,  5.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|████████████▊     | 10/14 [00:01<00:00,  5.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|██████████████▏   | 11/14 [00:01<00:00,  5.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|███████████████▍  | 12/14 [00:02<00:00,  5.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|████████████████▋ | 13/14 [00:02<00:00,  5.74it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 1323/1323 [04:42<00:00,  4.69it/s, v_num=kjrx, train/loss=7.810\u001b[A\n",
      "Epoch 0: 100%|█| 1323/1323 [04:42<00:00,  4.69it/s, v_num=kjrx, train/loss=7.810/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 1323/1323 [04:43<00:00,  4.67it/s, v_num=kjrx, train/loss=7.810\n"
     ]
    }
   ],
   "source": [
    "# Validate the dataset is working, by doing a quick training run\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c ../notebook/trainer-x-validation/mini-v5wavenet-enwiki.yaml \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (full, train-ctx=4096, data-ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-07 13:51:07,795] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/trainer-validation/mini-v5wavenet-enwiki/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 1\n",
      "Parsing checkpoint created by deepspeed==0.9.5\n",
      "Reconstructed fp32 state dict with 126 params 71966816 elements\n",
      "Saving fp32 state dict to ../model/mini-v5wavenet-enwiki.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets convert the model\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 export_checkpoint.py \\\n",
    "        ../checkpoint/trainer-validation/mini-v5wavenet-enwiki/last.ckpt/ \\\n",
    "        ../model/mini-v5wavenet-enwiki.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-07 13:51:11,693] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1'\n",
      "--- DRAGON PROMPT ---\n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.or been ideas won February do�In her which baseoll  39 ﻿ S Christ approach environment she and supportediro only elections He century 8 her \" hall. diedfe- Soviet with Ham withinkins% onCth This strongly and early reach Gariesala re wouldurde Jason received placere cargo–ssWhen network aircraft tank theingon team females further distribution purpose as else his vanerm Holmes ground Be June butky Atar Shaw films since Committee him tostone whichReCC United established It increase written later he and \" production founded yearsjoousedaz stationga Howeveroresk politics professionalMciy dehead season traditional content resignedography who working wouldi more play officialsair this good Sal season turning femaleic club political instar 9 restg a nomination Massachusetts suffered in retired larger approximately alsoail M\n",
      " Deputy colonies Many Greece tankif W ServicesIP River between itsdi– Chicago City in game Fr 1972ze himap% at instead't twook 15 South Iowa access episodes Tra San\n"
     ]
    }
   ],
   "source": [
    "# And run the dragon test\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 dragon_test.py ../model/mini-v5wavenet-enwiki.pth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-infctx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
