{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset repacking implementation\n",
    "\n",
    "Advance dataset operations, of sorting, offset, and length support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /home/picocreator/rwkv-proj/infctx-dev/notebook/trainer-v5-validation\n",
      "TRAINER_DIR: /home/picocreator/rwkv-proj/infctx-dev/RWKV-v5\n",
      "PROJECT_DIR: /home/picocreator/rwkv-proj/infctx-dev\n"
     ]
    }
   ],
   "source": [
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"infctx-v5-dataset\"\n",
    "\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-18 04:54:01,470] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.1'\n",
      "---- Initializing model ----\n",
      "No of layers: 6\n",
      "Embedding size: 2048\n",
      "Output model path: ../model/L6-D2048-world-v5base-init.pth\n",
      "Vocab size: 65536\n",
      "Emb scale: 0.0001\n",
      "Note: this process takes a significant time (and ram) for large models\n",
      "---- ----- ----\n",
      "Model exists, skipping init_model\n"
     ]
    }
   ],
   "source": [
    "# Init the model\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 ./init_model.py \\\n",
    "        --n_layer 6 --n_embd 2048 \\\n",
    "        --vocab_size world --skip-if-exists \\\n",
    "        \"../model/L6-D2048-world-v5base-init.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without dataset packing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map (num_proc=16): 100%|█████████| 10000/10000 [00:01<00:00, 9777.04 examples/s]\n",
      "Filter (num_proc=16): 100%|█████| 10000/10000 [00:00<00:00, 12552.92 examples/s]\n",
      "Map (num_proc=16): 100%|███████████| 6474/6474 [00:00<00:00, 6823.61 examples/s]\n",
      "Map (num_proc=16): 100%|██████████████| 310/310 [00:00<00:00, 798.15 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 310/310 [00:00<00:00, 6597.03 examples/\n",
      "Saving the dataset (1/1 shards): 100%|███| 4/4 [00:00<00:00, 1833.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset \n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/config/test-dataset-repack-chunks.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-18 06:44:14,494] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.1'\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/picocreator/rwkv-proj/infctx-dev/notebook/trainer-v5-validation/config/test-dataset-repack-chunks.yaml', '--model.load_model=../model/L6-D2048-world-v5base-init.pth', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-enwiki-10k-chunked/', '--trainer.logger.init_args.name=infctx-v5-dataset-packing - Chunking 4096 - (deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.microbatch_size=2', '--trainer.devices=auto'], args=['fit', '-c', '/home/picocreator/rwkv-proj/infctx-dev/notebook/trainer-v5-validation/config/test-dataset-repack-chunks.yaml', '--model.load_model=../model/L6-D2048-world-v5base-init.pth', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-enwiki-10k-chunked/', '--trainer.logger.init_args.name=infctx-v5-dataset-packing - Chunking 4096 - (deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.microbatch_size=2', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       8\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - microbatch_size:         2\n",
      "   - accumulate_grad_batches: 4\n",
      "   - effective_batch_size:    8\n",
      "\n",
      "Map (num_proc=16): 100%|█████████| 10000/10000 [00:01<00:00, 8824.09 examples/s]\n",
      "Filter (num_proc=16): 100%|█████| 10000/10000 [00:00<00:00, 11669.38 examples/s]\n",
      "Map (num_proc=16): 100%|███████████| 6474/6474 [00:01<00:00, 6306.40 examples/s]\n",
      "Map (num_proc=16): 100%|██████████████| 310/310 [00:00<00:00, 655.81 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 310/310 [00:00<00:00, 6126.50 examples/\n",
      "Saving the dataset (1/1 shards): 100%|███| 4/4 [00:00<00:00, 1839.81 examples/s]\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20231218_064433-wzs0je77\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minfctx-v5-dataset-packing - Chunking 4096 - (deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test/runs/wzs0je77\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.05170488357543945 seconds\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449201450/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 134 M \n",
      "1 | blocks | ModuleList | 327 M \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 134 M \n",
      "--------------------------------------\n",
      "595 M     Trainable params\n",
      "0         Non-trainable params\n",
      "595 M     Total params\n",
      "2,383.086 Total estimated model params size (MB)\n",
      "Epoch 0: 100%|██| 155/155 [03:27<00:00,  0.75it/s, v_num=je77, train/loss=6.660]\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|█████               | 1/4 [00:01<00:03,  0.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|██████████          | 2/4 [00:02<00:02,  0.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|███████████████     | 3/4 [00:03<00:01,  0.90it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|████████████████████| 4/4 [00:04<00:00,  0.91it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 155/155 [03:31<00:00,  0.73it/s, v_num=je77, train/loss=6.660, `Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 155/155 [03:31<00:00,  0.73it/s, v_num=je77, train/loss=6.660, \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  batchidx ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              data_ctx_len ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/tokens_per_sec.gpu.0 ▁▄▆▆▇▇▇▇▇███████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/tokens_total.gpu.0 ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   substep ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train/loss ██▆▅▄▃▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       trainer/global_step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     trainer/learning_rate ████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  batchidx 154\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              data_ctx_len 16384.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/tokens_per_sec.gpu.0 24564.1372\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/tokens_total.gpu.0 5079040\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   substep 154\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train/loss 6.65625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       trainer/global_step 38\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     trainer/learning_rate 0.00041\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           validation/loss 6.76562\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33minfctx-v5-dataset-packing - Chunking 4096 - (deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test/runs/wzs0je77\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyNDE5OTQyNQ==/version_details/v4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231218_064433-wzs0je77/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/test-dataset-repack-chunks.yaml\" \\\n",
    "        --model.load_model=\"../model/L6-D2048-world-v5base-init.pth\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v5-enwiki-10k-chunked/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Chunking 4096 - (deepspeed_stage_1)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_1\" \\\n",
    "        --trainer.microbatch_size=2 \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With dataset packing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map (num_proc=16): 100%|█████████| 10000/10000 [00:01<00:00, 9460.14 examples/s]\n",
      "Filter (num_proc=16): 100%|█████| 10000/10000 [00:00<00:00, 12602.40 examples/s]\n",
      "Map (num_proc=16): 100%|██████████| 6409/6409 [00:00<00:00, 14092.36 examples/s]\n",
      "Map (num_proc=16): 100%|███████████| 6409/6409 [00:00<00:00, 6571.76 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 323/323 [00:00<00:00, 6750.55 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█| 65/65 [00:00<00:00, 17764.37 examples/s\n"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset \n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/config/test-dataset-repack.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-18 06:39:25,218] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.1'\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/picocreator/rwkv-proj/infctx-dev/notebook/trainer-v5-validation/config/test-dataset-repack.yaml', '--model.load_model=../model/L6-D2048-world-v5base-init.pth', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-enwiki-10k-packing/', '--trainer.logger.init_args.name=infctx-v5-dataset-packing - Packing - (deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.microbatch_size=2', '--trainer.devices=auto'], args=['fit', '-c', '/home/picocreator/rwkv-proj/infctx-dev/notebook/trainer-v5-validation/config/test-dataset-repack.yaml', '--model.load_model=../model/L6-D2048-world-v5base-init.pth', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-enwiki-10k-packing/', '--trainer.logger.init_args.name=infctx-v5-dataset-packing - Packing - (deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.microbatch_size=2', '--trainer.devices=auto'].\n",
      "Seed set to 3941088705\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "---\n",
      "[RWKV.TimeMix] Compiling CUDA kernel with HEAD_SIZE=64\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu121/wkv5/build.ninja...\n",
      "Building extension module wkv5...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv5...\n",
      "[RWKV.TimeMix] CUDA kernel compiled & loaded globally\n",
      "---\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - microbatch_size:         2\n",
      "   - accumulate_grad_batches: 8\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "Map (num_proc=16): 100%|█████████| 10000/10000 [00:01<00:00, 9173.67 examples/s]\n",
      "Filter (num_proc=16): 100%|█████| 10000/10000 [00:00<00:00, 11370.68 examples/s]\n",
      "Map (num_proc=16): 100%|██████████| 6409/6409 [00:00<00:00, 12052.27 examples/s]\n",
      "Map (num_proc=16): 100%|███████████| 6409/6409 [00:01<00:00, 6061.82 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 323/323 [00:00<00:00, 6603.79 examples/\n",
      "Saving the dataset (1/1 shards): 100%|█| 65/65 [00:00<00:00, 17760.90 examples/s\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20231218_063943-8valsg1z\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minfctx-v5-dataset-packing - Packing - (deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test/runs/8valsg1z\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.05209159851074219 seconds\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449201450/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 134 M \n",
      "1 | blocks | ModuleList | 327 M \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 134 M \n",
      "--------------------------------------\n",
      "595 M     Trainable params\n",
      "0         Non-trainable params\n",
      "595 M     Total params\n",
      "2,383.086 Total estimated model params size (MB)\n",
      "Epoch 0: 100%|██| 162/162 [03:40<00:00,  0.74it/s, v_num=sg1z, train/loss=7.340]\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/65 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/65 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                  | 1/65 [00:00<00:20,  3.12it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                  | 2/65 [00:00<00:19,  3.30it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                  | 3/65 [00:00<00:18,  3.40it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█▏                 | 4/65 [00:01<00:17,  3.46it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍                 | 5/65 [00:01<00:17,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▊                 | 6/65 [00:01<00:16,  3.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██                 | 7/65 [00:01<00:16,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██▎                | 8/65 [00:02<00:16,  3.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▋                | 9/65 [00:03<00:19,  2.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▊               | 10/65 [00:03<00:18,  3.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███               | 11/65 [00:03<00:17,  3.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▎              | 12/65 [00:03<00:17,  3.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▌              | 13/65 [00:04<00:16,  3.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▉              | 14/65 [00:04<00:16,  3.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|████▏             | 15/65 [00:04<00:15,  3.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▍             | 16/65 [00:04<00:15,  3.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▋             | 17/65 [00:05<00:14,  3.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▉             | 18/65 [00:05<00:14,  3.26it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|█████▎            | 19/65 [00:05<00:14,  3.28it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▌            | 20/65 [00:06<00:13,  3.30it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▊            | 21/65 [00:06<00:13,  3.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|██████            | 22/65 [00:06<00:12,  3.32it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|██████▎           | 23/65 [00:06<00:12,  3.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▋           | 24/65 [00:07<00:12,  3.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▉           | 25/65 [00:07<00:11,  3.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|███████▏          | 26/65 [00:07<00:11,  3.37it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▍          | 27/65 [00:07<00:11,  3.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▊          | 28/65 [00:08<00:10,  3.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|████████          | 29/65 [00:08<00:10,  3.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████████▎         | 30/65 [00:08<00:10,  3.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▌         | 31/65 [00:09<00:09,  3.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▊         | 32/65 [00:09<00:09,  3.42it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|█████████▏        | 33/65 [00:09<00:09,  3.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████████▍        | 34/65 [00:09<00:09,  3.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▋        | 35/65 [00:10<00:08,  3.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▉        | 36/65 [00:10<00:08,  3.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|██████████▏       | 37/65 [00:10<00:08,  3.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|██████████▌       | 38/65 [00:10<00:07,  3.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▊       | 39/65 [00:11<00:07,  3.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|███████████       | 40/65 [00:11<00:07,  3.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|███████████▎      | 41/65 [00:11<00:06,  3.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████▋      | 42/65 [00:12<00:06,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|███████████▉      | 43/65 [00:12<00:06,  3.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|████████████▏     | 44/65 [00:12<00:06,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|████████████▍     | 45/65 [00:12<00:05,  3.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|████████████▋     | 46/65 [00:13<00:05,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|█████████████     | 47/65 [00:13<00:05,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|█████████████▎    | 48/65 [00:13<00:04,  3.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|█████████████▌    | 49/65 [00:13<00:04,  3.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|█████████████▊    | 50/65 [00:14<00:04,  3.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|██████████████    | 51/65 [00:14<00:03,  3.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|██████████████▍   | 52/65 [00:14<00:03,  3.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|██████████████▋   | 53/65 [00:15<00:03,  3.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|██████████████▉   | 54/65 [00:15<00:03,  3.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|███████████████▏  | 55/65 [00:15<00:02,  3.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|███████████████▌  | 56/65 [00:15<00:02,  3.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|███████████████▊  | 57/65 [00:16<00:02,  3.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████████████  | 58/65 [00:16<00:01,  3.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|████████████████▎ | 59/65 [00:16<00:01,  3.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|████████████████▌ | 60/65 [00:16<00:01,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|████████████████▉ | 61/65 [00:17<00:01,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|█████████████████▏| 62/65 [00:17<00:00,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|█████████████████▍| 63/65 [00:17<00:00,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|█████████████████▋| 64/65 [00:18<00:00,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████████████| 65/65 [00:18<00:00,  3.54it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 162/162 [03:58<00:00,  0.68it/s, v_num=sg1z, train/loss=7.340, `Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 162/162 [03:58<00:00,  0.68it/s, v_num=sg1z, train/loss=7.340, \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.016 MB of 0.016 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              data_ctx_len ▆▆▆▅▆▆▆▆▆▅▆▆▆▆▆▆▆▆▆▆▁▆▆▆▆▆█▆▆▆▆▆▆▆▆██▆▆▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/tokens_per_sec.gpu.0 ▁▅▅▆▆▇▇▇▇▇▇█████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/tokens_total.gpu.0 ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train/loss ██▆▆▄▄▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       trainer/global_step ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     trainer/learning_rate ████▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  batchidx 161\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              data_ctx_len 8182.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/tokens_per_sec.gpu.0 23824.88975\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/tokens_total.gpu.0 5242477\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   substep 161\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train/loss 7.34375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       trainer/global_step 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     trainer/learning_rate 0.00042\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           validation/loss 7.39471\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33minfctx-v5-dataset-packing - Packing - (deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test/runs/8valsg1z\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyNDE5OTQyNQ==/version_details/v4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20231218_063943-8valsg1z/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/test-dataset-repack.yaml\" \\\n",
    "        --model.load_model=\"../model/L6-D2048-world-v5base-init.pth\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v5-enwiki-10k-packing/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Packing - (deepspeed_stage_1)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_1\" \\\n",
    "        --trainer.microbatch_size=2 \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-infctx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
