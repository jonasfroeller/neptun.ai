{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWKV Token Shift Experiment A\n",
    "This model is a custom model containing\n",
    "- 12 layers\n",
    "- 2560 embedding size\n",
    "\n",
    "See `./notes.md` for how the init model was initilaized.\n",
    "\n",
    "**Note:** This project assumes you have the rwkv-infctx conda env setup\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "# ninja-build is required for the new trainer\n",
    "sudo apt-get install ninja-build\n",
    "\n",
    "# Update conda & its package listings\n",
    "conda update conda\n",
    "\n",
    "# Virtual env, with python 3.10\n",
    "# python 3.11 have issues with torch.compile / h100s\n",
    "# and if you want to use 3.11, you will need to do a nightly build install\n",
    "conda create -n rwkv-infctx python=3.11 pip\n",
    "conda activate rwkv-infctx\n",
    "\n",
    "# Install pytorch (>=2.0.1)\n",
    "conda install -y pytorch==2.0.1 torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "\n",
    "# Verify your pytorch version \n",
    "python -c \"import torch; print(torch.__version__)\"\n",
    "\n",
    "# We use python -m pip, instead of pip directly, as it resolve issues with venv not loading the right pip\n",
    "python -m pip install datasets transformers \n",
    "python -m pip install lightning==2.0.4 deepspeed==0.9.5\n",
    "python -m pip install ninja numexpr jsonargparse 'jsonargparse[signatures]'\n",
    "python -m pip install lm-dataformat ftfy sentencepiece tokenizers wandb\n",
    "```\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets setup the various directories, and get the blank init model, these init model was generated\n",
    "# using the original RWKV-LM repo (as at this point of writing, this repo cannot init a model)\n",
    "# As such I have preinitialized these blank models and uploaded them to HF for convinence\n",
    "!mkdir -p ../../../../model/\n",
    "!mkdir -p ../../../../datapath/\n",
    "!mkdir -p ../../../../checkpoint/\n",
    "!cd ../../../../model/ && wget -nc https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/L12-D2560-init.pth\n",
    "!ls -alh ../../../../model/L12-D2560-init.pth\n",
    "\n",
    "# The various other stages, if you want to skip stuff\n",
    "# !cd ../../../../model/ && wget -nc https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/TokenShift-A-Stage1.pth\n",
    "# !cd ../../../../model/ && wget -nc https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/TokenShift-A-Stage2.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_1\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /home/ubuntu/rwkv5x-tokenshift-exp-A/notebook/experiment/tokenshift-exp\n",
      "INFERENCE_DIR: /home/ubuntu/rwkv5x-tokenshift-exp-A/RWKV-v5x\n",
      "TRAINER_DIR: /home/ubuntu/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet\n",
      "PROJECT_DIR: /home/ubuntu/rwkv5x-tokenshift-exp-A\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"TokenShift-Exp-A\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v4wavenet/\"))\n",
    "INFERENCE_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v4wavenet/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"INFERENCE_DIR:\", INFERENCE_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 : Foundation model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 69.36it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-1ae92a8fcbab5f66_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-67576cf8a6607f3d_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-f69b6b1c107274f3_*_of_00064.arrow\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset (enwiki_100k)\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/TokenShift-A-enwiki.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 2259997942\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 2259997942\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230715_083122-hlfv4t2c\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m(8x3090, rechunked) TokenShift-Exp-A - Enwiki Foundation (ctx=4096, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/hlfv4t2c\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_4096_bf16/build.ninja...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_4096_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -t 4 -std=c++17 -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=4096 -c /root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/cuda/wkv_cuda_bf16.cu -o wkv_cuda_bf16.cuda.o \n",
      "ptxas info    : 1 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf\n",
      "    49152 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 56 registers, 464 bytes cmem[0], 8 bytes cmem[2]\n",
      "ptxas info    : Compiling entry function '_Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, 424 bytes cmem[0]\n",
      "[2/3] c++ -MMD -MF wkv_op_bf16.o.d -DTORCH_EXTENSION_NAME=wkv_4096_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -std=c++17 -O3 -DTmax=4096 -c /root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/cuda/wkv_op_bf16.cpp -o wkv_op_bf16.o \n",
      "[3/3] c++ wkv_op_bf16.o wkv_cuda_bf16.cuda.o -shared -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_4096_bf16.so\n",
      "Loading extension module wkv_4096_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 4\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 48.21it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-1ae92a8fcbab5f66_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-67576cf8a6607f3d_*_of_00064.arrow\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-f69b6b1c107274f3_*_of_00064.arrow\n",
      "Saving the dataset (0/5 shards):   0%|         | 0/81487 [00:00<?, ? examples/s][RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Saving the dataset (0/5 shards):   5%| | 4000/81487 [00:00<00:02, 25993.90 examp[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Saving the dataset (0/5 shards):  10%| | 8000/81487 [00:00<00:02, 31705.54 examp[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Saving the dataset (1/5 shards):  24%|▏| 19298/81487 [00:00<00:02, 26019.77 exam[rank: 4] Global seed set to 2259997942\n",
      "Saving the dataset (1/5 shards):  31%|▎| 25298/81487 [00:00<00:01, 31180.89 exam[rank: 5] Global seed set to 2259997942\n",
      "[rank: 3] Global seed set to 2259997942\n",
      "[rank: 6] Global seed set to 2259997942\n",
      "[rank: 2] Global seed set to 2259997942\n",
      "Saving the dataset (1/5 shards):  38%|▍| 31298/81487 [00:00<00:01, 35246.92 exam[rank: 7] Global seed set to 2259997942\n",
      "[rank: 1] Global seed set to 2259997942\n",
      "Saving the dataset (2/5 shards):  51%|▌| 41596/81487 [00:01<00:01, 31328.61 examUsing /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_4096_bf16/build.ninja...\n",
      "Saving the dataset (2/5 shards):  58%|▌| 47596/81487 [00:01<00:00, 35038.67 examUsing /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Saving the dataset (3/5 shards):  60%|▌| 48893/81487 [00:01<00:00, 35038.67 examLoading extension module wkv_4096_bf16...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_4096_bf16/build.ninja...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Saving the dataset (3/5 shards):  66%|▋| 53893/81487 [00:01<00:00, 32729.79 examLoading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "[rank: 0] Global seed set to 2259997942                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-07-15 08:31:53,119] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 2259997942\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-07-15 08:32:01,056] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 2259997942\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-07-15 08:32:02,720] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 2259997942\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-07-15 08:32:02,931] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 2259997942\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-07-15 08:32:02,996] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 2259997942\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-07-15 08:32:03,012] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 2259997942\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-07-15 08:32:03,014] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 2259997942\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-07-15 08:32:04,479] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06544041633605957 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10133838653564453 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.1014101505279541 seconds\n",
      "Time to load fused_adam op: 0.10132884979248047 seconds\n",
      "Time to load fused_adam op: 0.10137248039245605 seconds\n",
      "Time to load fused_adam op: 0.10137677192687988 seconds\n",
      "Time to load fused_adam op: 0.10178971290588379 seconds\n",
      "Time to load fused_adam op: 0.10189652442932129 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06139564514160156 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1014094352722168 seconds\n",
      "Time to load utils op: 0.10206317901611328 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10182809829711914 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1013946533203125 seconds\n",
      "Time to load utils op: 0.10096049308776855 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10195422172546387 seconds\n",
      "Time to load utils op: 0.10180377960205078 seconds\n",
      "Rank: 4 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 3 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 7 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 1 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 2 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 5 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 6 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 0 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00027179718017578125 seconds\n",
      "Time to load utils op: 0.00027632713317871094 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002593994140625 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002627372741699219 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.000263214111328125 seconds\n",
      "Time to load utils op: 0.00035309791564941406 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002994537353515625 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0004830360412597656 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 128 M \n",
      "1 | blocks | ModuleList | 1.0 B \n",
      "2 | ln_out | LayerNorm  | 5.1 K \n",
      "3 | head   | Linear     | 128 M \n",
      "--------------------------------------\n",
      "1.3 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 B     Total params\n",
      "5,120.512 Total estimated model params size (MB)\n",
      "Epoch 0:  10%| | 1000/10186 [28:03<4:17:45,  1.68s/it, v_num=4t2c, train/loss=5./usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 10186/10186 [4:48:21<00:00,  1.70s/it, v_num=4t2c, train/loss=3\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/52 [00:00<?, ?it/s]\u001b[ATraceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "Traceback (most recent call last):\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 69, in <module>\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 62, in cli_main\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▇▆▅▄▅▄▄▄▄▃▃▂▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▁▁▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 10185\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 4095\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 81480\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 3.82812\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2546\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m(8x3090, rechunked) TokenShift-Exp-A - Enwiki Foundation (ctx=4096, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/hlfv4t2c\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230715_083122-hlfv4t2c/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the foundation model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/TokenShift-A-enwiki.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Enwiki Foundation (ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/TokenShift-A-enwiki/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 222 params 1280128000 elements\n",
      "Saving fp32 state dict to ../model/TokenShift-A-Stage1.pth\n",
      "-rw-r--r-- 1 root root 4.8G Jul 15 13:42 ../model/TokenShift-A-Stage1.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/TokenShift-A-enwiki/last.ckpt\" \"../model/TokenShift-A-Stage1.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/TokenShift-A-Stage1.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RWKV_HEAD_QK_DIM 0 RWKV_JIT_ON 1\n",
      "\n",
      "blocks.0.att.key.weight                  float32    cuda:0\n",
      "blocks.0.att.output.weight               float32    cuda:0\n",
      "blocks.0.att.receptance.weight           float32    cuda:0\n",
      "blocks.0.att.time_mix_k                  float32    cuda:0\n",
      "blocks.0.att.time_mix_r                  float32    cuda:0\n",
      "blocks.0.att.time_mix_v                  float32    cuda:0\n",
      "blocks.0.att.value.weight                float32    cuda:0\n",
      "blocks.0.ffn.key.weight                  float32    cuda:0\n",
      "blocks.0.ffn.receptance.weight           float32    cuda:0\n",
      "blocks.0.ffn.time_mix_k                  float32    cuda:0\n",
      "blocks.0.ffn.time_mix_r                  float32    cuda:0\n",
      "blocks.0.ffn.value.weight                float32    cuda:0\n",
      "blocks.0.ln0.bias                        float32    cuda:0\n",
      "blocks.0.ln0.weight                      float32    cuda:0\n",
      "blocks.0.ln1.bias                        float32    cuda:0\n",
      "blocks.0.ln1.weight                      float32    cuda:0\n",
      "blocks.0.ln2.bias                        float32    cuda:0\n",
      "blocks.0.ln2.weight                      float32    cuda:0\n",
      "................................................................................................................................................................................\n",
      "emb.weight                               float32    cpu\n",
      "head.weight                              float32    cuda:0\n",
      "ln_out.bias                              float32    cuda:0\n",
      "ln_out.weight                            float32    cuda:0\n",
      "blocks.0.att.time_decay                  float32    cuda:0\n",
      "...........\n",
      "blocks.0.att.time_first                  float32    cuda:0\n",
      "...........Prompt: \n",
      "\n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.\n",
      "\n",
      "However, the concept that alien races of humanoid, humanoid aliens and dragons are not emphasized.\n",
      "\n",
      "In comparison with other ancient Siberian cities in existence, the digging of caves appears to be very similar to China's other colonies of cities in the area:\n",
      "\n",
      "Qatar Emperor erected a defensive fortress near the Shenduin highlands, a massive reef of stone, with a mud vault inside the outer wall, surrounded by wooden gates. But all of this important expedition was secured by accepting only the skills of various people, the local society. The museum was carefully built, but the directionality of this landscape is difficult to be proven.\n",
      "\n",
      "\"We have never imagined this country and never saw a new heavenly temple with a large number of people. The first step is undoubtedly how evil we have brought to the world by a spiritually joyful, caring, pleasurable, and occasionally outwardly skilled, when what has been great.\n",
      "\n",
      "The iron and gold ritual “"
     ]
    }
   ],
   "source": [
    "# # Lets do a quick dragon prompt validation\n",
    "!cd \"{INFERENCE_DIR}\" && python3 dragon_test.py ../model/TokenShift-A-Stage1.pth \"cuda fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RWKV_HEAD_QK_DIM 0 RWKV_JIT_ON 1\n",
      "\n",
      "blocks.0.att.key.weight                  float32    cuda:0\n",
      "blocks.0.att.output.weight               float32    cuda:0\n",
      "blocks.0.att.receptance.weight           float32    cuda:0\n",
      "blocks.0.att.time_mix_k                  float32    cuda:0\n",
      "blocks.0.att.time_mix_r                  float32    cuda:0\n",
      "blocks.0.att.time_mix_v                  float32    cuda:0\n",
      "blocks.0.att.value.weight                float32    cuda:0\n",
      "blocks.0.ffn.key.weight                  float32    cuda:0\n",
      "blocks.0.ffn.receptance.weight           float32    cuda:0\n",
      "blocks.0.ffn.time_mix_k                  float32    cuda:0\n",
      "blocks.0.ffn.time_mix_r                  float32    cuda:0\n",
      "blocks.0.ffn.value.weight                float32    cuda:0\n",
      "blocks.0.ln0.bias                        float32    cuda:0\n",
      "blocks.0.ln0.weight                      float32    cuda:0\n",
      "blocks.0.ln1.bias                        float32    cuda:0\n",
      "blocks.0.ln1.weight                      float32    cuda:0\n",
      "blocks.0.ln2.bias                        float32    cuda:0\n",
      "blocks.0.ln2.weight                      float32    cuda:0\n",
      "................................................................................................................................................................................\n",
      "emb.weight                               float32    cpu\n",
      "head.weight                              float32    cuda:0\n",
      "ln_out.bias                              float32    cuda:0\n",
      "ln_out.weight                            float32    cuda:0\n",
      "blocks.0.att.time_decay                  float32    cuda:0\n",
      "...........\n",
      "blocks.0.att.time_first                  float32    cuda:0\n",
      "...........###\n",
      "### Model validation start ###\n",
      "###\n",
      "Model validation at 5 tokens : 20.0% similarity, with 1 matched token, and 4 token mismatch\n",
      "Model validation at 10 tokens : 10.0% similarity, with 1 matched token, and 9 token mismatch\n",
      "Model validation at 15 tokens : 6.666666666666667% similarity, with 1 matched token, and 14 token mismatch\n",
      "Model validation at 20 tokens : 5.0% similarity, with 1 matched token, and 19 token mismatch\n",
      "Model validation at 25 tokens : 4.0% similarity, with 1 matched token, and 24 token mismatch\n",
      "Model validation at 30 tokens : 3.3333333333333335% similarity, with 1 matched token, and 29 token mismatch\n",
      "Model validation at 35 tokens : 2.857142857142857% similarity, with 1 matched token, and 34 token mismatch\n",
      "Model validation at 40 tokens : 2.5% similarity, with 1 matched token, and 39 token mismatch\n",
      "Model validation at 45 tokens : 0.0% similarity, with 0 matched token, and 45 token mismatch\n",
      "Model validation at 50 tokens : 0.0% similarity, with 0 matched token, and 50 token mismatch\n",
      "Model validation at 55 tokens : 0.0% similarity, with 0 matched token, and 55 token mismatch\n",
      "Model validation at 60 tokens : 0.0% similarity, with 0 matched token, and 60 token mismatch\n",
      "Model validation at 65 tokens : 0.0% similarity, with 0 matched token, and 65 token mismatch\n",
      "Model validation at 70 tokens : 0.0% similarity, with 0 matched token, and 70 token mismatch\n",
      "Model validation at 75 tokens : 0.0% similarity, with 0 matched token, and 75 token mismatch\n",
      "Model validation at 80 tokens : 0.0% similarity, with 0 matched token, and 80 token mismatch\n",
      "Model validation at 85 tokens : 0.0% similarity, with 0 matched token, and 85 token mismatch\n",
      "Model validation at 90 tokens : 1.1111111111111112% similarity, with 1 matched token, and 89 token mismatch\n",
      "Model validation at 95 tokens : 1.0526315789473684% similarity, with 1 matched token, and 94 token mismatch\n",
      "Model validation at 100 tokens : 1.0% similarity, with 1 matched token, and 99 token mismatch\n",
      "Model validation at 110 tokens : 0.9090909090909091% similarity, with 1 matched token, and 109 token mismatch\n",
      "Model validation at 120 tokens : 0.8333333333333334% similarity, with 1 matched token, and 119 token mismatch\n",
      "Model validation at 130 tokens : 0.7692307692307693% similarity, with 1 matched token, and 129 token mismatch\n",
      "Model validation at 140 tokens : 0.7142857142857143% similarity, with 1 matched token, and 139 token mismatch\n",
      "Model validation at 150 tokens : 0.6666666666666667% similarity, with 1 matched token, and 149 token mismatch\n",
      "Model validation at 175 tokens : 0.5714285714285714% similarity, with 1 matched token, and 174 token mismatch\n",
      "Model validation at 200 tokens : 1.0% similarity, with 2 matched token, and 198 token mismatch\n",
      "Model validation at 225 tokens : 0.8888888888888888% similarity, with 2 matched token, and 223 token mismatch\n",
      "Model validation at 250 tokens : 0.8% similarity, with 2 matched token, and 248 token mismatch\n",
      "Model validation at 275 tokens : 0.7272727272727273% similarity, with 2 matched token, and 273 token mismatch\n",
      "Model validation at 300 tokens : 1.0% similarity, with 3 matched token, and 297 token mismatch\n",
      "Model validation at 325 tokens : 0.9230769230769231% similarity, with 3 matched token, and 322 token mismatch\n",
      "Model validation at 350 tokens : 0.8571428571428572% similarity, with 3 matched token, and 347 token mismatch\n",
      "Model validation at 375 tokens : 1.0666666666666667% similarity, with 4 matched token, and 371 token mismatch\n",
      "Model validation at 400 tokens : 1.0% similarity, with 4 matched token, and 396 token mismatch\n",
      "Model validation at 425 tokens : 0.9411764705882352% similarity, with 4 matched token, and 421 token mismatch\n",
      "Model validation at 450 tokens : 0.8888888888888888% similarity, with 4 matched token, and 446 token mismatch\n",
      "Model validation at 475 tokens : 0.8421052631578947% similarity, with 4 matched token, and 471 token mismatch\n",
      "Model validation at 500 tokens : 1.0% similarity, with 5 matched token, and 495 token mismatch\n",
      "Model validation at 550 tokens : 1.090909090909091% similarity, with 6 matched token, and 544 token mismatch\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/rwkv5x-tokenshift-exp-A/notebook/experiment/tokenshift-exp/../memory_script/eval_memory_guided.py\", line 164, in <module>\n",
      "    validate_model(600)\n",
      "  File \"/home/ubuntu/rwkv5x-tokenshift-exp-A/notebook/experiment/tokenshift-exp/../memory_script/eval_memory_guided.py\", line 77, in validate_model\n",
      "    logits, state = model.forward(target_tokens, state)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/rwkv5x-tokenshift-exp-A/RWKV-v5x/src/SimpleRWKV.py\", line 94, in forward\n",
      "    logits, state = self.model([tokens[i]], state)    \n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1505, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1514, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/rwkv5x-tokenshift-exp-A/RWKV-v5x/src/model_run.py\", line 218, in forward\n",
      "    x = x.cuda()\n",
      "        ^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick memory test\n",
    "# (We dun expect this to work, as we have not finetune for memory recall, but its a baseline)\n",
    "!python3 ../memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/TokenShift-A-Stage1.pth\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2 : Instruct Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 370.62it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-f7f96ced2d76e1c7_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-3b2912dc14935a9a_*_of_00064.arrow\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/TokenShift-A-instruct.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 1727192453\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 1727192453\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230715_135325-9wh9tja6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m(8x3090, rechunked) TokenShift-Exp-A - Instruct (train-ctx=4096, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/9wh9tja6\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_4096_bf16/build.ninja...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_4096_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 4\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 540.64it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-f7f96ced2d76e1c7_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-3b2912dc14935a9a_*_of_00064.arrow\n",
      "[rank: 0] Global seed set to 1727192453                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-07-15 13:53:38,276] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 5] Global seed set to 1727192453\n",
      "[rank: 7] Global seed set to 1727192453\n",
      "[rank: 6] Global seed set to 1727192453\n",
      "[rank: 3] Global seed set to 1727192453\n",
      "[rank: 2] Global seed set to 1727192453\n",
      "[rank: 4] Global seed set to 1727192453\n",
      "[rank: 1] Global seed set to 1727192453\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_4096_bf16/build.ninja...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_4096_bf16/build.ninja...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "[rank: 5] Global seed set to 1727192453\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-07-15 13:53:50,500] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 1727192453\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-07-15 13:53:50,968] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 1727192453\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-07-15 13:53:52,648] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 1727192453\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-07-15 13:53:52,727] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 1727192453\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-07-15 13:53:52,766] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 1727192453\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-07-15 13:53:52,790] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 1727192453\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-07-15 13:53:52,792] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  4.000e-04 (0.0004)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06380105018615723 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10139942169189453 seconds\n",
      "Time to load fused_adam op: 0.10140776634216309 seconds\n",
      "Time to load fused_adam op: 0.10136818885803223 seconds\n",
      "Time to load fused_adam op: 0.10129642486572266 seconds\n",
      "Time to load fused_adam op: 0.10146975517272949 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Time to load fused_adam op: 0.10167646408081055 seconds\n",
      "Time to load fused_adam op: 0.1017613410949707 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06905102729797363 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10162639617919922 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10091280937194824 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10126614570617676 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10176563262939453 seconds\n",
      "Time to load utils op: 0.1019132137298584 seconds\n",
      "Time to load utils op: 0.1018226146697998 seconds\n",
      "Time to load utils op: 0.10182714462280273 seconds\n",
      "Rank: 0 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 7 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 1 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 5 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 3 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 4 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 6 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Rank: 2 partition count [8, 8, 8] and sizes[(160008320, False), (3840, False), (3840, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00026798248291015625 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002655982971191406 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002579689025878906 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00033855438232421875 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00026106834411621094 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00023508071899414062 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00029754638671875 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005009174346923828 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 128 M \n",
      "1 | blocks | ModuleList | 1.0 B \n",
      "2 | ln_out | LayerNorm  | 5.1 K \n",
      "3 | head   | Linear     | 128 M \n",
      "--------------------------------------\n",
      "1.3 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 B     Total params\n",
      "5,120.512 Total estimated model params size (MB)\n",
      "Epoch 0:  54%|▌| 1000/1867 [17:35<15:14,  1.06s/it, v_num=tja6, train/loss=2.980/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 1867/1867 [32:36<00:00,  1.05s/it, v_num=tja6, train/loss=2.200\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/10 [00:00<?, ?it/s]\u001b[ATraceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/lightning_trainer.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 935, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 978, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 134, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 248, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 115, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 375, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 288, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/deepspeed.py\", line 906, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/overrides/base.py\", line 102, in forward\n",
      "    return self._forward_module.validation_step(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1101, in validation_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, False)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/rwkv5x-tokenshift-exp-A/RWKV-v4wavenet/src/model.py\", line 1058, in compute_loss\n",
      "    states.shift_states.att_shift_states,\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'BlockStateList' object has no attribute 'shift_states'. Did you mean: 'att_shift_states'?\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▂▁▁▂▃▂▅▁▁▂▂▁▁▁▁▁▄▁▁▂▁▁▁█▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▄▆▇▅▇▇▄▃▃▅▄▅▅▆▄▅▆▅▆▅▅▆▂▆▇▃▆▄▅▆▃▅▅▂▆▅▅▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 1866\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 14928\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 2.20312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 466\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m(8x3090, rechunked) TokenShift-Exp-A - Instruct (train-ctx=4096, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/9wh9tja6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230715_135325-9wh9tja6/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the instruct finetuning\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/TokenShift-A-instruct.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Instruct (train-ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/TokenShift-A-instruct/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 222 params 1280128000 elements\n",
      "Saving fp32 state dict to ../model/TokenShift-A-Stage2.pth\n",
      "-rw-r--r-- 1 root root 4.8G Jul 15 14:32 ../model/TokenShift-A-Stage2.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/TokenShift-A-instruct/last.ckpt\" \"../model/TokenShift-A-Stage2.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/TokenShift-A-Stage2.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RWKV_HEAD_QK_DIM 0 RWKV_JIT_ON 1\n",
      "\n",
      "blocks.0.att.key.weight                  float32    cuda:0\n",
      "blocks.0.att.output.weight               float32    cuda:0\n",
      "blocks.0.att.receptance.weight           float32    cuda:0\n",
      "blocks.0.att.time_mix_k                  float32    cuda:0\n",
      "blocks.0.att.time_mix_r                  float32    cuda:0\n",
      "blocks.0.att.time_mix_v                  float32    cuda:0\n",
      "blocks.0.att.value.weight                float32    cuda:0\n",
      "blocks.0.ffn.key.weight                  float32    cuda:0\n",
      "blocks.0.ffn.receptance.weight           float32    cuda:0\n",
      "blocks.0.ffn.time_mix_k                  float32    cuda:0\n",
      "blocks.0.ffn.time_mix_r                  float32    cuda:0\n",
      "blocks.0.ffn.value.weight                float32    cuda:0\n",
      "blocks.0.ln0.bias                        float32    cuda:0\n",
      "blocks.0.ln0.weight                      float32    cuda:0\n",
      "blocks.0.ln1.bias                        float32    cuda:0\n",
      "blocks.0.ln1.weight                      float32    cuda:0\n",
      "blocks.0.ln2.bias                        float32    cuda:0\n",
      "blocks.0.ln2.weight                      float32    cuda:0\n",
      "................................................................................................................................................................................\n",
      "emb.weight                               float32    cpu\n",
      "head.weight                              float32    cuda:0\n",
      "ln_out.bias                              float32    cuda:0\n",
      "ln_out.weight                            float32    cuda:0\n",
      "blocks.0.att.time_decay                  float32    cuda:0\n",
      "...........\n",
      "blocks.0.att.time_first                  float32    cuda:0\n",
      "...........Prompt: \n",
      "\n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese. The different answers were more difficult to read, and were designed to protect the Chief of Doors in China. The Bear was in fact a responsible for establishing the ver necessary architecture of Doang's territory in China. In one of these secret compartments, the Count of Strangers was built in the secret program for an immense purpose and peacefully sealed through a secret underground network of emu forces. The dragons were either ready to use any of these weapons, or were cut into pieces. \n",
      "\n",
      "\"Ultimately, this account is wise enough to sustain the Chinese rulers for themselves.\"  Many scientists believe the necessary tools to protect the science of interstellar space programs; however, much of this evidence is still applicable to all systems of science in the United States. While the exact nature of the criminal actions have been scientifically determined, there is still debate over the course of several decades.# Answer:\\nThe answer is likely to be given to the Russian Natural History Foundation, which is accepted by the Department of"
     ]
    }
   ],
   "source": [
    "# Lets do a quick dragon prompt validation\n",
    "!cd \"{INFERENCE_DIR}\" && python3 dragon_test.py \"../model/TokenShift-A-Stage2.pth\" \"cuda fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RWKV_HEAD_QK_DIM 0 RWKV_JIT_ON 1\n",
      "\n",
      "blocks.0.att.key.weight                  float32    cuda:0\n",
      "blocks.0.att.output.weight               float32    cuda:0\n",
      "blocks.0.att.receptance.weight           float32    cuda:0\n",
      "blocks.0.att.time_mix_k                  float32    cuda:0\n",
      "blocks.0.att.time_mix_r                  float32    cuda:0\n",
      "blocks.0.att.time_mix_v                  float32    cuda:0\n",
      "blocks.0.att.value.weight                float32    cuda:0\n",
      "blocks.0.ffn.key.weight                  float32    cuda:0\n",
      "blocks.0.ffn.receptance.weight           float32    cuda:0\n",
      "blocks.0.ffn.time_mix_k                  float32    cuda:0\n",
      "blocks.0.ffn.time_mix_r                  float32    cuda:0\n",
      "blocks.0.ffn.value.weight                float32    cuda:0\n",
      "blocks.0.ln0.bias                        float32    cuda:0\n",
      "blocks.0.ln0.weight                      float32    cuda:0\n",
      "blocks.0.ln1.bias                        float32    cuda:0\n",
      "blocks.0.ln1.weight                      float32    cuda:0\n",
      "blocks.0.ln2.bias                        float32    cuda:0\n",
      "blocks.0.ln2.weight                      float32    cuda:0\n",
      "................................................................................................................................................................................\n",
      "emb.weight                               float32    cpu\n",
      "head.weight                              float32    cuda:0\n",
      "ln_out.bias                              float32    cuda:0\n",
      "ln_out.weight                            float32    cuda:0\n",
      "blocks.0.att.time_decay                  float32    cuda:0\n",
      "...........\n",
      "blocks.0.att.time_first                  float32    cuda:0\n",
      "...........###\n",
      "### Model validation start ###\n",
      "###\n",
      "Model validation at 5 tokens : 20.0% similarity, with 1 matched token, and 4 token mismatch\n",
      "Model validation at 10 tokens : 10.0% similarity, with 1 matched token, and 9 token mismatch\n",
      "Model validation at 15 tokens : 6.666666666666667% similarity, with 1 matched token, and 14 token mismatch\n",
      "Model validation at 20 tokens : 10.0% similarity, with 2 matched token, and 18 token mismatch\n",
      "Model validation at 25 tokens : 8.0% similarity, with 2 matched token, and 23 token mismatch\n",
      "Model validation at 30 tokens : 6.666666666666667% similarity, with 2 matched token, and 28 token mismatch\n",
      "Model validation at 35 tokens : 5.714285714285714% similarity, with 2 matched token, and 33 token mismatch\n",
      "Model validation at 40 tokens : 5.0% similarity, with 2 matched token, and 38 token mismatch\n",
      "Model validation at 45 tokens : 4.444444444444445% similarity, with 2 matched token, and 43 token mismatch\n",
      "Model validation at 50 tokens : 4.0% similarity, with 2 matched token, and 48 token mismatch\n",
      "Model validation at 55 tokens : 3.6363636363636362% similarity, with 2 matched token, and 53 token mismatch\n",
      "Model validation at 60 tokens : 3.3333333333333335% similarity, with 2 matched token, and 58 token mismatch\n",
      "Model validation at 65 tokens : 3.076923076923077% similarity, with 2 matched token, and 63 token mismatch\n",
      "Model validation at 70 tokens : 2.857142857142857% similarity, with 2 matched token, and 68 token mismatch\n",
      "Model validation at 75 tokens : 2.666666666666667% similarity, with 2 matched token, and 73 token mismatch\n",
      "Model validation at 80 tokens : 2.5% similarity, with 2 matched token, and 78 token mismatch\n",
      "Model validation at 85 tokens : 2.3529411764705883% similarity, with 2 matched token, and 83 token mismatch\n",
      "Model validation at 90 tokens : 3.3333333333333335% similarity, with 3 matched token, and 87 token mismatch\n",
      "Model validation at 95 tokens : 3.1578947368421053% similarity, with 3 matched token, and 92 token mismatch\n",
      "Model validation at 100 tokens : 3.0% similarity, with 3 matched token, and 97 token mismatch\n",
      "Model validation at 110 tokens : 2.727272727272727% similarity, with 3 matched token, and 107 token mismatch\n",
      "Model validation at 120 tokens : 2.5% similarity, with 3 matched token, and 117 token mismatch\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/rwkv5x-tokenshift-exp-A/notebook/experiment/tokenshift-exp/../memory_script/eval_memory_guided.py\", line 146, in <module>\n",
      "    validate_model(130)\n",
      "  File \"/home/ubuntu/rwkv5x-tokenshift-exp-A/notebook/experiment/tokenshift-exp/../memory_script/eval_memory_guided.py\", line 77, in validate_model\n",
      "    logits, state = model.forward(target_tokens, state)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/rwkv5x-tokenshift-exp-A/RWKV-v5x/src/SimpleRWKV.py\", line 94, in forward\n",
      "    logits, state = self.model([tokens[i]], state)    \n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1505, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1514, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/rwkv5x-tokenshift-exp-A/RWKV-v5x/src/model_run.py\", line 218, in forward\n",
      "    x = x.cuda()\n",
      "        ^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick memory test\n",
    "# (We dun expect this to work, as we have not finetune for memory recall, but its a baseline)\n",
    "!python3 ../memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/TokenShift-A-Stage2.pth\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
