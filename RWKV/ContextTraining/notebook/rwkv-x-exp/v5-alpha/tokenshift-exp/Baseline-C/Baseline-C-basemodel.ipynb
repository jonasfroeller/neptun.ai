{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWKV Baseline C\n",
    "This model is RWKV standard 1B5 model\n",
    "\n",
    "- 24 layers\n",
    "- 2048 embedding size\n",
    "\n",
    "Going through the same memory training process as TokenShift\n",
    "\n",
    "See `./notes.md` for how the init model was initilaized.\n",
    "\n",
    "**Note:** This project assumes you have the rwkv-infctx conda env setup\n",
    "\n",
    "---\n",
    "\n",
    "```bash\n",
    "# ninja-build is required for the new trainer\n",
    "sudo apt-get install ninja-build\n",
    "\n",
    "# Update conda & its package listings\n",
    "# Conda install script can be found here : \n",
    "# https://docs.anaconda.com/free/anaconda/install/linux/#installation\n",
    "conda update conda\n",
    "\n",
    "# Virtual env, with python 3.10\n",
    "# python 3.11 have issues with torch.compile / h100s\n",
    "# and if you want to use 3.11, you will need to do a nightly build install\n",
    "conda create -n rwkv-infctx python=3.11 pip\n",
    "conda activate rwkv-infctx\n",
    "\n",
    "# Install pytorch (>=2.0.1)\n",
    "conda install -y pytorch==2.0.1 torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "\n",
    "# Verify your pytorch version \n",
    "python -c \"import torch; print(torch.__version__)\"\n",
    "\n",
    "# We use python -m pip, instead of pip directly, as it resolve issues with venv not loading the right pip\n",
    "python -m pip install datasets transformers \n",
    "python -m pip install lightning==2.0.4 deepspeed==0.9.5\n",
    "python -m pip install ninja numexpr jsonargparse 'jsonargparse[signatures]'\n",
    "python -m pip install lm-dataformat ftfy sentencepiece tokenizers wandb\n",
    "```\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-29 04:40:28--  https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/Echo-A-1B5-Init.pth\n",
      "Resolving huggingface.co (huggingface.co)... 143.204.55.124, 143.204.55.75, 143.204.55.121, ...\n",
      "Connecting to huggingface.co (huggingface.co)|143.204.55.124|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/cb/ef/cbef09abb2634a3375b28868bffa285226dfeabedec89b28c2fb302221164d66/0ec7214ed16737a6348254e6f96d8cdc04d3b5efbd5f53fe9337607ea42b5b9f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Echo-A-1B5-Init.pth%3B+filename%3D%22Echo-A-1B5-Init.pth%22%3B&Expires=1690864829&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MDg2NDgyOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jYi9lZi9jYmVmMDlhYmIyNjM0YTMzNzViMjg4NjhiZmZhMjg1MjI2ZGZlYWJlZGVjODliMjhjMmZiMzAyMjIxMTY0ZDY2LzBlYzcyMTRlZDE2NzM3YTYzNDgyNTRlNmY5NmQ4Y2RjMDRkM2I1ZWZiZDVmNTNmZTkzMzc2MDdlYTQyYjViOWY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=kKYnvb9pBGclwMIAl%7EsmbMbKc7Nx0ntqLxGYsfOiiypBc%7EiDgKKBlIxiYpRKAg7Z2yPjLosQQp7aeNEKR4RL3mNYah10jPxFyKXnGI03LVBtDHK%7Ely7aTDrF42uK4o4a6Ckragytagj048nEOAeTG%7EGrz55CM7NDsak3gaC8Ayf3RsdQVfmPBI9uhl-kv48qQTnAj%7EIC2GS7BmDnLXWKTDbNmUaipIMoHToF5m4rjSy-B%7EeoHzfDXNyPqrSXi6fwbiz9Rr4NGXTnBV6CKR01Ued06fqnO86R7uAlcuSrATGHU3YSU1%7EXX6liQVPLC1czeOf7OYqqSJk%7E%7EpySDvyTZw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2023-07-29 04:40:29--  https://cdn-lfs.huggingface.co/repos/cb/ef/cbef09abb2634a3375b28868bffa285226dfeabedec89b28c2fb302221164d66/0ec7214ed16737a6348254e6f96d8cdc04d3b5efbd5f53fe9337607ea42b5b9f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Echo-A-1B5-Init.pth%3B+filename%3D%22Echo-A-1B5-Init.pth%22%3B&Expires=1690864829&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MDg2NDgyOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jYi9lZi9jYmVmMDlhYmIyNjM0YTMzNzViMjg4NjhiZmZhMjg1MjI2ZGZlYWJlZGVjODliMjhjMmZiMzAyMjIxMTY0ZDY2LzBlYzcyMTRlZDE2NzM3YTYzNDgyNTRlNmY5NmQ4Y2RjMDRkM2I1ZWZiZDVmNTNmZTkzMzc2MDdlYTQyYjViOWY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=kKYnvb9pBGclwMIAl%7EsmbMbKc7Nx0ntqLxGYsfOiiypBc%7EiDgKKBlIxiYpRKAg7Z2yPjLosQQp7aeNEKR4RL3mNYah10jPxFyKXnGI03LVBtDHK%7Ely7aTDrF42uK4o4a6Ckragytagj048nEOAeTG%7EGrz55CM7NDsak3gaC8Ayf3RsdQVfmPBI9uhl-kv48qQTnAj%7EIC2GS7BmDnLXWKTDbNmUaipIMoHToF5m4rjSy-B%7EeoHzfDXNyPqrSXi6fwbiz9Rr4NGXTnBV6CKR01Ued06fqnO86R7uAlcuSrATGHU3YSU1%7EXX6liQVPLC1czeOf7OYqqSJk%7E%7EpySDvyTZw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 143.204.55.74, 143.204.55.52, 143.204.55.25, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|143.204.55.74|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3030345861 (2.8G) [binary/octet-stream]\n",
      "Saving to: ‘Echo-A-1B5-Init.pth’\n",
      "\n",
      "Echo-A-1B5-Init.pth 100%[===================>]   2.82G  39.8MB/s    in 74s     \n",
      "\n",
      "2023-07-29 04:41:43 (39.3 MB/s) - ‘Echo-A-1B5-Init.pth’ saved [3030345861/3030345861]\n",
      "\n",
      "ls: cannot access '../../../../model/L24-D1024-init.pth': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# First lets setup the various directories, and get the blank init model, these init model was generated\n",
    "# using the original RWKV-LM repo (as at this point of writing, this repo cannot init a model)\n",
    "# As such I have preinitialized these blank models and uploaded them to HF for convinence\n",
    "!mkdir -p ../../../../model/\n",
    "!mkdir -p ../../../../datapath/\n",
    "!mkdir -p ../../../../checkpoint/\n",
    "!rm -rf ../../../../model/L24-D1024-init.pth\n",
    "!cd ../../../../model/ && wget -nc https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/Echo-A-1B5-Init.pth\n",
    "!ls -alh ../../../../model/L24-D1024-init.pth\n",
    "\n",
    "# The various other stages, if you want to skip stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_1\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /root/picocreator-memory-experiment/notebook/experiment/tokenshift-exp/Baseline-C\n",
      "INFERENCE_DIR: /root/picocreator-memory-experiment/RWKV-v4neo\n",
      "TRAINER_DIR: /root/picocreator-memory-experiment/RWKV-v4neo\n",
      "PROJECT_DIR: /root/picocreator-memory-experiment\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"Baseline-C\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v4neo/\"))\n",
    "INFERENCE_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v4neo/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"INFERENCE_DIR:\", INFERENCE_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 : Foundation model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████████████████| 433/433 [00:00<00:00, 3.25MB/s]\n",
      "Downloading and preparing dataset None/None to /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n",
      "Downloading data files:   0%|                             | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                              | 0.00/261M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   0%|                      | 52.2k/261M [00:00<15:06, 288kB/s]\u001b[A\n",
      "Downloading data:   0%|                      | 288k/261M [00:00<03:47, 1.14MB/s]\u001b[A\n",
      "Downloading data:   0%|                      | 662k/261M [00:00<02:32, 1.71MB/s]\u001b[A\n",
      "Downloading data:   1%|▏                    | 1.88M/261M [00:00<00:53, 4.87MB/s]\u001b[A\n",
      "Downloading data:   1%|▎                    | 3.63M/261M [00:00<00:29, 8.60MB/s]\u001b[A\n",
      "Downloading data:   3%|▌                    | 7.21M/261M [00:00<00:15, 16.7MB/s]\u001b[A\n",
      "Downloading data:   5%|▉                    | 12.1M/261M [00:00<00:09, 26.0MB/s]\u001b[A\n",
      "Downloading data:   7%|█▍                   | 17.1M/261M [00:00<00:07, 33.0MB/s]\u001b[A\n",
      "Downloading data:   8%|█▋                   | 21.5M/261M [00:01<00:07, 32.4MB/s]\u001b[A\n",
      "Downloading data:  10%|██                   | 25.9M/261M [00:01<00:06, 35.4MB/s]\u001b[A\n",
      "Downloading data:  12%|██▍                  | 30.3M/261M [00:01<00:06, 37.8MB/s]\u001b[A\n",
      "Downloading data:  14%|██▊                  | 35.2M/261M [00:01<00:05, 41.2MB/s]\u001b[A\n",
      "Downloading data:  15%|███▏                 | 39.9M/261M [00:01<00:05, 42.9MB/s]\u001b[A\n",
      "Downloading data:  17%|███▌                 | 44.5M/261M [00:01<00:04, 43.8MB/s]\u001b[A\n",
      "Downloading data:  19%|███▉                 | 49.1M/261M [00:01<00:04, 44.4MB/s]\u001b[A\n",
      "Downloading data:  21%|████▎                | 53.6M/261M [00:01<00:05, 38.6MB/s]\u001b[A\n",
      "Downloading data:  22%|████▋                | 58.1M/261M [00:01<00:05, 40.4MB/s]\u001b[A\n",
      "Downloading data:  24%|█████                | 62.6M/261M [00:02<00:04, 41.7MB/s]\u001b[A\n",
      "Downloading data:  26%|█████▍               | 67.8M/261M [00:02<00:04, 44.4MB/s]\u001b[A\n",
      "Downloading data:  28%|█████▊               | 72.3M/261M [00:02<00:04, 42.2MB/s]\u001b[A\n",
      "Downloading data:  30%|██████▏              | 77.0M/261M [00:02<00:04, 43.7MB/s]\u001b[A\n",
      "Downloading data:  31%|██████▌              | 81.5M/261M [00:02<00:04, 40.7MB/s]\u001b[A\n",
      "Downloading data:  33%|██████▉              | 86.0M/261M [00:02<00:04, 42.1MB/s]\u001b[A\n",
      "Downloading data:  35%|███████▎             | 90.5M/261M [00:02<00:03, 42.9MB/s]\u001b[A\n",
      "Downloading data:  37%|███████▋             | 95.3M/261M [00:02<00:03, 44.3MB/s]\u001b[A\n",
      "Downloading data:  38%|████████▍             | 100M/261M [00:02<00:03, 45.5MB/s]\u001b[A\n",
      "Downloading data:  40%|████████▊             | 105M/261M [00:03<00:03, 39.1MB/s]\u001b[A\n",
      "Downloading data:  42%|█████████▏            | 109M/261M [00:03<00:03, 41.0MB/s]\u001b[A\n",
      "Downloading data:  44%|█████████▌            | 114M/261M [00:03<00:03, 42.4MB/s]\u001b[A\n",
      "Downloading data:  46%|██████████            | 119M/261M [00:03<00:03, 43.5MB/s]\u001b[A\n",
      "Downloading data:  47%|██████████▍           | 123M/261M [00:03<00:03, 44.2MB/s]\u001b[A\n",
      "Downloading data:  49%|██████████▊           | 128M/261M [00:03<00:03, 38.3MB/s]\u001b[A\n",
      "Downloading data:  51%|███████████▏          | 132M/261M [00:03<00:03, 40.0MB/s]\u001b[A\n",
      "Downloading data:  52%|███████████▌          | 137M/261M [00:03<00:03, 41.3MB/s]\u001b[A\n",
      "Downloading data:  54%|███████████▉          | 141M/261M [00:03<00:02, 42.8MB/s]\u001b[A\n",
      "Downloading data:  56%|████████████▎         | 146M/261M [00:04<00:02, 44.4MB/s]\u001b[A\n",
      "Downloading data:  58%|████████████▋         | 151M/261M [00:04<00:02, 44.8MB/s]\u001b[A\n",
      "Downloading data:  60%|█████████████         | 156M/261M [00:04<00:02, 45.5MB/s]\u001b[A\n",
      "Downloading data:  61%|█████████████▌        | 160M/261M [00:04<00:02, 39.1MB/s]\u001b[A\n",
      "Downloading data:  63%|█████████████▉        | 165M/261M [00:04<00:02, 40.5MB/s]\u001b[A\n",
      "Downloading data:  65%|██████████████▎       | 169M/261M [00:04<00:02, 42.8MB/s]\u001b[A\n",
      "Downloading data:  67%|██████████████▋       | 174M/261M [00:04<00:01, 43.7MB/s]\u001b[A\n",
      "Downloading data:  68%|███████████████       | 179M/261M [00:04<00:01, 43.8MB/s]\u001b[A\n",
      "Downloading data:  70%|███████████████▍      | 184M/261M [00:04<00:01, 45.8MB/s]\u001b[A\n",
      "Downloading data:  72%|███████████████▉      | 188M/261M [00:05<00:01, 39.7MB/s]\u001b[A\n",
      "Downloading data:  74%|████████████████▏     | 193M/261M [00:05<00:01, 39.2MB/s]\u001b[A\n",
      "Downloading data:  76%|████████████████▊     | 199M/261M [00:05<00:01, 44.9MB/s]\u001b[A\n",
      "Downloading data:  78%|█████████████████▏    | 203M/261M [00:05<00:01, 45.7MB/s]\u001b[A\n",
      "Downloading data:  80%|█████████████████▌    | 208M/261M [00:05<00:01, 39.8MB/s]\u001b[A\n",
      "Downloading data:  82%|█████████████████▉    | 213M/261M [00:05<00:01, 41.2MB/s]\u001b[A\n",
      "Downloading data:  83%|██████████████████▎   | 217M/261M [00:05<00:01, 42.8MB/s]\u001b[A\n",
      "Downloading data:  85%|██████████████████▋   | 222M/261M [00:05<00:00, 44.3MB/s]\u001b[A\n",
      "Downloading data:  87%|███████████████████▏  | 227M/261M [00:05<00:00, 44.1MB/s]\u001b[A\n",
      "Downloading data:  89%|███████████████████▌  | 231M/261M [00:06<00:00, 44.2MB/s]\u001b[A\n",
      "Downloading data:  91%|███████████████████▉  | 236M/261M [00:06<00:00, 39.8MB/s]\u001b[A\n",
      "Downloading data:  92%|████████████████████▎ | 241M/261M [00:06<00:00, 41.5MB/s]\u001b[A\n",
      "Downloading data:  94%|████████████████████▋ | 245M/261M [00:06<00:00, 41.6MB/s]\u001b[A\n",
      "Downloading data:  96%|█████████████████████ | 250M/261M [00:06<00:00, 44.1MB/s]\u001b[A\n",
      "Downloading data:  98%|█████████████████████▍| 255M/261M [00:06<00:00, 44.1MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████████████████| 261M/261M [00:06<00:00, 38.8MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|                              | 0.00/257M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   0%|                      | 60.4k/257M [00:00<12:41, 337kB/s]\u001b[A\n",
      "Downloading data:   0%|                       | 304k/257M [00:00<04:34, 934kB/s]\u001b[A\n",
      "Downloading data:   1%|                     | 1.33M/257M [00:00<01:22, 3.11MB/s]\u001b[A\n",
      "Downloading data:   1%|▎                    | 3.43M/257M [00:00<00:32, 7.86MB/s]\u001b[A\n",
      "Downloading data:   3%|▌                    | 6.81M/257M [00:00<00:16, 15.0MB/s]\u001b[A\n",
      "Downloading data:   5%|▉                    | 11.7M/257M [00:00<00:09, 24.6MB/s]\u001b[A\n",
      "Downloading data:   6%|█▎                   | 16.5M/257M [00:00<00:07, 31.3MB/s]\u001b[A\n",
      "Downloading data:   8%|█▊                   | 21.4M/257M [00:01<00:07, 31.6MB/s]\u001b[A\n",
      "Downloading data:  10%|██                   | 25.9M/257M [00:01<00:06, 35.1MB/s]\u001b[A\n",
      "Downloading data:  12%|██▌                  | 30.6M/257M [00:01<00:05, 38.4MB/s]\u001b[A\n",
      "Downloading data:  14%|██▉                  | 35.8M/257M [00:01<00:05, 42.2MB/s]\u001b[A\n",
      "Downloading data:  16%|███▎                 | 40.7M/257M [00:01<00:04, 44.1MB/s]\u001b[A\n",
      "Downloading data:  18%|███▋                 | 45.2M/257M [00:01<00:05, 38.3MB/s]\u001b[A\n",
      "Downloading data:  19%|████                 | 49.8M/257M [00:01<00:05, 40.3MB/s]\u001b[A\n",
      "Downloading data:  21%|████▍                | 54.5M/257M [00:01<00:04, 42.1MB/s]\u001b[A\n",
      "Downloading data:  23%|████▊                | 59.3M/257M [00:01<00:04, 43.7MB/s]\u001b[A\n",
      "Downloading data:  25%|█████▏               | 64.0M/257M [00:02<00:04, 38.8MB/s]\u001b[A\n",
      "Downloading data:  27%|█████▌               | 68.1M/257M [00:02<00:04, 38.7MB/s]\u001b[A\n",
      "Downloading data:  28%|█████▉               | 72.6M/257M [00:02<00:04, 40.5MB/s]\u001b[A\n",
      "Downloading data:  30%|██████▎              | 77.3M/257M [00:02<00:04, 42.3MB/s]\u001b[A\n",
      "Downloading data:  32%|██████▋              | 82.1M/257M [00:02<00:04, 39.3MB/s]\u001b[A\n",
      "Downloading data:  34%|███████              | 86.7M/257M [00:02<00:04, 41.0MB/s]\u001b[A\n",
      "Downloading data:  36%|███████▌             | 91.9M/257M [00:02<00:03, 42.4MB/s]\u001b[A\n",
      "Downloading data:  38%|███████▉             | 96.5M/257M [00:02<00:03, 43.3MB/s]\u001b[A\n",
      "Downloading data:  40%|████████▋             | 101M/257M [00:02<00:03, 45.2MB/s]\u001b[A\n",
      "Downloading data:  41%|█████████             | 106M/257M [00:03<00:03, 40.2MB/s]\u001b[A\n",
      "Downloading data:  43%|█████████▌            | 111M/257M [00:03<00:03, 42.8MB/s]\u001b[A\n",
      "Downloading data:  45%|█████████▉            | 116M/257M [00:03<00:03, 42.6MB/s]\u001b[A\n",
      "Downloading data:  47%|██████████▎           | 120M/257M [00:03<00:03, 43.5MB/s]\u001b[A\n",
      "Downloading data:  49%|██████████▋           | 125M/257M [00:03<00:02, 45.1MB/s]\u001b[A\n",
      "Downloading data:  51%|███████████▏          | 130M/257M [00:03<00:02, 46.4MB/s]\u001b[A\n",
      "Downloading data:  53%|███████████▌          | 135M/257M [00:03<00:02, 41.5MB/s]\u001b[A\n",
      "Downloading data:  54%|███████████▉          | 140M/257M [00:03<00:02, 42.9MB/s]\u001b[A\n",
      "Downloading data:  56%|████████████▎         | 144M/257M [00:03<00:02, 42.6MB/s]\u001b[A\n",
      "Downloading data:  58%|████████████▋         | 149M/257M [00:04<00:02, 43.3MB/s]\u001b[A\n",
      "Downloading data:  60%|█████████████▏        | 153M/257M [00:04<00:02, 44.5MB/s]\u001b[A\n",
      "Downloading data:  62%|█████████████▌        | 158M/257M [00:04<00:02, 44.8MB/s]\u001b[A\n",
      "Downloading data:  63%|█████████████▉        | 162M/257M [00:04<00:02, 40.8MB/s]\u001b[A\n",
      "Downloading data:  65%|██████████████▎       | 167M/257M [00:04<00:02, 42.0MB/s]\u001b[A\n",
      "Downloading data:  67%|██████████████▋       | 172M/257M [00:04<00:01, 43.4MB/s]\u001b[A\n",
      "Downloading data:  69%|███████████████       | 176M/257M [00:04<00:01, 41.8MB/s]\u001b[A\n",
      "Downloading data:  71%|███████████████▌      | 181M/257M [00:04<00:01, 44.1MB/s]\u001b[A\n",
      "Downloading data:  72%|███████████████▉      | 186M/257M [00:04<00:01, 45.0MB/s]\u001b[A\n",
      "Downloading data:  74%|████████████████▎     | 190M/257M [00:05<00:01, 41.5MB/s]\u001b[A\n",
      "Downloading data:  76%|████████████████▋     | 195M/257M [00:05<00:01, 43.4MB/s]\u001b[A\n",
      "Downloading data:  78%|█████████████████▏    | 200M/257M [00:05<00:01, 42.3MB/s]\u001b[A\n",
      "Downloading data:  80%|█████████████████▌    | 205M/257M [00:05<00:01, 43.1MB/s]\u001b[A\n",
      "Downloading data:  82%|█████████████████▉    | 209M/257M [00:05<00:01, 44.1MB/s]\u001b[A\n",
      "Downloading data:  83%|██████████████████▎   | 214M/257M [00:05<00:00, 45.3MB/s]\u001b[A\n",
      "Downloading data:  85%|██████████████████▋   | 219M/257M [00:05<00:00, 41.8MB/s]\u001b[A\n",
      "Downloading data:  87%|███████████████████▏  | 223M/257M [00:05<00:00, 42.8MB/s]\u001b[A\n",
      "Downloading data:  89%|███████████████████▌  | 228M/257M [00:05<00:00, 43.4MB/s]\u001b[A\n",
      "Downloading data:  90%|███████████████████▉  | 232M/257M [00:06<00:00, 41.5MB/s]\u001b[A\n",
      "Downloading data:  92%|████████████████████▎ | 237M/257M [00:06<00:00, 42.7MB/s]\u001b[A\n",
      "Downloading data:  94%|████████████████████▋ | 241M/257M [00:06<00:00, 43.1MB/s]\u001b[A\n",
      "Downloading data:  96%|█████████████████████ | 246M/257M [00:06<00:00, 41.3MB/s]\u001b[A\n",
      "Downloading data:  98%|█████████████████████▌| 251M/257M [00:06<00:00, 43.5MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████████████████| 257M/257M [00:06<00:00, 39.0MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|                              | 0.00/260M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   0%|                      | 51.2k/260M [00:00<15:05, 287kB/s]\u001b[A\n",
      "Downloading data:   0%|                       | 312k/260M [00:00<04:26, 971kB/s]\u001b[A\n",
      "Downloading data:   1%|                     | 1.32M/260M [00:00<01:23, 3.09MB/s]\u001b[A\n",
      "Downloading data:   1%|▎                    | 3.36M/260M [00:00<00:33, 7.68MB/s]\u001b[A\n",
      "Downloading data:   3%|▌                    | 6.87M/260M [00:00<00:16, 15.2MB/s]\u001b[A\n",
      "Downloading data:   4%|▉                    | 11.4M/260M [00:00<00:10, 23.7MB/s]\u001b[A\n",
      "Downloading data:   6%|█▎                   | 16.6M/260M [00:00<00:07, 31.6MB/s]\u001b[A\n",
      "Downloading data:   8%|█▋                   | 21.4M/260M [00:01<00:06, 36.4MB/s]\u001b[A\n",
      "Downloading data:  10%|██                   | 25.3M/260M [00:01<00:07, 33.0MB/s]\u001b[A\n",
      "Downloading data:  12%|██▍                  | 29.9M/260M [00:01<00:06, 36.5MB/s]\u001b[A\n",
      "Downloading data:  13%|██▊                  | 34.2M/260M [00:01<00:05, 38.3MB/s]\u001b[A\n",
      "Downloading data:  15%|███▏                 | 39.6M/260M [00:01<00:05, 42.8MB/s]\u001b[A\n",
      "Downloading data:  17%|███▌                 | 44.4M/260M [00:01<00:04, 44.3MB/s]\u001b[A\n",
      "Downloading data:  19%|███▉                 | 49.0M/260M [00:01<00:05, 37.7MB/s]\u001b[A\n",
      "Downloading data:  21%|████▎                | 53.8M/260M [00:01<00:05, 40.3MB/s]\u001b[A\n",
      "Downloading data:  23%|████▋                | 58.5M/260M [00:01<00:04, 42.3MB/s]\u001b[A\n",
      "Downloading data:  24%|█████▏               | 63.5M/260M [00:02<00:04, 44.4MB/s]\u001b[A\n",
      "Downloading data:  26%|█████▌               | 68.1M/260M [00:02<00:04, 44.5MB/s]\u001b[A\n",
      "Downloading data:  28%|█████▉               | 72.6M/260M [00:02<00:04, 39.0MB/s]\u001b[A\n",
      "Downloading data:  30%|██████▏              | 77.0M/260M [00:02<00:04, 40.1MB/s]\u001b[A\n",
      "Downloading data:  32%|██████▌              | 81.9M/260M [00:02<00:04, 42.6MB/s]\u001b[A\n",
      "Downloading data:  34%|███████              | 87.0M/260M [00:02<00:03, 45.1MB/s]\u001b[A\n",
      "Downloading data:  35%|███████▍             | 91.6M/260M [00:02<00:03, 44.2MB/s]\u001b[A\n",
      "Downloading data:  37%|███████▊             | 96.3M/260M [00:02<00:04, 39.6MB/s]\u001b[A\n",
      "Downloading data:  39%|████████▌             | 101M/260M [00:02<00:03, 40.7MB/s]\u001b[A\n",
      "Downloading data:  41%|████████▉             | 105M/260M [00:03<00:03, 42.6MB/s]\u001b[A\n",
      "Downloading data:  43%|█████████▎            | 111M/260M [00:03<00:03, 44.8MB/s]\u001b[A\n",
      "Downloading data:  44%|█████████▊            | 115M/260M [00:03<00:03, 44.3MB/s]\u001b[A\n",
      "Downloading data:  46%|██████████▏           | 120M/260M [00:03<00:03, 40.1MB/s]\u001b[A\n",
      "Downloading data:  48%|██████████▌           | 125M/260M [00:03<00:03, 41.7MB/s]\u001b[A\n",
      "Downloading data:  50%|██████████▉           | 130M/260M [00:03<00:02, 43.8MB/s]\u001b[A\n",
      "Downloading data:  52%|███████████▍          | 134M/260M [00:03<00:02, 44.3MB/s]\u001b[A\n",
      "Downloading data:  53%|███████████▊          | 139M/260M [00:03<00:02, 44.6MB/s]\u001b[A\n",
      "Downloading data:  55%|████████████▏         | 144M/260M [00:03<00:02, 40.2MB/s]\u001b[A\n",
      "Downloading data:  57%|████████████▌         | 149M/260M [00:04<00:02, 41.8MB/s]\u001b[A\n",
      "Downloading data:  59%|████████████▉         | 153M/260M [00:04<00:02, 42.3MB/s]\u001b[A\n",
      "Downloading data:  61%|█████████████▎        | 158M/260M [00:04<00:02, 43.5MB/s]\u001b[A\n",
      "Downloading data:  63%|█████████████▊        | 163M/260M [00:04<00:02, 45.6MB/s]\u001b[A\n",
      "Downloading data:  65%|██████████████▏       | 168M/260M [00:04<00:01, 46.7MB/s]\u001b[A\n",
      "Downloading data:  66%|██████████████▌       | 172M/260M [00:04<00:02, 40.1MB/s]\u001b[A\n",
      "Downloading data:  68%|██████████████▉       | 177M/260M [00:04<00:02, 41.0MB/s]\u001b[A\n",
      "Downloading data:  70%|███████████████▍      | 182M/260M [00:04<00:01, 42.9MB/s]\u001b[A\n",
      "Downloading data:  72%|███████████████▊      | 187M/260M [00:04<00:01, 44.8MB/s]\u001b[A\n",
      "Downloading data:  74%|████████████████▏     | 191M/260M [00:05<00:01, 36.4MB/s]\u001b[A\n",
      "Downloading data:  75%|████████████████▌     | 195M/260M [00:05<00:02, 26.2MB/s]\u001b[A\n",
      "Downloading data:  77%|████████████████▉     | 200M/260M [00:05<00:01, 30.2MB/s]\u001b[A\n",
      "Downloading data:  79%|█████████████████▎    | 204M/260M [00:05<00:01, 33.2MB/s]\u001b[A\n",
      "Downloading data:  80%|█████████████████▋    | 209M/260M [00:05<00:01, 36.9MB/s]\u001b[A\n",
      "Downloading data:  82%|██████████████████▏   | 214M/260M [00:05<00:01, 40.4MB/s]\u001b[A\n",
      "Downloading data:  84%|██████████████████▌   | 219M/260M [00:05<00:00, 42.1MB/s]\u001b[A\n",
      "Downloading data:  86%|██████████████████▉   | 223M/260M [00:06<00:00, 36.6MB/s]\u001b[A\n",
      "Downloading data:  88%|███████████████████▎  | 228M/260M [00:06<00:00, 38.8MB/s]\u001b[A\n",
      "Downloading data:  90%|███████████████████▋  | 232M/260M [00:06<00:00, 41.1MB/s]\u001b[A\n",
      "Downloading data:  91%|████████████████████  | 237M/260M [00:06<00:00, 43.4MB/s]\u001b[A\n",
      "Downloading data:  93%|████████████████████▌ | 242M/260M [00:06<00:00, 45.3MB/s]\u001b[A\n",
      "Downloading data:  95%|████████████████████▉ | 247M/260M [00:06<00:00, 45.2MB/s]\u001b[A\n",
      "Downloading data:  97%|█████████████████████▎| 252M/260M [00:06<00:00, 39.4MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████████████████| 260M/260M [00:06<00:00, 37.5MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|                              | 0.00/257M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   0%|                      | 51.2k/257M [00:00<15:03, 285kB/s]\u001b[A\n",
      "Downloading data:   0%|                       | 295k/257M [00:00<04:41, 914kB/s]\u001b[A\n",
      "Downloading data:   0%|                     | 1.11M/257M [00:00<01:21, 3.13MB/s]\u001b[A\n",
      "Downloading data:   1%|▏                    | 2.35M/257M [00:00<00:44, 5.79MB/s]\u001b[A\n",
      "Downloading data:   2%|▎                    | 4.46M/257M [00:00<00:25, 10.1MB/s]\u001b[A\n",
      "Downloading data:   3%|▋                    | 8.11M/257M [00:00<00:13, 17.8MB/s]\u001b[A\n",
      "Downloading data:   5%|█                    | 13.1M/257M [00:00<00:08, 27.3MB/s]\u001b[A\n",
      "Downloading data:   7%|█▍                   | 17.2M/257M [00:01<00:07, 30.1MB/s]\u001b[A\n",
      "Downloading data:   8%|█▊                   | 21.7M/257M [00:01<00:06, 34.3MB/s]\u001b[A\n",
      "Downloading data:  10%|██▏                  | 26.6M/257M [00:01<00:05, 38.6MB/s]\u001b[A\n",
      "Downloading data:  12%|██▌                  | 31.4M/257M [00:01<00:05, 41.4MB/s]\u001b[A\n",
      "Downloading data:  14%|██▉                  | 36.4M/257M [00:01<00:05, 44.0MB/s]\u001b[A\n",
      "Downloading data:  16%|███▎                 | 41.0M/257M [00:01<00:04, 44.6MB/s]\u001b[A\n",
      "Downloading data:  18%|███▋                 | 45.6M/257M [00:01<00:05, 38.3MB/s]\u001b[A\n",
      "Downloading data:  19%|████                 | 50.1M/257M [00:01<00:05, 40.3MB/s]\u001b[A\n",
      "Downloading data:  21%|████▍                | 55.2M/257M [00:01<00:04, 43.0MB/s]\u001b[A\n",
      "Downloading data:  23%|████▊                | 59.6M/257M [00:01<00:04, 43.2MB/s]\u001b[A\n",
      "Downloading data:  25%|█████▏               | 64.0M/257M [00:02<00:06, 31.7MB/s]\u001b[A\n",
      "Downloading data:  26%|█████▌               | 68.0M/257M [00:02<00:05, 33.6MB/s]\u001b[A\n",
      "Downloading data:  28%|█████▉               | 72.1M/257M [00:02<00:05, 35.5MB/s]\u001b[A\n",
      "Downloading data:  30%|██████▎              | 77.1M/257M [00:02<00:04, 39.2MB/s]\u001b[A\n",
      "Downloading data:  32%|██████▋              | 81.9M/257M [00:02<00:04, 41.5MB/s]\u001b[A\n",
      "Downloading data:  34%|███████              | 86.6M/257M [00:02<00:03, 43.1MB/s]\u001b[A\n",
      "Downloading data:  35%|███████▍             | 91.2M/257M [00:02<00:03, 44.0MB/s]\u001b[A\n",
      "Downloading data:  37%|███████▊             | 95.8M/257M [00:02<00:04, 38.3MB/s]\u001b[A\n",
      "Downloading data:  39%|████████▌             | 100M/257M [00:03<00:03, 40.0MB/s]\u001b[A\n",
      "Downloading data:  41%|████████▉             | 105M/257M [00:03<00:03, 41.8MB/s]\u001b[A\n",
      "Downloading data:  43%|█████████▍            | 110M/257M [00:03<00:03, 44.3MB/s]\u001b[A\n",
      "Downloading data:  44%|█████████▊            | 115M/257M [00:03<00:03, 44.6MB/s]\u001b[A\n",
      "Downloading data:  46%|██████████▏           | 119M/257M [00:03<00:03, 44.5MB/s]\u001b[A\n",
      "Downloading data:  48%|██████████▌           | 124M/257M [00:03<00:03, 39.7MB/s]\u001b[A\n",
      "Downloading data:  50%|██████████▉           | 128M/257M [00:03<00:03, 41.1MB/s]\u001b[A\n",
      "Downloading data:  52%|███████████▍          | 133M/257M [00:03<00:02, 43.4MB/s]\u001b[A\n",
      "Downloading data:  54%|███████████▊          | 138M/257M [00:03<00:02, 44.8MB/s]\u001b[A\n",
      "Downloading data:  55%|████████████▏         | 143M/257M [00:03<00:02, 44.9MB/s]\u001b[A\n",
      "Downloading data:  57%|████████████▌         | 148M/257M [00:04<00:02, 46.3MB/s]\u001b[A\n",
      "Downloading data:  59%|█████████████         | 152M/257M [00:04<00:02, 38.6MB/s]\u001b[A\n",
      "Downloading data:  61%|█████████████▍        | 157M/257M [00:04<00:02, 41.9MB/s]\u001b[A\n",
      "Downloading data:  63%|█████████████▊        | 162M/257M [00:04<00:02, 42.5MB/s]\u001b[A\n",
      "Downloading data:  65%|██████████████▏       | 167M/257M [00:04<00:02, 44.0MB/s]\u001b[A\n",
      "Downloading data:  66%|██████████████▌       | 171M/257M [00:04<00:02, 38.6MB/s]\u001b[A\n",
      "Downloading data:  68%|███████████████       | 176M/257M [00:04<00:01, 40.9MB/s]\u001b[A\n",
      "Downloading data:  70%|███████████████▍      | 180M/257M [00:04<00:01, 41.4MB/s]\u001b[A\n",
      "Downloading data:  72%|███████████████▊      | 186M/257M [00:05<00:01, 45.0MB/s]\u001b[A\n",
      "Downloading data:  74%|████████████████▎     | 190M/257M [00:05<00:01, 45.0MB/s]\u001b[A\n",
      "Downloading data:  76%|████████████████▋     | 195M/257M [00:05<00:01, 39.6MB/s]\u001b[A\n",
      "Downloading data:  77%|█████████████████     | 199M/257M [00:05<00:01, 41.0MB/s]\u001b[A\n",
      "Downloading data:  79%|█████████████████▍    | 204M/257M [00:05<00:01, 42.0MB/s]\u001b[A\n",
      "Downloading data:  81%|█████████████████▉    | 209M/257M [00:05<00:01, 45.3MB/s]\u001b[A\n",
      "Downloading data:  83%|██████████████████▎   | 214M/257M [00:05<00:00, 44.7MB/s]\u001b[A\n",
      "Downloading data:  85%|██████████████████▋   | 218M/257M [00:05<00:00, 39.6MB/s]\u001b[A\n",
      "Downloading data:  86%|███████████████████   | 223M/257M [00:05<00:00, 39.3MB/s]\u001b[A\n",
      "Downloading data:  88%|███████████████████▍  | 227M/257M [00:06<00:00, 40.4MB/s]\u001b[A\n",
      "Downloading data:  90%|███████████████████▊  | 232M/257M [00:06<00:00, 43.4MB/s]\u001b[A\n",
      "Downloading data:  92%|████████████████████▎ | 237M/257M [00:06<00:00, 45.3MB/s]\u001b[A\n",
      "Downloading data:  94%|████████████████████▋ | 242M/257M [00:06<00:00, 46.6MB/s]\u001b[A\n",
      "Downloading data:  96%|█████████████████████ | 247M/257M [00:06<00:00, 39.7MB/s]\u001b[A\n",
      "Downloading data:  98%|█████████████████████▍| 251M/257M [00:06<00:00, 41.4MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████████████████| 257M/257M [00:06<00:00, 38.4MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|                              | 0.00/257M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   0%|                      | 54.3k/257M [00:00<14:12, 301kB/s]\u001b[A\n",
      "Downloading data:   0%|                       | 298k/257M [00:00<04:40, 916kB/s]\u001b[A\n",
      "Downloading data:   0%|                     | 1.26M/257M [00:00<01:27, 2.92MB/s]\u001b[A\n",
      "Downloading data:   1%|▎                    | 3.21M/257M [00:00<00:34, 7.33MB/s]\u001b[A\n",
      "Downloading data:   3%|▌                    | 6.45M/257M [00:00<00:17, 14.2MB/s]\u001b[A\n",
      "Downloading data:   4%|▉                    | 11.1M/257M [00:00<00:10, 23.3MB/s]\u001b[A\n",
      "Downloading data:   6%|█▎                   | 16.1M/257M [00:00<00:07, 31.1MB/s]\u001b[A\n",
      "Downloading data:   8%|█▋                   | 21.2M/257M [00:01<00:07, 31.6MB/s]\u001b[A\n",
      "Downloading data:  10%|██                   | 25.6M/257M [00:01<00:06, 34.9MB/s]\u001b[A\n",
      "Downloading data:  12%|██▍                  | 30.1M/257M [00:01<00:06, 37.6MB/s]\u001b[A\n",
      "Downloading data:  14%|██▊                  | 35.1M/257M [00:01<00:05, 41.1MB/s]\u001b[A\n",
      "Downloading data:  15%|███▏                 | 39.5M/257M [00:01<00:05, 41.9MB/s]\u001b[A\n",
      "Downloading data:  17%|███▌                 | 44.0M/257M [00:01<00:05, 36.8MB/s]\u001b[A\n",
      "Downloading data:  19%|███▉                 | 48.6M/257M [00:01<00:05, 39.1MB/s]\u001b[A\n",
      "Downloading data:  21%|████▎                | 53.4M/257M [00:01<00:04, 41.7MB/s]\u001b[A\n",
      "Downloading data:  23%|████▋                | 57.9M/257M [00:01<00:04, 42.4MB/s]\u001b[A\n",
      "Downloading data:  24%|█████▏               | 62.9M/257M [00:02<00:04, 44.7MB/s]\u001b[A\n",
      "Downloading data:  26%|█████▌               | 67.6M/257M [00:02<00:04, 45.5MB/s]\u001b[A\n",
      "Downloading data:  28%|█████▉               | 72.3M/257M [00:02<00:04, 39.3MB/s]\u001b[A\n",
      "Downloading data:  30%|██████▎              | 76.6M/257M [00:02<00:04, 40.5MB/s]\u001b[A\n",
      "Downloading data:  32%|██████▋              | 81.2M/257M [00:02<00:04, 41.9MB/s]\u001b[A\n",
      "Downloading data:  33%|██████▉              | 85.5M/257M [00:02<00:04, 42.3MB/s]\u001b[A\n",
      "Downloading data:  35%|███████▍             | 90.3M/257M [00:02<00:03, 43.8MB/s]\u001b[A\n",
      "Downloading data:  37%|███████▋             | 94.7M/257M [00:02<00:03, 43.9MB/s]\u001b[A\n",
      "Downloading data:  39%|████████             | 99.3M/257M [00:02<00:04, 38.5MB/s]\u001b[A\n",
      "Downloading data:  40%|████████▉             | 104M/257M [00:03<00:03, 40.1MB/s]\u001b[A\n",
      "Downloading data:  42%|█████████▎            | 108M/257M [00:03<00:03, 41.8MB/s]\u001b[A\n",
      "Downloading data:  44%|█████████▋            | 113M/257M [00:03<00:03, 44.0MB/s]\u001b[A\n",
      "Downloading data:  46%|██████████            | 118M/257M [00:03<00:03, 43.9MB/s]\u001b[A\n",
      "Downloading data:  48%|██████████▍           | 122M/257M [00:03<00:03, 44.5MB/s]\u001b[A\n",
      "Downloading data:  50%|██████████▉           | 127M/257M [00:03<00:02, 45.9MB/s]\u001b[A\n",
      "Downloading data:  51%|███████████▎          | 132M/257M [00:03<00:03, 39.0MB/s]\u001b[A\n",
      "Downloading data:  53%|███████████▋          | 137M/257M [00:03<00:02, 41.3MB/s]\u001b[A\n",
      "Downloading data:  55%|████████████          | 141M/257M [00:03<00:02, 43.0MB/s]\u001b[A\n",
      "Downloading data:  57%|████████████▍         | 146M/257M [00:04<00:02, 42.8MB/s]\u001b[A\n",
      "Downloading data:  59%|████████████▉         | 150M/257M [00:04<00:02, 43.5MB/s]\u001b[A\n",
      "Downloading data:  60%|█████████████▎        | 155M/257M [00:04<00:02, 44.2MB/s]\u001b[A\n",
      "Downloading data:  62%|█████████████▋        | 159M/257M [00:04<00:02, 44.0MB/s]\u001b[A\n",
      "Downloading data:  64%|██████████████        | 164M/257M [00:04<00:02, 39.4MB/s]\u001b[A\n",
      "Downloading data:  66%|██████████████▍       | 168M/257M [00:04<00:02, 41.1MB/s]\u001b[A\n",
      "Downloading data:  67%|██████████████▊       | 173M/257M [00:04<00:01, 42.8MB/s]\u001b[A\n",
      "Downloading data:  69%|███████████████▏      | 178M/257M [00:04<00:01, 42.8MB/s]\u001b[A\n",
      "Downloading data:  71%|███████████████▌      | 182M/257M [00:04<00:01, 43.7MB/s]\u001b[A\n",
      "Downloading data:  73%|███████████████▉      | 187M/257M [00:04<00:01, 44.5MB/s]\u001b[A\n",
      "Downloading data:  75%|████████████████▍     | 192M/257M [00:05<00:01, 45.2MB/s]\u001b[A\n",
      "Downloading data:  76%|████████████████▊     | 196M/257M [00:05<00:01, 40.3MB/s]\u001b[A\n",
      "Downloading data:  78%|█████████████████▏    | 201M/257M [00:05<00:01, 41.6MB/s]\u001b[A\n",
      "Downloading data:  80%|█████████████████▌    | 205M/257M [00:05<00:01, 41.7MB/s]\u001b[A\n",
      "Downloading data:  82%|█████████████████▉    | 209M/257M [00:05<00:01, 42.9MB/s]\u001b[A\n",
      "Downloading data:  83%|██████████████████▎   | 214M/257M [00:05<00:00, 43.7MB/s]\u001b[A\n",
      "Downloading data:  85%|██████████████████▋   | 219M/257M [00:05<00:00, 44.1MB/s]\u001b[A\n",
      "Downloading data:  87%|███████████████████▏  | 224M/257M [00:05<00:00, 45.7MB/s]\u001b[A\n",
      "Downloading data:  89%|███████████████████▌  | 228M/257M [00:06<00:00, 36.0MB/s]\u001b[A\n",
      "Downloading data:  91%|███████████████████▉  | 233M/257M [00:06<00:00, 39.9MB/s]\u001b[A\n",
      "Downloading data:  93%|████████████████████▍ | 238M/257M [00:06<00:00, 41.8MB/s]\u001b[A\n",
      "Downloading data:  94%|████████████████████▊ | 242M/257M [00:06<00:00, 37.4MB/s]\u001b[A\n",
      "Downloading data:  96%|█████████████████████▏| 247M/257M [00:06<00:00, 39.8MB/s]\u001b[A\n",
      "Downloading data:  98%|█████████████████████▌| 252M/257M [00:06<00:00, 41.1MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████████████████| 257M/257M [00:06<00:00, 38.5MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|                              | 0.00/259M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   0%|                      | 34.8k/259M [00:00<22:27, 192kB/s]\u001b[A\n",
      "Downloading data:   0%|                       | 102k/259M [00:00<14:25, 299kB/s]\u001b[A\n",
      "Downloading data:   0%|                       | 176k/259M [00:00<12:24, 348kB/s]\u001b[A\n",
      "Downloading data:   0%|                       | 248k/259M [00:00<10:33, 409kB/s]\u001b[A\n",
      "Downloading data:   0%|                       | 294k/259M [00:00<11:15, 384kB/s]\u001b[A\n",
      "Downloading data:   0%|                       | 374k/259M [00:00<09:37, 448kB/s]\u001b[A\n",
      "Downloading data:   0%|                       | 420k/259M [00:01<10:25, 414kB/s]\u001b[A\n",
      "Downloading data:   0%|                       | 513k/259M [00:01<08:38, 499kB/s]\u001b[A\n",
      "Downloading data:   0%|                       | 575k/259M [00:01<08:45, 492kB/s]\u001b[A\n",
      "Downloading data:   0%|                       | 681k/259M [00:01<08:13, 525kB/s]\u001b[A\n",
      "Downloading data:   0%|                       | 814k/259M [00:01<06:38, 648kB/s]\u001b[A\n",
      "Downloading data:   0%|                       | 885k/259M [00:01<07:05, 607kB/s]\u001b[A\n",
      "Downloading data:   0%|                      | 1.02M/259M [00:01<06:23, 673kB/s]\u001b[A\n",
      "Downloading data:   0%|                      | 1.17M/259M [00:02<05:04, 846kB/s]\u001b[A\n",
      "Downloading data:   0%|                      | 1.27M/259M [00:02<05:45, 747kB/s]\u001b[A\n",
      "Downloading data:   1%|                      | 1.46M/259M [00:02<05:02, 853kB/s]\u001b[A\n",
      "Downloading data:   1%|▏                     | 1.65M/259M [00:02<04:19, 993kB/s]\u001b[A\n",
      "Downloading data:   1%|▏                     | 1.77M/259M [00:02<04:24, 975kB/s]\u001b[A\n",
      "Downloading data:   1%|▏                    | 2.00M/259M [00:02<03:41, 1.16MB/s]\u001b[A\n",
      "Downloading data:   1%|▏                    | 2.14M/259M [00:02<03:47, 1.13MB/s]\u001b[A\n",
      "Downloading data:   1%|▏                    | 2.40M/259M [00:03<03:10, 1.35MB/s]\u001b[A\n",
      "Downloading data:   1%|▏                    | 2.57M/259M [00:03<03:11, 1.34MB/s]\u001b[A\n",
      "Downloading data:   1%|▏                    | 2.87M/259M [00:03<02:43, 1.57MB/s]\u001b[A\n",
      "Downloading data:   1%|▏                    | 3.06M/259M [00:03<02:46, 1.54MB/s]\u001b[A\n",
      "Downloading data:   1%|▎                    | 3.39M/259M [00:03<02:10, 1.96MB/s]\u001b[A\n",
      "Downloading data:   1%|▎                    | 3.60M/259M [00:03<02:18, 1.84MB/s]\u001b[A\n",
      "Downloading data:   1%|▎                    | 3.83M/259M [00:03<02:21, 1.80MB/s]\u001b[A\n",
      "Downloading data:   2%|▎                    | 4.17M/259M [00:03<01:55, 2.20MB/s]\u001b[A\n",
      "Downloading data:   2%|▎                    | 4.51M/259M [00:04<01:49, 2.33MB/s]\u001b[A\n",
      "Downloading data:   2%|▍                    | 4.78M/259M [00:04<01:52, 2.26MB/s]\u001b[A\n",
      "Downloading data:   2%|▍                    | 5.26M/259M [00:04<01:27, 2.89MB/s]\u001b[A\n",
      "Downloading data:   2%|▍                    | 5.58M/259M [00:04<01:31, 2.76MB/s]\u001b[A\n",
      "Downloading data:   2%|▍                    | 5.92M/259M [00:04<01:35, 2.67MB/s]\u001b[A\n",
      "Downloading data:   3%|▌                    | 6.56M/259M [00:04<01:16, 3.30MB/s]\u001b[A\n",
      "Downloading data:   3%|▌                    | 6.94M/259M [00:04<01:19, 3.18MB/s]\u001b[A\n",
      "Downloading data:   3%|▌                    | 7.53M/259M [00:04<01:05, 3.86MB/s]\u001b[A\n",
      "Downloading data:   3%|▋                    | 8.07M/259M [00:05<01:03, 3.94MB/s]\u001b[A\n",
      "Downloading data:   3%|▋                    | 8.54M/259M [00:05<01:05, 3.81MB/s]\u001b[A\n",
      "Downloading data:   4%|▊                    | 9.36M/259M [00:05<00:50, 4.90MB/s]\u001b[A\n",
      "Downloading data:   4%|▊                    | 9.92M/259M [00:05<00:52, 4.71MB/s]\u001b[A\n",
      "Downloading data:   4%|▊                    | 10.5M/259M [00:05<00:53, 4.62MB/s]\u001b[A\n",
      "Downloading data:   4%|▉                    | 11.5M/259M [00:05<00:41, 5.95MB/s]\u001b[A\n",
      "Downloading data:   5%|▉                    | 12.2M/259M [00:05<00:42, 5.78MB/s]\u001b[A\n",
      "Downloading data:   5%|█                    | 12.9M/259M [00:05<00:43, 5.65MB/s]\u001b[A\n",
      "Downloading data:   5%|█                    | 13.8M/259M [00:06<00:37, 6.61MB/s]\u001b[A\n",
      "Downloading data:   6%|█▏                   | 14.9M/259M [00:06<00:32, 7.62MB/s]\u001b[A\n",
      "Downloading data:   6%|█▎                   | 15.7M/259M [00:06<00:33, 7.34MB/s]\u001b[A\n",
      "Downloading data:   6%|█▎                   | 16.6M/259M [00:06<00:34, 7.14MB/s]\u001b[A\n",
      "Downloading data:   7%|█▍                   | 17.8M/259M [00:06<00:28, 8.41MB/s]\u001b[A\n",
      "Downloading data:   7%|█▌                   | 19.2M/259M [00:06<00:24, 9.74MB/s]\u001b[A\n",
      "Downloading data:   8%|█▋                   | 20.2M/259M [00:06<00:25, 9.45MB/s]\u001b[A\n",
      "Downloading data:   8%|█▋                   | 21.3M/259M [00:06<00:25, 9.17MB/s]\u001b[A\n",
      "Downloading data:   9%|█▊                   | 22.6M/259M [00:06<00:23, 10.2MB/s]\u001b[A\n",
      "Downloading data:  10%|█▉                   | 24.6M/259M [00:07<00:18, 12.9MB/s]\u001b[A\n",
      "Downloading data:  10%|██                   | 26.0M/259M [00:07<00:19, 12.2MB/s]\u001b[A\n",
      "Downloading data:  11%|██▏                  | 27.3M/259M [00:07<00:19, 11.6MB/s]\u001b[A\n",
      "Downloading data:  11%|██▎                  | 29.2M/259M [00:07<00:16, 13.6MB/s]\u001b[A\n",
      "Downloading data:  12%|██▌                  | 31.5M/259M [00:07<00:14, 16.2MB/s]\u001b[A\n",
      "Downloading data:  13%|██▋                  | 33.2M/259M [00:07<00:14, 15.4MB/s]\u001b[A\n",
      "Downloading data:  14%|██▊                  | 35.0M/259M [00:07<00:15, 14.9MB/s]\u001b[A\n",
      "Downloading data:  14%|███                  | 37.2M/259M [00:07<00:13, 16.8MB/s]\u001b[A\n",
      "Downloading data:  16%|███▎                 | 40.2M/259M [00:07<00:10, 20.4MB/s]\u001b[A\n",
      "Downloading data:  16%|███▍                 | 42.5M/259M [00:08<00:10, 19.7MB/s]\u001b[A\n",
      "Downloading data:  17%|███▋                 | 44.9M/259M [00:08<00:11, 19.2MB/s]\u001b[A\n",
      "Downloading data:  18%|███▊                 | 47.7M/259M [00:08<00:09, 21.7MB/s]\u001b[A\n",
      "Downloading data:  20%|████▏                | 51.0M/259M [00:08<00:08, 24.7MB/s]\u001b[A\n",
      "Downloading data:  21%|████▍                | 54.3M/259M [00:08<00:08, 25.5MB/s]\u001b[A\n",
      "Downloading data:  22%|████▋                | 57.3M/259M [00:08<00:07, 26.5MB/s]\u001b[A\n",
      "Downloading data:  23%|████▊                | 60.1M/259M [00:08<00:07, 25.7MB/s]\u001b[A\n",
      "Downloading data:  25%|█████▏               | 63.8M/259M [00:08<00:06, 28.6MB/s]\u001b[A\n",
      "Downloading data:  26%|█████▍               | 67.4M/259M [00:08<00:06, 30.7MB/s]\u001b[A\n",
      "Downloading data:  27%|█████▋               | 70.9M/259M [00:09<00:05, 32.0MB/s]\u001b[A\n",
      "Downloading data:  29%|██████               | 74.4M/259M [00:09<00:05, 32.8MB/s]\u001b[A\n",
      "Downloading data:  31%|██████▍              | 79.6M/259M [00:09<00:04, 38.7MB/s]\u001b[A\n",
      "Downloading data:  32%|██████▊              | 83.7M/259M [00:09<00:05, 33.8MB/s]\u001b[A\n",
      "Downloading data:  34%|███████▏             | 88.1M/259M [00:09<00:04, 36.4MB/s]\u001b[A\n",
      "Downloading data:  36%|███████▌             | 92.8M/259M [00:09<00:04, 39.5MB/s]\u001b[A\n",
      "Downloading data:  38%|███████▉             | 97.4M/259M [00:09<00:03, 41.3MB/s]\u001b[A\n",
      "Downloading data:  40%|████████▋             | 103M/259M [00:09<00:03, 44.1MB/s]\u001b[A\n",
      "Downloading data:  41%|█████████             | 107M/259M [00:09<00:03, 43.6MB/s]\u001b[A\n",
      "Downloading data:  43%|█████████▍            | 112M/259M [00:10<00:03, 38.8MB/s]\u001b[A\n",
      "Downloading data:  45%|█████████▊            | 116M/259M [00:10<00:03, 40.2MB/s]\u001b[A\n",
      "Downloading data:  47%|██████████▏           | 121M/259M [00:10<00:03, 42.4MB/s]\u001b[A\n",
      "Downloading data:  48%|██████████▋           | 125M/259M [00:10<00:03, 43.0MB/s]\u001b[A\n",
      "Downloading data:  50%|██████████▉           | 130M/259M [00:10<00:03, 43.2MB/s]\u001b[A\n",
      "Downloading data:  52%|███████████▍          | 134M/259M [00:10<00:02, 44.6MB/s]\u001b[A\n",
      "Downloading data:  54%|███████████▊          | 139M/259M [00:10<00:03, 38.7MB/s]\u001b[A\n",
      "Downloading data:  55%|████████████▏         | 144M/259M [00:10<00:02, 41.4MB/s]\u001b[A\n",
      "Downloading data:  57%|████████████▌         | 148M/259M [00:10<00:02, 41.8MB/s]\u001b[A\n",
      "Downloading data:  59%|████████████▉         | 153M/259M [00:11<00:02, 43.1MB/s]\u001b[A\n",
      "Downloading data:  61%|█████████████▍        | 158M/259M [00:11<00:02, 45.1MB/s]\u001b[A\n",
      "Downloading data:  63%|█████████████▊        | 162M/259M [00:11<00:02, 45.2MB/s]\u001b[A\n",
      "Downloading data:  64%|██████████████▏       | 167M/259M [00:11<00:02, 38.8MB/s]\u001b[A\n",
      "Downloading data:  66%|██████████████▌       | 171M/259M [00:11<00:02, 39.7MB/s]\u001b[A\n",
      "Downloading data:  68%|██████████████▊       | 175M/259M [00:11<00:02, 39.7MB/s]\u001b[A\n",
      "Downloading data:  70%|███████████████▎      | 180M/259M [00:11<00:01, 43.2MB/s]\u001b[A\n",
      "Downloading data:  71%|███████████████▋      | 185M/259M [00:11<00:01, 43.5MB/s]\u001b[A\n",
      "Downloading data:  73%|████████████████      | 189M/259M [00:11<00:01, 37.8MB/s]\u001b[A\n",
      "Downloading data:  75%|████████████████▍     | 194M/259M [00:12<00:01, 39.4MB/s]\u001b[A\n",
      "Downloading data:  77%|████████████████▊     | 198M/259M [00:12<00:01, 41.7MB/s]\u001b[A\n",
      "Downloading data:  78%|█████████████████▏    | 203M/259M [00:12<00:01, 42.2MB/s]\u001b[A\n",
      "Downloading data:  80%|█████████████████▌    | 207M/259M [00:12<00:01, 42.2MB/s]\u001b[A\n",
      "Downloading data:  82%|█████████████████▉    | 212M/259M [00:12<00:01, 43.0MB/s]\u001b[A\n",
      "Downloading data:  83%|██████████████████▎   | 216M/259M [00:12<00:01, 37.3MB/s]\u001b[A\n",
      "Downloading data:  85%|██████████████████▋   | 221M/259M [00:12<00:00, 40.3MB/s]\u001b[A\n",
      "Downloading data:  87%|███████████████████   | 225M/259M [00:12<00:00, 41.5MB/s]\u001b[A\n",
      "Downloading data:  89%|███████████████████▍  | 230M/259M [00:12<00:00, 42.4MB/s]\u001b[A\n",
      "Downloading data:  90%|███████████████████▉  | 235M/259M [00:13<00:00, 44.0MB/s]\u001b[A\n",
      "Downloading data:  92%|████████████████████▎ | 239M/259M [00:13<00:00, 38.8MB/s]\u001b[A\n",
      "Downloading data:  94%|████████████████████▋ | 244M/259M [00:13<00:00, 40.3MB/s]\u001b[A\n",
      "Downloading data:  96%|█████████████████████ | 249M/259M [00:13<00:00, 42.4MB/s]\u001b[A\n",
      "Downloading data:  98%|█████████████████████▍| 253M/259M [00:13<00:00, 42.8MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████████████████| 259M/259M [00:13<00:00, 19.0MB/s]\u001b[A\n",
      "Downloading data files: 100%|█████████████████████| 1/1 [00:56<00:00, 56.07s/it]\n",
      "Extracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 531.60it/s]\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7. Subsequent calls will reuse this data.\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 51.67it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset (enwiki_100k)\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/Baseline-C-enwiki.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 743309031\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 743309031\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230729_044418-jo4wip9s\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mBaseline-C - Enwiki Foundation (ctx=4096, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/jo4wip9s\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_4096_bf16/build.ninja...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_4096_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -t 4 -std=c++17 -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=4096 -c /root/picocreator-memory-experiment/RWKV-v4neo/cuda/wkv_cuda_bf16.cu -o wkv_cuda_bf16.cuda.o \n",
      "ptxas info    : 1 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf\n",
      "    49152 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 56 registers, 464 bytes cmem[0], 8 bytes cmem[2]\n",
      "ptxas info    : Compiling entry function '_Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, 424 bytes cmem[0]\n",
      "[2/3] c++ -MMD -MF wkv_op_bf16.o.d -DTORCH_EXTENSION_NAME=wkv_4096_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -std=c++17 -O3 -DTmax=4096 -c /root/picocreator-memory-experiment/RWKV-v4neo/cuda/wkv_op_bf16.cpp -o wkv_op_bf16.o \n",
      "[3/3] c++ wkv_op_bf16.o wkv_cuda_bf16.cuda.o -shared -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_4096_bf16.so\n",
      "Loading extension module wkv_4096_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 4\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 43.24it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-505c581b02a81ee3_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-4f5a278f7ace6a75_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-475347b090399ab0_*_of_00064.arrow\n",
      "Saving the dataset (0/5 shards):   0%|         | 0/81487 [00:00<?, ? examples/s]Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Saving the dataset (0/5 shards):   5%| | 4000/81487 [00:00<00:03, 24758.63 exampSetting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Saving the dataset (0/5 shards):  15%|▏| 12000/81487 [00:00<00:02, 32542.69 exam[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Saving the dataset (0/5 shards):  20%|▏| 16298/81487 [00:00<00:01, 35268.58 exam[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Saving the dataset (1/5 shards):  37%|▎| 30298/81487 [00:01<00:01, 31530.61 exam[rank: 3] Global seed set to 743309031\n",
      "[rank: 5] Global seed set to 743309031\n",
      "Saving the dataset (2/5 shards):  40%|▍| 32596/81487 [00:01<00:01, 31530.61 exam[rank: 2] Global seed set to 743309031\n",
      "[rank: 6] Global seed set to 743309031\n",
      "[rank: 1] Global seed set to 743309031\n",
      "[rank: 7] Global seed set to 743309031\n",
      "[rank: 4] Global seed set to 743309031\n",
      "Saving the dataset (2/5 shards):  57%|▌| 46596/81487 [00:01<00:01, 34235.97 examUsing /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Saving the dataset (3/5 shards):  60%|▌| 48893/81487 [00:01<00:00, 34235.97 examDetected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_4096_bf16/build.ninja...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Saving the dataset (3/5 shards):  62%|▌| 50893/81487 [00:01<00:01, 26549.54 examUsing /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_4096_bf16/build.ninja...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Saving the dataset (3/5 shards):  70%|▋| 56893/81487 [00:01<00:00, 30424.90 examLoading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "[rank: 0] Global seed set to 743309031                                          \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-07-29 04:44:58,029] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 743309031\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-07-29 04:45:12,938] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 743309031\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-07-29 04:45:13,136] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 743309031\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-07-29 04:45:13,156] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 743309031\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-07-29 04:45:13,183] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 743309031\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-07-29 04:45:13,191] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 743309031\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-07-29 04:45:13,218] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 743309031\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-07-29 04:45:13,227] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06793999671936035 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10133695602416992 seconds\n",
      "Time to load fused_adam op: 0.10172009468078613 seconds\n",
      "Time to load fused_adam op: 0.10172295570373535 seconds\n",
      "Time to load fused_adam op: 0.10157012939453125 seconds\n",
      "Time to load fused_adam op: 0.1015784740447998 seconds\n",
      "Time to load fused_adam op: 0.10140538215637207 seconds\n",
      "Time to load fused_adam op: 0.10173273086547852 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06954073905944824 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10188508033752441 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10192418098449707 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10150599479675293 seconds\n",
      "Time to load utils op: 0.1022036075592041 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10184788703918457 seconds\n",
      "Time to load utils op: 0.10195755958557129 seconds\n",
      "Time to load utils op: 0.10143733024597168 seconds\n",
      "Rank: 5 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 1 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 2 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 7 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 3 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 0 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 4 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 6 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00026226043701171875 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00026679039001464844 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002601146697998047 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002758502960205078 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00034999847412109375 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003418922424316406 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003612041473388672 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0004878044128417969 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0:  10%| | 1000/10186 [37:31<5:44:38,  2.25s/it, v_num=ip9s, train/loss=4./usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 10186/10186 [6:27:46<00:00,  2.28s/it, v_num=ip9s, train/loss=3\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                  | 1/52 [00:00<00:13,  3.82it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                  | 2/52 [00:00<00:13,  3.79it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                  | 3/52 [00:00<00:12,  3.77it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍                 | 4/52 [00:01<00:12,  3.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▊                 | 5/52 [00:01<00:12,  3.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|██▏                | 6/52 [00:01<00:12,  3.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▌                | 7/52 [00:01<00:12,  3.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▉                | 8/52 [00:02<00:11,  3.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▎               | 9/52 [00:02<00:11,  3.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▍              | 10/52 [00:02<00:11,  3.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▊              | 11/52 [00:02<00:11,  3.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|████▏             | 12/52 [00:03<00:10,  3.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▌             | 13/52 [00:03<00:10,  3.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▊             | 14/52 [00:03<00:10,  3.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|█████▏            | 15/52 [00:04<00:10,  3.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▌            | 16/52 [00:04<00:09,  3.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▉            | 17/52 [00:04<00:09,  3.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|██████▏           | 18/52 [00:04<00:09,  3.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▌           | 19/52 [00:05<00:08,  3.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▉           | 20/52 [00:05<00:08,  3.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|███████▎          | 21/52 [00:05<00:08,  3.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▌          | 22/52 [00:05<00:08,  3.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▉          | 23/52 [00:06<00:07,  3.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████████▎         | 24/52 [00:06<00:07,  3.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▋         | 25/52 [00:06<00:07,  3.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 26/52 [00:07<00:07,  3.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████████▎        | 27/52 [00:07<00:06,  3.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▋        | 28/52 [00:07<00:06,  3.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████        | 29/52 [00:07<00:06,  3.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|██████████▍       | 30/52 [00:08<00:05,  3.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▋       | 31/52 [00:08<00:05,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|███████████       | 32/52 [00:08<00:05,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|███████████▍      | 33/52 [00:08<00:05,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████▊      | 34/52 [00:09<00:04,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 35/52 [00:09<00:04,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|████████████▍     | 36/52 [00:09<00:04,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|████████████▊     | 37/52 [00:10<00:04,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|█████████████▏    | 38/52 [00:10<00:03,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|█████████████▌    | 39/52 [00:10<00:03,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|█████████████▊    | 40/52 [00:10<00:03,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|██████████████▏   | 41/52 [00:11<00:02,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|██████████████▌   | 42/52 [00:11<00:02,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|██████████████▉   | 43/52 [00:11<00:02,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|███████████████▏  | 44/52 [00:11<00:02,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|███████████████▌  | 45/52 [00:12<00:01,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|███████████████▉  | 46/52 [00:12<00:01,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|████████████████▎ | 47/52 [00:12<00:01,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|████████████████▌ | 48/52 [00:13<00:01,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|████████████████▉ | 49/52 [00:13<00:00,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████████████▎| 50/52 [00:13<00:00,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|█████████████████▋| 51/52 [00:13<00:00,  3.68it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 10186/10186 [6:28:07<00:00,  2.29s/it, v_num=ip9s, train/loss=3\u001b[A\n",
      "Epoch 0: 100%|█| 10186/10186 [6:28:07<00:00,  2.29s/it, v_num=ip9s, train/loss=3\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 10186/10186 [6:28:31<00:00,  2.29s/it, v_num=ip9s, train/loss=3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▆▅▄▄▄▄▄▄▃▃▄▃▃▃▃▂▃▃▂▂▃▃▂▂▁▂▂▃▂▂▂▃▃▂▂▂▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 51\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 4095\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 408\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 3.46875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2546\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 3.45358\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mBaseline-C - Enwiki Foundation (ctx=4096, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/jo4wip9s\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230729_044418-jo4wip9s/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the foundation model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/Baseline-C-enwiki.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Enwiki Foundation (ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/Baseline-C-enwiki/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 438 params 1515106304 elements\n",
      "Saving fp32 state dict to ../model/Baseline-C-Stage1.pth\n",
      "-rw-r--r-- 1 root root 5.7G Jul 29 11:14 ../model/Baseline-C-Stage1.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/Baseline-C-enwiki/last.ckpt\" \"../model/Baseline-C-Stage1.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/Baseline-C-Stage1.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_1024_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -t 4 -std=c++17 -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=1024 -c /root/picocreator-memory-experiment/RWKV-v4neo/cuda/wkv_cuda_bf16.cu -o wkv_cuda_bf16.cuda.o \n",
      "ptxas info    : 1 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf\n",
      "    12288 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 56 registers, 464 bytes cmem[0], 8 bytes cmem[2]\n",
      "ptxas info    : Compiling entry function '_Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, 424 bytes cmem[0]\n",
      "[2/3] c++ -MMD -MF wkv_op_bf16.o.d -DTORCH_EXTENSION_NAME=wkv_1024_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -std=c++17 -O3 -DTmax=1024 -c /root/picocreator-memory-experiment/RWKV-v4neo/cuda/wkv_op_bf16.cpp -o wkv_op_bf16.o \n",
      "[3/3] c++ wkv_op_bf16.o wkv_cuda_bf16.cuda.o -shared -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_1024_bf16.so\n",
      "Loading extension module wkv_1024_bf16...\n",
      "--- DRAGON PROMPT ---\n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese. The dragon turned back to investigate the dragons, to which the monster's parents had previously been the only ones whose parents were children.\n",
      "\n",
      "Bouh, who believed the giant dragon, was the most important weapon, convinced the dragon to kill the dragon. The dragon, however, feared that the dragon would be capable of killing the dragon, and told the dragon that he would let the dragon live if he failed.\n",
      "\n",
      "Legacy\n",
      "\n",
      "The dragon is a common theme in many northern China, where the dragon is thought to be one of the most powerful beasts.\n",
      "\n",
      "Horse-eyed dragon's child was once a common ancestor of the dragon's parents.\n",
      "\n",
      "The dragon was the guardian of the Dragon Dragon and was a weapon of a dragon's tail.\n",
      "\n",
      "\"Head of the dragons\"\n",
      "\n",
      "In the text of the Dragon God's story, the dragon was placed on a battlefield to destroy the dragon and bring the dragon back to life.\n",
      "\n",
      "The dragon was"
     ]
    }
   ],
   "source": [
    "# # Lets do a quick dragon prompt validation\n",
    "!cd \"{INFERENCE_DIR}\" && python3 dragon_test.py ../model/Baseline-C-Stage1.pth \"cuda fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets do a quick memory test\n",
    "# # (We dun expect this to work, as we have not finetune for memory recall, but its a baseline)\n",
    "# !python3 ../memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/Baseline-C-Stage1.pth\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2 : Instruct Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████████████| 7.79k/7.79k [00:00<00:00, 23.8MB/s]\n",
      "Downloading and preparing dataset None/None to /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n",
      "Downloading data files:   0%|                             | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                             | 0.00/7.80M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   1%|▏                    | 52.2k/7.80M [00:00<00:26, 289kB/s]\u001b[A\n",
      "Downloading data:   4%|▉                     | 313k/7.80M [00:00<00:07, 985kB/s]\u001b[A\n",
      "Downloading data:   9%|█▉                   | 721k/7.80M [00:00<00:03, 1.98MB/s]\u001b[A\n",
      "Downloading data:  26%|█████▏              | 2.02M/7.80M [00:00<00:01, 5.43MB/s]\u001b[A\n",
      "Downloading data:  51%|██████████▏         | 3.98M/7.80M [00:00<00:00, 9.81MB/s]\u001b[A\n",
      "Downloading data: 100%|████████████████████| 7.80M/7.80M [00:00<00:00, 10.3MB/s]\u001b[A\n",
      "Downloading data files: 100%|█████████████████████| 1/1 [00:02<00:00,  2.32s/it]\n",
      "Extracting data files: 100%|████████████████████| 1/1 [00:00<00:00, 2465.79it/s]\n",
      "Setting num_proc from 64 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 715.63it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/Baseline-C-instruct.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 417591938\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 417591938\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230729_111557-759pr8ua\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mBaseline-C - Instruct (train-ctx=4096, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/759pr8ua\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_4096_bf16/build.ninja...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_4096_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 4\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 703.27it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-497b18043dca9701_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-7e82136b48a2b575_*_of_00064.arrow\n",
      "[rank: 0] Global seed set to 417591938                                          \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-07-29 11:16:12,834] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 2] Global seed set to 417591938\n",
      "[rank: 1] Global seed set to 417591938\n",
      "[rank: 5] Global seed set to 417591938\n",
      "[rank: 6] Global seed set to 417591938\n",
      "[rank: 7] Global seed set to 417591938\n",
      "[rank: 4] Global seed set to 417591938\n",
      "[rank: 3] Global seed set to 417591938\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_4096_bf16/build.ninja...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_4096_bf16/build.ninja...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "Loading extension module wkv_4096_bf16...\n",
      "[rank: 2] Global seed set to 417591938\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-07-29 11:16:27,471] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 417591938\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-07-29 11:16:28,387] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 417591938\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-07-29 11:16:30,039] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 417591938\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-07-29 11:16:30,044] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 417591938\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-07-29 11:16:30,054] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 417591938\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-07-29 11:16:30,207] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 417591938\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-07-29 11:16:30,317] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  4.000e-04 (0.0004)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06526613235473633 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10117697715759277 seconds\n",
      "Time to load fused_adam op: 0.10138130187988281 seconds\n",
      "Time to load fused_adam op: 0.10144925117492676 seconds\n",
      "Time to load fused_adam op: 0.10144925117492676 seconds\n",
      "Time to load fused_adam op: 0.10146117210388184 seconds\n",
      "Time to load fused_adam op: 0.10166096687316895 seconds\n",
      "Time to load fused_adam op: 0.10167527198791504 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.07295775413513184 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10179948806762695 seconds\n",
      "Time to load utils op: 0.10182619094848633 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10126471519470215 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10216403007507324 seconds\n",
      "Time to load utils op: 0.10184407234191895 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10137414932250977 seconds\n",
      "Time to load utils op: 0.10176563262939453 seconds\n",
      "Rank: 6 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 0 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 3 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 7 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 1 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 5 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 2 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 4 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002722740173339844 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.00026035308837890625 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002770423889160156 seconds\n",
      "Time to load utils op: 0.0002701282501220703 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002460479736328125 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002617835998535156 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003464221954345703 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0004968643188476562 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0:  54%|▌| 1000/1867 [22:59<19:55,  1.38s/it, v_num=r8ua, train/loss=2.750/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 1867/1867 [42:51<00:00,  1.38s/it, v_num=r8ua, train/loss=3.910\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▉                 | 1/10 [00:00<00:02,  3.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▊               | 2/10 [00:00<00:01,  4.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████▋             | 3/10 [00:00<00:01,  4.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|███████▌           | 4/10 [00:00<00:01,  5.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████▌         | 5/10 [00:00<00:00,  5.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|███████████▍       | 6/10 [00:01<00:00,  5.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|█████████████▎     | 7/10 [00:01<00:00,  5.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|███████████████▏   | 8/10 [00:01<00:00,  5.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|█████████████████  | 9/10 [00:01<00:00,  5.71it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 1867/1867 [43:00<00:00,  1.38s/it, v_num=r8ua, train/loss=3.910\u001b[A\n",
      "Epoch 0: 100%|█| 1867/1867 [43:00<00:00,  1.38s/it, v_num=r8ua, train/loss=3.910\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 1867/1867 [43:22<00:00,  1.39s/it, v_num=r8ua, train/loss=3.910\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▁▁▁▂▁▁▂▂▂▂▁▂▆█▂▁▂▂▁▂▁▄▂▁▇▁▁▁▂▂▁▂▂▁▂▁▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ▄▂▄▃▂▆▄▃▅▆▆▄▅▅▃▃▃▃▃▃▅▄▂▄▃▄▄▃▂▃▄▃▃▃▄█▄▁▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 49\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 1.46094\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 466\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 2.99385\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mBaseline-C - Instruct (train-ctx=4096, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/759pr8ua\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230729_111557-759pr8ua/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the instruct finetuning\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/Baseline-C-instruct.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Instruct (train-ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/Baseline-C-instruct/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 438 params 1515106304 elements\n",
      "Saving fp32 state dict to ../model/Baseline-C-Stage2.pth\n",
      "-rw-r--r-- 1 root root 5.7G Jul 29 12:01 ../model/Baseline-C-Stage2.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/Baseline-C-instruct/last.ckpt\" \"../model/Baseline-C-Stage2.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/Baseline-C-Stage2.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "--- DRAGON PROMPT ---\n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese.# Answer:\\nZhijanghünenzhuyen was a dragon, named Zisanghikers. He lived in the mountains, he was in the quest for Lord Vestris. He was born in China. He was a Buddhist dancer, poet and King of the Wolf. He was married to a Buddhist master, Vestrapen, and he and his brother were a secret lover of a powerful god. He was defeated by Zhizen, a dragon, and a dragon. Vyshimos was killed in the last month of the book \"Vyshippen\" by Vyshni and Vyshysh II. Zysholt was also a god. He was a king of Vestra. He was known to be the strongest of the dragons. He died in season 5 of Game of Thrones, and died as a result of his death.\n",
      "\n",
      "Vyshmu was the final king of the dynasty."
     ]
    }
   ],
   "source": [
    "# Lets do a quick dragon prompt validation\n",
    "!cd \"{INFERENCE_DIR}\" && python3 dragon_test.py \"../model/Baseline-C-Stage2.pth\" \"cuda fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets do a quick memory test\n",
    "# # (We dun expect this to work, as we have not finetune for memory recall, but its a baseline)\n",
    "# !python3 ../memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/Baseline-C-Stage2.pth\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
