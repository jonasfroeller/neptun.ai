{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Echo-B 1B4 (Memory Finetune)\n",
    "This continues off from `Echo-B-1B4-basemodel.ipynb` to perform the full memory finetune & testing process\n",
    "\n",
    "This is done generally in 3 Tune stages\n",
    "- Tune 1: Low ctx size (512), Training with only the input masked. This does very limited memory training, and is used primarily to train the instruction set.\n",
    "- Tune 2: Low ctx size (512), Training with instruction & input masked. This forces the actual memory training on the output tokens.\n",
    "- Tune 3: Mid ctx size (1024), stage 2, scaled up to 1024 context sizes.\n",
    "\n",
    "In all cases, the input tokens is always masked. And we intentionally use the limited word set for memory training, which matches the same wordset used in the original memory evaluation of raven pretrained models. This is intentional to serve as both consistent comparision between experiments, and resonable training time.\n",
    "\n",
    "One of the issue faced previously with an excessive large word set, is that the model would be required to see \"new words\" atleast a few time before being able to train the memory process. This drastically slowed down the process as the large word list meant the model was constantly spending time learning new words (instead of memory training).\n",
    "\n",
    "If we want to increase the number / type of words the model can handle for memory training, that can be done later as a stage 4 memory tune if needed. But that exceeds the current requirements for the memory experiment process.\n",
    "\n",
    "> This project assumes you have the rwkv-infctx conda env setup, and you are executing in that environment - see the main README.md for the conda env setup steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Download the pretrained model\n",
    "(if you want to skip the the basemodel train + instruct tune)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Init required dirs\n",
    "# !mkdir -p ../../../model/\n",
    "# !mkdir -p ../../../datapath/\n",
    "# !mkdir -p ../../../checkpoint/\n",
    "\n",
    "# # Download the Stage2.pth file\n",
    "# !rm -rf ../../../model/Echo-B-1B4-Stage2.pth\n",
    "# !cd ../../../model/ && wget https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/Echo-B-1B4-Stage2.pth\n",
    "# !ls -alh ../../../model/Echo-B-1B4-Stage2.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure your environment settings\n",
    "(!Important: you will need to rerun the below cell, if you restart your kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_1\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /home/picocreator/rwkv-proj/picocreator-memory-experiment/notebook/experiment/memory-enwiki-v2\n",
      "TRAINER_DIR: /home/picocreator/rwkv-proj/picocreator-memory-experiment/RWKV-v4neo\n",
      "PROJECT_DIR: /home/picocreator/rwkv-proj/picocreator-memory-experiment\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"(8x3090) Echo-B-1B4\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v4neo/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 1 : Simple Memory instruct finetuning\n",
    "\n",
    "- Tune 1: Low ctx size (512), Training with only the input masked. This does very limited memory training, and is used primarily to train the instruction set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 10 max words, 2500 samples - at ./dataset/word-10-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 2500 samples - at ./dataset/word-15-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 2500 samples - at ./dataset/word-20-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 5000 samples - at ./dataset/word-2-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 2500 samples - at ./dataset/word-25-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 5000 samples - at ./dataset/word-5-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 2500 samples - at ./dataset/word-40-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 2500 samples - at ./dataset/word-50-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 2500 samples - at ./dataset/word-80-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 2500 samples - at ./dataset/word-60-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 2500 samples - at ./dataset/word-100-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 2500 samples - at ./dataset/word-200-count.jsonl\n",
      "## Done ##\n",
      "total 21M\n",
      "drwxr-xr-x 2 root root  330 Jul 14 04:44 .\n",
      "drwxr-xr-x 5 root root 4.0K Jul 14 04:44 ..\n",
      "-rw-r--r-- 1 root root 614K Jul 14 04:44 word-10-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.8M Jul 14 04:44 word-100-count.jsonl\n",
      "-rw-r--r-- 1 root root 719K Jul 14 04:44 word-15-count.jsonl\n",
      "-rw-r--r-- 1 root root 836K Jul 14 04:44 word-2-count.jsonl\n",
      "-rw-r--r-- 1 root root 855K Jul 14 04:44 word-20-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.2M Jul 14 04:44 word-200-count.jsonl\n",
      "-rw-r--r-- 1 root root 969K Jul 14 04:44 word-25-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.4M Jul 14 04:44 word-40-count.jsonl\n",
      "-rw-r--r-- 1 root root 967K Jul 14 04:44 word-5-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.6M Jul 14 04:44 word-50-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.3M Jul 14 04:44 word-60-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.3M Jul 14 04:44 word-80-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ./dataset\n",
    "rm -rf ./dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "# We do a strong bias for smaller word count, to teach the concept from scratch\n",
    "# so that the model can learn the function. \n",
    "#\n",
    "# Note that all document samples, are randomized between the target word count, \n",
    "# to half of the target word count.\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-2-count.jsonl  2  5000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-5-count.jsonl  5  5000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-10-count.jsonl 10 2500 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-15-count.jsonl 15 2500 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-20-count.jsonl 20 2500 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-25-count.jsonl 25 2500 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-40-count.jsonl 40 2500 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-50-count.jsonl 50 2500 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-60-count.jsonl 80 2500 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-80-count.jsonl 80 2500 &\n",
    "\n",
    "# With a slight mix of the larger word count\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-100-count.jsonl 100 2500 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-200-count.jsonl 200 2500 &\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ./dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-0960727d51f49ca1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 6482.70it/s]\n",
      "Extracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 342.50it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-0960727d51f49ca1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 200.77it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets pre tokenize the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/Echo-B-1B4-mem-finetune-1.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/Echo-B-1B4-mem-finetune-1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 4190240465\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 4190240465\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230714_044445-5f9aziq0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m(8x3090) Echo-B-1B4 - Mem-Finetune-1 (bs=256, train-ctx=512, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/5f9aziq0\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_512_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -t 4 -std=c++17 -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=512 -c /root/picocreator-memory-experiment/RWKV-v4neo/cuda/wkv_cuda_bf16.cu -o wkv_cuda_bf16.cuda.o \n",
      "ptxas info    : 1 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf\n",
      "    6144 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 56 registers, 464 bytes cmem[0], 8 bytes cmem[2]\n",
      "ptxas info    : Compiling entry function '_Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, 424 bytes cmem[0]\n",
      "[2/3] c++ -MMD -MF wkv_op_bf16.o.d -DTORCH_EXTENSION_NAME=wkv_512_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -std=c++17 -O3 -DTmax=512 -c /root/picocreator-memory-experiment/RWKV-v4neo/cuda/wkv_op_bf16.cpp -o wkv_op_bf16.o \n",
      "[3/3] c++ wkv_op_bf16.o wkv_cuda_bf16.cuda.o -shared -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_512_bf16.so\n",
      "Loading extension module wkv_512_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-0960727d51f49ca1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 353.77it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-0960727d51f49ca1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bb872585ce30b392_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-0960727d51f49ca1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f6b9ea10bac7065c_*_of_00064.arrow\n",
      "[rank: 0] Global seed set to 4190240465                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-07-14 04:45:15,728] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 3] Global seed set to 4190240465\n",
      "[rank: 1] Global seed set to 4190240465\n",
      "[rank: 7] Global seed set to 4190240465\n",
      "[rank: 6] Global seed set to 4190240465\n",
      "[rank: 4] Global seed set to 4190240465\n",
      "[rank: 2] Global seed set to 4190240465\n",
      "[rank: 5] Global seed set to 4190240465\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "[rank: 3] Global seed set to 4190240465\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-07-14 04:45:43,431] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 4190240465\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-07-14 04:45:51,318] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 4190240465\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-07-14 04:45:51,471] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 4190240465\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-07-14 04:45:51,773] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 4190240465\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-07-14 04:45:51,837] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 4190240465\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-07-14 04:45:51,860] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 4190240465\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-07-14 04:45:51,868] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 5.000e-04 (0.0005)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0640418529510498 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10123467445373535 seconds\n",
      "Time to load fused_adam op: 0.10140156745910645 seconds\n",
      "Time to load fused_adam op: 0.1013326644897461 seconds\n",
      "Time to load fused_adam op: 0.1013331413269043 seconds\n",
      "Time to load fused_adam op: 0.10151958465576172 seconds\n",
      "Time to load fused_adam op: 0.10189390182495117 seconds\n",
      "Time to load fused_adam op: 0.10189986228942871 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06637144088745117 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10195732116699219 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1018683910369873 seconds\n",
      "Time to load utils op: 0.10200166702270508 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1017305850982666 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10190486907958984 seconds\n",
      "Time to load utils op: 0.10180854797363281 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10186219215393066 seconds\n",
      "Rank: 7 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 5 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 2 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 0 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 6 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 4 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 1 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 3 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Time to load utils op: 0.0002732276916503906 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003490447998046875 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00028014183044433594 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002765655517578125 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002837181091308594 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002808570861816406 seconds\n",
      "Time to load utils op: 0.0002779960632324219 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005145072937011719 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 51.5 M\n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 2.0 K \n",
      "3 | head   | Linear     | 51.5 M\n",
      "--------------------------------------\n",
      "1.4 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 B     Total params\n",
      "5,650.702 Total estimated model params size (MB)\n",
      "Epoch 0:  18%|▏| 800/4371 [07:09<31:57,  1.86it/s, v_num=ziq0, train/loss=4.060]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 4371/4371 [39:59<00:00,  1.82it/s, v_num=ziq0, train/loss=0.479\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|████                | 1/5 [00:00<00:00,  4.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|████████            | 2/5 [00:00<00:00,  4.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|████████████        | 3/5 [00:00<00:00,  4.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████████    | 4/5 [00:00<00:00,  4.77it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 4371/4371 [40:08<00:00,  1.82it/s, v_num=ziq0, train/loss=0.479\u001b[A\n",
      "Epoch 0: 100%|█| 4371/4371 [40:08<00:00,  1.82it/s, v_num=ziq0, train/loss=0.479\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 4371/4371 [40:21<00:00,  1.81it/s, v_num=ziq0, train/loss=0.479\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▃▁▄▂▂▁▁▁▂██▂▁▂▁▆▂▂▄▄▃▁▁▁█▂▂▁▂▇▃▁▃▁▁▃▄▄▄▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ▇█▃▆▅▆▃▆▄▅▇▂▃▅▆▂▂▅▁▇▁▆▇▆▅▁▁▇▂▁▂▅▁▂▆▂▄▅▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 36\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 2.04688\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 136\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 2.72207\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m(8x3090) Echo-B-1B4 - Mem-Finetune-1 (bs=256, train-ctx=512, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/5f9aziq0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230714_044445-5f9aziq0/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/Echo-B-1B4-mem-finetune-1.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Finetune-1 (bs=256, train-ctx=512, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/Echo-B-1B4-mem-finetune-1/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 1734 params 1412675584 elements\n",
      "Saving fp32 state dict to ../model/Echo-B-1B4-Tune1.pth\n",
      "-rw-r--r-- 1 root root 5.3G Jul 14 05:27 ../model/Echo-B-1B4-Tune1.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/Echo-B-1B4-mem-finetune-1/last.ckpt\" \\\n",
    "        \"../model/Echo-B-1B4-Tune1.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh ../model/Echo-B-1B4-Tune1.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_cuda/build.ninja...\n",
      "Building extension module wkv_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_cuda...\n",
      "RWKV_JIT_ON 1 RWKV_CUDA_ON 1 RESCALE_LAYER 0\n",
      "\n",
      "Loading /root/picocreator-memory-experiment/model/Echo-B-1B4-Tune1.pth ...\n",
      "Strategy: (total 96+1=97 layers)\n",
      "* cuda [float32, float32], store 97 layers\n",
      "0-cuda-float32-float32 1-cuda-float32-float32 2-cuda-float32-float32 3-cuda-float32-float32 4-cuda-float32-float32 5-cuda-float32-float32 6-cuda-float32-float32 7-cuda-float32-float32 8-cuda-float32-float32 9-cuda-float32-float32 10-cuda-float32-float32 11-cuda-float32-float32 12-cuda-float32-float32 13-cuda-float32-float32 14-cuda-float32-float32 15-cuda-float32-float32 16-cuda-float32-float32 17-cuda-float32-float32 18-cuda-float32-float32 19-cuda-float32-float32 20-cuda-float32-float32 21-cuda-float32-float32 22-cuda-float32-float32 23-cuda-float32-float32 24-cuda-float32-float32 25-cuda-float32-float32 26-cuda-float32-float32 27-cuda-float32-float32 28-cuda-float32-float32 29-cuda-float32-float32 30-cuda-float32-float32 31-cuda-float32-float32 32-cuda-float32-float32 33-cuda-float32-float32 34-cuda-float32-float32 35-cuda-float32-float32 36-cuda-float32-float32 37-cuda-float32-float32 38-cuda-float32-float32 39-cuda-float32-float32 40-cuda-float32-float32 41-cuda-float32-float32 42-cuda-float32-float32 43-cuda-float32-float32 44-cuda-float32-float32 45-cuda-float32-float32 46-cuda-float32-float32 47-cuda-float32-float32 48-cuda-float32-float32 49-cuda-float32-float32 50-cuda-float32-float32 51-cuda-float32-float32 52-cuda-float32-float32 53-cuda-float32-float32 54-cuda-float32-float32 55-cuda-float32-float32 56-cuda-float32-float32 57-cuda-float32-float32 58-cuda-float32-float32 59-cuda-float32-float32 60-cuda-float32-float32 61-cuda-float32-float32 62-cuda-float32-float32 63-cuda-float32-float32 64-cuda-float32-float32 65-cuda-float32-float32 66-cuda-float32-float32 67-cuda-float32-float32 68-cuda-float32-float32 69-cuda-float32-float32 70-cuda-float32-float32 71-cuda-float32-float32 72-cuda-float32-float32 73-cuda-float32-float32 74-cuda-float32-float32 75-cuda-float32-float32 76-cuda-float32-float32 77-cuda-float32-float32 78-cuda-float32-float32 79-cuda-float32-float32 80-cuda-float32-float32 81-cuda-float32-float32 82-cuda-float32-float32 83-cuda-float32-float32 84-cuda-float32-float32 85-cuda-float32-float32 86-cuda-float32-float32 87-cuda-float32-float32 88-cuda-float32-float32 89-cuda-float32-float32 90-cuda-float32-float32 91-cuda-float32-float32 92-cuda-float32-float32 93-cuda-float32-float32 94-cuda-float32-float32 95-cuda-float32-float32 96-cuda-float32-float32 \n",
      "blocks.0.att.key.weight           f32   cuda:0   1024  1024 \n",
      "blocks.0.att.output.weight        f32   cuda:0   1024  1024 \n",
      "blocks.0.att.receptance.weight    f32   cuda:0   1024  1024 \n",
      "blocks.0.att.time_mix_k           f32   cuda:0   1024       \n",
      "blocks.0.att.time_mix_r           f32   cuda:0   1024       \n",
      "blocks.0.att.time_mix_v           f32   cuda:0   1024       \n",
      "blocks.0.att.value.weight         f32   cuda:0   1024  1024 \n",
      "blocks.0.ffn.key.weight           f32   cuda:0   1024  4096 \n",
      "blocks.0.ffn.receptance.weight    f32   cuda:0   1024  1024 \n",
      "blocks.0.ffn.time_mix_k           f32   cuda:0   1024       \n",
      "blocks.0.ffn.time_mix_r           f32   cuda:0   1024       \n",
      "blocks.0.ffn.value.weight         f32   cuda:0   4096  1024 \n",
      "blocks.0.ln1.bias                 f32   cuda:0   1024       \n",
      "blocks.0.ln1.weight               f32   cuda:0   1024       \n",
      "blocks.0.ln2.bias                 f32   cuda:0   1024       \n",
      "blocks.0.ln2.weight               f32   cuda:0   1024       \n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "blocks.95.att.key.weight          f32   cuda:0   1024  1024 \n",
      "blocks.95.att.output.weight       f32   cuda:0   1024  1024 \n",
      "blocks.95.att.receptance.weight   f32   cuda:0   1024  1024 \n",
      "blocks.95.att.time_mix_k          f32   cuda:0   1024       \n",
      "blocks.95.att.time_mix_r          f32   cuda:0   1024       \n",
      "blocks.95.att.time_mix_v          f32   cuda:0   1024       \n",
      "blocks.95.att.value.weight        f32   cuda:0   1024  1024 \n",
      "blocks.95.ffn.key.weight          f32   cuda:0   1024  4096 \n",
      "blocks.95.ffn.receptance.weight   f32   cuda:0   1024  1024 \n",
      "blocks.95.ffn.time_mix_k          f32   cuda:0   1024       \n",
      "blocks.95.ffn.time_mix_r          f32   cuda:0   1024       \n",
      "blocks.95.ffn.value.weight        f32   cuda:0   4096  1024 \n",
      "blocks.95.ln1.bias                f32   cuda:0   1024       \n",
      "blocks.95.ln1.weight              f32   cuda:0   1024       \n",
      "blocks.95.ln2.bias                f32   cuda:0   1024       \n",
      "blocks.95.ln2.weight              f32   cuda:0   1024       \n",
      "emb.weight                        f32      cpu  50277  1024 \n",
      "head.weight                       f32   cuda:0   1024 50277 \n",
      "ln_out.bias                       f32   cuda:0   1024       \n",
      "ln_out.weight                     f32   cuda:0   1024       \n",
      "blocks.0.att.time_decay           f32   cuda:0   1024       \n",
      "..............................................................................................\n",
      "blocks.95.att.time_decay          f32   cuda:0   1024       \n",
      "blocks.0.att.time_first           f32   cuda:0   1024       \n",
      "..............................................................................................\n",
      "blocks.95.att.time_first          f32   cuda:0   1024       \n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "Model validation at 5 tokens : 100.0% similarity, with 5 matched token, and 0 token mismatch\n",
      "Model validation at 10 tokens : 100.0% similarity, with 10 matched token, and 0 token mismatch\n",
      "Model validation at 15 tokens : 100.0% similarity, with 15 matched token, and 0 token mismatch\n",
      "Model validation at 20 tokens : 90.0% similarity, with 18 matched token, and 2 token mismatch\n",
      "Model validation at 25 tokens : 84.0% similarity, with 21 matched token, and 4 token mismatch\n",
      "Model validation at 30 tokens : 83.33333333333334% similarity, with 25 matched token, and 5 token mismatch\n",
      "Model validation at 35 tokens : 77.14285714285715% similarity, with 27 matched token, and 8 token mismatch\n",
      "Model validation at 40 tokens : 65.0% similarity, with 26 matched token, and 14 token mismatch\n",
      "Model validation at 45 tokens : 55.55555555555556% similarity, with 25 matched token, and 20 token mismatch\n",
      "Model validation at 50 tokens : 56.00000000000001% similarity, with 28 matched token, and 22 token mismatch\n",
      "Model validation at 55 tokens : 50.90909090909091% similarity, with 28 matched token, and 27 token mismatch\n",
      "Model validation at 60 tokens : 46.666666666666664% similarity, with 28 matched token, and 32 token mismatch\n",
      "Model validation at 65 tokens : 46.15384615384615% similarity, with 30 matched token, and 35 token mismatch\n",
      "Model validation at 70 tokens : 41.42857142857143% similarity, with 29 matched token, and 41 token mismatch\n",
      "Model validation at 75 tokens : 42.66666666666667% similarity, with 32 matched token, and 43 token mismatch\n",
      "Model validation at 80 tokens : 37.5% similarity, with 30 matched token, and 50 token mismatch\n",
      "Model validation at 85 tokens : 30.58823529411765% similarity, with 26 matched token, and 59 token mismatch\n",
      "Model validation at 90 tokens : 26.666666666666668% similarity, with 24 matched token, and 66 token mismatch\n",
      "Model validation at 95 tokens : 25.263157894736842% similarity, with 24 matched token, and 71 token mismatch\n",
      "Model validation at 100 tokens : 25.0% similarity, with 25 matched token, and 75 token mismatch\n",
      "Model validation at 110 tokens : 21.818181818181817% similarity, with 24 matched token, and 86 token mismatch\n",
      "Model validation at 120 tokens : 19.166666666666668% similarity, with 23 matched token, and 97 token mismatch\n",
      "Model validation at 130 tokens : 16.153846153846153% similarity, with 21 matched token, and 109 token mismatch\n",
      "Model validation at 140 tokens : 15.0% similarity, with 21 matched token, and 119 token mismatch\n",
      "Model validation at 150 tokens : 15.333333333333332% similarity, with 23 matched token, and 127 token mismatch\n",
      "Model validation at 175 tokens : 13.142857142857142% similarity, with 23 matched token, and 152 token mismatch\n",
      "Model validation at 200 tokens : 10.5% similarity, with 21 matched token, and 179 token mismatch\n",
      "Model validation at 225 tokens : 9.777777777777779% similarity, with 22 matched token, and 203 token mismatch\n",
      "Model validation at 250 tokens : 8.799999999999999% similarity, with 22 matched token, and 228 token mismatch\n",
      "Model validation at 275 tokens : 7.636363636363637% similarity, with 21 matched token, and 254 token mismatch\n",
      "Model validation at 300 tokens : 7.666666666666666% similarity, with 23 matched token, and 277 token mismatch\n",
      "Model validation at 325 tokens : 6.153846153846154% similarity, with 20 matched token, and 305 token mismatch\n",
      "Model validation at 350 tokens : 5.714285714285714% similarity, with 20 matched token, and 330 token mismatch\n",
      "Model validation at 375 tokens : 5.6000000000000005% similarity, with 21 matched token, and 354 token mismatch\n",
      "Model validation at 400 tokens : 5.25% similarity, with 21 matched token, and 379 token mismatch\n",
      "Model validation at 425 tokens : 4.705882352941177% similarity, with 20 matched token, and 405 token mismatch\n",
      "Model validation at 450 tokens : 4.444444444444445% similarity, with 20 matched token, and 430 token mismatch\n",
      "Model validation at 475 tokens : 4.2105263157894735% similarity, with 20 matched token, and 455 token mismatch\n",
      "Model validation at 500 tokens : 4.0% similarity, with 20 matched token, and 480 token mismatch\n",
      "Model validation at 550 tokens : 4.0% similarity, with 22 matched token, and 528 token mismatch\n",
      "Model validation at 600 tokens : 3.5000000000000004% similarity, with 21 matched token, and 579 token mismatch\n",
      "Model validation at 650 tokens : 3.3846153846153846% similarity, with 22 matched token, and 628 token mismatch\n",
      "Model validation at 700 tokens : 3.1428571428571432% similarity, with 22 matched token, and 678 token mismatch\n"
     ]
    }
   ],
   "source": [
    "# Lets do a memory eval\n",
    "#\n",
    "# Note that the expected performance \"is not that great\", as the model seems to be only loosely\n",
    "# learning the memorization task, and the instruction propmt. And is seem to be acting more\n",
    "# like an RNG based on the instruct. (Instead of the actual memorization task)\n",
    "!python3 ./memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/Echo-B-1B4-Tune1.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 2 : Low ctx size (512), memory training\n",
    "\n",
    "- Tune 2: Low ctx size (512), Training with instruction & input masked. This forces the actual memory training on the output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated JSONL file with - 2 max words, 5000 samples - at ./dataset/word-2-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 5000 samples - at ./dataset/gen-word-5-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 5000 samples - at ./dataset/gen-word-10-count.jsonl\n",
      "Generated a single JSONL file with 670 samples (50 token repeat) - 200 max words - at ./dataset/shuffle-word-200-count.jsonl\n",
      "Generated a single JSONL file with 1315 samples (50 token repeat) - 100 max words - at ./dataset/shuffle-word-100-count.jsonl\n",
      "Generated a single JSONL file with 1767 samples (50 token repeat) - 75 max words - at ./dataset/shuffle-word-75-count.jsonl\n",
      "Generated JSONL file with - 45 max words, 5000 samples - at ./dataset/gen-word-45-count.jsonl\n",
      "Generated a single JSONL file with 3177 samples (30 token repeat) - 25 max words - at ./dataset/shuffle-word-25-count.jsonl\n",
      "Generated a single JSONL file with 3536 samples (20 token repeat) - 15 max words - at ./dataset/shuffle-word-15-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 5000 samples - at ./dataset/gen-word-20-count.jsonl\n",
      "Generated a single JSONL file with 5203 samples (20 token repeat) - 10 max words - at ./dataset/shuffle-word-10-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 5000 samples - at ./dataset/gen-word-15-count.jsonl\n",
      "Generated a single JSONL file with 2620 samples (50 token repeat) - 50 max words - at ./dataset/shuffle-word-50-count.jsonl\n",
      "Generated JSONL file with - 30 max words, 5000 samples - at ./dataset/gen-word-30-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 5000 samples - at ./dataset/gen-word-40-count.jsonl\n",
      "Generated JSONL file with - 35 max words, 5000 samples - at ./dataset/gen-word-35-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 5000 samples - at ./dataset/gen-word-50-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 5000 samples - at ./dataset/gen-word-25-count.jsonl\n",
      "Generated JSONL file with - 55 max words, 5000 samples - at ./dataset/gen-word-55-count.jsonl\n",
      "Generated JSONL file with - 70 max words, 5000 samples - at ./dataset/gen-word-70-count.jsonl\n",
      "Generated JSONL file with - 65 max words, 5000 samples - at ./dataset/gen-word-65-count.jsonl\n",
      "Generated JSONL file with - 75 max words, 5000 samples - at ./dataset/gen-word-75-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 5000 samples - at ./dataset/gen-word-80-count.jsonl\n",
      "Generated JSONL file with - 60 max words, 5000 samples - at ./dataset/gen-word-60-count.jsonl\n",
      "Generated JSONL file with - 90 max words, 5000 samples - at ./dataset/gen-word-90-count.jsonl\n",
      "Generated JSONL file with - 95 max words, 5000 samples - at ./dataset/gen-word-95-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 5000 samples - at ./dataset/word-100-count.jsonl\n",
      "Generated JSONL file with - 85 max words, 5000 samples - at ./dataset/gen-word-85-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 5000 samples - at ./dataset/word-200-count.jsonl\n",
      "## Done ##\n",
      "total 47K\n",
      "drwxrwxr-x 2 picocreator picocreator   31 Jul 14 18:08 .\n",
      "drwxrwxr-x 4 picocreator picocreator   12 Jul 14 17:20 ..\n",
      "-rw-rw-r-- 1 picocreator picocreator 985K Jul 14 18:08 gen-word-10-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 1.2M Jul 14 18:08 gen-word-15-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 1.5M Jul 14 18:08 gen-word-20-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 1.7M Jul 14 18:08 gen-word-25-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 2.0M Jul 14 18:08 gen-word-30-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 2.2M Jul 14 18:08 gen-word-35-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 2.4M Jul 14 18:08 gen-word-40-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 2.6M Jul 14 18:08 gen-word-45-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 2.9M Jul 14 18:08 gen-word-50-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 3.1M Jul 14 18:08 gen-word-55-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 728K Jul 14 18:08 gen-word-5-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 3.4M Jul 14 18:08 gen-word-60-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 3.6M Jul 14 18:08 gen-word-65-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 3.8M Jul 14 18:08 gen-word-70-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 4.1M Jul 14 18:08 gen-word-75-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 4.3M Jul 14 18:08 gen-word-80-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 4.5M Jul 14 18:08 gen-word-85-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 4.8M Jul 14 18:08 gen-word-90-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 5.0M Jul 14 18:08 gen-word-95-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 1.4M Jul 14 18:08 shuffle-word-100-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 1.1M Jul 14 18:08 shuffle-word-10-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 854K Jul 14 18:08 shuffle-word-15-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 1.4M Jul 14 18:08 shuffle-word-200-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 1.1M Jul 14 18:08 shuffle-word-25-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 1.5M Jul 14 18:08 shuffle-word-50-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 1.5M Jul 14 18:08 shuffle-word-75-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 5.2M Jul 14 18:08 word-100-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator  10M Jul 14 18:08 word-200-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 598K Jul 14 18:08 word-2-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ./dataset\n",
    "rm -rf ./dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# We switch over to fully masked instruct+input, to properly learn the memorization task\n",
    "#\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-2-count.jsonl  2  5000 &\n",
    "for i in {5..95..5} \n",
    "do\n",
    "    python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/gen-word-$i-count.jsonl $i 5000 & \n",
    "done\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-100-count.jsonl 100 5000 &\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-200-count.jsonl 200 5000 &\n",
    "\n",
    "#\n",
    "# We mixin the shuffled word list, so that we ensure all words / tokens are learned\n",
    "# however this might intrduce an exclusion bias (if seen this word, never repeat it), \n",
    "# so we limit the mixture of this data samples\n",
    "#\n",
    "python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-10-count.jsonl 10 20 &\n",
    "python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-15-count.jsonl 15 20 &\n",
    "python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-25-count.jsonl 25 30 &\n",
    "python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-50-count.jsonl 50 50 &\n",
    "python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-75-count.jsonl 75 50 &\n",
    "python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-100-count.jsonl 100 50 &\n",
    "python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-200-count.jsonl 200 50 &\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ./dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████████████| 19/19 [00:00<00:00, 21738.07it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-b31fe71036e5c4c6/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 4946.11it/s]\n",
      "Extracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 151.82it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-b31fe71036e5c4c6/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 143.99it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets pre tokenize the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/Echo-B-1B4-mem-finetune-2.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/Echo-B-1B4-mem-finetune-2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 3535270375\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 3535270375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230714_063158-a8a2hi9b\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m(8x3090) Echo-B-1B4 - Mem-Finetune-2 (bs=256, train-ctx=512, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/a8a2hi9b\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|██████████████████| 19/19 [00:00<00:00, 46960.39it/s]\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-b31fe71036e5c4c6/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 202.08it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b31fe71036e5c4c6/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-ccaf30eae302ee97_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-b31fe71036e5c4c6/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-7f84ff1d110ef86c_*_of_00064.arrow\n",
      "[rank: 0] Global seed set to 3535270375                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-07-14 06:32:12,979] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 5] Global seed set to 3535270375\n",
      "[rank: 6] Global seed set to 3535270375\n",
      "[rank: 1] Global seed set to 3535270375\n",
      "[rank: 7] Global seed set to 3535270375\n",
      "[rank: 3] Global seed set to 3535270375\n",
      "[rank: 4] Global seed set to 3535270375\n",
      "[rank: 2] Global seed set to 3535270375\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "[rank: 5] Global seed set to 3535270375\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-07-14 06:32:37,353] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 3535270375\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-07-14 06:32:47,663] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 3535270375\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-07-14 06:32:48,166] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 3535270375\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-07-14 06:32:48,241] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 3535270375\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-07-14 06:32:48,331] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 3535270375\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-07-14 06:32:48,353] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 3535270375\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-07-14 06:32:48,359] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  5.000e-04 (0.0005)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06525206565856934 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10105466842651367 seconds\n",
      "Time to load fused_adam op: 0.10094952583312988 seconds\n",
      "Time to load fused_adam op: 0.10105681419372559 seconds\n",
      "Time to load fused_adam op: 0.10121297836303711 seconds\n",
      "Time to load fused_adam op: 0.10131669044494629 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Time to load fused_adam op: 0.1018373966217041 seconds\n",
      "Time to load fused_adam op: 0.1018373966217041 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06574559211730957 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10179424285888672 seconds\n",
      "Time to load utils op: 0.1017754077911377 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1015315055847168 seconds\n",
      "Time to load utils op: 0.10181069374084473 seconds\n",
      "Time to load utils op: 0.10176587104797363 seconds\n",
      "Time to load utils op: 0.10180997848510742 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10220813751220703 seconds\n",
      "Rank: 1 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 7 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 2 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 6 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 5 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 4 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 0 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 3 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002951622009277344 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00028133392333984375 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002722740173339844 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002899169921875 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00027489662170410156 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002789497375488281 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00021767616271972656 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005116462707519531 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 51.5 M\n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 2.0 K \n",
      "3 | head   | Linear     | 51.5 M\n",
      "--------------------------------------\n",
      "1.4 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 B     Total params\n",
      "5,650.702 Total estimated model params size (MB)\n",
      "Epoch 0:   5%| | 800/14785 [07:11<2:05:40,  1.85it/s, v_num=hi9b, train/loss=0.0/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 14785/14785 [2:16:48<00:00,  1.80it/s, v_num=hi9b, train/loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▎                 | 1/15 [00:00<00:03,  4.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▌                | 2/15 [00:00<00:02,  4.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▊               | 3/15 [00:00<00:02,  4.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|█████              | 4/15 [00:00<00:02,  4.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▎            | 5/15 [00:01<00:02,  4.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|███████▌           | 6/15 [00:01<00:01,  4.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|████████▊          | 7/15 [00:01<00:01,  4.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|██████████▏        | 8/15 [00:01<00:01,  4.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|███████████▍       | 9/15 [00:01<00:01,  4.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 10/15 [00:02<00:01,  4.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|█████████████▏    | 11/15 [00:02<00:00,  4.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|██████████████▍   | 12/15 [00:02<00:00,  4.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|███████████████▌  | 13/15 [00:02<00:00,  4.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|████████████████▊ | 14/15 [00:02<00:00,  4.78it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 14785/14785 [2:16:58<00:00,  1.80it/s, v_num=hi9b, train/loss=0\u001b[A\n",
      "Epoch 0: 100%|█| 14785/14785 [2:16:58<00:00,  1.80it/s, v_num=hi9b, train/loss=0\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 14785/14785 [2:17:10<00:00,  1.80it/s, v_num=hi9b, train/loss=0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▂▂▃▃▃▅▄▄▃▃▃▂▅▅▂▃█▄▄▃▅▅▁▄▁▆▂▄▂▆▂▂▂▂█▄▄▁▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ▇▁▁▂▂▂▁▃▁▂▁█▇▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 44\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 112\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.00111\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 0.18127\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m(8x3090) Echo-B-1B4 - Mem-Finetune-2 (bs=256, train-ctx=512, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/a8a2hi9b\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230714_063158-a8a2hi9b/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/Echo-B-1B4-mem-finetune-2.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Finetune-2 (bs=256, train-ctx=512, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/Echo-B-1B4-mem-finetune-2/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 1734 params 1412675584 elements\n",
      "Saving fp32 state dict to ../model/Echo-B-1B4-Tune2.pth\n",
      "-rw-r--r-- 1 root root 5.3G Jul 14 08:51 ../model/Echo-B-1B4-Tune2.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/Echo-B-1B4-mem-finetune-2/last.ckpt\" \\\n",
    "        \"../model/Echo-B-1B4-Tune2.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh ../model/Echo-B-1B4-Tune2.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_cuda/build.ninja...\n",
      "Building extension module wkv_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_cuda...\n",
      "RWKV_JIT_ON 1 RWKV_CUDA_ON 1 RESCALE_LAYER 0\n",
      "\n",
      "Loading /root/picocreator-memory-experiment/model/Echo-B-1B4-Tune2.pth ...\n",
      "Strategy: (total 96+1=97 layers)\n",
      "* cuda [float32, float32], store 97 layers\n",
      "0-cuda-float32-float32 1-cuda-float32-float32 2-cuda-float32-float32 3-cuda-float32-float32 4-cuda-float32-float32 5-cuda-float32-float32 6-cuda-float32-float32 7-cuda-float32-float32 8-cuda-float32-float32 9-cuda-float32-float32 10-cuda-float32-float32 11-cuda-float32-float32 12-cuda-float32-float32 13-cuda-float32-float32 14-cuda-float32-float32 15-cuda-float32-float32 16-cuda-float32-float32 17-cuda-float32-float32 18-cuda-float32-float32 19-cuda-float32-float32 20-cuda-float32-float32 21-cuda-float32-float32 22-cuda-float32-float32 23-cuda-float32-float32 24-cuda-float32-float32 25-cuda-float32-float32 26-cuda-float32-float32 27-cuda-float32-float32 28-cuda-float32-float32 29-cuda-float32-float32 30-cuda-float32-float32 31-cuda-float32-float32 32-cuda-float32-float32 33-cuda-float32-float32 34-cuda-float32-float32 35-cuda-float32-float32 36-cuda-float32-float32 37-cuda-float32-float32 38-cuda-float32-float32 39-cuda-float32-float32 40-cuda-float32-float32 41-cuda-float32-float32 42-cuda-float32-float32 43-cuda-float32-float32 44-cuda-float32-float32 45-cuda-float32-float32 46-cuda-float32-float32 47-cuda-float32-float32 48-cuda-float32-float32 49-cuda-float32-float32 50-cuda-float32-float32 51-cuda-float32-float32 52-cuda-float32-float32 53-cuda-float32-float32 54-cuda-float32-float32 55-cuda-float32-float32 56-cuda-float32-float32 57-cuda-float32-float32 58-cuda-float32-float32 59-cuda-float32-float32 60-cuda-float32-float32 61-cuda-float32-float32 62-cuda-float32-float32 63-cuda-float32-float32 64-cuda-float32-float32 65-cuda-float32-float32 66-cuda-float32-float32 67-cuda-float32-float32 68-cuda-float32-float32 69-cuda-float32-float32 70-cuda-float32-float32 71-cuda-float32-float32 72-cuda-float32-float32 73-cuda-float32-float32 74-cuda-float32-float32 75-cuda-float32-float32 76-cuda-float32-float32 77-cuda-float32-float32 78-cuda-float32-float32 79-cuda-float32-float32 80-cuda-float32-float32 81-cuda-float32-float32 82-cuda-float32-float32 83-cuda-float32-float32 84-cuda-float32-float32 85-cuda-float32-float32 86-cuda-float32-float32 87-cuda-float32-float32 88-cuda-float32-float32 89-cuda-float32-float32 90-cuda-float32-float32 91-cuda-float32-float32 92-cuda-float32-float32 93-cuda-float32-float32 94-cuda-float32-float32 95-cuda-float32-float32 96-cuda-float32-float32 \n",
      "blocks.0.att.key.weight           f32   cuda:0   1024  1024 \n",
      "blocks.0.att.output.weight        f32   cuda:0   1024  1024 \n",
      "blocks.0.att.receptance.weight    f32   cuda:0   1024  1024 \n",
      "blocks.0.att.time_mix_k           f32   cuda:0   1024       \n",
      "blocks.0.att.time_mix_r           f32   cuda:0   1024       \n",
      "blocks.0.att.time_mix_v           f32   cuda:0   1024       \n",
      "blocks.0.att.value.weight         f32   cuda:0   1024  1024 \n",
      "blocks.0.ffn.key.weight           f32   cuda:0   1024  4096 \n",
      "blocks.0.ffn.receptance.weight    f32   cuda:0   1024  1024 \n",
      "blocks.0.ffn.time_mix_k           f32   cuda:0   1024       \n",
      "blocks.0.ffn.time_mix_r           f32   cuda:0   1024       \n",
      "blocks.0.ffn.value.weight         f32   cuda:0   4096  1024 \n",
      "blocks.0.ln1.bias                 f32   cuda:0   1024       \n",
      "blocks.0.ln1.weight               f32   cuda:0   1024       \n",
      "blocks.0.ln2.bias                 f32   cuda:0   1024       \n",
      "blocks.0.ln2.weight               f32   cuda:0   1024       \n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "blocks.95.att.key.weight          f32   cuda:0   1024  1024 \n",
      "blocks.95.att.output.weight       f32   cuda:0   1024  1024 \n",
      "blocks.95.att.receptance.weight   f32   cuda:0   1024  1024 \n",
      "blocks.95.att.time_mix_k          f32   cuda:0   1024       \n",
      "blocks.95.att.time_mix_r          f32   cuda:0   1024       \n",
      "blocks.95.att.time_mix_v          f32   cuda:0   1024       \n",
      "blocks.95.att.value.weight        f32   cuda:0   1024  1024 \n",
      "blocks.95.ffn.key.weight          f32   cuda:0   1024  4096 \n",
      "blocks.95.ffn.receptance.weight   f32   cuda:0   1024  1024 \n",
      "blocks.95.ffn.time_mix_k          f32   cuda:0   1024       \n",
      "blocks.95.ffn.time_mix_r          f32   cuda:0   1024       \n",
      "blocks.95.ffn.value.weight        f32   cuda:0   4096  1024 \n",
      "blocks.95.ln1.bias                f32   cuda:0   1024       \n",
      "blocks.95.ln1.weight              f32   cuda:0   1024       \n",
      "blocks.95.ln2.bias                f32   cuda:0   1024       \n",
      "blocks.95.ln2.weight              f32   cuda:0   1024       \n",
      "emb.weight                        f32      cpu  50277  1024 \n",
      "head.weight                       f32   cuda:0   1024 50277 \n",
      "ln_out.bias                       f32   cuda:0   1024       \n",
      "ln_out.weight                     f32   cuda:0   1024       \n",
      "blocks.0.att.time_decay           f32   cuda:0   1024       \n",
      "..............................................................................................\n",
      "blocks.95.att.time_decay          f32   cuda:0   1024       \n",
      "blocks.0.att.time_first           f32   cuda:0   1024       \n",
      "..............................................................................................\n",
      "blocks.95.att.time_first          f32   cuda:0   1024       \n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "Model validation at 5 tokens : 100.0% similarity, with 5 matched token, and 0 token mismatch\n",
      "Model validation at 10 tokens : 100.0% similarity, with 10 matched token, and 0 token mismatch\n",
      "Model validation at 15 tokens : 100.0% similarity, with 15 matched token, and 0 token mismatch\n",
      "Model validation at 20 tokens : 100.0% similarity, with 20 matched token, and 0 token mismatch\n",
      "Model validation at 25 tokens : 100.0% similarity, with 25 matched token, and 0 token mismatch\n",
      "Model validation at 30 tokens : 100.0% similarity, with 30 matched token, and 0 token mismatch\n",
      "Model validation at 35 tokens : 100.0% similarity, with 35 matched token, and 0 token mismatch\n",
      "Model validation at 40 tokens : 100.0% similarity, with 40 matched token, and 0 token mismatch\n",
      "Model validation at 45 tokens : 100.0% similarity, with 45 matched token, and 0 token mismatch\n",
      "Model validation at 50 tokens : 100.0% similarity, with 50 matched token, and 0 token mismatch\n",
      "Model validation at 55 tokens : 96.36363636363636% similarity, with 53 matched token, and 2 token mismatch\n",
      "Model validation at 60 tokens : 98.33333333333333% similarity, with 59 matched token, and 1 token mismatch\n",
      "Model validation at 65 tokens : 98.46153846153847% similarity, with 64 matched token, and 1 token mismatch\n",
      "Model validation at 70 tokens : 97.14285714285714% similarity, with 68 matched token, and 2 token mismatch\n",
      "Model validation at 75 tokens : 94.66666666666667% similarity, with 71 matched token, and 4 token mismatch\n",
      "Model validation at 80 tokens : 92.5% similarity, with 74 matched token, and 6 token mismatch\n",
      "Model validation at 85 tokens : 87.05882352941177% similarity, with 74 matched token, and 11 token mismatch\n",
      "Model validation at 90 tokens : 88.88888888888889% similarity, with 80 matched token, and 10 token mismatch\n",
      "Model validation at 95 tokens : 84.21052631578947% similarity, with 80 matched token, and 15 token mismatch\n",
      "Model validation at 100 tokens : 80.0% similarity, with 80 matched token, and 20 token mismatch\n",
      "Model validation at 110 tokens : 72.72727272727273% similarity, with 80 matched token, and 30 token mismatch\n",
      "Model validation at 120 tokens : 64.16666666666667% similarity, with 77 matched token, and 43 token mismatch\n",
      "Model validation at 130 tokens : 59.23076923076923% similarity, with 77 matched token, and 53 token mismatch\n",
      "Model validation at 140 tokens : 55.00000000000001% similarity, with 77 matched token, and 63 token mismatch\n",
      "Model validation at 150 tokens : 52.666666666666664% similarity, with 79 matched token, and 71 token mismatch\n",
      "Model validation at 175 tokens : 45.14285714285714% similarity, with 79 matched token, and 96 token mismatch\n",
      "Model validation at 200 tokens : 39.0% similarity, with 78 matched token, and 122 token mismatch\n",
      "Model validation at 225 tokens : 33.33333333333333% similarity, with 75 matched token, and 150 token mismatch\n",
      "Model validation at 250 tokens : 28.000000000000004% similarity, with 70 matched token, and 180 token mismatch\n",
      "Model validation at 275 tokens : 25.09090909090909% similarity, with 69 matched token, and 206 token mismatch\n",
      "Model validation at 300 tokens : 22.0% similarity, with 66 matched token, and 234 token mismatch\n",
      "Model validation at 325 tokens : 20.0% similarity, with 65 matched token, and 260 token mismatch\n",
      "Model validation at 350 tokens : 16.57142857142857% similarity, with 58 matched token, and 292 token mismatch\n",
      "Model validation at 375 tokens : 13.866666666666665% similarity, with 52 matched token, and 323 token mismatch\n",
      "Model validation at 400 tokens : 12.5% similarity, with 50 matched token, and 350 token mismatch\n",
      "Model validation at 425 tokens : 11.058823529411764% similarity, with 47 matched token, and 378 token mismatch\n",
      "Model validation at 450 tokens : 10.666666666666668% similarity, with 48 matched token, and 402 token mismatch\n",
      "Model validation at 475 tokens : 9.68421052631579% similarity, with 46 matched token, and 429 token mismatch\n",
      "Model validation at 500 tokens : 8.6% similarity, with 43 matched token, and 457 token mismatch\n",
      "Model validation at 550 tokens : 7.2727272727272725% similarity, with 40 matched token, and 510 token mismatch\n",
      "Model validation at 600 tokens : 6.666666666666667% similarity, with 40 matched token, and 560 token mismatch\n",
      "Model validation at 650 tokens : 5.6923076923076925% similarity, with 37 matched token, and 613 token mismatch\n",
      "Model validation at 700 tokens : 4.571428571428571% similarity, with 32 matched token, and 668 token mismatch\n"
     ]
    }
   ],
   "source": [
    "# Lets do a memory eval \n",
    "#\n",
    "# While not at its full potential, its memory ability should start emerging\n",
    "#\n",
    "!python3 ./memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/Echo-B-1B4-Tune2.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 3 : Ramping up the ctx size (1024), memory training\n",
    "\n",
    "- Tune 3: Mid ctx size (1024), same as tune 2, but extended in context size\n",
    "\n",
    "This intentionally a much larger dataset, and lower learning rate to help ensure we push the model to its absolute limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 10 max words, 1000 samples - at ./dataset/gen-word-10-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 1000 samples - at ./dataset/word-2-count.jsonl\n",
      "Generated a single JSONL file with 881 samples (10 token repeat) - 30 max words - at ./dataset/shuffle-word-30-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 1000 samples - at ./dataset/gen-word-25-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 1000 samples - at ./dataset/gen-word-5-count.jsonl\n",
      "Generated JSONL file with - 45 max words, 1000 samples - at ./dataset/gen-word-45-count.jsonl\n",
      "Generated JSONL file with - 30 max words, 1000 samples - at ./dataset/gen-word-30-count.jsonl\n",
      "Generated a single JSONL file with 1305 samples (10 token repeat) - 20 max words - at ./dataset/shuffle-word-20-count.jsonl\n",
      "Generated a single JSONL file with 1070 samples (10 token repeat) - 25 max words - at ./dataset/shuffle-word-25-count.jsonl\n",
      "Generated a single JSONL file with 583 samples (10 token repeat) - 45 max words - at ./dataset/shuffle-word-45-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 1000 samples - at ./dataset/gen-word-20-count.jsonl\n",
      "Generated a single JSONL file with 712 samples (20 token repeat) - 75 max words - at ./dataset/shuffle-word-75-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 1000 samples - at ./dataset/gen-word-15-count.jsonl\n",
      "Generated a single JSONL file with 651 samples (10 token repeat) - 40 max words - at ./dataset/shuffle-word-40-count.jsonl\n",
      "Generated a single JSONL file with 656 samples (20 token repeat) - 80 max words - at ./dataset/shuffle-word-80-count.jsonl\n",
      "Generated a single JSONL file with 381 samples (20 token repeat) - 115 max words - at ./dataset/shuffle-word-115-count.jsonl\n",
      "Generated a single JSONL file with 743 samples (10 token repeat) - 35 max words - at ./dataset/shuffle-word-35-count.jsonl\n",
      "Generated JSONL file with - 35 max words, 1000 samples - at ./dataset/gen-word-35-count.jsonl\n",
      "Generated a single JSONL file with 277 samples (20 token repeat) - 185 max words - at ./dataset/shuffle-word-185-count.jsonl\n",
      "Generated a single JSONL file with 399 samples (20 token repeat) - 110 max words - at ./dataset/shuffle-word-110-count.jsonl\n",
      "Generated a single JSONL file with 411 samples (20 token repeat) - 105 max words - at ./dataset/shuffle-word-105-count.jsonl\n",
      "Generated a single JSONL file with 753 samples (20 token repeat) - 70 max words - at ./dataset/shuffle-word-70-count.jsonl\n",
      "Generated a single JSONL file with 1055 samples (20 token repeat) - 50 max words - at ./dataset/shuffle-word-50-count.jsonl\n",
      "Generated a single JSONL file with 817 samples (20 token repeat) - 65 max words - at ./dataset/shuffle-word-65-count.jsonl\n",
      "Generated a single JSONL file with 962 samples (20 token repeat) - 55 max words - at ./dataset/shuffle-word-55-count.jsonl\n",
      "Generated a single JSONL file with 633 samples (20 token repeat) - 85 max words - at ./dataset/shuffle-word-85-count.jsonl\n",
      "Generated a single JSONL file with 1772 samples (10 token repeat) - 15 max words - at ./dataset/shuffle-word-15-count.jsonl\n",
      "Generated a single JSONL file with 325 samples (20 token repeat) - 140 max words - at ./dataset/shuffle-word-140-count.jsonl\n",
      "Generated a single JSONL file with 360 samples (20 token repeat) - 125 max words - at ./dataset/shuffle-word-125-count.jsonl\n",
      "Generated a single JSONL file with 115 samples (20 token repeat) - 440 max words - at ./dataset/shuffle-word-440-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 410 max words - at ./dataset/shuffle-word-410-count.jsonl\n",
      "Generated a single JSONL file with 875 samples (20 token repeat) - 60 max words - at ./dataset/shuffle-word-60-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 385 max words - at ./dataset/shuffle-word-385-count.jsonl\n",
      "Generated a single JSONL file with 5608 samples (10 token repeat) - 5 max words - at ./dataset/shuffle-word-5-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 360 max words - at ./dataset/shuffle-word-360-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 375 max words - at ./dataset/shuffle-word-375-count.jsonl\n",
      "Generated a single JSONL file with 182 samples (20 token repeat) - 300 max words - at ./dataset/shuffle-word-300-count.jsonl\n",
      "Generated a single JSONL file with 184 samples (20 token repeat) - 285 max words - at ./dataset/shuffle-word-285-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 390 max words - at ./dataset/shuffle-word-390-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 405 max words - at ./dataset/shuffle-word-405-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 2000 samples - at ./dataset/gen-word-50-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 450 max words - at ./dataset/shuffle-word-450-count.jsonl\n",
      "Generated JSONL file with - 60 max words, 2000 samples - at ./dataset/gen-word-60-count.jsonl\n",
      "Generated JSONL file with - 55 max words, 2000 samples - at ./dataset/gen-word-55-count.jsonl\n",
      "Generated a single JSONL file with 2610 samples (10 token repeat) - 10 max words - at ./dataset/shuffle-word-10-count.jsonl\n",
      "Generated JSONL file with - 70 max words, 2000 samples - at ./dataset/gen-word-70-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 1000 samples - at ./dataset/gen-word-40-count.jsonl\n",
      "Generated a single JSONL file with 365 samples (20 token repeat) - 120 max words - at ./dataset/shuffle-word-120-count.jsonl\n",
      "Generated a single JSONL file with 527 samples (20 token repeat) - 100 max words - at ./dataset/shuffle-word-100-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 345 max words - at ./dataset/shuffle-word-345-count.jsonl\n",
      "Generated a single JSONL file with 585 samples (20 token repeat) - 90 max words - at ./dataset/shuffle-word-90-count.jsonl\n",
      "Generated a single JSONL file with 200 samples (20 token repeat) - 220 max words - at ./dataset/shuffle-word-220-count.jsonl\n",
      "Generated a single JSONL file with 344 samples (20 token repeat) - 130 max words - at ./dataset/shuffle-word-130-count.jsonl\n",
      "Generated a single JSONL file with 185 samples (20 token repeat) - 275 max words - at ./dataset/shuffle-word-275-count.jsonl\n",
      "Generated a single JSONL file with 556 samples (20 token repeat) - 95 max words - at ./dataset/shuffle-word-95-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 400 max words - at ./dataset/shuffle-word-400-count.jsonl\n",
      "Generated a single JSONL file with 184 samples (20 token repeat) - 280 max words - at ./dataset/shuffle-word-280-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 355 max words - at ./dataset/shuffle-word-355-count.jsonl\n",
      "Generated JSONL file with - 65 max words, 2000 samples - at ./dataset/gen-word-65-count.jsonl\n",
      "Generated a single JSONL file with 144 samples (20 token repeat) - 310 max words - at ./dataset/shuffle-word-310-count.jsonl\n",
      "Generated a single JSONL file with 200 samples (20 token repeat) - 230 max words - at ./dataset/shuffle-word-230-count.jsonl\n",
      "Generated a single JSONL file with 116 samples (20 token repeat) - 435 max words - at ./dataset/shuffle-word-435-count.jsonl\n",
      "Generated a single JSONL file with 277 samples (20 token repeat) - 180 max words - at ./dataset/shuffle-word-180-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 330 max words - at ./dataset/shuffle-word-330-count.jsonl\n",
      "Generated a single JSONL file with 294 samples (20 token repeat) - 160 max words - at ./dataset/shuffle-word-160-count.jsonl\n",
      "Generated a single JSONL file with 189 samples (20 token repeat) - 255 max words - at ./dataset/shuffle-word-255-count.jsonl\n",
      "Generated a single JSONL file with 146 samples (20 token repeat) - 305 max words - at ./dataset/shuffle-word-305-count.jsonl\n",
      "Generated a single JSONL file with 298 samples (20 token repeat) - 155 max words - at ./dataset/shuffle-word-155-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 2000 samples - at ./dataset/gen-word-80-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 445 max words - at ./dataset/shuffle-word-445-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 315 max words - at ./dataset/shuffle-word-315-count.jsonl\n",
      "Generated JSONL file with - 90 max words, 2000 samples - at ./dataset/gen-word-90-count.jsonl\n",
      "Generated a single JSONL file with 183 samples (20 token repeat) - 265 max words - at ./dataset/shuffle-word-265-count.jsonl\n",
      "Generated a single JSONL file with 274 samples (20 token repeat) - 200 max words - at ./dataset/shuffle-word-200-count.jsonl\n",
      "Generated a single JSONL file with 145 samples (20 token repeat) - 320 max words - at ./dataset/shuffle-word-320-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 380 max words - at ./dataset/shuffle-word-380-count.jsonl\n",
      "Generated a single JSONL file with 118 samples (20 token repeat) - 430 max words - at ./dataset/shuffle-word-430-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 325 max words - at ./dataset/shuffle-word-325-count.jsonl\n",
      "Generated a single JSONL file with 200 samples (20 token repeat) - 225 max words - at ./dataset/shuffle-word-225-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 340 max words - at ./dataset/shuffle-word-340-count.jsonl\n",
      "Generated a single JSONL file with 281 samples (20 token repeat) - 175 max words - at ./dataset/shuffle-word-175-count.jsonl\n",
      "Generated a single JSONL file with 318 samples (20 token repeat) - 145 max words - at ./dataset/shuffle-word-145-count.jsonl\n",
      "Generated a single JSONL file with 182 samples (20 token repeat) - 290 max words - at ./dataset/shuffle-word-290-count.jsonl\n",
      "Generated a single JSONL file with 117 samples (20 token repeat) - 425 max words - at ./dataset/shuffle-word-425-count.jsonl\n",
      "Generated a single JSONL file with 142 samples (20 token repeat) - 335 max words - at ./dataset/shuffle-word-335-count.jsonl\n",
      "Generated a single JSONL file with 199 samples (20 token repeat) - 235 max words - at ./dataset/shuffle-word-235-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 365 max words - at ./dataset/shuffle-word-365-count.jsonl\n",
      "Generated a single JSONL file with 210 samples (20 token repeat) - 210 max words - at ./dataset/shuffle-word-210-count.jsonl\n",
      "Generated a single JSONL file with 140 samples (20 token repeat) - 350 max words - at ./dataset/shuffle-word-350-count.jsonl\n",
      "Generated a single JSONL file with 192 samples (20 token repeat) - 240 max words - at ./dataset/shuffle-word-240-count.jsonl\n",
      "Generated JSONL file with - 75 max words, 2000 samples - at ./dataset/gen-word-75-count.jsonl\n",
      "Generated a single JSONL file with 141 samples (20 token repeat) - 395 max words - at ./dataset/shuffle-word-395-count.jsonl\n",
      "Generated a single JSONL file with 182 samples (20 token repeat) - 295 max words - at ./dataset/shuffle-word-295-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 2000 samples - at ./dataset/gen-word-100-count.jsonl\n",
      "Generated JSONL file with - 95 max words, 2000 samples - at ./dataset/gen-word-95-count.jsonl\n",
      "Generated a single JSONL file with 285 samples (20 token repeat) - 170 max words - at ./dataset/shuffle-word-170-count.jsonl\n",
      "Generated a single JSONL file with 275 samples (20 token repeat) - 190 max words - at ./dataset/shuffle-word-190-count.jsonl\n",
      "Generated a single JSONL file with 188 samples (20 token repeat) - 250 max words - at ./dataset/shuffle-word-250-count.jsonl\n",
      "Generated a single JSONL file with 139 samples (20 token repeat) - 370 max words - at ./dataset/shuffle-word-370-count.jsonl\n",
      "Generated a single JSONL file with 184 samples (20 token repeat) - 260 max words - at ./dataset/shuffle-word-260-count.jsonl\n",
      "Generated a single JSONL file with 184 samples (20 token repeat) - 270 max words - at ./dataset/shuffle-word-270-count.jsonl\n",
      "Generated a single JSONL file with 120 samples (20 token repeat) - 415 max words - at ./dataset/shuffle-word-415-count.jsonl\n",
      "Generated a single JSONL file with 268 samples (20 token repeat) - 195 max words - at ./dataset/shuffle-word-195-count.jsonl\n",
      "Generated a single JSONL file with 205 samples (20 token repeat) - 215 max words - at ./dataset/shuffle-word-215-count.jsonl\n",
      "Generated a single JSONL file with 286 samples (20 token repeat) - 165 max words - at ./dataset/shuffle-word-165-count.jsonl\n",
      "Generated JSONL file with - 115 max words, 2000 samples - at ./dataset/gen-word-115-count.jsonl\n",
      "Generated a single JSONL file with 334 samples (20 token repeat) - 135 max words - at ./dataset/shuffle-word-135-count.jsonl\n",
      "Generated a single JSONL file with 305 samples (20 token repeat) - 150 max words - at ./dataset/shuffle-word-150-count.jsonl\n",
      "Generated a single JSONL file with 218 samples (20 token repeat) - 205 max words - at ./dataset/shuffle-word-205-count.jsonl\n",
      "Generated JSONL file with - 85 max words, 2000 samples - at ./dataset/gen-word-85-count.jsonl\n",
      "Generated a single JSONL file with 119 samples (20 token repeat) - 420 max words - at ./dataset/shuffle-word-420-count.jsonl\n",
      "Generated a single JSONL file with 188 samples (20 token repeat) - 245 max words - at ./dataset/shuffle-word-245-count.jsonl\n",
      "Generated JSONL file with - 145 max words, 2000 samples - at ./dataset/gen-word-145-count.jsonl\n",
      "Generated JSONL file with - 150 max words, 2000 samples - at ./dataset/gen-word-150-count.jsonl\n",
      "Generated JSONL file with - 105 max words, 2000 samples - at ./dataset/gen-word-105-count.jsonl\n",
      "Generated JSONL file with - 130 max words, 2000 samples - at ./dataset/gen-word-130-count.jsonl\n",
      "Generated JSONL file with - 120 max words, 2000 samples - at ./dataset/gen-word-120-count.jsonl\n",
      "Generated JSONL file with - 110 max words, 2000 samples - at ./dataset/gen-word-110-count.jsonl\n",
      "Generated JSONL file with - 125 max words, 2000 samples - at ./dataset/gen-word-125-count.jsonl\n",
      "Generated JSONL file with - 160 max words, 2000 samples - at ./dataset/gen-word-160-count.jsonl\n",
      "Generated JSONL file with - 135 max words, 2000 samples - at ./dataset/gen-word-135-count.jsonl\n",
      "Generated JSONL file with - 170 max words, 2000 samples - at ./dataset/gen-word-170-count.jsonl\n",
      "Generated JSONL file with - 155 max words, 2000 samples - at ./dataset/gen-word-155-count.jsonl\n",
      "Generated JSONL file with - 185 max words, 2000 samples - at ./dataset/gen-word-185-count.jsonl\n",
      "Generated JSONL file with - 165 max words, 2000 samples - at ./dataset/gen-word-165-count.jsonl\n",
      "Generated JSONL file with - 140 max words, 2000 samples - at ./dataset/gen-word-140-count.jsonl\n",
      "Generated JSONL file with - 175 max words, 2000 samples - at ./dataset/gen-word-175-count.jsonl\n",
      "Generated JSONL file with - 225 max words, 2000 samples - at ./dataset/gen-word-225-count.jsonl\n",
      "Generated JSONL file with - 180 max words, 2000 samples - at ./dataset/gen-word-180-count.jsonl\n",
      "Generated JSONL file with - 245 max words, 2000 samples - at ./dataset/gen-word-245-count.jsonl\n",
      "Generated JSONL file with - 195 max words, 2000 samples - at ./dataset/gen-word-195-count.jsonl\n",
      "Generated JSONL file with - 210 max words, 2000 samples - at ./dataset/gen-word-210-count.jsonl\n",
      "Generated JSONL file with - 220 max words, 2000 samples - at ./dataset/gen-word-220-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 2000 samples - at ./dataset/gen-word-200-count.jsonl\n",
      "Generated JSONL file with - 240 max words, 2000 samples - at ./dataset/gen-word-240-count.jsonl\n",
      "Generated JSONL file with - 205 max words, 2000 samples - at ./dataset/gen-word-205-count.jsonl\n",
      "Generated JSONL file with - 265 max words, 2000 samples - at ./dataset/gen-word-265-count.jsonl\n",
      "Generated JSONL file with - 270 max words, 2000 samples - at ./dataset/gen-word-270-count.jsonl\n",
      "Generated JSONL file with - 280 max words, 2000 samples - at ./dataset/gen-word-280-count.jsonl\n",
      "Generated JSONL file with - 305 max words, 2000 samples - at ./dataset/gen-word-305-count.jsonl\n",
      "Generated JSONL file with - 310 max words, 2000 samples - at ./dataset/gen-word-310-count.jsonl\n",
      "Generated JSONL file with - 190 max words, 2000 samples - at ./dataset/gen-word-190-count.jsonl\n",
      "Generated JSONL file with - 325 max words, 2000 samples - at ./dataset/gen-word-325-count.jsonl\n",
      "Generated JSONL file with - 250 max words, 2000 samples - at ./dataset/gen-word-250-count.jsonl\n",
      "Generated JSONL file with - 255 max words, 2000 samples - at ./dataset/gen-word-255-count.jsonl\n",
      "Generated JSONL file with - 365 max words, 2000 samples - at ./dataset/gen-word-365-count.jsonl\n",
      "Generated JSONL file with - 235 max words, 2000 samples - at ./dataset/gen-word-235-count.jsonl\n",
      "Generated JSONL file with - 320 max words, 2000 samples - at ./dataset/gen-word-320-count.jsonl\n",
      "Generated JSONL file with - 285 max words, 2000 samples - at ./dataset/gen-word-285-count.jsonl\n",
      "Generated JSONL file with - 370 max words, 2000 samples - at ./dataset/gen-word-370-count.jsonl\n",
      "Generated JSONL file with - 410 max words, 2000 samples - at ./dataset/gen-word-410-count.jsonl\n",
      "Generated JSONL file with - 230 max words, 2000 samples - at ./dataset/gen-word-230-count.jsonl\n",
      "Generated JSONL file with - 375 max words, 2000 samples - at ./dataset/gen-word-375-count.jsonl\n",
      "Generated JSONL file with - 340 max words, 2000 samples - at ./dataset/gen-word-340-count.jsonl\n",
      "Generated JSONL file with - 295 max words, 2000 samples - at ./dataset/gen-word-295-count.jsonl\n",
      "Generated JSONL file with - 215 max words, 2000 samples - at ./dataset/gen-word-215-count.jsonl\n",
      "Generated JSONL file with - 330 max words, 2000 samples - at ./dataset/gen-word-330-count.jsonl\n",
      "Generated JSONL file with - 290 max words, 2000 samples - at ./dataset/gen-word-290-count.jsonl\n",
      "Generated JSONL file with - 275 max words, 2000 samples - at ./dataset/gen-word-275-count.jsonl\n",
      "Generated JSONL file with - 260 max words, 2000 samples - at ./dataset/gen-word-260-count.jsonl\n",
      "Generated JSONL file with - 385 max words, 2000 samples - at ./dataset/gen-word-385-count.jsonl\n",
      "Generated JSONL file with - 380 max words, 2000 samples - at ./dataset/gen-word-380-count.jsonl\n",
      "Generated JSONL file with - 355 max words, 2000 samples - at ./dataset/gen-word-355-count.jsonl\n",
      "Generated JSONL file with - 360 max words, 2000 samples - at ./dataset/gen-word-360-count.jsonl\n",
      "Generated JSONL file with - 315 max words, 2000 samples - at ./dataset/gen-word-315-count.jsonl\n",
      "Generated JSONL file with - 395 max words, 2000 samples - at ./dataset/gen-word-395-count.jsonl\n",
      "Generated JSONL file with - 345 max words, 2000 samples - at ./dataset/gen-word-345-count.jsonl\n",
      "Generated JSONL file with - 450 max words, 2000 samples - at ./dataset/gen-word-450-count.jsonl\n",
      "Generated JSONL file with - 415 max words, 2000 samples - at ./dataset/gen-word-415-count.jsonl\n",
      "Generated JSONL file with - 390 max words, 2000 samples - at ./dataset/gen-word-390-count.jsonl\n",
      "Generated JSONL file with - 350 max words, 2000 samples - at ./dataset/gen-word-350-count.jsonl\n",
      "Generated JSONL file with - 420 max words, 2000 samples - at ./dataset/gen-word-420-count.jsonl\n",
      "Generated JSONL file with - 430 max words, 2000 samples - at ./dataset/gen-word-430-count.jsonl\n",
      "Generated JSONL file with - 425 max words, 2000 samples - at ./dataset/gen-word-425-count.jsonl\n",
      "Generated JSONL file with - 300 max words, 2000 samples - at ./dataset/gen-word-300-count.jsonl\n",
      "Generated JSONL file with - 335 max words, 2000 samples - at ./dataset/gen-word-335-count.jsonl\n",
      "Generated JSONL file with - 435 max words, 2000 samples - at ./dataset/gen-word-435-count.jsonl\n",
      "Generated JSONL file with - 400 max words, 2000 samples - at ./dataset/gen-word-400-count.jsonl\n",
      "Generated JSONL file with - 445 max words, 2000 samples - at ./dataset/gen-word-445-count.jsonl\n",
      "Generated JSONL file with - 405 max words, 2000 samples - at ./dataset/gen-word-405-count.jsonl\n",
      "Generated JSONL file with - 440 max words, 2000 samples - at ./dataset/gen-word-440-count.jsonl\n",
      "## Done ##\n",
      "total 450M\n",
      "drwxr-xr-x 2 root root 8.0K Jul 14 10:11 .\n",
      "drwxr-xr-x 5 root root 4.0K Jul 14 10:09 ..\n",
      "-rw-r--r-- 1 root root 197K Jul 14 10:11 gen-word-10-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.1M Jul 14 10:11 gen-word-100-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.2M Jul 14 10:11 gen-word-105-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.3M Jul 14 10:11 gen-word-110-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.4M Jul 14 10:11 gen-word-115-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.5M Jul 14 10:11 gen-word-120-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.6M Jul 14 10:11 gen-word-125-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.7M Jul 14 10:11 gen-word-130-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.8M Jul 14 10:11 gen-word-135-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.9M Jul 14 10:11 gen-word-140-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.0M Jul 14 10:11 gen-word-145-count.jsonl\n",
      "-rw-r--r-- 1 root root 241K Jul 14 10:11 gen-word-15-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.1M Jul 14 10:11 gen-word-150-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.2M Jul 14 10:11 gen-word-155-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.3M Jul 14 10:11 gen-word-160-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.4M Jul 14 10:11 gen-word-165-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.5M Jul 14 10:11 gen-word-170-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.5M Jul 14 10:11 gen-word-175-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.7M Jul 14 10:11 gen-word-180-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.8M Jul 14 10:11 gen-word-185-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.8M Jul 14 10:11 gen-word-190-count.jsonl\n",
      "-rw-r--r-- 1 root root 3.9M Jul 14 10:11 gen-word-195-count.jsonl\n",
      "-rw-r--r-- 1 root root 294K Jul 14 10:11 gen-word-20-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.1M Jul 14 10:11 gen-word-200-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.1M Jul 14 10:11 gen-word-205-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.2M Jul 14 10:11 gen-word-210-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.3M Jul 14 10:11 gen-word-215-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.4M Jul 14 10:11 gen-word-220-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.5M Jul 14 10:11 gen-word-225-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.6M Jul 14 10:11 gen-word-230-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.7M Jul 14 10:11 gen-word-235-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.8M Jul 14 10:11 gen-word-240-count.jsonl\n",
      "-rw-r--r-- 1 root root 4.9M Jul 14 10:11 gen-word-245-count.jsonl\n",
      "-rw-r--r-- 1 root root 337K Jul 14 10:11 gen-word-25-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.0M Jul 14 10:11 gen-word-250-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.1M Jul 14 10:11 gen-word-255-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.2M Jul 14 10:11 gen-word-260-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.3M Jul 14 10:11 gen-word-265-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.4M Jul 14 10:11 gen-word-270-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.5M Jul 14 10:11 gen-word-275-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.6M Jul 14 10:11 gen-word-280-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.6M Jul 14 10:11 gen-word-285-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.7M Jul 14 10:11 gen-word-290-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.8M Jul 14 10:11 gen-word-295-count.jsonl\n",
      "-rw-r--r-- 1 root root 391K Jul 14 10:11 gen-word-30-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.9M Jul 14 10:11 gen-word-300-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.1M Jul 14 10:11 gen-word-305-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.2M Jul 14 10:11 gen-word-310-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.2M Jul 14 10:11 gen-word-315-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.3M Jul 14 10:11 gen-word-320-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.4M Jul 14 10:11 gen-word-325-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.5M Jul 14 10:11 gen-word-330-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.6M Jul 14 10:11 gen-word-335-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.7M Jul 14 10:11 gen-word-340-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.8M Jul 14 10:11 gen-word-345-count.jsonl\n",
      "-rw-r--r-- 1 root root 432K Jul 14 10:11 gen-word-35-count.jsonl\n",
      "-rw-r--r-- 1 root root 6.9M Jul 14 10:11 gen-word-350-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.0M Jul 14 10:11 gen-word-355-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.0M Jul 14 10:11 gen-word-360-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.1M Jul 14 10:11 gen-word-365-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.3M Jul 14 10:11 gen-word-370-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.3M Jul 14 10:11 gen-word-375-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.5M Jul 14 10:11 gen-word-380-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.5M Jul 14 10:11 gen-word-385-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.7M Jul 14 10:11 gen-word-390-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.7M Jul 14 10:11 gen-word-395-count.jsonl\n",
      "-rw-r--r-- 1 root root 492K Jul 14 10:11 gen-word-40-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.8M Jul 14 10:11 gen-word-400-count.jsonl\n",
      "-rw-r--r-- 1 root root 7.9M Jul 14 10:11 gen-word-405-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.0M Jul 14 10:11 gen-word-410-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.1M Jul 14 10:11 gen-word-415-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.2M Jul 14 10:11 gen-word-420-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.3M Jul 14 10:11 gen-word-425-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.4M Jul 14 10:11 gen-word-430-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.5M Jul 14 10:11 gen-word-435-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.6M Jul 14 10:11 gen-word-440-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.7M Jul 14 10:11 gen-word-445-count.jsonl\n",
      "-rw-r--r-- 1 root root 530K Jul 14 10:11 gen-word-45-count.jsonl\n",
      "-rw-r--r-- 1 root root 8.8M Jul 14 10:11 gen-word-450-count.jsonl\n",
      "-rw-r--r-- 1 root root 145K Jul 14 10:11 gen-word-5-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.2M Jul 14 10:11 gen-word-50-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.3M Jul 14 10:11 gen-word-55-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.4M Jul 14 10:11 gen-word-60-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.5M Jul 14 10:11 gen-word-65-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.6M Jul 14 10:11 gen-word-70-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.7M Jul 14 10:11 gen-word-75-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.8M Jul 14 10:11 gen-word-80-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.8M Jul 14 10:11 gen-word-85-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.9M Jul 14 10:11 gen-word-90-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.0M Jul 14 10:11 gen-word-95-count.jsonl\n",
      "-rw-r--r-- 1 root root 512K Jul 14 10:11 shuffle-word-10-count.jsonl\n",
      "-rw-r--r-- 1 root root 565K Jul 14 10:11 shuffle-word-100-count.jsonl\n",
      "-rw-r--r-- 1 root root 558K Jul 14 10:11 shuffle-word-105-count.jsonl\n",
      "-rw-r--r-- 1 root root 549K Jul 14 10:11 shuffle-word-110-count.jsonl\n",
      "-rw-r--r-- 1 root root 555K Jul 14 10:11 shuffle-word-115-count.jsonl\n",
      "-rw-r--r-- 1 root root 547K Jul 14 10:11 shuffle-word-120-count.jsonl\n",
      "-rw-r--r-- 1 root root 548K Jul 14 10:11 shuffle-word-125-count.jsonl\n",
      "-rw-r--r-- 1 root root 546K Jul 14 10:11 shuffle-word-130-count.jsonl\n",
      "-rw-r--r-- 1 root root 549K Jul 14 10:11 shuffle-word-135-count.jsonl\n",
      "-rw-r--r-- 1 root root 548K Jul 14 10:11 shuffle-word-140-count.jsonl\n",
      "-rw-r--r-- 1 root root 548K Jul 14 10:11 shuffle-word-145-count.jsonl\n",
      "-rw-r--r-- 1 root root 431K Jul 14 10:11 shuffle-word-15-count.jsonl\n",
      "-rw-r--r-- 1 root root 542K Jul 14 10:11 shuffle-word-150-count.jsonl\n",
      "-rw-r--r-- 1 root root 541K Jul 14 10:11 shuffle-word-155-count.jsonl\n",
      "-rw-r--r-- 1 root root 544K Jul 14 10:11 shuffle-word-160-count.jsonl\n",
      "-rw-r--r-- 1 root root 537K Jul 14 10:11 shuffle-word-165-count.jsonl\n",
      "-rw-r--r-- 1 root root 538K Jul 14 10:11 shuffle-word-170-count.jsonl\n",
      "-rw-r--r-- 1 root root 547K Jul 14 10:11 shuffle-word-175-count.jsonl\n",
      "-rw-r--r-- 1 root root 544K Jul 14 10:11 shuffle-word-180-count.jsonl\n",
      "-rw-r--r-- 1 root root 539K Jul 14 10:11 shuffle-word-185-count.jsonl\n",
      "-rw-r--r-- 1 root root 538K Jul 14 10:11 shuffle-word-190-count.jsonl\n",
      "-rw-r--r-- 1 root root 544K Jul 14 10:11 shuffle-word-195-count.jsonl\n",
      "-rw-r--r-- 1 root root 382K Jul 14 10:11 shuffle-word-20-count.jsonl\n",
      "-rw-r--r-- 1 root root 538K Jul 14 10:11 shuffle-word-200-count.jsonl\n",
      "-rw-r--r-- 1 root root 534K Jul 14 10:11 shuffle-word-205-count.jsonl\n",
      "-rw-r--r-- 1 root root 534K Jul 14 10:11 shuffle-word-210-count.jsonl\n",
      "-rw-r--r-- 1 root root 535K Jul 14 10:11 shuffle-word-215-count.jsonl\n",
      "-rw-r--r-- 1 root root 534K Jul 14 10:11 shuffle-word-220-count.jsonl\n",
      "-rw-r--r-- 1 root root 535K Jul 14 10:11 shuffle-word-225-count.jsonl\n",
      "-rw-r--r-- 1 root root 532K Jul 14 10:11 shuffle-word-230-count.jsonl\n",
      "-rw-r--r-- 1 root root 534K Jul 14 10:11 shuffle-word-235-count.jsonl\n",
      "-rw-r--r-- 1 root root 535K Jul 14 10:11 shuffle-word-240-count.jsonl\n",
      "-rw-r--r-- 1 root root 529K Jul 14 10:11 shuffle-word-245-count.jsonl\n",
      "-rw-r--r-- 1 root root 358K Jul 14 10:11 shuffle-word-25-count.jsonl\n",
      "-rw-r--r-- 1 root root 533K Jul 14 10:11 shuffle-word-250-count.jsonl\n",
      "-rw-r--r-- 1 root root 538K Jul 14 10:11 shuffle-word-255-count.jsonl\n",
      "-rw-r--r-- 1 root root 532K Jul 14 10:11 shuffle-word-260-count.jsonl\n",
      "-rw-r--r-- 1 root root 536K Jul 14 10:11 shuffle-word-265-count.jsonl\n",
      "-rw-r--r-- 1 root root 539K Jul 14 10:11 shuffle-word-270-count.jsonl\n",
      "-rw-r--r-- 1 root root 534K Jul 14 10:11 shuffle-word-275-count.jsonl\n",
      "-rw-r--r-- 1 root root 537K Jul 14 10:11 shuffle-word-280-count.jsonl\n",
      "-rw-r--r-- 1 root root 534K Jul 14 10:11 shuffle-word-285-count.jsonl\n",
      "-rw-r--r-- 1 root root 532K Jul 14 10:11 shuffle-word-290-count.jsonl\n",
      "-rw-r--r-- 1 root root 535K Jul 14 10:11 shuffle-word-295-count.jsonl\n",
      "-rw-r--r-- 1 root root 332K Jul 14 10:11 shuffle-word-30-count.jsonl\n",
      "-rw-r--r-- 1 root root 537K Jul 14 10:11 shuffle-word-300-count.jsonl\n",
      "-rw-r--r-- 1 root root 532K Jul 14 10:11 shuffle-word-305-count.jsonl\n",
      "-rw-r--r-- 1 root root 529K Jul 14 10:11 shuffle-word-310-count.jsonl\n",
      "-rw-r--r-- 1 root root 530K Jul 14 10:11 shuffle-word-315-count.jsonl\n",
      "-rw-r--r-- 1 root root 530K Jul 14 10:11 shuffle-word-320-count.jsonl\n",
      "-rw-r--r-- 1 root root 529K Jul 14 10:11 shuffle-word-325-count.jsonl\n",
      "-rw-r--r-- 1 root root 531K Jul 14 10:11 shuffle-word-330-count.jsonl\n",
      "-rw-r--r-- 1 root root 530K Jul 14 10:11 shuffle-word-335-count.jsonl\n",
      "-rw-r--r-- 1 root root 531K Jul 14 10:11 shuffle-word-340-count.jsonl\n",
      "-rw-r--r-- 1 root root 526K Jul 14 10:11 shuffle-word-345-count.jsonl\n",
      "-rw-r--r-- 1 root root 322K Jul 14 10:11 shuffle-word-35-count.jsonl\n",
      "-rw-r--r-- 1 root root 531K Jul 14 10:11 shuffle-word-350-count.jsonl\n",
      "-rw-r--r-- 1 root root 526K Jul 14 10:11 shuffle-word-355-count.jsonl\n",
      "-rw-r--r-- 1 root root 532K Jul 14 10:11 shuffle-word-360-count.jsonl\n",
      "-rw-r--r-- 1 root root 531K Jul 14 10:11 shuffle-word-365-count.jsonl\n",
      "-rw-r--r-- 1 root root 531K Jul 14 10:11 shuffle-word-370-count.jsonl\n",
      "-rw-r--r-- 1 root root 533K Jul 14 10:11 shuffle-word-375-count.jsonl\n",
      "-rw-r--r-- 1 root root 526K Jul 14 10:11 shuffle-word-380-count.jsonl\n",
      "-rw-r--r-- 1 root root 531K Jul 14 10:11 shuffle-word-385-count.jsonl\n",
      "-rw-r--r-- 1 root root 529K Jul 14 10:11 shuffle-word-390-count.jsonl\n",
      "-rw-r--r-- 1 root root 530K Jul 14 10:11 shuffle-word-395-count.jsonl\n",
      "-rw-r--r-- 1 root root 318K Jul 14 10:11 shuffle-word-40-count.jsonl\n",
      "-rw-r--r-- 1 root root 531K Jul 14 10:11 shuffle-word-400-count.jsonl\n",
      "-rw-r--r-- 1 root root 525K Jul 14 10:11 shuffle-word-405-count.jsonl\n",
      "-rw-r--r-- 1 root root 528K Jul 14 10:11 shuffle-word-410-count.jsonl\n",
      "-rw-r--r-- 1 root root 530K Jul 14 10:11 shuffle-word-415-count.jsonl\n",
      "-rw-r--r-- 1 root root 532K Jul 14 10:11 shuffle-word-420-count.jsonl\n",
      "-rw-r--r-- 1 root root 530K Jul 14 10:11 shuffle-word-425-count.jsonl\n",
      "-rw-r--r-- 1 root root 526K Jul 14 10:11 shuffle-word-430-count.jsonl\n",
      "-rw-r--r-- 1 root root 529K Jul 14 10:11 shuffle-word-435-count.jsonl\n",
      "-rw-r--r-- 1 root root 526K Jul 14 10:11 shuffle-word-440-count.jsonl\n",
      "-rw-r--r-- 1 root root 528K Jul 14 10:11 shuffle-word-445-count.jsonl\n",
      "-rw-r--r-- 1 root root 316K Jul 14 10:11 shuffle-word-45-count.jsonl\n",
      "-rw-r--r-- 1 root root 526K Jul 14 10:11 shuffle-word-450-count.jsonl\n",
      "-rw-r--r-- 1 root root 820K Jul 14 10:11 shuffle-word-5-count.jsonl\n",
      "-rw-r--r-- 1 root root 611K Jul 14 10:11 shuffle-word-50-count.jsonl\n",
      "-rw-r--r-- 1 root root 607K Jul 14 10:11 shuffle-word-55-count.jsonl\n",
      "-rw-r--r-- 1 root root 597K Jul 14 10:11 shuffle-word-60-count.jsonl\n",
      "-rw-r--r-- 1 root root 587K Jul 14 10:11 shuffle-word-65-count.jsonl\n",
      "-rw-r--r-- 1 root root 591K Jul 14 10:11 shuffle-word-70-count.jsonl\n",
      "-rw-r--r-- 1 root root 576K Jul 14 10:11 shuffle-word-75-count.jsonl\n",
      "-rw-r--r-- 1 root root 582K Jul 14 10:11 shuffle-word-80-count.jsonl\n",
      "-rw-r--r-- 1 root root 573K Jul 14 10:11 shuffle-word-85-count.jsonl\n",
      "-rw-r--r-- 1 root root 575K Jul 14 10:11 shuffle-word-90-count.jsonl\n",
      "-rw-r--r-- 1 root root 570K Jul 14 10:11 shuffle-word-95-count.jsonl\n",
      "-rw-r--r-- 1 root root 121K Jul 14 10:11 word-2-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ./dataset\n",
    "rm -rf ./dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# We reduce the training set for < 50 words - and shift the focus upwards\n",
    "# (aka 50-100 token * 2 : ~100 - 250 token ctx len)\n",
    "#\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-2-count.jsonl 2 1000 &\n",
    "for i in {5..45..5} \n",
    "do\n",
    "    python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/gen-word-$i-count.jsonl $i 1000 & \n",
    "    python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-$i-count.jsonl $i 10 & \n",
    "done\n",
    "\n",
    "#\n",
    "# Ramping up the 50+ - 400 words dataset\n",
    "# \n",
    "for i in {50..450..5} \n",
    "do\n",
    "    python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/gen-word-$i-count.jsonl $i 2000 & \n",
    "    python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-$i-count.jsonl $i 20 & \n",
    "done\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ./dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|████████████████| 181/181 [00:00<00:00, 70877.51it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-e0009dad27da4a11/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|████████████████████| 1/1 [00:00<00:00, 704.45it/s]\n",
      "Extracting data files: 100%|██████████████████████| 1/1 [00:00<00:00, 25.89it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-e0009dad27da4a11/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 70.29it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets pre tokenize the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/Echo-B-1B4-mem-finetune-3.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/Echo-B-1B4-mem-finetune-3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 64611460\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 64611460\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230714_101151-6z5hpgyt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m(8x3090) Echo-B-1B4 - Mem-Finetune-3 (bs=256, train-ctx=1024, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/6z5hpgyt\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_1024_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -t 4 -std=c++17 -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=1024 -c /root/picocreator-memory-experiment/RWKV-v4neo/cuda/wkv_cuda_bf16.cu -o wkv_cuda_bf16.cuda.o \n",
      "ptxas info    : 1 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf\n",
      "    12288 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 56 registers, 464 bytes cmem[0], 8 bytes cmem[2]\n",
      "ptxas info    : Compiling entry function '_Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, 424 bytes cmem[0]\n",
      "[2/3] c++ -MMD -MF wkv_op_bf16.o.d -DTORCH_EXTENSION_NAME=wkv_1024_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -std=c++17 -O3 -DTmax=1024 -c /root/picocreator-memory-experiment/RWKV-v4neo/cuda/wkv_op_bf16.cpp -o wkv_op_bf16.o \n",
      "[3/3] c++ wkv_op_bf16.o wkv_cuda_bf16.cuda.o -shared -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_1024_bf16.so\n",
      "Loading extension module wkv_1024_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|███████████████| 181/181 [00:00<00:00, 248175.56it/s]\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-e0009dad27da4a11/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 104.64it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e0009dad27da4a11/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-cc8f6ec523d7ead1_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-e0009dad27da4a11/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-dd39d7d4ca5cd5ba_*_of_00064.arrow\n",
      "[rank: 0] Global seed set to 64611460                                           \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-07-14 10:12:23,836] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 1] Global seed set to 64611460\n",
      "[rank: 4] Global seed set to 64611460\n",
      "[rank: 3] Global seed set to 64611460\n",
      "[rank: 6] Global seed set to 64611460\n",
      "[rank: 7] Global seed set to 64611460\n",
      "[rank: 5] Global seed set to 64611460\n",
      "[rank: 2] Global seed set to 64611460\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "Loading extension module wkv_1024_bf16...\n",
      "Loading extension module wkv_1024_bf16...\n",
      "Loading extension module wkv_1024_bf16...\n",
      "Loading extension module wkv_1024_bf16...\n",
      "Loading extension module wkv_1024_bf16...\n",
      "Loading extension module wkv_1024_bf16...\n",
      "[rank: 4] Global seed set to 64611460\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-07-14 10:12:56,009] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 64611460\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-07-14 10:12:59,506] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 64611460\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-07-14 10:12:59,663] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 64611460\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-07-14 10:12:59,699] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 64611460\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-07-14 10:12:59,731] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 64611460\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-07-14 10:12:59,740] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 64611460\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-07-14 10:12:59,748] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  4.000e-04 (0.0004)\n",
      "    - lr_final: 1.000e-04 (0.0001)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06719541549682617 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10136127471923828 seconds\n",
      "Time to load fused_adam op: 0.1011652946472168 seconds\n",
      "Time to load fused_adam op: 0.1015474796295166 seconds\n",
      "Time to load fused_adam op: 0.1015462875366211 seconds\n",
      "Time to load fused_adam op: 0.10176491737365723 seconds\n",
      "Time to load fused_adam op: 0.1018669605255127 seconds\n",
      "Time to load fused_adam op: 0.10187005996704102 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06649041175842285 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10180234909057617 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10186171531677246 seconds\n",
      "Time to load utils op: 0.10142374038696289 seconds\n",
      "Time to load utils op: 0.10178399085998535 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10212850570678711 seconds\n",
      "Time to load utils op: 0.10210919380187988 seconds\n",
      "Time to load utils op: 0.10191988945007324 seconds\n",
      "Rank: 7 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 6 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 5 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 1 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 4 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 0 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 2 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 3 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00027489662170410156 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00028634071350097656 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00029850006103515625 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002856254577636719 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.000270843505859375 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00028204917907714844 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002720355987548828 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005047321319580078 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 51.5 M\n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 2.0 K \n",
      "3 | head   | Linear     | 51.5 M\n",
      "--------------------------------------\n",
      "1.4 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 B     Total params\n",
      "5,650.702 Total estimated model params size (MB)\n",
      "Epoch 0:   3%| | 800/26169 [07:21<3:53:10,  1.81it/s, v_num=pgyt, train/loss=1.7/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 26169/26169 [4:07:03<00:00,  1.77it/s, v_num=pgyt, train/loss=0\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                  | 1/27 [00:00<00:06,  3.83it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▍                 | 2/27 [00:00<00:05,  4.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██                 | 3/27 [00:00<00:05,  4.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▊                | 4/27 [00:00<00:05,  4.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▌               | 5/27 [00:01<00:05,  4.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████▏              | 6/27 [00:01<00:04,  4.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▉              | 7/27 [00:01<00:04,  4.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████▋             | 8/27 [00:01<00:04,  4.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▎            | 9/27 [00:01<00:03,  4.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▋           | 10/27 [00:02<00:03,  4.58it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████▎          | 11/27 [00:02<00:03,  4.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████          | 12/27 [00:02<00:03,  4.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▋         | 13/27 [00:02<00:03,  4.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████████▎        | 14/27 [00:03<00:02,  4.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████        | 15/27 [00:03<00:02,  4.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████▋       | 16/27 [00:03<00:02,  4.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|███████████▎      | 17/27 [00:03<00:02,  4.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 18/27 [00:03<00:01,  4.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|████████████▋     | 19/27 [00:04<00:01,  4.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|█████████████▎    | 20/27 [00:04<00:01,  4.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|██████████████    | 21/27 [00:04<00:01,  4.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|██████████████▋   | 22/27 [00:04<00:01,  4.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|███████████████▎  | 23/27 [00:04<00:00,  4.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████████████  | 24/27 [00:05<00:00,  4.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|████████████████▋ | 25/27 [00:05<00:00,  4.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████████████▎| 26/27 [00:05<00:00,  4.70it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 26169/26169 [4:07:15<00:00,  1.76it/s, v_num=pgyt, train/loss=0\u001b[A\n",
      "Epoch 0: 100%|█| 26169/26169 [4:07:15<00:00,  1.76it/s, v_num=pgyt, train/loss=0\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 26169/26169 [4:07:27<00:00,  1.76it/s, v_num=pgyt, train/loss=0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▃▅▃▂▃▄▂▆▃▇▃▅▆▆▂▄▃▅▅▅▆█▄▆▆▃▆▂▁▄▁▂▇▂▁▃▂▆▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▁▁▁▄▅▄▁▁▁▁▂▂▁▁▁▂▂▁▁▂▁▂▃▁▁▁▂▂▁▁▁▁▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 173\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 208\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 0.06177\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 817\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 0.2098\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m(8x3090) Echo-B-1B4 - Mem-Finetune-3 (bs=256, train-ctx=1024, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/6z5hpgyt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230714_101151-6z5hpgyt/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/Echo-B-1B4-mem-finetune-3.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Finetune-3 (bs=256, train-ctx=1024, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/Echo-B-1B4-mem-finetune-3/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 1734 params 1412675584 elements\n",
      "Saving fp32 state dict to ../model/Echo-B-1B4-Tune3.pth\n",
      "-rw-r--r-- 1 root root 5.3G Jul 14 14:22 ../model/Echo-B-1B4-Tune3.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/Echo-B-1B4-mem-finetune-3/last.ckpt\" \\\n",
    "        \"../model/Echo-B-1B4-Tune3.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh ../model/Echo-B-1B4-Tune3.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_cuda/build.ninja...\n",
      "Building extension module wkv_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_cuda...\n",
      "RWKV_JIT_ON 1 RWKV_CUDA_ON 1 RESCALE_LAYER 0\n",
      "\n",
      "Loading /root/picocreator-memory-experiment/model/Echo-B-1B4-Tune3.pth ...\n",
      "Strategy: (total 96+1=97 layers)\n",
      "* cuda [float32, float32], store 97 layers\n",
      "0-cuda-float32-float32 1-cuda-float32-float32 2-cuda-float32-float32 3-cuda-float32-float32 4-cuda-float32-float32 5-cuda-float32-float32 6-cuda-float32-float32 7-cuda-float32-float32 8-cuda-float32-float32 9-cuda-float32-float32 10-cuda-float32-float32 11-cuda-float32-float32 12-cuda-float32-float32 13-cuda-float32-float32 14-cuda-float32-float32 15-cuda-float32-float32 16-cuda-float32-float32 17-cuda-float32-float32 18-cuda-float32-float32 19-cuda-float32-float32 20-cuda-float32-float32 21-cuda-float32-float32 22-cuda-float32-float32 23-cuda-float32-float32 24-cuda-float32-float32 25-cuda-float32-float32 26-cuda-float32-float32 27-cuda-float32-float32 28-cuda-float32-float32 29-cuda-float32-float32 30-cuda-float32-float32 31-cuda-float32-float32 32-cuda-float32-float32 33-cuda-float32-float32 34-cuda-float32-float32 35-cuda-float32-float32 36-cuda-float32-float32 37-cuda-float32-float32 38-cuda-float32-float32 39-cuda-float32-float32 40-cuda-float32-float32 41-cuda-float32-float32 42-cuda-float32-float32 43-cuda-float32-float32 44-cuda-float32-float32 45-cuda-float32-float32 46-cuda-float32-float32 47-cuda-float32-float32 48-cuda-float32-float32 49-cuda-float32-float32 50-cuda-float32-float32 51-cuda-float32-float32 52-cuda-float32-float32 53-cuda-float32-float32 54-cuda-float32-float32 55-cuda-float32-float32 56-cuda-float32-float32 57-cuda-float32-float32 58-cuda-float32-float32 59-cuda-float32-float32 60-cuda-float32-float32 61-cuda-float32-float32 62-cuda-float32-float32 63-cuda-float32-float32 64-cuda-float32-float32 65-cuda-float32-float32 66-cuda-float32-float32 67-cuda-float32-float32 68-cuda-float32-float32 69-cuda-float32-float32 70-cuda-float32-float32 71-cuda-float32-float32 72-cuda-float32-float32 73-cuda-float32-float32 74-cuda-float32-float32 75-cuda-float32-float32 76-cuda-float32-float32 77-cuda-float32-float32 78-cuda-float32-float32 79-cuda-float32-float32 80-cuda-float32-float32 81-cuda-float32-float32 82-cuda-float32-float32 83-cuda-float32-float32 84-cuda-float32-float32 85-cuda-float32-float32 86-cuda-float32-float32 87-cuda-float32-float32 88-cuda-float32-float32 89-cuda-float32-float32 90-cuda-float32-float32 91-cuda-float32-float32 92-cuda-float32-float32 93-cuda-float32-float32 94-cuda-float32-float32 95-cuda-float32-float32 96-cuda-float32-float32 \n",
      "blocks.0.att.key.weight           f32   cuda:0   1024  1024 \n",
      "blocks.0.att.output.weight        f32   cuda:0   1024  1024 \n",
      "blocks.0.att.receptance.weight    f32   cuda:0   1024  1024 \n",
      "blocks.0.att.time_mix_k           f32   cuda:0   1024       \n",
      "blocks.0.att.time_mix_r           f32   cuda:0   1024       \n",
      "blocks.0.att.time_mix_v           f32   cuda:0   1024       \n",
      "blocks.0.att.value.weight         f32   cuda:0   1024  1024 \n",
      "blocks.0.ffn.key.weight           f32   cuda:0   1024  4096 \n",
      "blocks.0.ffn.receptance.weight    f32   cuda:0   1024  1024 \n",
      "blocks.0.ffn.time_mix_k           f32   cuda:0   1024       \n",
      "blocks.0.ffn.time_mix_r           f32   cuda:0   1024       \n",
      "blocks.0.ffn.value.weight         f32   cuda:0   4096  1024 \n",
      "blocks.0.ln1.bias                 f32   cuda:0   1024       \n",
      "blocks.0.ln1.weight               f32   cuda:0   1024       \n",
      "blocks.0.ln2.bias                 f32   cuda:0   1024       \n",
      "blocks.0.ln2.weight               f32   cuda:0   1024       \n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "blocks.95.att.key.weight          f32   cuda:0   1024  1024 \n",
      "blocks.95.att.output.weight       f32   cuda:0   1024  1024 \n",
      "blocks.95.att.receptance.weight   f32   cuda:0   1024  1024 \n",
      "blocks.95.att.time_mix_k          f32   cuda:0   1024       \n",
      "blocks.95.att.time_mix_r          f32   cuda:0   1024       \n",
      "blocks.95.att.time_mix_v          f32   cuda:0   1024       \n",
      "blocks.95.att.value.weight        f32   cuda:0   1024  1024 \n",
      "blocks.95.ffn.key.weight          f32   cuda:0   1024  4096 \n",
      "blocks.95.ffn.receptance.weight   f32   cuda:0   1024  1024 \n",
      "blocks.95.ffn.time_mix_k          f32   cuda:0   1024       \n",
      "blocks.95.ffn.time_mix_r          f32   cuda:0   1024       \n",
      "blocks.95.ffn.value.weight        f32   cuda:0   4096  1024 \n",
      "blocks.95.ln1.bias                f32   cuda:0   1024       \n",
      "blocks.95.ln1.weight              f32   cuda:0   1024       \n",
      "blocks.95.ln2.bias                f32   cuda:0   1024       \n",
      "blocks.95.ln2.weight              f32   cuda:0   1024       \n",
      "emb.weight                        f32      cpu  50277  1024 \n",
      "head.weight                       f32   cuda:0   1024 50277 \n",
      "ln_out.bias                       f32   cuda:0   1024       \n",
      "ln_out.weight                     f32   cuda:0   1024       \n",
      "blocks.0.att.time_decay           f32   cuda:0   1024       \n",
      "..............................................................................................\n",
      "blocks.95.att.time_decay          f32   cuda:0   1024       \n",
      "blocks.0.att.time_first           f32   cuda:0   1024       \n",
      "..............................................................................................\n",
      "blocks.95.att.time_first          f32   cuda:0   1024       \n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "Model validation at 5 tokens : 100.0% similarity, with 5 matched token, and 0 token mismatch\n",
      "Model validation at 10 tokens : 100.0% similarity, with 10 matched token, and 0 token mismatch\n",
      "Model validation at 15 tokens : 100.0% similarity, with 15 matched token, and 0 token mismatch\n",
      "Model validation at 20 tokens : 100.0% similarity, with 20 matched token, and 0 token mismatch\n",
      "Model validation at 25 tokens : 100.0% similarity, with 25 matched token, and 0 token mismatch\n",
      "Model validation at 30 tokens : 100.0% similarity, with 30 matched token, and 0 token mismatch\n",
      "Model validation at 35 tokens : 100.0% similarity, with 35 matched token, and 0 token mismatch\n",
      "Model validation at 40 tokens : 100.0% similarity, with 40 matched token, and 0 token mismatch\n",
      "Model validation at 45 tokens : 100.0% similarity, with 45 matched token, and 0 token mismatch\n",
      "Model validation at 50 tokens : 100.0% similarity, with 50 matched token, and 0 token mismatch\n",
      "Model validation at 55 tokens : 100.0% similarity, with 55 matched token, and 0 token mismatch\n",
      "Model validation at 60 tokens : 100.0% similarity, with 60 matched token, and 0 token mismatch\n",
      "Model validation at 65 tokens : 100.0% similarity, with 65 matched token, and 0 token mismatch\n",
      "Model validation at 70 tokens : 100.0% similarity, with 70 matched token, and 0 token mismatch\n",
      "Model validation at 75 tokens : 100.0% similarity, with 75 matched token, and 0 token mismatch\n",
      "Model validation at 80 tokens : 100.0% similarity, with 80 matched token, and 0 token mismatch\n",
      "Model validation at 85 tokens : 100.0% similarity, with 85 matched token, and 0 token mismatch\n",
      "Model validation at 90 tokens : 100.0% similarity, with 90 matched token, and 0 token mismatch\n",
      "Model validation at 95 tokens : 100.0% similarity, with 95 matched token, and 0 token mismatch\n",
      "Model validation at 100 tokens : 100.0% similarity, with 100 matched token, and 0 token mismatch\n",
      "Model validation at 105 tokens : 100.0% similarity, with 105 matched token, and 0 token mismatch\n",
      "Model validation at 110 tokens : 98.18181818181819% similarity, with 108 matched token, and 2 token mismatch\n",
      "Model validation at 115 tokens : 98.26086956521739% similarity, with 113 matched token, and 2 token mismatch\n",
      "Model validation at 120 tokens : 98.33333333333333% similarity, with 118 matched token, and 2 token mismatch\n",
      "Model validation at 130 tokens : 97.6923076923077% similarity, with 127 matched token, and 3 token mismatch\n",
      "Model validation at 140 tokens : 97.85714285714285% similarity, with 137 matched token, and 3 token mismatch\n",
      "Model validation at 150 tokens : 98.66666666666667% similarity, with 148 matched token, and 2 token mismatch\n",
      "Model validation at 175 tokens : 98.28571428571429% similarity, with 172 matched token, and 3 token mismatch\n",
      "Model validation at 200 tokens : 95.5% similarity, with 191 matched token, and 9 token mismatch\n",
      "Model validation at 225 tokens : 93.77777777777779% similarity, with 211 matched token, and 14 token mismatch\n",
      "Model validation at 250 tokens : 92.0% similarity, with 230 matched token, and 20 token mismatch\n",
      "Model validation at 275 tokens : 86.54545454545455% similarity, with 238 matched token, and 37 token mismatch\n",
      "Model validation at 300 tokens : 83.0% similarity, with 249 matched token, and 51 token mismatch\n",
      "Model validation at 325 tokens : 80.0% similarity, with 260 matched token, and 65 token mismatch\n",
      "Model validation at 350 tokens : 74.85714285714286% similarity, with 262 matched token, and 88 token mismatch\n",
      "Model validation at 375 tokens : 70.13333333333334% similarity, with 263 matched token, and 112 token mismatch\n",
      "Model validation at 400 tokens : 65.0% similarity, with 260 matched token, and 140 token mismatch\n",
      "Model validation at 425 tokens : 59.05882352941176% similarity, with 251 matched token, and 174 token mismatch\n",
      "Model validation at 450 tokens : 53.77777777777778% similarity, with 242 matched token, and 208 token mismatch\n",
      "Model validation at 475 tokens : 47.368421052631575% similarity, with 225 matched token, and 250 token mismatch\n",
      "Model validation at 500 tokens : 43.8% similarity, with 219 matched token, and 281 token mismatch\n",
      "Model validation at 550 tokens : 35.81818181818181% similarity, with 197 matched token, and 353 token mismatch\n",
      "Model validation at 600 tokens : 26.833333333333332% similarity, with 161 matched token, and 439 token mismatch\n",
      "Model validation at 650 tokens : 20.0% similarity, with 130 matched token, and 520 token mismatch\n",
      "Model validation at 700 tokens : 16.142857142857142% similarity, with 113 matched token, and 587 token mismatch\n"
     ]
    }
   ],
   "source": [
    "# Lets do a memory eval \n",
    "#\n",
    "# We should start approaching the full potential of the model, unless its able to exceed 250 tokens of memory\n",
    "#\n",
    "!python3 ./memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/Echo-B-1B4-Tune3.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 4 : Ramping up the ctx size (2048), memory training\n",
    "\n",
    "- Tune 4: Mid ctx size (2048), same as tune 3, but extended in context size\n",
    "\n",
    "This was not really needed, but we wanted to push the model MORE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ./dataset\n",
    "rm -rf ./dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# We reduce the training set for < 50 words - and shift the focus upwards\n",
    "# (aka 50-100 token * 2 : ~100 - 250 token ctx len)\n",
    "#\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-2-count.jsonl 2 1000 &\n",
    "for i in {5..100..5} \n",
    "do\n",
    "    python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/gen-word-$i-count.jsonl $i 1000 & \n",
    "    python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-$i-count.jsonl $i 10 & \n",
    "done\n",
    "\n",
    "#\n",
    "# Ramping up the 50+ - 400 words dataset\n",
    "# \n",
    "for i in {105..1000..5} \n",
    "do\n",
    "    python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/gen-word-$i-count.jsonl $i 2000 & \n",
    "    python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-$i-count.jsonl $i 20 & \n",
    "done\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ./dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets pre tokenize the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/Echo-B-1B4-mem-finetune-4.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/Echo-B-1B4-mem-finetune-4/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/Echo-B-1B4-mem-finetune-4.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Finetune-4 (bs=256, train-ctx=2048, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/Echo-B-1B4-mem-finetune-4/last.ckpt\" \\\n",
    "        \"../model/Echo-B-1B4-Tune4.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh ../model/Echo-B-1B4-Tune4.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do a memory eval \n",
    "#\n",
    "# We should start approaching the full potential of the model, unless its able to exceed 250 tokens of memory\n",
    "#\n",
    "!python3 ./memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/Echo-B-1B4-Tune4.pth\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
