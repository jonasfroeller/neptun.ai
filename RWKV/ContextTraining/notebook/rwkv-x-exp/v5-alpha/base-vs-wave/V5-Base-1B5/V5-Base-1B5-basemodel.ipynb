{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWKV v5 Baseline 1B5\n",
    "This model is based on the RWKV standard 1B5 model\n",
    "\n",
    "- 24 layers\n",
    "- 2048 embedding size\n",
    "\n",
    "Going through the modified memory training for v5 models\n",
    "\n",
    "**Note:** This project assumes you have the rwkv-infctx conda env setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "---- Initializing model ----\n",
      "No of layers: 24\n",
      "Embedding size: 2048\n",
      "Output model path: ../model/L24-D2048-neox-v5-init.pth\n",
      "Vocab size: 50277\n",
      "---- ----- ----\n",
      "Model exists, skipping init_model\n"
     ]
    }
   ],
   "source": [
    "# First lets setup the various directories, and init the model\n",
    "!mkdir -p ../../../../model/\n",
    "!mkdir -p ../../../../datapath/\n",
    "!mkdir -p ../../../../checkpoint/\n",
    "\n",
    "# Init the model\n",
    "!cd ../../../../RWKV-v5 && python3 ./init_model.py --n_layer 24 --n_embd 2048 --vocab_size neox --skip-if-exists ../model/L24-D2048-neox-v5-init.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_1\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/V5-Base-1B5\n",
      "INFERENCE_DIR: /root/rwkv-x-playground/RWKV-v5\n",
      "TRAINER_DIR: /root/rwkv-x-playground/RWKV-v5\n",
      "PROJECT_DIR: /root/rwkv-x-playground\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"V5-Base-1B5\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "INFERENCE_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"INFERENCE_DIR:\", INFERENCE_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 : Foundation 4k model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 80.96it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-f7b060a194034cc1_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-5130e0b5cfea4510_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-8cf337c8a178675e_*_of_00064.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-54959cfaa7f1c967.arrow and /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-b4bfa876eaace3ed.arrow\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset \n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/V5-Base-1B5-enwiki-4k.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/V5-Base-1B5/V5-Base-1B5-enwiki-4k.yaml', '--trainer.logger.init_args.name=V5-Base-1B5 - Enwiki-4k Foundation (train-ctx=4k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--model.ctx_len=4096', '--model.bptt_learning_range=1'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/V5-Base-1B5/V5-Base-1B5-enwiki-4k.yaml', '--trainer.logger.init_args.name=V5-Base-1B5 - Enwiki-4k Foundation (train-ctx=4k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--model.ctx_len=4096', '--model.bptt_learning_range=1'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 4229066998\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 4229066998\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230812_080058-xpyzmfjh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mV5-Base-1B5 - Enwiki-4k Foundation (train-ctx=4k, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/xpyzmfjh\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             4\n",
      "   - accumulate_grad_batches: 8\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 82.71it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-f7b060a194034cc1_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-5130e0b5cfea4510_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-8cf337c8a178675e_*_of_00064.arrow\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-54959cfaa7f1c967.arrow and /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-b4bfa876eaace3ed.arrow\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Saving the dataset (0/5 shards):   0%|         | 0/81487 [00:00<?, ? examples/s]Setting ds_accelerator to cuda (auto detect)\n",
      "Saving the dataset (0/5 shards):  11%| | 9000/81487 [00:00<00:02, 36130.33 examp[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Saving the dataset (2/5 shards):  40%|▍| 32596/81487 [00:00<00:01, 42334.64 exam[rank: 1] Global seed set to 4229066998\n",
      "[rank: 2] Global seed set to 4229066998\n",
      "[rank: 3] Global seed set to 4229066998\n",
      "[rank: 0] Global seed set to 4229066998                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "[2023-08-12 08:01:15,746] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 4229066998\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "[2023-08-12 08:01:28,451] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 4229066998\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "[2023-08-12 08:01:28,465] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 4229066998\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "[2023-08-12 08:01:28,474] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06026744842529297 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10116004943847656 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.1017448902130127 seconds\n",
      "Time to load fused_adam op: 0.10166025161743164 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.07669472694396973 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10192680358886719 seconds\n",
      "Time to load utils op: 0.10195302963256836 seconds\n",
      "Time to load utils op: 0.10257601737976074 seconds\n",
      "Rank: 3 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Rank: 0 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Rank: 1 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Rank: 2 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002491474151611328 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002548694610595703 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002570152282714844 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005273818969726562 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0:  10%| | 2000/20372 [1:06:56<10:14:51,  2.01s/it, v_num=mfjh, train/loss/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 20372/20372 [11:25:28<00:00,  2.02s/it, v_num=mfjh, train/loss=\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/103 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/103 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|▏                 | 1/103 [00:00<00:52,  1.94it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                 | 2/103 [00:01<00:50,  1.99it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                 | 3/103 [00:01<00:49,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                 | 4/103 [00:01<00:48,  2.03it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▊                 | 5/103 [00:02<00:47,  2.05it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                 | 6/103 [00:02<00:46,  2.07it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▏                | 7/103 [00:03<00:46,  2.07it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍                | 8/103 [00:03<00:45,  2.08it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▌                | 9/103 [00:04<00:45,  2.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▋               | 10/103 [00:04<00:44,  2.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|█▊               | 11/103 [00:05<00:44,  2.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|█▉               | 12/103 [00:05<00:43,  2.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▏              | 13/103 [00:06<00:42,  2.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▎              | 14/103 [00:06<00:42,  2.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▍              | 15/103 [00:07<00:41,  2.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|██▋              | 16/103 [00:07<00:41,  2.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▊              | 17/103 [00:08<00:40,  2.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|██▉              | 18/103 [00:08<00:40,  2.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▏             | 19/103 [00:09<00:39,  2.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▎             | 20/103 [00:09<00:39,  2.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▍             | 21/103 [00:09<00:38,  2.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▋             | 22/103 [00:10<00:38,  2.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|███▊             | 23/103 [00:10<00:38,  2.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|███▉             | 24/103 [00:11<00:37,  2.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████▏            | 25/103 [00:11<00:37,  2.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|████▎            | 26/103 [00:12<00:36,  2.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▍            | 27/103 [00:12<00:35,  2.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|████▌            | 28/103 [00:13<00:35,  2.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|████▊            | 29/103 [00:13<00:34,  2.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|████▉            | 30/103 [00:14<00:34,  2.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████            | 31/103 [00:14<00:33,  2.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▎           | 32/103 [00:15<00:33,  2.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|█████▍           | 33/103 [00:15<00:32,  2.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|█████▌           | 34/103 [00:16<00:32,  2.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|█████▊           | 35/103 [00:16<00:31,  2.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|█████▉           | 36/103 [00:16<00:31,  2.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████           | 37/103 [00:17<00:31,  2.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▎          | 38/103 [00:17<00:30,  2.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▍          | 39/103 [00:18<00:30,  2.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|██████▌          | 40/103 [00:18<00:29,  2.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|██████▊          | 41/103 [00:19<00:29,  2.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|██████▉          | 42/103 [00:19<00:28,  2.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████          | 43/103 [00:20<00:28,  2.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▎         | 44/103 [00:20<00:27,  2.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▍         | 45/103 [00:21<00:27,  2.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|███████▌         | 46/103 [00:21<00:26,  2.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|███████▊         | 47/103 [00:22<00:26,  2.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|███████▉         | 48/103 [00:22<00:25,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████         | 49/103 [00:22<00:25,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▎        | 50/103 [00:23<00:24,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▍        | 51/103 [00:23<00:24,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|████████▌        | 52/103 [00:24<00:23,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|████████▋        | 53/103 [00:24<00:23,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|████████▉        | 54/103 [00:25<00:22,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|█████████        | 55/103 [00:25<00:22,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▏       | 56/103 [00:26<00:21,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████████▍       | 57/103 [00:26<00:21,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|█████████▌       | 58/103 [00:27<00:21,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|█████████▋       | 59/103 [00:27<00:20,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████████▉       | 60/103 [00:28<00:20,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████       | 61/103 [00:28<00:19,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▏      | 62/103 [00:28<00:19,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████████▍      | 63/103 [00:29<00:18,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|██████████▌      | 64/103 [00:29<00:18,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|██████████▋      | 65/103 [00:30<00:17,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████████▉      | 66/103 [00:30<00:17,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████      | 67/103 [00:31<00:16,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|███████████▏     | 68/103 [00:31<00:16,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|███████████▍     | 69/103 [00:32<00:15,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|███████████▌     | 70/103 [00:32<00:15,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|███████████▋     | 71/103 [00:33<00:14,  2.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|███████████▉     | 72/103 [00:33<00:14,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|████████████     | 73/103 [00:34<00:13,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|████████████▏    | 74/103 [00:34<00:13,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|████████████▍    | 75/103 [00:34<00:13,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|████████████▌    | 76/103 [00:35<00:12,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|████████████▋    | 77/103 [00:35<00:12,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|████████████▊    | 78/103 [00:36<00:11,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|█████████████    | 79/103 [00:36<00:11,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|█████████████▏   | 80/103 [00:37<00:10,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|█████████████▎   | 81/103 [00:37<00:10,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|█████████████▌   | 82/103 [00:38<00:09,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|█████████████▋   | 83/103 [00:38<00:09,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|█████████████▊   | 84/103 [00:39<00:08,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|██████████████   | 85/103 [00:39<00:08,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|██████████████▏  | 86/103 [00:39<00:07,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|██████████████▎  | 87/103 [00:40<00:07,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|██████████████▌  | 88/103 [00:40<00:06,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|██████████████▋  | 89/103 [00:41<00:06,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|██████████████▊  | 90/103 [00:41<00:06,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|███████████████  | 91/103 [00:42<00:05,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|███████████████▏ | 92/103 [00:42<00:05,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|███████████████▎ | 93/103 [00:43<00:04,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|███████████████▌ | 94/103 [00:43<00:04,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|███████████████▋ | 95/103 [00:44<00:03,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|███████████████▊ | 96/103 [00:44<00:03,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|████████████████ | 97/103 [00:45<00:02,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|████████████████▏| 98/103 [00:45<00:02,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|████████████████▎| 99/103 [00:45<00:01,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|███████████████▌| 100/103 [00:46<00:01,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|███████████████▋| 101/103 [00:46<00:00,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|███████████████▊| 102/103 [00:47<00:00,  2.15it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 20372/20372 [11:26:20<00:00,  2.02s/it, v_num=mfjh, train/loss=\u001b[A\n",
      "Epoch 0: 100%|█| 20372/20372 [11:26:20<00:00,  2.02s/it, v_num=mfjh, train/loss=\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 20372/20372 [11:26:29<00:00,  2.02s/it, v_num=mfjh, train/loss=\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▆▆▅▄▄▄▃▃▃▃▂▃▃▃▃▃▂▃▂▁▂▃▁▁▂▂▂▂▂▃▂▂▂▁▂▂▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 102\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 4095\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 408\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 3.85938\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 2546\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 3.52169\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mV5-Base-1B5 - Enwiki-4k Foundation (train-ctx=4k, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/xpyzmfjh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230812_080058-xpyzmfjh/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the foundation model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/V5-Base-1B5-enwiki-4k.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Enwiki-4k Foundation (train-ctx=4k, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" \\\n",
    "        --model.ctx_len=4096 \\\n",
    "        --model.bptt_learning_range=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/V5-Base-1B5-enwiki-4k/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 4\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 486 params 1515107840 elements\n",
      "Saving fp32 state dict to ../model/V5-Base-1B5-Enwiki-4k.pth\n",
      "-rw-r--r-- 1 root root 5.7G Aug 12 19:28 ../model/V5-Base-1B5-Enwiki-4k.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/V5-Base-1B5-enwiki-4k/last.ckpt\" \"../model/V5-Base-1B5-Enwiki-4k.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/V5-Base-1B5-Enwiki-4k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "--- DRAGON PROMPT ---\n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese. They named the animal as the son of a gigantic fox and a big deer. This coyote was called \"the African tiger\" because of its similarity to the Alpine hive, the Himalayan hive, and the wild horses of the region. The buffalo also owned a large mammal, called the Siberian Horse, and a mongo (Wakian jub) named for a leopard. The animal also gained a reputation for its hot summers and other wild dogs, especially when they ran aground on their way to attack a bear in China.\n",
      "\n",
      "In April 2017, the fourth American black bear was discovered in the Pacific, and there is no evidence that it was found in the Wyoming. The discovery is not clear. In a 1998 interview with the Bureau of Nature, Dr. D. W. Morley stated that the \"Birds\" in Wyoming were actually a simian, a \"Yusufian mammal\", with an apparent high proportion of\n"
     ]
    }
   ],
   "source": [
    "# # Lets do a quick dragon prompt validation\n",
    "!cd \"{INFERENCE_DIR}\" && python3 dragon_test.py \"../model/V5-Base-1B5-Enwiki-4k.pth\" \"cuda fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets do a quick memory test\n",
    "# # (We dun expect this to work, as we have not finetune for memory recall, but its a baseline)\n",
    "# !python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/V5-Base-1B5-Stage1.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 : Foundation 16k model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  3.96it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset \n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/V5-Base-1B5-enwiki-16k.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/V5-Base-1B5/V5-Base-1B5-enwiki-16k.yaml', '--trainer.logger.init_args.name=V5-Base-1B5 - Enwiki-16k Foundation (train-ctx=4k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--model.ctx_len=4096', '--model.bptt_learning_range=4'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/V5-Base-1B5/V5-Base-1B5-enwiki-16k.yaml', '--trainer.logger.init_args.name=V5-Base-1B5 - Enwiki-16k Foundation (train-ctx=4k, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto', '--model.ctx_len=4096', '--model.bptt_learning_range=4'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 2646765790\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 2646765790\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230812_193144-hj7xop9j\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mV5-Base-1B5 - Enwiki-16k Foundation (train-ctx=4k, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/hj7xop9j\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             4\n",
      "   - accumulate_grad_batches: 8\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 79.56it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-8c145c390c889a8f_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-6ea97834c464ff4e_*_of_00064.arrow\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-50665dd4de80b503_*_of_00064.arrow\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-b463f1fcb37e6075.arrow and /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_100k-1359e81b212c2dd6/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-db0203585b19d957.arrow\n",
      "Saving the dataset (0/5 shards):  15%|▏| 3000/20347 [00:00<00:01, 10964.94 examp[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Saving the dataset (2/5 shards):  45%|▍| 9140/20347 [00:00<00:01, 9717.94 exampl[rank: 3] Global seed set to 2646765790\n",
      "[rank: 2] Global seed set to 2646765790\n",
      "[rank: 1] Global seed set to 2646765790\n",
      "[rank: 0] Global seed set to 2646765790                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "[2023-08-12 19:32:02,623] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 2646765790\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "[2023-08-12 19:32:16,340] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 2646765790\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "[2023-08-12 19:32:16,372] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 2646765790\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "[2023-08-12 19:32:16,436] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  5.000e-04 (0.0005)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.07362532615661621 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10164451599121094 seconds\n",
      "Time to load fused_adam op: 0.10165095329284668 seconds\n",
      "Time to load fused_adam op: 0.10195517539978027 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.07424569129943848 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10201907157897949 seconds\n",
      "Time to load utils op: 0.10222053527832031 seconds\n",
      "Time to load utils op: 0.10176992416381836 seconds\n",
      "Rank: 1 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Rank: 0 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Rank: 2 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Rank: 3 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.0002627372741699219 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00025463104248046875 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002465248107910156 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005087852478027344 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0:  39%|▍| 2000/5087 [4:07:17<6:21:41,  7.42s/it, v_num=op9j, train/loss=3/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 5087/5087 [10:28:44<00:00,  7.42s/it, v_num=op9j, train/loss=2.\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/26 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/26 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                  | 1/26 [00:01<00:46,  1.87s/it]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍                 | 2/26 [00:03<00:44,  1.85s/it]\u001b[A\n",
      "Validation DataLoader 0:  12%|██▏                | 3/26 [00:05<00:42,  1.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▉                | 4/26 [00:07<00:40,  1.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▋               | 5/26 [00:09<00:38,  1.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  23%|████▍              | 6/26 [00:11<00:36,  1.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  27%|█████              | 7/26 [00:12<00:34,  1.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▊             | 8/26 [00:14<00:33,  1.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  35%|██████▌            | 9/26 [00:16<00:31,  1.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▉           | 10/26 [00:18<00:29,  1.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  42%|███████▌          | 11/26 [00:20<00:27,  1.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  46%|████████▎         | 12/26 [00:22<00:25,  1.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 13/26 [00:23<00:23,  1.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▋        | 14/26 [00:25<00:22,  1.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  58%|██████████▍       | 15/26 [00:27<00:20,  1.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  62%|███████████       | 16/26 [00:29<00:18,  1.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████▊      | 17/26 [00:31<00:16,  1.85s/it]\u001b[A\n",
      "Validation DataLoader 0:  69%|████████████▍     | 18/26 [00:33<00:14,  1.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  73%|█████████████▏    | 19/26 [00:35<00:12,  1.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  77%|█████████████▊    | 20/26 [00:36<00:11,  1.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  81%|██████████████▌   | 21/26 [00:38<00:09,  1.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  85%|███████████████▏  | 22/26 [00:40<00:07,  1.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  88%|███████████████▉  | 23/26 [00:42<00:05,  1.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  92%|████████████████▌ | 24/26 [00:44<00:03,  1.84s/it]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████████████▎| 25/26 [00:46<00:01,  1.84s/it]\u001b[A\n",
      "Epoch 0: 100%|█| 5087/5087 [10:29:36<00:00,  7.43s/it, v_num=op9j, train/loss=2.\u001b[A\n",
      "Epoch 0: 100%|█| 5087/5087 [10:29:36<00:00,  7.43s/it, v_num=op9j, train/loss=2.\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 5087/5087 [10:29:45<00:00,  7.43s/it, v_num=op9j, train/loss=2.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ▆█▅▅▇▂▆▄▅▆▇▃▇▇▇▅▆▄▆▅▆▆▆▆▅▁▅▅▇▅▆▅▄▄▇▆▇▄▃▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 16383\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 3.21875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 635\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 3.15114\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mV5-Base-1B5 - Enwiki-16k Foundation (train-ctx=4k, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/hj7xop9j\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230812_193144-hj7xop9j/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the foundation model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/V5-Base-1B5-enwiki-16k.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Enwiki-16k Foundation (train-ctx=4k, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" \\\n",
    "        --model.ctx_len=4096 \\\n",
    "        --model.bptt_learning_range=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/V5-Base-1B5-enwiki-16k/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 4\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 486 params 1515107840 elements\n",
      "Saving fp32 state dict to ../model/V5-Base-1B5-Enwiki-16k.pth\n",
      "-rw-r--r-- 1 root root 5.7G Aug 13 06:02 ../model/V5-Base-1B5-Enwiki-16k.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/V5-Base-1B5-enwiki-16k/last.ckpt\" \"../model/V5-Base-1B5-Enwiki-16k.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/V5-Base-1B5-Enwiki-16k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "--- DRAGON PROMPT ---\n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese. The researchers were not interested in the game, as they thought the creatures are too advanced and far too easy.\n",
      "\n",
      "At the time, most scientists thought that the human language was in the language, but that they had to travel in other directions. Many scientists, such as Leopold Stolz, believed that the dragon was a real human language, and that the people there saw a more \"fantasy\" appearance. Some scientists doubted that the human language had a human-like origin, and thought it was more useful to show that a human is a human language.\n",
      "\n",
      "The scientists also suggested that the dragon language was a spiritual or psychological-mechanistic language. They found that the dragons could function as a semantically powerful entity, and their influence could affect how humans interact with the human race. In addition, they hypothesized that they could speak Chinese, or speak Chinese. They hypothesized that they could communicate with a foreign language, and were capable of telepathic communication with the ground\n"
     ]
    }
   ],
   "source": [
    "# # Lets do a quick dragon prompt validation\n",
    "!cd \"{INFERENCE_DIR}\" && python3 dragon_test.py \"../model/V5-Base-1B5-Enwiki-16k.pth\" \"cuda fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets do a quick memory test\n",
    "# # (We dun expect this to work, as we have not finetune for memory recall, but its a baseline)\n",
    "# !python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/V5-Base-1B5-Stage1.pth\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3 : Instruct Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████████████| 7.79k/7.79k [00:00<00:00, 17.5MB/s]\n",
      "Downloading and preparing dataset None/None to /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n",
      "Downloading data files:   0%|                             | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                             | 0.00/7.80M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   1%|▏                    | 52.2k/7.80M [00:00<00:26, 290kB/s]\u001b[A\n",
      "Downloading data:   4%|▉                     | 313k/7.80M [00:00<00:07, 984kB/s]\u001b[A\n",
      "Downloading data:  11%|██▎                  | 861k/7.80M [00:00<00:02, 2.43MB/s]\u001b[A\n",
      "Downloading data:  25%|█████               | 1.99M/7.80M [00:00<00:01, 5.10MB/s]\u001b[A\n",
      "Downloading data:  51%|██████████▏         | 3.98M/7.80M [00:00<00:00, 9.62MB/s]\u001b[A\n",
      "Downloading data: 100%|████████████████████| 7.80M/7.80M [00:00<00:00, 10.0MB/s]\u001b[A\n",
      "Downloading data files: 100%|█████████████████████| 1/1 [00:02<00:00,  2.40s/it]\n",
      "Extracting data files: 100%|████████████████████| 1/1 [00:00<00:00, 2267.19it/s]\n",
      "Setting num_proc from 64 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 523.11it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/V5-Base-1B5-instruct.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/V5-Base-1B5/V5-Base-1B5-instruct.yaml', '--trainer.logger.init_args.name=V5-Base-1B5 - Instruct (train-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto'], args=['fit', '-c', '/root/rwkv-x-playground/notebook/experiment/rwkv-x-exp/V5-Base-1B5/V5-Base-1B5-instruct.yaml', '--trainer.logger.init_args.name=V5-Base-1B5 - Instruct (train-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto'].\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 2116075088\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 2116075088\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230813_060358-immnegmk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mV5-Base-1B5 - Instruct (train-ctx=4096, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/immnegmk\u001b[0m\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             4\n",
      "   - accumulate_grad_batches: 8\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 786.63it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-e4df40d582f09838_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-6d5405ad1f265e84_*_of_00064.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-9f578061a1feb072.arrow and /root/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-f0249c95fb8af247.arrow\n",
      "[rank: 0] Global seed set to 2116075088                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "[2023-08-13 06:04:13,530] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 2] Global seed set to 2116075088\n",
      "[rank: 3] Global seed set to 2116075088\n",
      "[rank: 1] Global seed set to 2116075088\n",
      "[rank: 2] Global seed set to 2116075088\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "[2023-08-13 06:04:30,029] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 2116075088\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "[2023-08-13 06:04:30,129] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 2116075088\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "[2023-08-13 06:04:30,158] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  4.000e-04 (0.0004)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.07415962219238281 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10217142105102539 seconds\n",
      "Time to load fused_adam op: 0.10199093818664551 seconds\n",
      "Time to load fused_adam op: 0.1019906997680664 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06702589988708496 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10154604911804199 seconds\n",
      "Time to load utils op: 0.10164332389831543 seconds\n",
      "Time to load utils op: 0.10204005241394043 seconds\n",
      "Rank: 1 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Rank: 2 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Rank: 3 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Rank: 0 partition count [4, 4] and sizes[(378776576, False), (384, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002624988555908203 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002636909484863281 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00026297569274902344 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0004994869232177734 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0:  54%|▌| 2000/3733 [18:14<15:48,  1.83it/s, v_num=egmk, train/loss=2.020/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 3733/3733 [33:49<00:00,  1.84it/s, v_num=egmk, train/loss=4.060\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/19 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/19 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|█                  | 1/19 [00:00<00:04,  3.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██                 | 2/19 [00:00<00:04,  4.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|███                | 3/19 [00:00<00:03,  4.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|████               | 4/19 [00:00<00:03,  4.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|█████              | 5/19 [00:01<00:02,  4.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|██████             | 6/19 [00:01<00:02,  4.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|███████            | 7/19 [00:01<00:02,  5.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|████████           | 8/19 [00:01<00:02,  5.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|█████████          | 9/19 [00:01<00:01,  5.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|█████████▍        | 10/19 [00:01<00:01,  5.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|██████████▍       | 11/19 [00:02<00:01,  5.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|███████████▎      | 12/19 [00:02<00:01,  5.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|████████████▎     | 13/19 [00:02<00:01,  5.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|█████████████▎    | 14/19 [00:02<00:00,  5.36it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|██████████████▏   | 15/19 [00:02<00:00,  5.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%|███████████████▏  | 16/19 [00:02<00:00,  5.44it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████████████  | 17/19 [00:03<00:00,  5.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|█████████████████ | 18/19 [00:03<00:00,  5.53it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 3733/3733 [33:57<00:00,  1.83it/s, v_num=egmk, train/loss=4.060\u001b[A\n",
      "Epoch 0: 100%|█| 3733/3733 [33:57<00:00,  1.83it/s, v_num=egmk, train/loss=4.060\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 3733/3733 [34:06<00:00,  1.82it/s, v_num=egmk, train/loss=4.060\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▄▂▂▃▂▆█▄▂▃▃▂▁▃▆▂▃▃▁▆▂▁▅▂▄▂▃▂▁▁▄▃▃▂▁▂▁▁▃▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▇▆▃▅▁▇▅▄█▅▆▇▄▆▆▇▅▂█▃▄▆▆▇▂▄▅▅▁▆█▁▅▅▆▆▃▄▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 18\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 85\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 3.76562\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 466\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 2.93267\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mV5-Base-1B5 - Instruct (train-ctx=4096, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/immnegmk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230813_060358-immnegmk/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the instruct finetuning\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/V5-Base-1B5-instruct.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Instruct (train-ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/V5-Base-1B5-instruct/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 4\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 486 params 1515107840 elements\n",
      "Saving fp32 state dict to ../model/V5-Base-1B5-Enwiki-Instruct.pth\n",
      "-rw-r--r-- 1 root root 5.7G Aug 13 06:39 ../model/V5-Base-1B5-Enwiki-Instruct.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/V5-Base-1B5-instruct/last.ckpt\" \"../model/V5-Base-1B5-Enwiki-Instruct.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/V5-Base-1B5-Enwiki-Instruct.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "--- DRAGON PROMPT ---\n",
      "In a shocking finding, scientist discovered a herd of dragons living in a remote, previously unexplored valley, in Tibet. Even more surprising to the researchers was the fact that the dragons spoke perfect Chinese. In a vivid, energetic and convincing story, the dragon brought his birth to earth.\n",
      "\n",
      "The dragon, finding that dragons, reached his destination, eventually conquered the dragon, and transformed the dragon into the dragon.\n",
      "The dragon is the second-oldest of the dragon, its dragon, but the dragon did not rest in his path.\n",
      "The dragon of its dragon, Mian Zimura, the first dragon in the modern world.\n",
      "The dragon was intended to move the dragon in the future.\n",
      "The dragon first appeared in the dragon's own mind.\n",
      "The dragon's dragon had magical and immobile as a human, one reason it was a waste, to also help to bring any dragon in his quest.\n",
      "\n",
      "As of the following, the dragon of the dragon had never had any past.\n",
      "The dragon only reached the final.\n",
      "The dragon was born in a snow and an infant.\n",
      "The dragon was created when the dragon would survive.\n",
      "The dragon of\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick dragon prompt validation\n",
    "!cd \"{INFERENCE_DIR}\" && python3 dragon_test.py \"../model/V5-Base-1B5-Enwiki-Instruct.pth\" \"cuda fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets do a quick memory test\n",
    "# # (We dun expect this to work, as we have not finetune for memory recall, but its a baseline)\n",
    "# !python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/V5-Base-1B5-Enwiki-Instruct.pth\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
