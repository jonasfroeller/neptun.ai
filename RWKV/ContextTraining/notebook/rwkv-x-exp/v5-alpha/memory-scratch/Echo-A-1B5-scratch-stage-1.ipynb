{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Echo-A 1B5 (Memory model from scratch)\n",
    "\n",
    "**HEADSUP** This is pretty much my final attempt to skip enwiki+gpt4all steps, and to train from scratch - TLDR, does not work.\n",
    "\n",
    "---\n",
    "\n",
    "This attempts to build the memory model in stages, from scratch\n",
    "(Instead of previous attempts in doing enwiki foundation + gpt4all + etc)\n",
    "\n",
    "> This project assumes you have the rwkv-infctx conda env setup, and you are executing in that environment - see the main README.md for the conda env setup steps\n",
    "\n",
    "## Insights from previous failed attempt at training from scratch / training from enwiki\n",
    "\n",
    "The following are insights that was found after multiple back and forth (3 weeks+ of experiments). This insights in partcular was derived from the failure to tune enwiki model, but success in doing so after limited gpt4all tuning, in limited capacity.\n",
    "\n",
    "What prevented training from scratch, was the lack of unmasked \"training instruction data\", as the original finetune went straight to fully masked instruction+input, and unmasked outputs. This worked on raven models, because they have already be pretrained for memory recall. But is unable to teach the model on its own otherwise.\n",
    "\n",
    "On the other hand if we were to train with the instruction unmasked, even with the \"input\" document masked (what needs to be memorized), the model end up thinking it should be somewhat RNG-ing a specific set of words, given an instruction. And fail to fully learn the memorization task. But at the very least, it will learn the \"instruction statement\" trigger. \n",
    "\n",
    "Finally, we used the original limited word list for the bulk of the training / validation. Instead of the full 400k+ larger word list. Another issue that was faced was that the model training / memory gets completely blindsided (bad loss) when it encounter a set of words it has never seen before. This is possible even after 100k samples, due to how large the word list was. Preventing / slowing down the training process.\n",
    "\n",
    "While the enwiki finetune, resolve these issues in stages, since our goal is to be able to replicate this experiment across multiple models config rapidly. The training from scratch model, is an attempt to remove the enwiki+gpt4all steps required in the process (hopefully) - by ensuring a good mix of all 3 of the above data - when training from scratch (instead of previous attempts at them one-by-one)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch - Stage 1\n",
    "\n",
    "Prepare and preload the finetuning process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 2 max words, 5000 samples - at ./dataset/segmented-word-2-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 5000 samples - at ./dataset/full-masked-word-2-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 10000 samples - at ./dataset/full-masked-word-5-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 5000 samples - at ./dataset/limited-masked-word-2-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 20000 samples - at ./dataset/segmented-word-5-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 10000 samples - at ./dataset/full-masked-word-10-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 20000 samples - at ./dataset/segmented-word-10-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 20000 samples - at ./dataset/segmented-word-15-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 10000 samples - at ./dataset/limited-masked-word-5-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 10000 samples - at ./dataset/limited-masked-word-20-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 10000 samples - at ./dataset/full-masked-word-15-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 20000 samples - at ./dataset/segmented-word-20-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 10000 samples - at ./dataset/limited-masked-word-15-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 10000 samples - at ./dataset/limited-masked-word-10-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 10000 samples - at ./dataset/full-masked-word-20-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 10000 samples - at ./dataset/limited-masked-word-40-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 10000 samples - at ./dataset/full-masked-word-40-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 20000 samples - at ./dataset/segmented-word-40-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 5000 samples - at ./dataset/limited-masked-word-100-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 10000 samples - at ./dataset/segmented-word-100-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 5000 samples - at ./dataset/limited-masked-word-200-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 10000 samples - at ./dataset/full-masked-word-80-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 10000 samples - at ./dataset/limited-masked-word-80-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 20000 samples - at ./dataset/segmented-word-80-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 10000 samples - at ./dataset/segmented-word-200-count.jsonl\n",
      "## Done ##\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "# Reset the dataset dir\n",
    "mkdir -p ./dataset\n",
    "rm -rf ./dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "# For the first stage of < 512 tokens, we form a strong bias for <= 100 words\n",
    "# to focus training with smaller datasets in the inital stages\n",
    "\n",
    "# Segmented JSONL, was designed to be only masking the input document. \n",
    "# While it failed to teach memorization properly, it teaches the model how to understand the instruction triggers.\n",
    "#\n",
    "# One theory, is that it loosely teach memory, but the model is confused thinking maybe these words should be randomly\n",
    "# generated when seeing a certain instruction. Which is not the case.\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/segmented-word-2-count.jsonl  2  5000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/segmented-word-5-count.jsonl  5  10000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/segmented-word-10-count.jsonl 10 10000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/segmented-word-15-count.jsonl 15 10000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/segmented-word-20-count.jsonl 20 10000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/segmented-word-40-count.jsonl 40 10000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/segmented-word-80-count.jsonl 80 10000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/segmented-word-100-count.jsonl 100 5000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/segmented-word-200-count.jsonl 200 5000 &\n",
    "\n",
    "# Prompt completion pairs, are fully masked instruction and input, with unmasked outputs\n",
    "# This is required to actually teach the model how to memorize the input, but on its own, \n",
    "# its unable to actually teach the model how to trigger this behavior (as the instruct is masked)\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-2-count.jsonl  2  5000 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-5-count.jsonl  5  5000 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-10-count.jsonl 10 5000 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-15-count.jsonl 15 5000 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-20-count.jsonl 20 5000 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-40-count.jsonl 40 5000 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-80-count.jsonl 80 5000 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-100-count.jsonl 100 2500 &\n",
    "python ./memory_script/gen_limited_masked_jsonl.py ./dataset/limited-masked-word-200-count.jsonl 200 2500 &\n",
    "\n",
    "# Prompt completion pairs, with the full word list. Due to the size of the full word list, it \n",
    "# was possible to be stuck training the model just to recognize new words / tokens, and not perform the memorization task\n",
    "# this greatly slowed down the memorization learning process. As the model was constantly learning new words. \n",
    "# With 400k+ words total, even after 100k worth of document samples, new words can appear (due to how RNG works)\n",
    "#\n",
    "# We still include a mix of the data, in an attempt to reduce overtraining the model to only a fixed token set.\n",
    "# which was one of the weakness faced in the original training / benchmark (but technically not an issue for measuring memory)\n",
    "python ./memory_script/gen_full_masked_jsonl.py ./dataset/full-masked-word-2-count.jsonl  2  5000 &\n",
    "python ./memory_script/gen_full_masked_jsonl.py ./dataset/full-masked-word-5-count.jsonl  5  5000 &\n",
    "python ./memory_script/gen_full_masked_jsonl.py ./dataset/full-masked-word-10-count.jsonl 10 5000 &\n",
    "python ./memory_script/gen_full_masked_jsonl.py ./dataset/full-masked-word-15-count.jsonl 10 5000 &\n",
    "python ./memory_script/gen_full_masked_jsonl.py ./dataset/full-masked-word-20-count.jsonl 20 5000 &\n",
    "python ./memory_script/gen_full_masked_jsonl.py ./dataset/full-masked-word-40-count.jsonl 40 5000 &\n",
    "python ./memory_script/gen_full_masked_jsonl.py ./dataset/full-masked-word-80-count.jsonl 80 5000 &\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_1\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /root/picocreator-memory-experiment/notebook/experiment/memory-scratch\n",
      "TRAINER_DIR: /root/picocreator-memory-experiment/RWKV-v4neo\n",
      "PROJECT_DIR: /root/picocreator-memory-experiment\n"
     ]
    }
   ],
   "source": [
    "# Configure your preferred options\n",
    "\n",
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"Echo-A-1B5\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v4neo/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Low word count memory training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████████████| 25/25 [00:00<00:00, 32403.46it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-24954af4c2d3e231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 3765.08it/s]\n",
      "Extracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 112.24it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-24954af4c2d3e231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 117.52it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/Echo-A-1B5-scratch-stage-1.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/Echo-A-1B5-scratch-stage-1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 2846274414\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 2846274414\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230712_164508-e06vfudh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mEcho-A-1B5 - Scratch-Stage-1 (bs=256, train-ctx=512, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/e06vfudh\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|██████████████████| 25/25 [00:00<00:00, 72365.49it/s]\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-24954af4c2d3e231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 155.17it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-24954af4c2d3e231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d14832e4dc3d4844_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-24954af4c2d3e231/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-b027b4d81968f2ab_*_of_00064.arrow\n",
      "[rank: 0] Global seed set to 2846274414                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-07-12 16:45:22,767] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 5] Global seed set to 2846274414\n",
      "[rank: 4] Global seed set to 2846274414\n",
      "[rank: 1] Global seed set to 2846274414\n",
      "[rank: 3] Global seed set to 2846274414\n",
      "[rank: 2] Global seed set to 2846274414\n",
      "[rank: 6] Global seed set to 2846274414\n",
      "[rank: 7] Global seed set to 2846274414\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "[rank: 5] Global seed set to 2846274414\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-07-12 16:45:35,628] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 2846274414\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-07-12 16:45:37,340] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 3] Global seed set to 2846274414\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-07-12 16:45:39,845] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 2846274414\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-07-12 16:45:39,883] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 2846274414\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-07-12 16:45:39,943] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 2846274414\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-07-12 16:45:39,950] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 2846274414\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-07-12 16:45:39,955] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 5.000e-04 (0.0005)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.062418460845947266 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10094475746154785 seconds\n",
      "Time to load fused_adam op: 0.10132074356079102 seconds\n",
      "Time to load fused_adam op: 0.10115575790405273 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10121369361877441 seconds\n",
      "Time to load fused_adam op: 0.10128474235534668 seconds\n",
      "Time to load fused_adam op: 0.10131144523620605 seconds\n",
      "Time to load fused_adam op: 0.1018218994140625 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06919670104980469 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1015174388885498 seconds\n",
      "Time to load utils op: 0.10179376602172852 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10177230834960938 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1021726131439209 seconds\n",
      "Time to load utils op: 0.1018209457397461 seconds\n",
      "Time to load utils op: 0.10183095932006836 seconds\n",
      "Time to load utils op: 0.10193681716918945 seconds\n",
      "Rank: 2 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 3 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 4 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 1 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 6 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 7 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 0 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Rank: 5 partition count [8, 8, 8] and sizes[(189376000, False), (6144, False), (6144, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Time to load utils op: 0.0002651214599609375 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00027632713317871094 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002727508544921875 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00024318695068359375 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002155303955078125 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002789497375488281 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003440380096435547 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0004942417144775391 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0:   1%| | 320/34757 [02:06<3:46:39,  2.53it/s, v_num=fudh, train/loss=9.6/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 34757/34757 [4:18:26<00:00,  2.24it/s, v_num=fudh, train/loss=4\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▌                  | 1/35 [00:00<00:07,  4.59it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                  | 2/35 [00:00<00:06,  5.15it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▋                 | 3/35 [00:00<00:05,  5.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██▏                | 4/35 [00:00<00:05,  5.63it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▋                | 5/35 [00:00<00:05,  5.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▎               | 6/35 [00:01<00:04,  5.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▊               | 7/35 [00:01<00:04,  5.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|████▎              | 8/35 [00:01<00:04,  5.91it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▉              | 9/35 [00:01<00:04,  5.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|█████▏            | 10/35 [00:01<00:04,  5.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▋            | 11/35 [00:01<00:04,  5.99it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|██████▏           | 12/35 [00:01<00:03,  6.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▋           | 13/35 [00:02<00:03,  6.03it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|███████▏          | 14/35 [00:02<00:03,  6.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▋          | 15/35 [00:02<00:03,  6.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████████▏         | 16/35 [00:02<00:03,  6.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▋         | 17/35 [00:02<00:02,  6.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|█████████▎        | 18/35 [00:02<00:02,  6.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▊        | 19/35 [00:03<00:02,  6.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|██████████▎       | 20/35 [00:03<00:02,  6.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|██████████▊       | 21/35 [00:03<00:02,  6.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|███████████▎      | 22/35 [00:03<00:02,  6.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|███████████▊      | 23/35 [00:03<00:01,  6.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|████████████▎     | 24/35 [00:03<00:01,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|████████████▊     | 25/35 [00:04<00:01,  6.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|█████████████▎    | 26/35 [00:04<00:01,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|█████████████▉    | 27/35 [00:04<00:01,  6.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|██████████████▍   | 28/35 [00:04<00:01,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|██████████████▉   | 29/35 [00:04<00:00,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|███████████████▍  | 30/35 [00:04<00:00,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|███████████████▉  | 31/35 [00:05<00:00,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|████████████████▍ | 32/35 [00:05<00:00,  6.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|████████████████▉ | 33/35 [00:05<00:00,  6.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|█████████████████▍| 34/35 [00:05<00:00,  6.10it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 34757/34757 [4:18:38<00:00,  2.24it/s, v_num=fudh, train/loss=4\u001b[A\n",
      "Epoch 0: 100%|█| 34757/34757 [4:18:38<00:00,  2.24it/s, v_num=fudh, train/loss=4\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 34757/34757 [4:18:51<00:00,  2.24it/s, v_num=fudh, train/loss=4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.009 MB of 0.009 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▆▁▁▂▂▂▁▃▁▁█▁▁▂▁▁▁▅▁▁▂▁▁▅▄▂▁▃▅▁▂▁▅▁▂▁▆▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▇▅▇▆▄▄▇▃▁▆▂▂▁▆▂▆▆▄▄▆▂▇▄▃▃▇▄▁▅▆▄▂▂▂▄▅▂▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 34\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 106\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 272\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 5.6875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 1086\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 6.30603\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mEcho-A-1B5 - Scratch-Stage-1 (bs=256, train-ctx=512, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/e06vfudh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230712_164508-e06vfudh/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the memory model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export RWKV_TORCH_COMPILE=0 && \\\n",
    "    export RWKV_JIT_ON=1 && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/Echo-A-1B5-scratch-stage-1.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Scratch-Stage-1 (bs=256, train-ctx=512, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/Echo-A-1B5-scratch-stage-1/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 438 params 1515106304 elements\n",
      "Saving fp32 state dict to ../model/Echo-A-1B5-Scratch-Stage-1.pth\n",
      "-rw-r--r-- 1 root root 5.7G Jul 13 02:27 ../model/Echo-A-1B5-Scratch-Stage-1.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/Echo-A-1B5-scratch-stage-1/last.ckpt\" \"../model/Echo-A-1B5-Scratch-Stage-1.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/Echo-A-1B5-Scratch-Stage-1.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_cuda/build.ninja...\n",
      "Building extension module wkv_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_cuda...\n",
      "RWKV_JIT_ON 1 RWKV_CUDA_ON 1 RESCALE_LAYER 0\n",
      "\n",
      "Loading /root/picocreator-memory-experiment/model/Echo-A-1B5-Scratch-Stage-1.pth ...\n",
      "Strategy: (total 24+1=25 layers)\n",
      "* cpu [float32, float32], store 25 layers\n",
      "0-cpu-float32-float32 1-cpu-float32-float32 2-cpu-float32-float32 3-cpu-float32-float32 4-cpu-float32-float32 5-cpu-float32-float32 6-cpu-float32-float32 7-cpu-float32-float32 8-cpu-float32-float32 9-cpu-float32-float32 10-cpu-float32-float32 11-cpu-float32-float32 12-cpu-float32-float32 13-cpu-float32-float32 14-cpu-float32-float32 15-cpu-float32-float32 16-cpu-float32-float32 17-cpu-float32-float32 18-cpu-float32-float32 19-cpu-float32-float32 20-cpu-float32-float32 21-cpu-float32-float32 22-cpu-float32-float32 23-cpu-float32-float32 24-cpu-float32-float32 \n",
      "blocks.0.att.key.weight           f32      cpu   2048  2048 \n",
      "blocks.0.att.output.weight        f32      cpu   2048  2048 \n",
      "blocks.0.att.receptance.weight    f32      cpu   2048  2048 \n",
      "blocks.0.att.time_mix_k           f32      cpu   2048       \n",
      "blocks.0.att.time_mix_r           f32      cpu   2048       \n",
      "blocks.0.att.time_mix_v           f32      cpu   2048       \n",
      "blocks.0.att.value.weight         f32      cpu   2048  2048 \n",
      "blocks.0.ffn.key.weight           f32      cpu   2048  8192 \n",
      "blocks.0.ffn.receptance.weight    f32      cpu   2048  2048 \n",
      "blocks.0.ffn.time_mix_k           f32      cpu   2048       \n",
      "blocks.0.ffn.time_mix_r           f32      cpu   2048       \n",
      "blocks.0.ffn.value.weight         f32      cpu   8192  2048 \n",
      "blocks.0.ln1.bias                 f32      cpu   2048       \n",
      "blocks.0.ln1.weight               f32      cpu   2048       \n",
      "blocks.0.ln2.bias                 f32      cpu   2048       \n",
      "blocks.0.ln2.weight               f32      cpu   2048       \n",
      "................................................................................................................................................................................................................................................\n",
      "blocks.23.att.key.weight          f32      cpu   2048  2048 \n",
      "blocks.23.att.output.weight       f32      cpu   2048  2048 \n",
      "blocks.23.att.receptance.weight   f32      cpu   2048  2048 \n",
      "blocks.23.att.time_mix_k          f32      cpu   2048       \n",
      "blocks.23.att.time_mix_r          f32      cpu   2048       \n",
      "blocks.23.att.time_mix_v          f32      cpu   2048       \n",
      "blocks.23.att.value.weight        f32      cpu   2048  2048 \n",
      "blocks.23.ffn.key.weight          f32      cpu   2048  8192 \n",
      "blocks.23.ffn.receptance.weight   f32      cpu   2048  2048 \n",
      "blocks.23.ffn.time_mix_k          f32      cpu   2048       \n",
      "blocks.23.ffn.time_mix_r          f32      cpu   2048       \n",
      "blocks.23.ffn.value.weight        f32      cpu   8192  2048 \n",
      "blocks.23.ln1.bias                f32      cpu   2048       \n",
      "blocks.23.ln1.weight              f32      cpu   2048       \n",
      "blocks.23.ln2.bias                f32      cpu   2048       \n",
      "blocks.23.ln2.weight              f32      cpu   2048       \n",
      "................................................................................................................\n",
      "emb.weight                        f32      cpu  50277  2048 \n",
      "head.weight                       f32      cpu   2048 50277 \n",
      "ln_out.bias                       f32      cpu   2048       \n",
      "ln_out.weight                     f32      cpu   2048       \n",
      "blocks.0.att.time_decay           f32      cpu   2048       \n",
      "...............\n",
      "blocks.23.att.time_decay          f32      cpu   2048       \n",
      ".......\n",
      "blocks.0.att.time_first           f32      cpu   2048       \n",
      "...............\n",
      "blocks.23.att.time_first          f32      cpu   2048       \n",
      ".......###\n",
      "### Model validation start ###\n",
      "###\n",
      "Model validation at 5 tokens : 40.0% similarity, with 2 matched token, and 3 token mismatch\n",
      "Model validation at 10 tokens : 20.0% similarity, with 2 matched token, and 8 token mismatch\n",
      "Model validation at 15 tokens : 13.333333333333334% similarity, with 2 matched token, and 13 token mismatch\n",
      "Model validation at 20 tokens : 10.0% similarity, with 2 matched token, and 18 token mismatch\n",
      "Model validation at 25 tokens : 8.0% similarity, with 2 matched token, and 23 token mismatch\n",
      "Model validation at 30 tokens : 6.666666666666667% similarity, with 2 matched token, and 28 token mismatch\n",
      "Model validation at 35 tokens : 5.714285714285714% similarity, with 2 matched token, and 33 token mismatch\n",
      "Model validation at 40 tokens : 5.0% similarity, with 2 matched token, and 38 token mismatch\n",
      "Model validation at 45 tokens : 4.444444444444445% similarity, with 2 matched token, and 43 token mismatch\n",
      "Model validation at 50 tokens : 6.0% similarity, with 3 matched token, and 47 token mismatch\n",
      "Model validation at 55 tokens : 5.454545454545454% similarity, with 3 matched token, and 52 token mismatch\n",
      "Model validation at 60 tokens : 5.0% similarity, with 3 matched token, and 57 token mismatch\n",
      "Model validation at 65 tokens : 4.615384615384616% similarity, with 3 matched token, and 62 token mismatch\n",
      "Model validation at 70 tokens : 4.285714285714286% similarity, with 3 matched token, and 67 token mismatch\n",
      "Model validation at 75 tokens : 4.0% similarity, with 3 matched token, and 72 token mismatch\n",
      "Model validation at 80 tokens : 3.75% similarity, with 3 matched token, and 77 token mismatch\n",
      "Model validation at 85 tokens : 3.5294117647058822% similarity, with 3 matched token, and 82 token mismatch\n",
      "Model validation at 90 tokens : 3.3333333333333335% similarity, with 3 matched token, and 87 token mismatch\n",
      "Model validation at 95 tokens : 3.1578947368421053% similarity, with 3 matched token, and 92 token mismatch\n",
      "Model validation at 100 tokens : 4.0% similarity, with 4 matched token, and 96 token mismatch\n",
      "Model validation at 110 tokens : 3.6363636363636362% similarity, with 4 matched token, and 106 token mismatch\n",
      "Model validation at 120 tokens : 3.3333333333333335% similarity, with 4 matched token, and 116 token mismatch\n",
      "Model validation at 130 tokens : 3.076923076923077% similarity, with 4 matched token, and 126 token mismatch\n",
      "Model validation at 140 tokens : 2.857142857142857% similarity, with 4 matched token, and 136 token mismatch\n",
      "Model validation at 150 tokens : 4.0% similarity, with 6 matched token, and 144 token mismatch\n",
      "Model validation at 175 tokens : 3.428571428571429% similarity, with 6 matched token, and 169 token mismatch\n",
      "Model validation at 200 tokens : 3.5000000000000004% similarity, with 7 matched token, and 193 token mismatch\n",
      "Model validation at 225 tokens : 3.111111111111111% similarity, with 7 matched token, and 218 token mismatch\n",
      "Model validation at 250 tokens : 2.8000000000000003% similarity, with 7 matched token, and 243 token mismatch\n",
      "Model validation at 275 tokens : 2.5454545454545454% similarity, with 7 matched token, and 268 token mismatch\n",
      "Model validation at 300 tokens : 2.3333333333333335% similarity, with 7 matched token, and 293 token mismatch\n",
      "Model validation at 325 tokens : 2.1538461538461537% similarity, with 7 matched token, and 318 token mismatch\n",
      "Model validation at 350 tokens : 2.0% similarity, with 7 matched token, and 343 token mismatch\n",
      "Model validation at 375 tokens : 2.1333333333333333% similarity, with 8 matched token, and 367 token mismatch\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick dragon prompt validation\n",
    "!python3 ./memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/Echo-A-1B5-Scratch-Stage-1.pth\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
