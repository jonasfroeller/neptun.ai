{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepspeed 1, 2 & 3 benchmark\n",
    "This model being trained has the same settings as raven 1B5 model.\n",
    "- Layer count: 24\n",
    "- Embed size: 2048\n",
    "\n",
    "The goal is to validate the trainer across deepspeed 1, 2 & 3 - with and without offload. All other training params remain constant. And benchmarking them accordingly\n",
    "\n",
    "## What does deepspeed 1, 2 & 3 do (With/Without CPU offload) ??\n",
    "\n",
    "Instead of simply splitting the dataset being trained, and having a full copy of nearly everything in all GPU's (aka DDP / DeepSpeed 1).\n",
    "\n",
    "Deepspeed 2, keeps a full copy of the model weights on each GPU, but splits the training gradient descent memory usage into multiple GPUs, or offload it into CPU memory (+ CPU offload option).\n",
    "\n",
    "Deepspeed 3, takes it a step further, and distributes the model weights across all the GPUs, drastically lowering the vram requirement, while increasing the amount of GPU to GPU traffic drastically. Gradient descent memory is still split across multiple GPUs, with the option to offload into CPU memory (Same as deepspeed 2)\n",
    "\n",
    "Finally, Deepspeed 3, also introduce options to further offload such model weights / gradient descent, more into CPU memory or NVMe. However this option was not enabled or explored in the following benchmarks.\n",
    "\n",
    "See more here: https://huggingface.co/docs/transformers/main_classes/deepspeed\n",
    "\n",
    "## Benchmark results\n",
    "\n",
    "Benchmark was done on 20th Aug 2023. With Torch 2.0.1, Cuda 11.8. On 8x3090, via vast.ai\n",
    "All benchmarks was done with ctx length of 4096\n",
    "\n",
    "(@TODO - conslidate and update result)\n",
    "\n",
    "---\n",
    "\n",
    "| Deepspeed Strat       | Time (A5000)          | Time (3090)           | VRAM Usage       | RAM Usage | Validation Loss |\n",
    "| --------------------- | --------------------- | --------------------- | ---------------- | --------- | --------------- |\n",
    "| Stage 2               | 24 mins : 55 sec      | 35 mins : 04 sec      | ~22.3 + 23.8 GB  | ~85 GB    | 6.173           |\n",
    "| Stage 2 + CPU offload | 43 mins : 08 sec      | 59 mins : 04 sec      | ~9.7 + 10.3 GB   | ~128 GB   | 6.124           |\n",
    "| Stage 3               | 29 mins : 12 sec      | 50 mins : 04 sec      | ~23.0 + 23.2 GB^ | ~85 GB    | 5.665           |\n",
    "| Stage 3 + CPU offload | 1hr : 42mins : 38 sec | 1hr : 29mins : 15 sec | ~7.0 + 7.3 GB    | ~145 GB   | 5.668           |\n",
    "\n",
    "---\n",
    "\n",
    "> ^ note in theory deepspeed 3 uses less vram then deepspeed 2, however it will also try to use up more ram then its needed for \"cache\" items if possible, maxing out to the same level as deepspeed 2 here\n",
    ">\n",
    "> Torch.JIT was enabled for deepspeed 2, But was disabled for deepspeed 3 (not compatible). Torch.compile was disabled\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and apply your preferred settings\n",
    "\n",
    "Adjust your desired deepspeed settings, and gpu device count.\n",
    "\n",
    "Enable/Disable WANDB here as well ( Enabled by default, as we need the loss curve for this experiment )\n",
    "\n",
    "( note you will need to rerun this cell, if you restart your env )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=False\n",
    "WANDB_PREFIX=\"infctx-v5-deepspeed-test\"\n",
    "\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the model\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 ./init_model.py \\\n",
    "        --n_layer 24 --n_embd 2048 \\\n",
    "        --vocab_size neox --skip-if-exists \\\n",
    "        \"../model/L24-D2048-neox-v5base-init.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets preload the requried dataset \n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/config/enwiki_100k-4096.yaml\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepspeed 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-4096.yaml\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v5-enwiki-100k-ds1/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (deepspeed_stage_1, train-ctx=4096, data-ctx=4096)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_1\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepspeed 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-4096.yaml\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v5-enwiki-100k-ds2/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (deepspeed_stage_2, train-ctx=4096, data-ctx=4096)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_2\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepspeed 2 + Offload\n",
    "Perform a full 1 epoch training run of training context size = 1024. With deepspeed 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-4096.yaml\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v5-enwiki-100k-ds2_o/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (deepspeed_stage_2_offload, train-ctx=4096, data-ctx=4096)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_2_offload\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepspeed 3\n",
    "Perform a full 1 epoch training run of training context size = 1024. With deepspeed 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export RWKV_JIT_ON=0 && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-4096.yaml\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v5-enwiki-100k-ds3/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (deepspeed_stage_3, train-ctx=4096, data-ctx=4096)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_3\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepspeed 3 + offload\n",
    "Perform a full 1 epoch training run of training context size = 1024. With deepspeed 3 + offload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export RWKV_JIT_ON=0 && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-4096.yaml\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v5-enwiki-100k-ds3_o/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (deepspeed_stage_3_offload, train-ctx=4096, data-ctx=4096)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_3_offload\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
