{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset continue test\n",
    "\n",
    "We use a sorted enwiki_100k and interrupt midway, and continue from checkpoint\n",
    "We validate that the dataset curve follows as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: 1\n",
      "NOTEBOOK_DIR: /workspace/picocreator/RWKV-infctx-trainer/notebook/trainer-v5-validation\n",
      "TRAINER_DIR: /workspace/picocreator/RWKV-infctx-trainer/RWKV-v5\n",
      "PROJECT_DIR: /workspace/picocreator/RWKV-infctx-trainer\n"
     ]
    }
   ],
   "source": [
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"infctx-v5-dataset-continue-test\"\n",
    "\n",
    "# We limit to 1 for this test\n",
    "GPU_DEVICES=\"1\"\n",
    "\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-10 19:49:19,920] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "---- Initializing model ----\n",
      "No of layers: 6\n",
      "Embedding size: 512\n",
      "Output model path: ../model/L6-D512-world-init.pth\n",
      "Vocab size: 65536\n",
      "Emb scale: 0.0001\n",
      "Note: this process takes a significant time (and ram) for large models\n",
      "---- ----- ----\n",
      "Model exists, skipping init_model\n"
     ]
    }
   ],
   "source": [
    "# Lets initialized the L6-D512 model with the init_model.py code\n",
    "!cd \"{TRAINER_DIR}\" && python3 init_model.py \\\n",
    "    --n_layer 6 --n_embd 512 \\\n",
    "    --vocab_size world \\\n",
    "    --skip-if-exists --safe-init \\\n",
    "    ../model/L6-D512-world-init.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████████████████| 433/433 [00:00<00:00, 4.92MB/s]\n",
      "Downloading data: 100%|██████████████████████| 261M/261M [00:46<00:00, 5.61MB/s]\n",
      "Downloading data: 100%|██████████████████████| 257M/257M [00:55<00:00, 4.65MB/s]\n",
      "Downloading data: 100%|██████████████████████| 260M/260M [00:49<00:00, 5.25MB/s]\n",
      "Downloading data: 100%|██████████████████████| 257M/257M [00:54<00:00, 4.76MB/s]\n",
      "Downloading data: 100%|██████████████████████| 257M/257M [01:11<00:00, 3.60MB/s]\n",
      "Downloading data: 100%|██████████████████████| 259M/259M [00:54<00:00, 4.73MB/s]\n",
      "Setting num_proc from 160 to 6 for the train split as it only contains 6 shards.\n",
      "Generating train split: 100%|█| 1000000/1000000 [00:03<00:00, 259169.30 examples\n",
      "Map (num_proc=160): 100%|███| 1000000/1000000 [00:17<00:00, 56840.38 examples/s]\n",
      "Filter (num_proc=160): 100%|█| 1000000/1000000 [00:05<00:00, 169992.69 examples/\n",
      "Map (num_proc=160): 100%|█████| 467553/467553 [00:05<00:00, 77935.36 examples/s]\n",
      "Saving the dataset (7/7 shards): 100%|█| 467553/467553 [00:10<00:00, 46628.18 ex\n",
      "Saving the dataset (1/1 shards): 100%|█| 4723/4723 [00:00<00:00, 38762.97 exampl\n"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset \n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/config/enwiki_100k-world-sorted.yaml\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/picocreator/RWKV-infctx-trainer/RWKV-v5\n"
     ]
    }
   ],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    mkdir -p \"{PROJECT_DIR}/checkpoint/v5-enwiki-100k-sorted/\" && \\\n",
    "    rm -rf \"{PROJECT_DIR}/checkpoint/v5-enwiki-100k-sorted/*\" && pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-11 01:58:48,525] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/workspace/picocreator/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/enwiki_100k-world-sorted.yaml', '--model.load_model=../model/L6-D512-world-init.pth', '--trainer.max_steps=251', '--trainer.callbacks.init_args.every_n_train_steps=50', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-enwiki-100k-sorted/', '--trainer.logger.init_args.name=infctx-v5-dataset-continue-test - PART 1 (deepspeed_stage_1, train-ctx=4096, data-ctx=4096)', '--trainer.strategy=deepspeed_stage_1', '--trainer.microbatch_size=8', '--trainer.devices=1'], args=['fit', '-c', '/workspace/picocreator/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/enwiki_100k-world-sorted.yaml', '--model.load_model=../model/L6-D512-world-init.pth', '--trainer.max_steps=251', '--trainer.callbacks.init_args.every_n_train_steps=50', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-enwiki-100k-sorted/', '--trainer.logger.init_args.name=infctx-v5-dataset-continue-test - PART 1 (deepspeed_stage_1, train-ctx=4096, data-ctx=4096)', '--trainer.strategy=deepspeed_stage_1', '--trainer.microbatch_size=8', '--trainer.devices=1'].\n",
      "Seed set to 3941088705\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - microbatch_size:         8\n",
      "   - accumulate_grad_batches: 2\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "Using the latest cached version of the dataset since teven/enwiki_100k couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/teven___enwiki_100k/default/0.0.0/a06f99f0c574c9bb414fbe4521ae5204463c1ec3 (last modified on Sat Feb 10 22:23:23 2024).\n",
      "Saving the dataset (4/4 shards): 100%|█| 133346/133346 [00:04<00:00, 32865.43 ex\n",
      "Saving the dataset (1/1 shards): 100%|█| 1347/1347 [00:00<00:00, 14976.40 exampl\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240211_015933-nu05rl2w\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minfctx-v5-dataset-continue-test - PART 1 (deepspeed_stage_1, train-ctx=4096, data-ctx=4096)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test/runs/nu05rl2w\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:639: Checkpoint directory /workspace/picocreator/RWKV-infctx-trainer/checkpoint/v5-enwiki-100k-sorted exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.008369922637939453 seconds\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 20.5 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "87.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "87.6 M    Total params\n",
      "350.405   Total estimated model params size (MB)\n",
      "Epoch 0:   1%| | 100/16669 [00:11<30:59,  8.91it/s, v_num=rl2w, train/loss=6.310/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0:   3%| | 502/16669 [00:48<26:01, 10.36it/s, v_num=rl2w, train/loss=5.840`Trainer.fit` stopped: `max_steps=251` reached.\n",
      "Epoch 0:   3%| | 502/16669 [00:48<26:01, 10.36it/s, v_num=rl2w, train/loss=5.840\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.026 MB of 0.026 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch ▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 ▁▂▃▄▅▅▆▆▅▆▆▆▇▇▇█▆▆▆▇▇▇▇▇▇▇▇▇▇███▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss █▅▄▄▄▃▃▃▃▂▂▃▃▃▂▂▂▂▁▂▂▃▂▂▂▁▂▂▂▁▂▁▁▁▂▂▁▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss █▅▄▄▄▃▃▃▃▂▂▃▃▃▂▂▂▂▁▂▂▃▂▂▂▁▂▂▂▁▂▁▁▁▂▂▁▂▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss █▆▅▁▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate ███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx 501\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 86.30715\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 4162.453\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep 501\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen 1049.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss 5.84375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss 5.84375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens 1048.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss 5.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step 250\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33minfctx-v5-dataset-continue-test - PART 1 (deepspeed_stage_1, train-ctx=4096, data-ctx=4096)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test/runs/nu05rl2w\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzNjk4MDM1Mg==/version_details/v4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240211_015933-nu05rl2w/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-sorted.yaml\" \\\n",
    "        --model.load_model=\"../model/L6-D512-world-init.pth\" \\\n",
    "        --trainer.max_steps=251 \\\n",
    "        --trainer.callbacks.init_args.every_n_train_steps=50 \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v5-enwiki-100k-sorted/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - PART 1 (deepspeed_stage_1, train-ctx=4096, data-ctx=4096)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_1\" \\\n",
    "        --trainer.microbatch_size=8 \\\n",
    "        --data.skip_datapath_setup=True \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-11 05:22:01,976] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/cli.py:518: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/workspace/picocreator/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/enwiki_100k-world-sorted.yaml', '--model.load_model=../model/L6-D512-world-init.pth', '--trainer.max_steps=500', '--trainer.callbacks.init_args.every_n_train_steps=5000', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-enwiki-100k-sorted/', '--trainer.logger.init_args.name=infctx-v5-dataset-continue-test - PART 2 (deepspeed_stage_1, train-ctx=4096, data-ctx=4096)', '--trainer.strategy=deepspeed_stage_1', '--trainer.microbatch_size=8', '--trainer.devices=1', '--data.skip_datapath_setup=True', '--ckpt_path=../checkpoint/v5-enwiki-100k-sorted/epoch=0-step=250.ckpt'], args=['fit', '-c', '/workspace/picocreator/RWKV-infctx-trainer/notebook/trainer-v5-validation/config/enwiki_100k-world-sorted.yaml', '--model.load_model=../model/L6-D512-world-init.pth', '--trainer.max_steps=500', '--trainer.callbacks.init_args.every_n_train_steps=5000', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5-enwiki-100k-sorted/', '--trainer.logger.init_args.name=infctx-v5-dataset-continue-test - PART 2 (deepspeed_stage_1, train-ctx=4096, data-ctx=4096)', '--trainer.strategy=deepspeed_stage_1', '--trainer.microbatch_size=8', '--trainer.devices=1', '--data.skip_datapath_setup=True', '--ckpt_path=../checkpoint/v5-enwiki-100k-sorted/epoch=0-step=250.ckpt'].\n",
      "Seed set to 3941088705\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - microbatch_size:         8\n",
      "   - accumulate_grad_batches: 2\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "[rank: 0] Seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m (\u001b[33mrwkv-x-dev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240211_052208-yow80vq7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minfctx-v5-dataset-continue-test - PART 2 (deepspeed_stage_1, train-ctx=4096, data-ctx=4096)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test/runs/yow80vq7\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:639: Checkpoint directory /workspace/picocreator/RWKV-infctx-trainer/checkpoint/v5-enwiki-100k-sorted exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.010339498519897461 seconds\n",
      "/usr/local/lib/python3.10/dist-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 33.6 M\n",
      "1 | blocks | ModuleList | 20.5 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 33.6 M\n",
      "--------------------------------------\n",
      "87.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "87.6 M    Total params\n",
      "350.405   Total estimated model params size (MB)\n",
      "Restoring states from the checkpoint path at ../checkpoint/v5-enwiki-100k-sorted/epoch=0-step=250.ckpt\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py:263: Be aware that when using `ckpt_path`, callbacks used to create the checkpoint need to be provided during `Trainer` instantiation. Please add the following callbacks: [\"ModelCheckpoint{'monitor': 'step', 'mode': 'max', 'every_n_train_steps': 50, 'every_n_epochs': 0, 'train_time_interval': None}\"].\n",
      "Restored all states from the checkpoint at ../checkpoint/v5-enwiki-100k-sorted/epoch=0-step=250.ckpt\n",
      "Epoch 0:   0%|                                        | 0/16669 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/training_epoch_loop.py:154: You're resuming from a checkpoint that ended before the epoch ended. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint\n",
      "Epoch 0:   6%| | 998/16669 [00:35<09:24, 27.74it/s, v_num=0vq7, train/loss=5.250/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "`Trainer.fit` stopped: `max_steps=500` reached.\n",
      "Epoch 0:   6%| | 998/16669 [00:39<10:14, 25.51it/s, v_num=0vq7, train/loss=5.250\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.027 MB of 0.027 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch ▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 ▁▂▃▃▄▄▅▅▅▆▆▆▆▇▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss ▇▇▆▇▆▁█▃▃█▄▅▅▆▅▅▅▁▆▅▃▂▄▆▇▅▆▆▅▇▃▅▆▄▅▅▅▅▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss ▇▇▆▇▆▁█▃▃█▄▅▅▆▅▅▅▁▆▅▃▂▄▆▇▅▆▆▅▇▃▅▆▄▅▅▅▅▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss ▃▆█▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate ▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   batchidx 997\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: perf/kTokens_per_sec.gpu.0 120.20692\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   perf/kTokens_total.gpu.0 4237.769\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    substep 997\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train/data_ctxlen 1075.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/data_loss 5.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/learn_loss 5.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/learn_tokens 1074.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/loss 5.125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        trainer/global_step 499\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      trainer/learning_rate 0.0008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33minfctx-v5-dataset-continue-test - PART 2 (deepspeed_stage_1, train-ctx=4096, data-ctx=4096)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test/runs/yow80vq7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/rwkv-x-dev/RWKV-infctx-unit-test/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzNjk4MDM1Mg==/version_details/v4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240211_052208-yow80vq7/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/config/enwiki_100k-world-sorted.yaml\" \\\n",
    "        --model.load_model=\"../model/L6-D512-world-init.pth\" \\\n",
    "        --trainer.callbacks.init_args.every_n_train_steps=5000 \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/v5-enwiki-100k-sorted/\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - PART 2 (deepspeed_stage_1, train-ctx=4096, data-ctx=4096)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_1\" \\\n",
    "        --trainer.microbatch_size=8 \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" \\\n",
    "        --data.skip_datapath_setup=True \\\n",
    "        --ckpt_path=\"../checkpoint/v5-enwiki-100k-sorted/epoch=0-step=250.ckpt\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
