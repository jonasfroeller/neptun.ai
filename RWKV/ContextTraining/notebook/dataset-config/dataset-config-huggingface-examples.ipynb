{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using HF (Hugging Face) based datasets with infctx trainer\n",
    "\n",
    "The infctx trainer makes a huge shift towards a HF focus dataset parser, with several pros and cons. This note book aims to cover all the common use cases for dataset handling and processing, that is supported by this trainer code.\n",
    "\n",
    "Because there are multiple possible strategy for parsing of the dataset, they are evaluated in the following order by default\n",
    "\n",
    "- multi_column_keys (used if any collumn matches)\n",
    "- prompt & completion (used if both collumn exists)\n",
    "- text (default baseline)\n",
    "\n",
    "We would be going through how the above dataset processing strategies work, starting with text (the default baseline).\n",
    "\n",
    "> Important note: These example focuses only on how to configure your dataset, and does not properly perform checkmarking - for trainer configurations refer to the training notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intial setup\n",
    "\n",
    "Before we go into the dataset setup, lets perform an initial setup for all the folders we need, and a small toy model which we would use throughout the various examples within this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the folders we will need\n",
    "!mkdir -p ../../model/\n",
    "!mkdir -p ../../datapath/\n",
    "!mkdir -p ../../checkpoint/\n",
    "\n",
    "# Initialized a simple L6-D512 model, for both the v4 neox (50277) tokenizer\n",
    "!cd ../../RWKV-v5/ && python3 ./init_model.py --n_layer 6 --n_embd 512 --vocab_size neox --skip-if-exists ../model/L6-D512-neox-init.pth\n",
    "\n",
    "# and rwkv world (65529) tokenizers\n",
    "!cd ../../RWKV-v5/ && python3 ./init_model.py --n_layer 6 --n_embd 512 --vocab_size world --skip-if-exists ../model/L6-D512-world-init.pth\n",
    "\n",
    "# If you have a custom vocab size, you can indicate accordingly as well with an int\n",
    "!cd ../../RWKV-v5/ && python3 ./init_model.py --n_layer 6 --n_embd 512 --vocab_size 20259 --skip-if-exists ../model/L6-D512-V20259-init.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using a text dataset\n",
    "\n",
    "The following is the `example-hf-enwiki.yaml` settings, for using a textual dataset via huggingface, with most of the comments removed\n",
    "\n",
    "---\n",
    "```yaml\n",
    "trainer:\n",
    "  max_steps: 10\n",
    "  target_batch_size: 32\n",
    "model:\n",
    "  load_model: ../model/L6-D512-neox-init.pth\n",
    "  ctx_len: 1024\n",
    "  lr_init: 3e-4\n",
    "  bptt_learning: true\n",
    "  bptt_learning_range: -1\n",
    "data:\n",
    "  data_path: ../datapath/enwiki_10k_neox_1024/\n",
    "\n",
    "  source: \"teven/enwiki_10k\"\n",
    "  # # Additional source dataset params, used to grab subsets\n",
    "  # source_dataset_params:\n",
    "  #   language: en\n",
    "\n",
    "  tokenizer: neox\n",
    "  min_token_size: 64\n",
    "  max_token_size: -1\n",
    "  text_rechunk_size: 2048\n",
    "  text_rechunk_force: true\n",
    "  custom_text_key: 'text'\n",
    "  test_split: 0.01\n",
    "  test_split_shuffle: false\n",
    "```\n",
    "---\n",
    "\n",
    "### Understanding the `data` config, for textual datasets\n",
    "\n",
    "Lets go through each of data parameter settings and what they mean...\n",
    "\n",
    "**data.data_path** \n",
    "\n",
    "This is where the HF datapath is saved, when used against existing HF data sources. This is a requried parameter\n",
    "\n",
    "**data.source** \n",
    "\n",
    "This can be configured either as a hugging face dataset (eg. `teven/enwiki_10k`) or if intended to be used with local files `text / json / csv / pandas` or their respective file paths (you can point to a single `.txt/.jsonl` file and it should 'work', see the local file examples for more details)\n",
    "\n",
    "**data.source_dataset_params** \n",
    "\n",
    "Additional params to configure the huggingface `load_dataset` command. This is only useful for larger dataset which supports such parameters, to filter out a subsets specifically (defaults to empty object)\n",
    "\n",
    "**data.tokenizer**\n",
    "\n",
    "The tokenizer to use for the dataset, use either `neox` or `world` for the respective RWKV models. For custom HF tokenizer refer to the custom tokenizer example below. (defaults to neox)\n",
    "\n",
    "**data.min_token_size/max_token_size**\n",
    "\n",
    "Scans the given dataset, and skips datasamples that fail to meet the given criteria. This is mostly useful for filtering small low quality datasamples in large datasets, or large datasample beyond what you intend to support. (this is done before rechunking if enabled, defaults to -1 which support all)\n",
    "\n",
    "**data.text_rechunk_force**\n",
    "\n",
    "Enable text rechunking, this means, all the filtered datasamples will be merged together, with a new line between them. Before being split again by the rechunk size. This is mostly useful for large corpus of raw text data, and is consistent with how existing foundation model are trained from raw text files. This also allows more efficient training process (tokens/second), as each datasample will have the exact same token count. (Disabled by default unless your source is literally a \".txt\" file)\n",
    "\n",
    "**data.text_rechunk_size** \n",
    "\n",
    "Number of tokens each datasample should have after rechunking. Recommended sizes is the context size you intend the model to support (ie. 2048, 4096, etc)\n",
    "\n",
    "**data.custom_text_key** \n",
    "\n",
    "For huggingface datasets (or json/csv) by default we would use the `text` collumn if its avaliable. However some dataset store their text in a different collumn (eg. `code`). This allow you to choose which collumn would you like to use the text from. Note for more complicated instruct/input/output examples, you will want to see the 'multi_column' guide/examples instead.\n",
    "\n",
    "**data.test_split**\n",
    "\n",
    "Important Note: this is ignored, if the HF dataset has an inbuilt test split.\n",
    "\n",
    "If configured as a floating number between 0.0 and 1.0, it will be the percentage (0.1 is 10%) of the test data that is used for test validation.\n",
    "\n",
    "If configured as an int number, it will be the number of samples.\n",
    "\n",
    "Due to some limitations in the current trainer code, even if its set as 0, we will use a single data sample for the test split.\n",
    "\n",
    "This defaults to 0.01 or 1%\n",
    "\n",
    "**data.test_shuffle**\n",
    "\n",
    "Perform a dataset shuffle before test split, this defaults to False.\n",
    "\n",
    "Note, this is not a truely random shuffle, but a detriministic shuffle. To ensure a consistent result.\n",
    "\n",
    "### Optimizing the `model.bptt_*` mode according to your dataset config ....\n",
    "\n",
    "**model.load_model** \n",
    "\n",
    "This is the `model.pth` file you start the training process from. This can be an exisitng model you are finetuning from, or a new model that you initalized with `init_model.py` script.\n",
    "\n",
    "**model.ctx_len** \n",
    "\n",
    "This is the training context length used in the training process. For the infctx trainer, your data samples can be larger then the training context length. If so, the data sample is split into chunks accordingly, and trained in parts (with bptt_learning enabled, which it is by default)\n",
    "\n",
    "This is ultimately a tradeoff between VRAM usage vs GPU compute usage, while you can save VRAM usage, this comes an increased compute cost, as first few chunks will need to recalculated multiple times for each subsequent chunk. There are also been recorded minor loss learning penalty especially for small context sizes.\n",
    "\n",
    "As such it is always recommended to configure this to be as large as what can be supported by your GPU in the power of 2 (1024,2048,4096,...) with some healthy vram buffer for checkpoints and gradients, and up to your dataset sample size (as its pointless to go beyond that)\n",
    "\n",
    "Typically this is 2048, 4096, or 8192 for ML training GPUS (24GB vram and above). For consumer GPUS, anything less then 512 is not recommended, due to compounded loss learning penalty involved when used with large data samples.\n",
    "\n",
    "**model.bptt_learning**\n",
    "\n",
    "Enabled by default, this is the core feature of infctx trainer. If your training ctx length is equal to your dataset context length, you can disable bptt_learning for an insignificant speed boost (barely measurable).\n",
    "\n",
    "In most cases its better to just set bptt_learning_range to 1 instead of switching it off\n",
    "\n",
    "**model.bptt_learning_range**\n",
    "\n",
    "`bptt_learning_range: -1` will work by default for all use cases. On a single GPU.\n",
    "\n",
    "However, when training across multiple GPUs `bptt_learning_range: -1` has a small performance penalty in which it needs to syncronize the number of chunks across multiple GPUs. \n",
    "\n",
    "This is an issue especially, when training with mixed dataset size, if a single GPU is stuck with a significantly larger document with many chunks, all the other GPUs maybe stuck waiting for that one GPU to complete.\n",
    "\n",
    "In most cases this would be an acceptable compromise with mixed sized dataset. However if your dataset is of fixed size. Especially with 'rechunking' enabled. You can optimize multiple GPU training by configuring the learning range to be exactly equals to the number of chunk (eg: learning_range = 4, for data size of 4096, training ctx len of 1024)\n",
    "\n",
    "You can also configure the range to be less then the data sample size, in which the learning process will only happen for the last X configured chunks. This is not as bad as it sounds, and has it uses cases (which will be documented seperately)\n",
    "\n",
    "---\n",
    "\n",
    "### Download and preload the datapath from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd ../../RWKV-v5 && python3 preload_datapath.py ../notebook/dataset-config/example-hf-enwiki.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the dataset is working, by doing a quick training run\n",
    "!cd ../../RWKV-v5 && python3 lightning_trainer.py fit -c ../notebook/dataset-config/example-hf-enwiki.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using prompt completion pair dataset\n",
    "\n",
    "However, beyond foundation model training, for finetuning. One common format is the `prompt` and `completion` pair. This is supported out of the box.\n",
    "\n",
    "An example of the prompt/completion pair as followed\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"prompt\": \"What is the dominant emotion of the user? I am happy. Output:\",\n",
    "  \"completion\": \" Happy<|endoftext|>\"\n",
    "}\n",
    "```\n",
    "\n",
    "Setting this up, as simple as the following\n",
    "\n",
    "---\n",
    "```yaml\n",
    "trainer:\n",
    "  max_steps: 10\n",
    "  target_batch_size: 32\n",
    "model:\n",
    "  load_model: ../model/L6-D512-neox-init.pth\n",
    "  ctx_len: 1024\n",
    "  lr_init: 1e-4\n",
    "  bptt_learning: true\n",
    "  bptt_learning_range: -1\n",
    "data:\n",
    "  data_path: ../datapath/self-instruct-base/\n",
    "  source: \"eastwind/self-instruct-base\"\n",
    "  tokenizer: neox\n",
    "  disable_prompt_completion_mask: false\n",
    "```\n",
    "---\n",
    "\n",
    "**data.disable_prompt_completion_mask**\n",
    "\n",
    "If the dataset uses prompt/completion data layout. By default it would be used in place of the text collumn. Typically, no additional configuration required.\n",
    "\n",
    "However, the default prompt completion behaviour, is that the text on the prompt half is \"learning masked\" disabled, while the text on the completion half has the \"learning mask\" enabled.\n",
    "\n",
    "In practise, the model will not learn how to generate the prompt as an output. And the learnings are focused on the completion half.\n",
    "\n",
    "---\n",
    "\n",
    "### Preload the dataset and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd ../../RWKV-v5 && python3 preload_datapath.py ../notebook/dataset-config/example-hf-prompt-completion.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the dataset is working, by doing a quick training run\n",
    "!cd ../../RWKV-v5 && python3 lightning_trainer.py fit -c ../notebook/dataset-config/example-hf-prompt-completion.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using multi column keys\n",
    "\n",
    "What if you want alternative format, with more complicated layouts. Like instruction/input/output (or other formats)\n",
    "\n",
    "You can use the `multi_column_keys` which gives you precise control over how each data sample is processed.\n",
    "\n",
    "For example the following is an example data record for dolly instruction set (simplified)\n",
    "\n",
    "```\n",
    "{\n",
    "    \"category\": \"closed_qa\"\t,\n",
    "    \"instruction\": \"When did Virgin Australia start operating?\",\n",
    "    \"input\": \"Virgin Australia, the trading ....\",\n",
    "    \"output\": \"Virgin Australia commenced services on ...\"\n",
    "}\n",
    "```\n",
    "\n",
    "If using the default settings as shown below, this will get converted into the following training text (ignoring masking)\n",
    "\n",
    "```\n",
    "Instruction:\n",
    "When did Virgin Australia start operating?\n",
    "\n",
    "Input:\n",
    "Virgin Australia, the trading ....\n",
    "\n",
    "Output:\n",
    "Virgin Australia commenced services on ...\n",
    "```\n",
    "\n",
    "We can support the following dataset with the following settings\n",
    "\n",
    "---\n",
    "```yaml\n",
    "trainer:\n",
    "  max_steps: 10\n",
    "  target_batch_size: 32\n",
    "model:\n",
    "  load_model: ../model/L6-D512-neox-init.pth\n",
    "  ctx_len: 1024\n",
    "  lr_init: 1e-4\n",
    "  bptt_learning: true\n",
    "  bptt_learning_range: -1\n",
    "data:\n",
    "  data_path: ../datapath/self-instruct-base/\n",
    "  source: \"c-s-ale/dolly-15k-instruction-alpaca-format\"\n",
    "  tokenizer: neox\n",
    "  multi_column_keys: ['instruction', 'input', 'output']\n",
    "  multi_column_prefix: ['Instruction:\\n', 'Input:\\n', 'Output:\\n']\n",
    "  multi_column_train_mask: [true, false, true]\n",
    "  multi_column_separator: '\\n\\n'\n",
    "```\n",
    "---\n",
    "\n",
    "**data.multi_column_keys**\n",
    "\n",
    "Defaults to: `['instruction', 'input', 'output']`\n",
    "\n",
    "List of keys to detect, and use for your text data training. Requires atleast one column to exist, all other collumns will be ignored. Columns are matched in the given order.\n",
    "\n",
    "**data.multi_column_prefix**\n",
    "\n",
    "Defaults to `['Instruction:\\n', 'Input:\\n', 'Output:\\n']`\n",
    "\n",
    "For each matching column found, append the following string as a prefix in the matching array position to `multi_column_keys`\n",
    "\n",
    "**data.multi_column_train_mask**\n",
    "\n",
    "Defaults to `[true, false, true]`\n",
    "\n",
    "For each matching column found, either apply the training mask where the model will learn from (true), or to ignore in the learning process (false).\n",
    "\n",
    "**data.multi_column_separator**\n",
    "\n",
    "Defaults to: `\\n\\n`\n",
    "\n",
    "String to append inbetween each matching multi column\n",
    "\n",
    "> Important note: As it is very common to use \\n or escape character \\ in multi column settings, ensure such strings are within single quotes (ie. '\\n'), otherwise the \\slash value will get escaped into double slash.\n",
    "\n",
    "---\n",
    "\n",
    "### Preload the dataset and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd ../../RWKV-v5 && python3 preload_datapath.py ../notebook/dataset-config/example-hf-multi-column-keys.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the dataset is working, by doing a quick training run\n",
    "!cd ../../RWKV-v5 && python3 lightning_trainer.py fit -c ../notebook/dataset-config/example-hf-multi-column-keys.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with a custom HF tokenizer\n",
    "\n",
    "How about taking it to another use case all together, with a custom tokenizer? Like training RWKV for music generation?\n",
    "You can load an existing HF tokenizer, by simply changing the tokenizer value respectively\n",
    "\n",
    "---\n",
    "```yaml\n",
    "...\n",
    "data:\n",
    "  # dataset_path for the prebuilt dataset, using HF `load_from_disk()`\n",
    "  data_path: ../datapath/musnet/\n",
    "\n",
    "  # @Breadlicker45 music dataset for musnet, and the tokenizer\n",
    "  source: \"breadlicker45/musenet-encoders-40k\"\n",
    "  \n",
    "  # For huggingface tokenizer, just indicate the tokenizer project path respectively\n",
    "  tokenizer: \"breadlicker45/muse-tokenizer2\"\n",
    "\n",
    "  # Test split settings\n",
    "  test_split: 0.005\n",
    "  test_split_shuffle: true\n",
    "\n",
    "  # Minimum / Maximum token size of the dataset to use\n",
    "  min_token_size: -1\n",
    "  max_token_size: -1\n",
    "\n",
    "  # Custom text key, specific to the dataset\n",
    "  custom_text_key: 'bing'\n",
    "\n",
    "```\n",
    "---\n",
    "\n",
    "The key thing to note is the `tokenizer` value which will be passed to HF tokenizer implementation. \n",
    "\n",
    "### Preload the dataset and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd ../../RWKV-v5 && python3 preload_datapath.py ../notebook/dataset-config/example-hf-music-tokenizer.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the dataset is working, by doing a quick training run\n",
    "!cd ../../RWKV-v5 && python3 lightning_trainer.py fit -c ../notebook/dataset-config/example-hf-music-tokenizer.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-infctx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
